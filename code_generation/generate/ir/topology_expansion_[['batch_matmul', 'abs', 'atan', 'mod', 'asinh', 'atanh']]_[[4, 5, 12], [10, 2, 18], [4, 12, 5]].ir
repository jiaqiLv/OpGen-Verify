# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(ph_0: T.Buffer((4, 5, 12), "float32"), ph_3: T.Buffer((4, 12, 5), "float32"), compute: T.Buffer((4, 5, 12), "float32"), T_mod: T.Buffer((4, 5, 12), "float32"), compute_1: T.Buffer((4, 5, 12), "float32"), compute_2: T.Buffer((4, 5, 5), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        auto_scheduler_layout_transform = T.allocate([240], "float32", "global")
        ph_0_1 = T.Buffer((240,), data=ph_0.data)
        for i0_i1_fused_i2_fused in T.parallel(240):
            compute_3 = T.Buffer((240,), data=compute.data)
            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])
        for ax0_ax1_fused_ax2_fused in T.parallel(240):
            T_mod_1 = T.Buffer((240,), data=T_mod.data)
            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])
        for i0_i1_fused_i2_fused in T.parallel(240):
            compute_3 = T.Buffer((240,), data=compute_1.data)
            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))
        auto_scheduler_layout_transform_1 = T.Buffer((240,), data=auto_scheduler_layout_transform)
        for ax0_ax1_fused_ax2_fused in T.parallel(4):
            for ax8, ax10, ax11 in T.grid(2, 5, 6):
                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 60 + ax8 * 30
                ph_3_1 = T.Buffer((240,), data=ph_3.data)
                auto_scheduler_layout_transform_1[cse_var_1 + ax10 * 6 + ax11] = ph_3_1[cse_var_1 + ax11 * 5 + ax10]
        for i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(4):
            T_batch_matmul_NN = T.allocate([25], "float32", "global")
            T_batch_matmul_NN_1 = T.Buffer((25,), data=T_batch_matmul_NN)
            for i_outer_inner_init, j_outer_inner_init in T.grid(5, 5):
                T_batch_matmul_NN_1[i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)
            for k_outer, i_outer_inner, j_outer_inner, k_inner in T.grid(2, 5, 5, 6):
                cse_var_3: T.int32 = i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 60
                cse_var_2: T.int32 = i_outer_inner * 5 + j_outer_inner
                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_3 + i_outer_inner * 12 + k_outer * 6 + k_inner] * auto_scheduler_layout_transform_1[cse_var_3 + k_outer * 30 + j_outer_inner * 6 + k_inner]
            for i1_inner, i2_inner_s in T.grid(5, 5):
                cse_var_4: T.int32 = i1_inner * 5
                compute_3 = T.Buffer((100,), data=compute_2.data)
                compute_3[i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 25 + cse_var_4 + i2_inner_s] = T.atanh(T_batch_matmul_NN_1[cse_var_4 + i2_inner_s])