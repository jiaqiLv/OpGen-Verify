# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(ph_0: T.Buffer((16, 12, 18), "float32"), ph_6: T.Buffer((16, 18, 5), "float32"), compute: T.Buffer((16, 12, 18), "float32"), T_add: T.Buffer((16, 12, 18), "float32"), compute_1: T.Buffer((16, 12, 5), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        auto_scheduler_layout_transform = T.allocate([1440], "float32", "global")
        ph_0_1 = T.Buffer((3456,), data=ph_0.data)
        for i0_i1_fused_i2_fused in T.parallel(3456):
            compute_2 = T.Buffer((3456,), data=compute.data)
            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])
        for ax0_ax1_fused_ax2_fused in T.parallel(3456):
            T_add_1 = T.Buffer((3456,), data=T_add.data)
            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]
        auto_scheduler_layout_transform_1 = T.Buffer((1440,), data=auto_scheduler_layout_transform)
        for ax3, ax8, ax9, ax11, ax12 in T.grid(5, 9, 4, 2, 4):
            ph_6_1 = T.Buffer((1440,), data=ph_6.data)
            auto_scheduler_layout_transform_1[ax3 * 288 + ax8 * 32 + ax9 * 8 + ax11 * 4 + ax12] = ph_6_1[ax9 * 360 + ax12 * 90 + ax8 * 10 + ax11 * 5 + ax3]
        for i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(10):
            T_batch_matmul_NN = T.allocate([96], "float32", "global")
            T_batch_matmul_NN_1 = T.Buffer((96,), data=T_batch_matmul_NN)
            for b_outer_inner_init, i_outer_inner_init, b_inner_init, i_inner_init in T.grid(4, 3, 4, 2):
                T_batch_matmul_NN_1[b_outer_inner_init * 24 + b_inner_init * 6 + i_outer_inner_init * 2 + i_inner_init] = T.float32(0)
            for k_outer, b_outer_inner, i_outer_inner, k_inner, b_inner, i_inner in T.grid(9, 4, 3, 2, 4, 2):
                cse_var_1: T.int32 = b_outer_inner * 24 + b_inner * 6 + i_outer_inner * 2 + i_inner
                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 864 + b_inner * 216 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 5 * 108 + i_outer_inner * 36 + i_inner * 18 + k_outer * 2 + k_inner] * auto_scheduler_layout_transform_1[i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5 * 288 + k_outer * 32 + b_outer_inner * 8 + k_inner * 4 + b_inner]
            for i0_inner, i1_inner in T.grid(16, 6):
                compute_2 = T.Buffer((960,), data=compute_1.data)
                compute_2[i0_inner * 60 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 5 * 30 + i1_inner * 5 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5] = T.fabs(T_batch_matmul_NN_1[i0_inner * 6 + i1_inner])