# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(ph_0: T.Buffer((11, 11, 19), "float32"), ph_5: T.Buffer((11, 19, 5), "float32"), compute: T.Buffer((11, 11, 19), "float32"), compute_1: T.Buffer((11, 11, 19), "float32"), T_batch_matmul_NN: T.Buffer((11, 11, 5), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        auto_scheduler_layout_transform = T.allocate([1045], "float32", "global")
        ph_0_1 = T.Buffer((2299,), data=ph_0.data)
        for i0_i1_fused_i2_fused in T.parallel(2299):
            compute_2 = T.Buffer((2299,), data=compute.data)
            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])
        for i0_i1_fused_i2_fused in T.parallel(2299):
            compute_2 = T.Buffer((2299,), data=compute_1.data)
            compute_2[i0_i1_fused_i2_fused] = T.sin(T.acos(ph_0_1[i0_i1_fused_i2_fused]))
        auto_scheduler_layout_transform_1 = T.Buffer((1045,), data=auto_scheduler_layout_transform)
        for ax4, ax6, ax8 in T.grid(19, 5, 11):
            ph_5_1 = T.Buffer((1045,), data=ph_5.data)
            auto_scheduler_layout_transform_1[ax4 * 55 + ax6 * 11 + ax8] = ph_5_1[ax8 * 95 + ax4 * 5 + ax6]
        T_batch_matmul_NN_1 = T.Buffer((605,), data=T_batch_matmul_NN.data)
        for i_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(11, 5, 11):
            T_batch_matmul_NN_1[b_inner_init * 55 + i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)
        for k_outer, i_outer_inner, j_outer_inner, b_inner in T.grid(19, 11, 5, 11):
            cse_var_1: T.int32 = b_inner * 55 + i_outer_inner * 5 + j_outer_inner
            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 209 + i_outer_inner * 19 + k_outer] * auto_scheduler_layout_transform_1[k_outer * 55 + j_outer_inner * 11 + b_inner]