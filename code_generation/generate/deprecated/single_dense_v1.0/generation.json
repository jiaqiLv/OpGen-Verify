{"op_name": "exp", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3690; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 93; ++i2) {\n      for (int32_t i3 = 0; i3 < 36; ++i3) {\n        compute[(((i0_i1_fused * 3348) + (i2 * 36)) + i3)] = expf(data[(((i0_i1_fused * 3348) + (i2 * 36)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 1544265) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((82, 45, 93, 36), \"float32\"), compute: T.Buffer((82, 45, 93, 36), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(3690):\n            for i2, i3 in T.grid(93, 36):\n                cse_var_1: T.int32 = i0_i1_fused * 3348 + i2 * 36 + i3\n                compute_1 = T.Buffer((12354120,), data=compute.data)\n                data_1 = T.Buffer((12354120,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])", "op_args": [82, 45, 93, 36], "input_shape": "[[82, 45, 93, 36]]", "output_shape": "[[82, 45, 93, 36]]"}{"op_name": "exp", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 106392; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 71; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 71) + i3)] = expf(data[((i0_i1_fused_i2_fused * 71) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((31, 78, 44, 71), \"float32\"), compute: T.Buffer((31, 78, 44, 71), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(106392):\n            for i3 in range(71):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 71 + i3\n                compute_1 = T.Buffer((7553832,), data=compute.data)\n                data_1 = T.Buffer((7553832,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])", "op_args": [31, 78, 44, 71], "input_shape": "[[31, 78, 44, 71]]", "output_shape": "[[31, 78, 44, 71]]"}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 11048400; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(62) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((66, 54, 50, 62), \"float32\"), T_fast_erf: T.Buffer((66, 54, 50, 62), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(11048400):\n            T_fast_erf_1 = T.Buffer((11048400,), data=T_fast_erf.data)\n            data_1 = T.Buffer((11048400,), data=data.data)\n            T_fast_erf_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [66, 54, 50, 62], "input_shape": "[[66, 54, 50, 62]]", "output_shape": "[[66, 54, 50, 62]]"}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 13812840; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(61) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 61) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((74, 60, 51, 61), \"float32\"), T_fast_erf: T.Buffer((74, 60, 51, 61), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(13812840):\n            T_fast_erf_1 = T.Buffer((13812840,), data=T_fast_erf.data)\n            data_1 = T.Buffer((13812840,), data=data.data)\n            T_fast_erf_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [74, 60, 51, 61], "input_shape": "[[74, 60, 51, 61]]", "output_shape": "[[74, 60, 51, 61]]"}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 133110; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 72; ++ax3) {\n        int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused_ax2_fused * 72) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 72) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused_ax2_fused * 72) + ax3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((58, 45, 51, 72), \"float32\"), T_fast_exp: T.Buffer((58, 45, 51, 72), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(133110):\n            for ax3 in range(72):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 72 + ax3\n                T_fast_exp_1 = T.Buffer((9583920,), data=T_fast_exp.data)\n                data_1 = T.Buffer((9583920,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [58, 45, 51, 72], "input_shape": "[[58, 45, 51, 72]]", "output_shape": "[[58, 45, 51, 72]]"}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 8074570; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n      int32_t v_ = ((int32_t)(floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[ax0_ax1_fused_ax2_fused_ax3_fused] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(62) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((70, 61, 61, 31), \"float32\"), T_fast_exp: T.Buffer((70, 61, 61, 31), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(8074570):\n            T_fast_exp_1 = T.Buffer((8074570,), data=T_fast_exp.data)\n            data_1 = T.Buffer((8074570,), data=data.data)\n            T_fast_exp_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [70, 61, 61, 31], "input_shape": "[[70, 61, 61, 31]]", "output_shape": "[[70, 61, 61, 31]]"}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 15594600; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_tanh[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((42, 79, 50, 94), \"float32\"), T_fast_tanh: T.Buffer((42, 79, 50, 94), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(15594600):\n            T_fast_tanh_1 = T.Buffer((15594600,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((15594600,), data=data.data)\n            T_fast_tanh_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [42, 79, 50, 94], "input_shape": "[[42, 79, 50, 94]]", "output_shape": "[[42, 79, 50, 94]]"}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 28274688; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_tanh[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((64, 59, 96, 78), \"float32\"), T_fast_tanh: T.Buffer((64, 59, 96, 78), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(28274688):\n            T_fast_tanh_1 = T.Buffer((28274688,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((28274688,), data=data.data)\n            T_fast_tanh_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [64, 59, 96, 78], "input_shape": "[[64, 59, 96, 78]]", "output_shape": "[[64, 59, 96, 78]]"}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 3723; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 60; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 89; ++ax3) {\n        T_reverse_sequence[(((ax0_ax1_fused * 5340) + (ax2 * 89)) + ax3)] = data[((((((ax0_ax1_fused % 51) * 5340) + (ax2 * 89)) + ax3) + 19608480) - ((ax0_ax1_fused / 51) * 272340))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 7565) * 36) + ((int)threadIdx.x)) + 19608480) - ((((int)blockIdx.x) / 7565) * 272340))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((73, 51, 60, 89), \"float32\"), T_reverse_sequence: T.Buffer((73, 51, 60, 89), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(3723):\n            for ax2, ax3 in T.grid(60, 89):\n                cse_var_1: T.int32 = ax2 * 89\n                T_reverse_sequence_1 = T.Buffer((19880820,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((19880820,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 5340 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 51 * 5340 + cse_var_1 + ax3 + 19608480 - ax0_ax1_fused // 51 * 272340]", "op_args": [73, 51, 60, 89], "input_shape": "[[73, 51, 60, 89]]", "output_shape": "[[73, 51, 60, 89]]"}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 5016; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 75; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 39; ++ax3) {\n        T_reverse_sequence[(((ax0_ax1_fused * 2925) + (ax2 * 39)) + ax3)] = data[((((((ax0_ax1_fused % 66) * 2925) + (ax2 * 39)) + ax3) + 14478750) - ((ax0_ax1_fused / 66) * 193050))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 3575) * 54) + ((int)threadIdx.x)) + 14478750) - ((((int)blockIdx.x) / 3575) * 193050))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((76, 66, 75, 39), \"float32\"), T_reverse_sequence: T.Buffer((76, 66, 75, 39), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(5016):\n            for ax2, ax3 in T.grid(75, 39):\n                cse_var_1: T.int32 = ax2 * 39\n                T_reverse_sequence_1 = T.Buffer((14671800,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((14671800,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 2925 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 66 * 2925 + cse_var_1 + ax3 + 14478750 - ax0_ax1_fused // 66 * 193050]", "op_args": [76, 66, 75, 39], "input_shape": "[[76, 66, 75, 39]]", "output_shape": "[[76, 66, 75, 39]]"}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 65; ++i2) {\n      for (int32_t i3 = 0; i3 < 30; ++i3) {\n        compute[(((i0_i1_fused * 1950) + (i2 * 30)) + i3)] = floorf(data[(((i0_i1_fused * 1950) + (i2 * 30)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((70, 59, 65, 30), \"float32\"), compute: T.Buffer((70, 59, 65, 30), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(4130):\n            for i2, i3 in T.grid(65, 30):\n                cse_var_1: T.int32 = i0_i1_fused * 1950 + i2 * 30 + i3\n                compute_1 = T.Buffer((8053500,), data=compute.data)\n                data_1 = T.Buffer((8053500,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])", "op_args": [70, 59, 65, 30], "input_shape": "[[70, 59, 65, 30]]", "output_shape": "[[70, 59, 65, 30]]"}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6889; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 48; ++i2) {\n      for (int32_t i3 = 0; i3 < 73; ++i3) {\n        compute[(((i0_i1_fused * 3504) + (i2 * 73)) + i3)] = floorf(data[(((i0_i1_fused * 3504) + (i2 * 73)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((83, 83, 48, 73), \"float32\"), compute: T.Buffer((83, 83, 48, 73), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(6889):\n            for i2, i3 in T.grid(48, 73):\n                cse_var_1: T.int32 = i0_i1_fused * 3504 + i2 * 73 + i3\n                compute_1 = T.Buffer((24139056,), data=compute.data)\n                data_1 = T.Buffer((24139056,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])", "op_args": [83, 83, 48, 73], "input_shape": "[[83, 83, 48, 73]]", "output_shape": "[[83, 83, 48, 73]]"}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 51; ++i0) {\n    for (int32_t i1 = 0; i1 < 44; ++i1) {\n      for (int32_t i2 = 0; i2 < 46; ++i2) {\n        for (int32_t i3 = 0; i3 < 97; ++i3) {\n          compute[((((i0 * 196328) + (i1 * 4462)) + (i2 * 97)) + i3)] = ((int8_t)(data[((((i0 * 196328) + (i1 * 4462)) + (i2 * 97)) + i3)] != data[((((i0 * 196328) + (i1 * 4462)) + (i2 * 97)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((51, 44, 46, 97), \"float32\"), compute: T.Buffer((51, 44, 46, 97), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(51):\n            for i1, i2, i3 in T.grid(44, 46, 97):\n                cse_var_1: T.int32 = i0 * 196328 + i1 * 4462 + i2 * 97 + i3\n                compute_1 = T.Buffer((10012728,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((10012728,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [51, 44, 46, 97], "input_shape": "[[51, 44, 46, 97]]", "output_shape": "[[51, 44, 46, 97]]"}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 76176; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 42; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 42) + i3)] = ((int8_t)(data[((i0_i1_fused_i2_fused * 42) + i3)] != data[((i0_i1_fused_i2_fused * 42) + i3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(46) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 46) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 46) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 46) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((36, 46, 46, 42), \"float32\"), compute: T.Buffer((36, 46, 46, 42), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(76176):\n            for i3 in range(42):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 42 + i3\n                compute_1 = T.Buffer((3199392,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((3199392,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [36, 46, 46, 42], "input_shape": "[[36, 46, 46, 42]]", "output_shape": "[[36, 46, 46, 42]]"}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 182988; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 71; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 71) + i3)] = logf(data[((i0_i1_fused_i2_fused * 71) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((39, 68, 69, 71), \"float32\"), compute: T.Buffer((39, 68, 69, 71), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(182988):\n            for i3 in range(71):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 71 + i3\n                compute_1 = T.Buffer((12992148,), data=compute.data)\n                data_1 = T.Buffer((12992148,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [39, 68, 69, 71], "input_shape": "[[39, 68, 69, 71]]", "output_shape": "[[39, 68, 69, 71]]"}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4212; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 90; ++i2) {\n      for (int32_t i3 = 0; i3 < 46; ++i3) {\n        compute[(((i0_i1_fused * 4140) + (i2 * 46)) + i3)] = logf(data[(((i0_i1_fused * 4140) + (i2 * 46)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((78, 54, 90, 46), \"float32\"), compute: T.Buffer((78, 54, 90, 46), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(4212):\n            for i2, i3 in T.grid(90, 46):\n                cse_var_1: T.int32 = i0_i1_fused * 4140 + i2 * 46 + i3\n                compute_1 = T.Buffer((17437680,), data=compute.data)\n                data_1 = T.Buffer((17437680,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [78, 54, 90, 46], "input_shape": "[[78, 54, 90, 46]]", "output_shape": "[[78, 54, 90, 46]]"}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 138600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 68; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 68) + i3)] = log10f(data[((i0_i1_fused_i2_fused * 68) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((30, 70, 66, 68), \"float32\"), compute: T.Buffer((30, 70, 66, 68), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(138600):\n            for i3 in range(68):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 68 + i3\n                compute_1 = T.Buffer((9424800,), data=compute.data)\n                data_1 = T.Buffer((9424800,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [30, 70, 66, 68], "input_shape": "[[30, 70, 66, 68]]", "output_shape": "[[30, 70, 66, 68]]"}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 154836; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 30; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 30) + i3)] = log10f(data[((i0_i1_fused_i2_fused * 30) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((69, 66, 34, 30), \"float32\"), compute: T.Buffer((69, 66, 34, 30), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(154836):\n            for i3 in range(30):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 30 + i3\n                compute_1 = T.Buffer((4645080,), data=compute.data)\n                data_1 = T.Buffer((4645080,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [69, 66, 34, 30], "input_shape": "[[69, 66, 34, 30]]", "output_shape": "[[69, 66, 34, 30]]"}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 330980; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 38; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 38) + i3)] = log2f(data[((i0_i1_fused_i2_fused * 38) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((95, 52, 67, 38), \"float32\"), compute: T.Buffer((95, 52, 67, 38), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(330980):\n            for i3 in range(38):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 38 + i3\n                compute_1 = T.Buffer((12577240,), data=compute.data)\n                data_1 = T.Buffer((12577240,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [95, 52, 67, 38], "input_shape": "[[95, 52, 67, 38]]", "output_shape": "[[95, 52, 67, 38]]"}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6264; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 76; ++i2) {\n      for (int32_t i3 = 0; i3 < 98; ++i3) {\n        compute[(((i0_i1_fused * 7448) + (i2 * 98)) + i3)] = log2f(data[(((i0_i1_fused * 7448) + (i2 * 98)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((72, 87, 76, 98), \"float32\"), compute: T.Buffer((72, 87, 76, 98), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(6264):\n            for i2, i3 in T.grid(76, 98):\n                cse_var_1: T.int32 = i0_i1_fused * 7448 + i2 * 98 + i3\n                compute_1 = T.Buffer((46654272,), data=compute.data)\n                data_1 = T.Buffer((46654272,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [72, 87, 76, 98], "input_shape": "[[72, 87, 76, 98]]", "output_shape": "[[72, 87, 76, 98]]"}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[332332];\n  #pragma omp parallel for\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 332332; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = -3.402823e+38f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 61; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer], data[((k0_k1_fused_k2_fused_k3_fused_outer * 61) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 332332; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 633508; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 8) + (((int)threadIdx.x) >> 2)) < 5068063) {\n      normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((61, 77, 83, 52), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([332332], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((332332,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(332332):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(-3.4028234663852886e+38)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(61):\n                data_1 = T.Buffer((20272252,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 61 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(332332):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v])", "op_args": [61, 77, 83, 52], "input_shape": "[[61, 77, 83, 52]]", "output_shape": "[[]]"}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[112320];\n  #pragma omp parallel for\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 112320; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = -3.402823e+38f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 57; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer], data[((k0_k1_fused_k2_fused_k3_fused_outer * 57) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 112320; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 200070; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((52, 45, 36, 76), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([112320], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((112320,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(112320):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(-3.4028234663852886e+38)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(57):\n                data_1 = T.Buffer((6402240,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 57 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(112320):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v])", "op_args": [52, 45, 36, 76], "input_shape": "[[52, 45, 36, 76]]", "output_shape": "[[]]"}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[365211];\n  #pragma omp parallel for\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 365211; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = 3.402823e+38f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 56; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer], data[((k0_k1_fused_k2_fused_k3_fused_outer * 56) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 365211; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 639120; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 2556477) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((62, 99, 49, 68), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([365211], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((365211,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(365211):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(3.4028234663852886e+38)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(56):\n                data_1 = T.Buffer((20451816,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 56 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(365211):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v])", "op_args": [62, 99, 49, 68], "input_shape": "[[62, 99, 49, 68]]", "output_shape": "[[]]"}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[636633];\n  #pragma omp parallel for\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 636633; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = 3.402823e+38f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer], data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 636633; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 90; ++k0) {\n    for (int k1 = 0; k1 < 68; ++k1) {\n      for (int k2 = 0; k2 < 57; ++k2) {\n        for (int k3 = 0; k3 < 73; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 282948) + (k1 * 4161)) + (k2 * 73)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((90, 68, 57, 73), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([636633], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((636633,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(636633):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(3.4028234663852886e+38)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(40):\n                data_1 = T.Buffer((25465320,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(636633):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v])", "op_args": [90, 68, 57, 73], "input_shape": "[[90, 68, 57, 73]]", "output_shape": "[[]]"}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 12196800; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((44, 77, 50, 72), \"float32\"), compute: T.Buffer((44, 77, 50, 72), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(12196800):\n            compute_1 = T.Buffer((12196800,), data=compute.data)\n            data_1 = T.Buffer((12196800,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [44, 77, 50, 72], "input_shape": "[[44, 77, 50, 72]]", "output_shape": "[[44, 77, 50, 72]]"}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 38998; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 88; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 88) + i3)] = (data[((i0_i1_fused_i2_fused * 88) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(62) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((37, 34, 31, 88), \"float32\"), compute: T.Buffer((37, 34, 31, 88), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(38998):\n            for i3 in range(88):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 88 + i3\n                compute_1 = T.Buffer((3431824,), data=compute.data)\n                data_1 = T.Buffer((3431824,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [37, 34, 31, 88], "input_shape": "[[37, 34, 31, 88]]", "output_shape": "[[37, 34, 31, 88]]"}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[482625];\n  #pragma omp parallel for\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 482625; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = 1.000000e+00f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 25; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 25) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 482625; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 377051; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x)) < 12065625) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((55, 75, 45, 65), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([482625], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((482625,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(482625):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(1)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(25):\n                data_1 = T.Buffer((12065625,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 25 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(482625):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v]", "op_args": [55, 75, 45, 65], "input_shape": "[[55, 75, 45, 65]]", "output_shape": "[[]]"}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[712101];\n  #pragma omp parallel for\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 712101; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = 1.000000e+00f;\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 30; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 30) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer_v = 0; k0_k1_fused_k2_fused_k3_fused_outer_v < 712101; ++k0_k1_fused_k2_fused_k3_fused_outer_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_outer_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 62; ++k0) {\n    for (int k1 = 0; k1 < 93; ++k1) {\n      for (int k2 = 0; k2 < 65; ++k2) {\n        for (int k3 = 0; k3 < 57; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 344565) + (k1 * 3705)) + (k2 * 57)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((62, 93, 65, 57), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([712101], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((712101,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_outer in T.parallel(712101):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = T.float32(1)\n            for k0_k1_fused_k2_fused_k3_fused_inner in range(30):\n                data_1 = T.Buffer((21363030,), data=data.data)\n                data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 30 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer_v in range(712101):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_outer_v]", "op_args": [62, 93, 65, 57], "input_shape": "[[62, 93, 65, 57]]", "output_shape": "[[]]"}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 476000; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 59; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 59) + i3)] = roundf(data[((i0_i1_fused_i2_fused * 59) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((80, 85, 70, 59), \"float32\"), compute: T.Buffer((80, 85, 70, 59), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(476000):\n            for i3 in range(59):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 59 + i3\n                compute_1 = T.Buffer((28084000,), data=compute.data)\n                data_1 = T.Buffer((28084000,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [80, 85, 70, 59], "input_shape": "[[80, 85, 70, 59]]", "output_shape": "[[80, 85, 70, 59]]"}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 43; ++i0) {\n    for (int32_t i1 = 0; i1 < 59; ++i1) {\n      for (int32_t i2 = 0; i2 < 86; ++i2) {\n        for (int32_t i3 = 0; i3 < 45; ++i3) {\n          compute[((((i0 * 228330) + (i1 * 3870)) + (i2 * 45)) + i3)] = roundf(data[((((i0 * 228330) + (i1 * 3870)) + (i2 * 45)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) < 4909095) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((43, 59, 86, 45), \"float32\"), compute: T.Buffer((43, 59, 86, 45), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(43):\n            for i1, i2, i3 in T.grid(59, 86, 45):\n                cse_var_1: T.int32 = i0 * 228330 + i1 * 3870 + i2 * 45 + i3\n                compute_1 = T.Buffer((9818190,), data=compute.data)\n                data_1 = T.Buffer((9818190,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [43, 59, 86, 45], "input_shape": "[[43, 59, 86, 45]]", "output_shape": "[[43, 59, 86, 45]]"}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 16915770; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / sqrtf(data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(62) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((45, 93, 43, 94), \"float32\"), compute: T.Buffer((45, 93, 43, 94), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(16915770):\n            compute_1 = T.Buffer((16915770,), data=compute.data)\n            data_1 = T.Buffer((16915770,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.rsqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [45, 93, 43, 94], "input_shape": "[[45, 93, 43, 94]]", "output_shape": "[[45, 93, 43, 94]]"}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 5292; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 38; ++i2) {\n      for (int32_t i3 = 0; i3 < 48; ++i3) {\n        compute[(((i0_i1_fused * 1824) + (i2 * 48)) + i3)] = (1.000000e+00f / sqrtf(data[(((i0_i1_fused * 1824) + (i2 * 48)) + i3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((98, 54, 38, 48), \"float32\"), compute: T.Buffer((98, 54, 38, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(5292):\n            for i2, i3 in T.grid(38, 48):\n                cse_var_1: T.int32 = i0_i1_fused * 1824 + i2 * 48 + i3\n                compute_1 = T.Buffer((9652608,), data=compute.data)\n                data_1 = T.Buffer((9652608,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [98, 54, 38, 48], "input_shape": "[[98, 54, 38, 48]]", "output_shape": "[[98, 54, 38, 48]]"}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 57 : ((ax0 == 2) ? 64 : ((ax0 == 1) ? 77 : 60)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 57 : ((((int)threadIdx.x) == 2) ? 64 : ((((int)threadIdx.x) == 1) ? 77 : 60)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((60, 77, 64, 57), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 57, T.if_then_else(ax0 == 2, 64, T.if_then_else(ax0 == 1, 77, 60)))", "op_args": [60, 77, 64, 57], "input_shape": "[[60, 77, 64, 57]]", "output_shape": "[[4]]"}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 40 : ((ax0 == 2) ? 75 : ((ax0 == 1) ? 87 : 91)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 40 : ((((int)threadIdx.x) == 2) ? 75 : ((((int)threadIdx.x) == 1) ? 87 : 91)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((91, 87, 75, 40), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 40, T.if_then_else(ax0 == 2, 75, T.if_then_else(ax0 == 1, 87, 91)))", "op_args": [91, 87, 75, 40], "input_shape": "[[91, 87, 75, 40]]", "output_shape": "[[4]]"}{"op_name": "sigmoid", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 254408; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 94; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 94) + i3)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[((i0_i1_fused_i2_fused * 94) + i3)]))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((88, 49, 59, 94), \"float32\"), compute: T.Buffer((88, 49, 59, 94), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(254408):\n            for i3 in range(94):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 94 + i3\n                compute_1 = T.Buffer((23914352,), data=compute.data)\n                data_1 = T.Buffer((23914352,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])", "op_args": [88, 49, 59, 94], "input_shape": "[[88, 49, 59, 94]]", "output_shape": "[[88, 49, 59, 94]]"}{"op_name": "sigmoid", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 123480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 97; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 97) + i3)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[((i0_i1_fused_i2_fused * 97) + i3)]))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((42, 30, 98, 97), \"float32\"), compute: T.Buffer((42, 30, 98, 97), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(123480):\n            for i3 in range(97):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 97 + i3\n                compute_1 = T.Buffer((11977560,), data=compute.data)\n                data_1 = T.Buffer((11977560,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])", "op_args": [42, 30, 98, 97], "input_shape": "[[42, 30, 98, 97]]", "output_shape": "[[42, 30, 98, 97]]"}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 8910; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 99; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 40; ++ax3) {\n        T_sign[(((ax0_ax1_fused * 3960) + (ax2 * 40)) + ax3)] = ((0.000000e+00f < data[(((ax0_ax1_fused * 3960) + (ax2 * 40)) + ax3)]) ? 1.000000e+00f : ((data[(((ax0_ax1_fused * 3960) + (ax2 * 40)) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((90, 99, 99, 40), \"float32\"), T_sign: T.Buffer((90, 99, 99, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(8910):\n            for ax2, ax3 in T.grid(99, 40):\n                cse_var_1: T.int32 = ax0_ax1_fused * 3960 + ax2 * 40 + ax3\n                T_sign_1 = T.Buffer((35283600,), data=T_sign.data)\n                data_1 = T.Buffer((35283600,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [90, 99, 99, 40], "input_shape": "[[90, 99, 99, 40]]", "output_shape": "[[90, 99, 99, 40]]"}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 5300; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 52; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 33; ++ax3) {\n        T_sign[(((ax0_ax1_fused * 1716) + (ax2 * 33)) + ax3)] = ((0.000000e+00f < data[(((ax0_ax1_fused * 1716) + (ax2 * 33)) + ax3)]) ? 1.000000e+00f : ((data[(((ax0_ax1_fused * 1716) + (ax2 * 33)) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(53) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 53) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 53) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 53) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((53, 100, 52, 33), \"float32\"), T_sign: T.Buffer((53, 100, 52, 33), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(5300):\n            for ax2, ax3 in T.grid(52, 33):\n                cse_var_1: T.int32 = ax0_ax1_fused * 1716 + ax2 * 33 + ax3\n                T_sign_1 = T.Buffer((9094800,), data=T_sign.data)\n                data_1 = T.Buffer((9094800,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [53, 100, 52, 33], "input_shape": "[[53, 100, 52, 33]]", "output_shape": "[[53, 100, 52, 33]]"}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 22761480; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((42, 70, 98, 79), \"float32\"), compute: T.Buffer((42, 70, 98, 79), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(22761480):\n            compute_1 = T.Buffer((22761480,), data=compute.data)\n            data_1 = T.Buffer((22761480,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [42, 70, 98, 79], "input_shape": "[[42, 70, 98, 79]]", "output_shape": "[[42, 70, 98, 79]]"}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720360; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 45; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 45) + i3)] = sinhf(data[((i0_i1_fused_i2_fused * 45) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 4052025) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((92, 90, 87, 45), \"float32\"), compute: T.Buffer((92, 90, 87, 45), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(720360):\n            for i3 in range(45):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 45 + i3\n                compute_1 = T.Buffer((32416200,), data=compute.data)\n                data_1 = T.Buffer((32416200,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])", "op_args": [92, 90, 87, 45], "input_shape": "[[92, 90, 87, 45]]", "output_shape": "[[92, 90, 87, 45]]"}{"op_name": "sqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 223146; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 96; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 96) + i3)] = sqrtf(data[((i0_i1_fused_i2_fused * 96) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((42, 69, 77, 96), \"float32\"), compute: T.Buffer((42, 69, 77, 96), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(223146):\n            for i3 in range(96):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 96 + i3\n                compute_1 = T.Buffer((21422016,), data=compute.data)\n                data_1 = T.Buffer((21422016,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])", "op_args": [42, 69, 77, 96], "input_shape": "[[42, 69, 77, 96]]", "output_shape": "[[42, 69, 77, 96]]"}{"op_name": "sqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5508300; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(43) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 43) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 43) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((30, 70, 61, 43), \"float32\"), compute: T.Buffer((30, 70, 61, 43), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(5508300):\n            compute_1 = T.Buffer((5508300,), data=compute.data)\n            data_1 = T.Buffer((5508300,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [30, 70, 61, 43], "input_shape": "[[30, 70, 61, 43]]", "output_shape": "[[30, 70, 61, 43]]"}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 121448; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 55; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 55) + i3)] = tanf(data[((i0_i1_fused_i2_fused * 55) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 21) + (((int)threadIdx.x) >> 1)) < 3339820) {\n    compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((68, 38, 47, 55), \"float32\"), compute: T.Buffer((68, 38, 47, 55), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(121448):\n            for i3 in range(55):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 55 + i3\n                compute_1 = T.Buffer((6679640,), data=compute.data)\n                data_1 = T.Buffer((6679640,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [68, 38, 47, 55], "input_shape": "[[68, 38, 47, 55]]", "output_shape": "[[68, 38, 47, 55]]"}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 165620; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 59; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 59) + i3)] = tanf(data[((i0_i1_fused_i2_fused * 59) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((52, 91, 35, 59), \"float32\"), compute: T.Buffer((52, 91, 35, 59), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(165620):\n            for i3 in range(59):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 59 + i3\n                compute_1 = T.Buffer((9771580,), data=compute.data)\n                data_1 = T.Buffer((9771580,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [52, 91, 35, 59], "input_shape": "[[52, 91, 35, 59]]", "output_shape": "[[52, 91, 35, 59]]"}{"op_name": "tanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 326274; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 49; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 49) + i3)] = tanhf(data[((i0_i1_fused_i2_fused * 49) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((78, 47, 89, 49), \"float32\"), compute: T.Buffer((78, 47, 89, 49), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(326274):\n            for i3 in range(49):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 49 + i3\n                compute_1 = T.Buffer((15987426,), data=compute.data)\n                data_1 = T.Buffer((15987426,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])", "op_args": [78, 47, 89, 49], "input_shape": "[[78, 47, 89, 49]]", "output_shape": "[[78, 47, 89, 49]]"}{"op_name": "tanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 267540; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 45; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 45) + i3)] = tanhf(data[((i0_i1_fused_i2_fused * 45) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((49, 84, 65, 45), \"float32\"), compute: T.Buffer((49, 84, 65, 45), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(267540):\n            for i3 in range(45):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 45 + i3\n                compute_1 = T.Buffer((12039300,), data=compute.data)\n                data_1 = T.Buffer((12039300,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])", "op_args": [49, 84, 65, 45], "input_shape": "[[49, 84, 65, 45]]", "output_shape": "[[49, 84, 65, 45]]"}{"op_name": "matmul", "c_code": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused = 0; ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused < 1444; ++ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused) {\n    for (int32_t ax0_outer_inner_init = 0; ax0_outer_inner_init < 2; ++ax0_outer_inner_init) {\n      for (int32_t ax0_inner_init = 0; ax0_inner_init < 2; ++ax0_inner_init) {\n        T_matmul[(((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 76) * 304) + (ax0_outer_inner_init * 152)) + (ax0_inner_init * 76)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 76))] = 0.000000e+00f;\n      }\n    }\n    for (int32_t ax0_outer_inner = 0; ax0_outer_inner < 2; ++ax0_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 48; ++k_inner) {\n        for (int32_t ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n          T_matmul[(((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 76) * 304) + (ax0_outer_inner * 152)) + (ax0_inner * 76)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 76))] = (T_matmul[(((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 76) * 304) + (ax0_outer_inner * 152)) + (ax0_inner * 76)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 76))] + (left_matrix[(((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 76) * 192) + (ax0_outer_inner * 96)) + (ax0_inner * 48)) + k_inner)] * right_matrix[((k_inner * 76) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 76))]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(361) default_function_kernel(float* __restrict__ T_matmul, float* __restrict__ left_matrix, float* __restrict__ right_matrix) {\n  float T_matmul_local[1];\n  __shared__ float left_matrix_shared[152];\n  __shared__ float right_matrix_shared[152];\n  T_matmul_local[0] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 6; ++k_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 76) {\n      *(float2*)(left_matrix_shared + (((int)threadIdx.x) * 2)) = *(float2*)(left_matrix + (((((((int)blockIdx.x) >> 2) * 912) + ((((int)threadIdx.x) >> 2) * 48)) + (k_outer_outer * 8)) + ((((int)threadIdx.x) & 3) * 2)));\n    }\n    if (((int)threadIdx.x) < 76) {\n      int2 __1;\n        int2 __2;\n          int2 __3;\n            int2 v_ = make_int2((k_outer_outer * 608), (k_outer_outer * 608));\n            int2 __4;\n              int2 __5;\n                int2 v__1 = make_int2(((((int)threadIdx.x) * 2))+(1*0), ((((int)threadIdx.x) * 2))+(1*1));\n                int2 v__2 = make_int2(19, 19);\n                __5.x = (v__1.x/v__2.x);\n                __5.y = (v__1.y/v__2.y);\n              int2 v__3 = make_int2(76, 76);\n              __4.x = (__5.x*v__3.x);\n              __4.y = (__5.y*v__3.y);\n            __3.x = (v_.x+__4.x);\n            __3.y = (v_.y+__4.y);\n          int2 v__4 = make_int2(((((int)blockIdx.x) & 3) * 19), ((((int)blockIdx.x) & 3) * 19));\n          __2.x = (__3.x+v__4.x);\n          __2.y = (__3.y+v__4.y);\n        int2 __6;\n          int2 v__5 = make_int2(((((int)threadIdx.x) * 2))+(1*0), ((((int)threadIdx.x) * 2))+(1*1));\n          int2 v__6 = make_int2(19, 19);\n          __6.x = (v__5.x%v__6.x);\n          __6.y = (v__5.y%v__6.y);\n        __1.x = (__2.x+__6.x);\n        __1.y = (__2.y+__6.y);\n      *(float2*)(right_matrix_shared + (((int)threadIdx.x) * 2)) = make_float2(right_matrix[__1.x],right_matrix[__1.y]);\n    }\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 4; ++k_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_matmul_local[0] = (T_matmul_local[0] + (left_matrix_shared[((((((int)threadIdx.x) / 19) * 8) + (k_outer_inner * 2)) + k_inner)] * right_matrix_shared[(((k_outer_inner * 38) + (k_inner * 19)) + (((int)threadIdx.x) % 19))]));\n      }\n    }\n  }\n  T_matmul[(((((((int)blockIdx.x) >> 2) * 1444) + ((((int)threadIdx.x) / 19) * 76)) + ((((int)blockIdx.x) & 3) * 19)) + (((int)threadIdx.x) % 19))] = T_matmul_local[0];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(left_matrix: T.Buffer((76, 48), \"float32\"), right_matrix: T.Buffer((48, 76), \"float32\"), T_matmul: T.Buffer((76, 76), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused in T.parallel(1444):\n            T_matmul_1 = T.Buffer((5776,), data=T_matmul.data)\n            for ax0_outer_inner_init, ax0_inner_init in T.grid(2, 2):\n                T_matmul_1[ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused // 76 * 304 + ax0_outer_inner_init * 152 + ax0_inner_init * 76 + ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 76] = T.float32(0)\n            for ax0_outer_inner, k_inner, ax0_inner in T.grid(2, 48, 2):\n                cse_var_3: T.int32 = ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused // 76\n                cse_var_2: T.int32 = ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 76\n                cse_var_1: T.int32 = cse_var_3 * 304 + ax0_outer_inner * 152 + ax0_inner * 76 + cse_var_2\n                left_matrix_1 = T.Buffer((3648,), data=left_matrix.data)\n                right_matrix_1 = T.Buffer((3648,), data=right_matrix.data)\n                T_matmul_1[cse_var_1] = T_matmul_1[cse_var_1] + left_matrix_1[cse_var_3 * 192 + ax0_outer_inner * 96 + ax0_inner * 48 + k_inner] * right_matrix_1[k_inner * 76 + cse_var_2]", "op_args": [99, 97, 76, 48], "input_shape": "[[76, 48], [48, 76]]", "output_shape": "[[76, 76]]"}{"op_name": "matmul", "c_code": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  #pragma omp parallel for\n  for (int32_t ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused = 0; ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused < 1058; ++ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused) {\n    for (int32_t ax0_inner_init = 0; ax0_inner_init < 2; ++ax0_inner_init) {\n      T_matmul[((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 46) * 92) + (ax0_inner_init * 46)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 46))] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 24; ++k_outer) {\n      for (int32_t k_inner = 0; k_inner < 3; ++k_inner) {\n        for (int32_t ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n          T_matmul[((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 46) * 92) + (ax0_inner * 46)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 46))] = (T_matmul[((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 46) * 92) + (ax0_inner * 46)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 46))] + (left_matrix[(((((ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused / 46) * 144) + (ax0_inner * 72)) + (k_outer * 3)) + k_inner)] * right_matrix[(((k_outer * 138) + (k_inner * 46)) + (ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 46))]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(23) default_function_kernel(float* __restrict__ T_matmul, float* __restrict__ left_matrix, float* __restrict__ right_matrix) {\n  float T_matmul_local[2];\n  __shared__ float left_matrix_shared[552];\n  __shared__ float right_matrix_shared[48];\n  for (int ax1_c_inner_init = 0; ax1_c_inner_init < 2; ++ax1_c_inner_init) {\n    T_matmul_local[ax1_c_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 3; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 24; ++ax0_ax1_fused_outer_outer) {\n      left_matrix_shared[((ax0_ax1_fused_outer_outer * 23) + ((int)threadIdx.x))] = left_matrix[(((((((int)blockIdx.x) / 23) * 1656) + ((((ax0_ax1_fused_outer_outer * 23) + ((int)threadIdx.x)) / 24) * 72)) + (k_outer_outer * 24)) + (((ax0_ax1_fused_outer_outer * 23) + ((int)threadIdx.x)) % 24))];\n    }\n    for (int ax0_ax1_fused_outer_outer_1 = 0; ax0_ax1_fused_outer_outer_1 < 3; ++ax0_ax1_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_outer_outer_1 * 23) + ((int)threadIdx.x)) < 48) {\n        right_matrix_shared[((ax0_ax1_fused_outer_outer_1 * 23) + ((int)threadIdx.x))] = right_matrix[((((k_outer_outer * 1104) + ((((ax0_ax1_fused_outer_outer_1 * 23) + ((int)threadIdx.x)) >> 1) * 46)) + ((((int)blockIdx.x) % 23) * 2)) + ((ax0_ax1_fused_outer_outer_1 + ((int)threadIdx.x)) & 1))];\n      }\n    }\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 8; ++k_outer_inner) {\n      for (int k_inner = 0; k_inner < 3; ++k_inner) {\n        for (int ax1_c_inner = 0; ax1_c_inner < 2; ++ax1_c_inner) {\n          T_matmul_local[ax1_c_inner] = (T_matmul_local[ax1_c_inner] + (left_matrix_shared[(((((int)threadIdx.x) * 24) + (k_outer_inner * 3)) + k_inner)] * right_matrix_shared[(((k_outer_inner * 6) + (k_inner * 2)) + ax1_c_inner)]));\n        }\n      }\n    }\n  }\n  for (int ax1_inner = 0; ax1_inner < 2; ++ax1_inner) {\n    T_matmul[(((((((int)blockIdx.x) / 23) * 1058) + (((int)threadIdx.x) * 46)) + ((((int)blockIdx.x) % 23) * 2)) + ax1_inner)] = T_matmul_local[ax1_inner];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(left_matrix: T.Buffer((46, 72), \"float32\"), right_matrix: T.Buffer((72, 46), \"float32\"), T_matmul: T.Buffer((46, 46), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused in T.parallel(1058):\n            T_matmul_1 = T.Buffer((2116,), data=T_matmul.data)\n            for ax0_inner_init in range(2):\n                T_matmul_1[ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused // 46 * 92 + ax0_inner_init * 46 + ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 46] = T.float32(0)\n            for k_outer, k_inner, ax0_inner in T.grid(24, 3, 2):\n                cse_var_3: T.int32 = ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused // 46\n                cse_var_2: T.int32 = ax0_outer_outer_outer_ax1_outer_outer_outer_fused_ax0_outer_outer_inner_fused_ax1_outer_outer_inner_fused % 46\n                cse_var_1: T.int32 = cse_var_3 * 92 + ax0_inner * 46 + cse_var_2\n                left_matrix_1 = T.Buffer((3312,), data=left_matrix.data)\n                right_matrix_1 = T.Buffer((3312,), data=right_matrix.data)\n                T_matmul_1[cse_var_1] = T_matmul_1[cse_var_1] + left_matrix_1[cse_var_3 * 144 + ax0_inner * 72 + k_outer * 3 + k_inner] * right_matrix_1[k_outer * 138 + k_inner * 46 + cse_var_2]", "op_args": [70, 33, 46, 72], "input_shape": "[[46, 72], [72, 46]]", "output_shape": "[[46, 46]]"}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 109800; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 61; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 61) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 61) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 61) + ax3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 837225) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((61, 40, 45, 61), \"float32\"), data_1: T.Buffer((61, 40, 45, 61), \"float32\"), T_add: T.Buffer((61, 40, 45, 61), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(109800):\n            for ax3 in range(61):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 61 + ax3\n                T_add_1 = T.Buffer((6697800,), data=T_add.data)\n                data_2 = T.Buffer((6697800,), data=data.data)\n                data_3 = T.Buffer((6697800,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [61, 40, 45, 61], "input_shape": "[[61, 40, 45, 61], [61, 40, 45, 61]]", "output_shape": "[[61, 40, 45, 61]]"}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 1216; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 30; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 99; ++ax3) {\n        T_add[(((ax0_ax1_fused * 2970) + (ax2 * 99)) + ax3)] = (sqrtf(data[(((ax0_ax1_fused * 2970) + (ax2 * 99)) + ax3)]) + cosf(data_1[(((ax0_ax1_fused * 2970) + (ax2 * 99)) + ax3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((38, 32, 30, 99), \"float32\"), data_1: T.Buffer((38, 32, 30, 99), \"float32\"), T_add: T.Buffer((38, 32, 30, 99), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(1216):\n            for ax2, ax3 in T.grid(30, 99):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2970 + ax2 * 99 + ax3\n                T_add_1 = T.Buffer((3611520,), data=T_add.data)\n                data_2 = T.Buffer((3611520,), data=data.data)\n                data_3 = T.Buffer((3611520,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [38, 32, 30, 99], "input_shape": "[[38, 32, 30, 99], [38, 32, 30, 99]]", "output_shape": "[[38, 32, 30, 99]]"}{"op_name": "multi_out_op", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 694260; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 35; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 35) + i3)] = sqrtf((data[((i0_i1_fused_i2_fused * 35) + i3)] + data_1[((i0_i1_fused_i2_fused * 35) + i3)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 7308; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 95; ++i2) {\n      for (int32_t i3_1 = 0; i3_1 < 35; ++i3_1) {\n        compute_1[(((i0_i1_fused * 3325) + (i2 * 35)) + i3_1)] = cosf((data[(((i0_i1_fused * 3325) + (i2 * 35)) + i3_1)] + data_1[(((i0_i1_fused * 3325) + (i2 * 35)) + i3_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 6074775) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((84, 87, 95, 35), \"float32\"), data_1: T.Buffer((84, 87, 95, 35), \"float32\"), compute: T.Buffer((84, 87, 95, 35), \"float32\"), compute_1: T.Buffer((84, 87, 95, 35), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((24299100,), data=data.data)\n        data_3 = T.Buffer((24299100,), data=data_1.data)\n        for i0_i1_fused_i2_fused in T.parallel(694260):\n            for i3 in range(35):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 35 + i3\n                compute_2 = T.Buffer((24299100,), data=compute.data)\n                compute_2[cse_var_1] = T.sqrt(data_2[cse_var_1] + data_3[cse_var_1])\n        for i0_i1_fused in T.parallel(7308):\n            for i2, i3 in T.grid(95, 35):\n                cse_var_2: T.int32 = i0_i1_fused * 3325 + i2 * 35 + i3\n                compute_2 = T.Buffer((24299100,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(data_2[cse_var_2] + data_3[cse_var_2])", "op_args": [84, 87, 95, 35], "input_shape": "[[84, 87, 95, 35], [84, 87, 95, 35], [84, 87, 95, 35]]", "output_shape": "[[84, 87, 95, 35]]"}{"op_name": "multi_out_op", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 23522400; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf((data[i0_i1_fused_i2_fused_i3_fused] + data_1[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 245025; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 96; ++i3) {\n      compute_1[((i0_i1_fused_i2_fused * 96) + i3)] = cosf((data[((i0_i1_fused_i2_fused * 96) + i3)] + data_1[((i0_i1_fused_i2_fused * 96) + i3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((45, 55, 99, 96), \"float32\"), data_1: T.Buffer((45, 55, 99, 96), \"float32\"), compute: T.Buffer((45, 55, 99, 96), \"float32\"), compute_1: T.Buffer((45, 55, 99, 96), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((23522400,), data=data.data)\n        data_3 = T.Buffer((23522400,), data=data_1.data)\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(23522400):\n            compute_2 = T.Buffer((23522400,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_2[i0_i1_fused_i2_fused_i3_fused] + data_3[i0_i1_fused_i2_fused_i3_fused])\n        for i0_i1_fused_i2_fused in T.parallel(245025):\n            for i3 in range(96):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 96 + i3\n                compute_2 = T.Buffer((23522400,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(data_2[cse_var_1] + data_3[cse_var_1])", "op_args": [45, 55, 99, 96], "input_shape": "[[45, 55, 99, 96], [45, 55, 99, 96], [45, 55, 99, 96]]", "output_shape": "[[45, 55, 99, 96]]"}