{"op_name": "conv1d", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_kernel_strides == NULL)) {\n  }\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  }\n  void* conv1d_ncw_local = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)160, 2, 32);\n  if (conv1d_ncw_local == NULL) {\n    return -1;\n  }\n  for (int32_t yy_c_outer_outer_inner = 0; yy_c_outer_outer_inner < 2; ++yy_c_outer_outer_inner) {\n    int32_t cse_var_1 = (yy_c_outer_outer_inner * 2);\n    *(float2*)(((float*)conv1d_ncw_local) + cse_var_1) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 4)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 8)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 12)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 16)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 20)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 24)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 28)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 32)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    *(float2*)(((float*)conv1d_ncw_local) + (cse_var_1 + 36)) = ((float2)(0.000000e+00f, 0.000000e+00f));\n    for (int32_t rc_inner = 0; rc_inner < 3; ++rc_inner) {\n      int32_t cse_var_31 = (rc_inner * 3);\n      int32_t cse_var_30 = (cse_var_1 + 8);\n      int32_t cse_var_29 = (cse_var_1 + 4);\n      int32_t cse_var_28 = (cse_var_1 + 36);\n      int32_t cse_var_27 = (cse_var_1 + 32);\n      int32_t cse_var_26 = (cse_var_1 + 28);\n      int32_t cse_var_25 = (cse_var_1 + 24);\n      int32_t cse_var_24 = (cse_var_1 + 20);\n      int32_t cse_var_23 = (cse_var_1 + 16);\n      int32_t cse_var_22 = (cse_var_1 + 12);\n      int32_t cse_var_21 = (cse_var_31 + 9);\n      int32_t cse_var_20 = (cse_var_31 + 38);\n      int32_t cse_var_19 = (cse_var_31 + 37);\n      int32_t cse_var_18 = (cse_var_31 + 36);\n      int32_t cse_var_17 = (cse_var_31 + 29);\n      int32_t cse_var_16 = (cse_var_31 + 28);\n      int32_t cse_var_15 = (cse_var_31 + 27);\n      int32_t cse_var_14 = (cse_var_31 + 20);\n      int32_t cse_var_13 = (cse_var_31 + 2);\n      int32_t cse_var_12 = (cse_var_31 + 19);\n      int32_t cse_var_11 = (cse_var_31 + 18);\n      int32_t cse_var_10 = (cse_var_31 + 11);\n      int32_t cse_var_9 = (cse_var_31 + 10);\n      int32_t cse_var_8 = (cse_var_31 + 1);\n      int32_t cse_var_7 = ((rc_inner * 10) + (yy_c_outer_outer_inner * 4));\n      int32_t cse_var_6 = (cse_var_7 + 32);\n      int32_t cse_var_5 = (cse_var_7 + 31);\n      int32_t cse_var_4 = (cse_var_7 + 30);\n      int32_t cse_var_3 = (cse_var_7 + 2);\n      int32_t cse_var_2 = (cse_var_7 + 1);\n      int32_t2 v_ = int32_t2((cse_var_1)+(1*0), (cse_var_1)+(1*1));\n      int32_t2 v__1 = int32_t2((cse_var_7)+(2*0), (cse_var_7)+(2*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_1) = ((float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[cse_var_31], ((float*)kernel_1)[cse_var_31]))));\n      int32_t2 v__2 = int32_t2((cse_var_29)+(1*0), (cse_var_29)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_29) = ((float2(((float*)conv1d_ncw_local)[v__2.s0],((float*)conv1d_ncw_local)[v__2.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[cse_var_21], ((float*)kernel_1)[cse_var_21]))));\n      int32_t2 v__3 = int32_t2((cse_var_30)+(1*0), (cse_var_30)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_30) = ((float2(((float*)conv1d_ncw_local)[v__3.s0],((float*)conv1d_ncw_local)[v__3.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[cse_var_11], ((float*)kernel_1)[cse_var_11]))));\n      int32_t2 v__4 = int32_t2((cse_var_22)+(1*0), (cse_var_22)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_22) = ((float2(((float*)conv1d_ncw_local)[v__4.s0],((float*)conv1d_ncw_local)[v__4.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[cse_var_15], ((float*)kernel_1)[cse_var_15]))));\n      int32_t2 v__5 = int32_t2((cse_var_23)+(1*0), (cse_var_23)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_23) = ((float2(((float*)conv1d_ncw_local)[v__5.s0],((float*)conv1d_ncw_local)[v__5.s1])) + ((float2(((float*)data_1)[v__1.s0],((float*)data_1)[v__1.s1])) * ((float2)(((float*)kernel_1)[cse_var_18], ((float*)kernel_1)[cse_var_18]))));\n      int32_t2 v__6 = int32_t2((cse_var_24)+(1*0), (cse_var_24)+(1*1));\n      int32_t2 v__7 = int32_t2((cse_var_4)+(2*0), (cse_var_4)+(2*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_24) = ((float2(((float*)conv1d_ncw_local)[v__6.s0],((float*)conv1d_ncw_local)[v__6.s1])) + ((float2(((float*)data_1)[v__7.s0],((float*)data_1)[v__7.s1])) * ((float2)(((float*)kernel_1)[cse_var_31], ((float*)kernel_1)[cse_var_31]))));\n      int32_t2 v__8 = int32_t2((cse_var_25)+(1*0), (cse_var_25)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_25) = ((float2(((float*)conv1d_ncw_local)[v__8.s0],((float*)conv1d_ncw_local)[v__8.s1])) + ((float2(((float*)data_1)[v__7.s0],((float*)data_1)[v__7.s1])) * ((float2)(((float*)kernel_1)[cse_var_21], ((float*)kernel_1)[cse_var_21]))));\n      int32_t2 v__9 = int32_t2((cse_var_26)+(1*0), (cse_var_26)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_26) = ((float2(((float*)conv1d_ncw_local)[v__9.s0],((float*)conv1d_ncw_local)[v__9.s1])) + ((float2(((float*)data_1)[v__7.s0],((float*)data_1)[v__7.s1])) * ((float2)(((float*)kernel_1)[cse_var_11], ((float*)kernel_1)[cse_var_11]))));\n      int32_t2 v__10 = int32_t2((cse_var_27)+(1*0), (cse_var_27)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_27) = ((float2(((float*)conv1d_ncw_local)[v__10.s0],((float*)conv1d_ncw_local)[v__10.s1])) + ((float2(((float*)data_1)[v__7.s0],((float*)data_1)[v__7.s1])) * ((float2)(((float*)kernel_1)[cse_var_15], ((float*)kernel_1)[cse_var_15]))));\n      int32_t2 v__11 = int32_t2((cse_var_28)+(1*0), (cse_var_28)+(1*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_28) = ((float2(((float*)conv1d_ncw_local)[v__11.s0],((float*)conv1d_ncw_local)[v__11.s1])) + ((float2(((float*)data_1)[v__7.s0],((float*)data_1)[v__7.s1])) * ((float2)(((float*)kernel_1)[cse_var_18], ((float*)kernel_1)[cse_var_18]))));\n      int32_t2 v__12 = int32_t2((cse_var_2)+(2*0), (cse_var_2)+(2*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_1) = ((float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1])) + ((float2(((float*)data_1)[v__12.s0],((float*)data_1)[v__12.s1])) * ((float2)(((float*)kernel_1)[cse_var_8], ((float*)kernel_1)[cse_var_8]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_29) = ((float2(((float*)conv1d_ncw_local)[v__2.s0],((float*)conv1d_ncw_local)[v__2.s1])) + ((float2(((float*)data_1)[v__12.s0],((float*)data_1)[v__12.s1])) * ((float2)(((float*)kernel_1)[cse_var_9], ((float*)kernel_1)[cse_var_9]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_30) = ((float2(((float*)conv1d_ncw_local)[v__3.s0],((float*)conv1d_ncw_local)[v__3.s1])) + ((float2(((float*)data_1)[v__12.s0],((float*)data_1)[v__12.s1])) * ((float2)(((float*)kernel_1)[cse_var_12], ((float*)kernel_1)[cse_var_12]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_22) = ((float2(((float*)conv1d_ncw_local)[v__4.s0],((float*)conv1d_ncw_local)[v__4.s1])) + ((float2(((float*)data_1)[v__12.s0],((float*)data_1)[v__12.s1])) * ((float2)(((float*)kernel_1)[cse_var_16], ((float*)kernel_1)[cse_var_16]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_23) = ((float2(((float*)conv1d_ncw_local)[v__5.s0],((float*)conv1d_ncw_local)[v__5.s1])) + ((float2(((float*)data_1)[v__12.s0],((float*)data_1)[v__12.s1])) * ((float2)(((float*)kernel_1)[cse_var_19], ((float*)kernel_1)[cse_var_19]))));\n      int32_t2 v__13 = int32_t2((cse_var_5)+(2*0), (cse_var_5)+(2*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_24) = ((float2(((float*)conv1d_ncw_local)[v__6.s0],((float*)conv1d_ncw_local)[v__6.s1])) + ((float2(((float*)data_1)[v__13.s0],((float*)data_1)[v__13.s1])) * ((float2)(((float*)kernel_1)[cse_var_8], ((float*)kernel_1)[cse_var_8]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_25) = ((float2(((float*)conv1d_ncw_local)[v__8.s0],((float*)conv1d_ncw_local)[v__8.s1])) + ((float2(((float*)data_1)[v__13.s0],((float*)data_1)[v__13.s1])) * ((float2)(((float*)kernel_1)[cse_var_9], ((float*)kernel_1)[cse_var_9]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_26) = ((float2(((float*)conv1d_ncw_local)[v__9.s0],((float*)conv1d_ncw_local)[v__9.s1])) + ((float2(((float*)data_1)[v__13.s0],((float*)data_1)[v__13.s1])) * ((float2)(((float*)kernel_1)[cse_var_12], ((float*)kernel_1)[cse_var_12]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_27) = ((float2(((float*)conv1d_ncw_local)[v__10.s0],((float*)conv1d_ncw_local)[v__10.s1])) + ((float2(((float*)data_1)[v__13.s0],((float*)data_1)[v__13.s1])) * ((float2)(((float*)kernel_1)[cse_var_16], ((float*)kernel_1)[cse_var_16]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_28) = ((float2(((float*)conv1d_ncw_local)[v__11.s0],((float*)conv1d_ncw_local)[v__11.s1])) + ((float2(((float*)data_1)[v__13.s0],((float*)data_1)[v__13.s1])) * ((float2)(((float*)kernel_1)[cse_var_19], ((float*)kernel_1)[cse_var_19]))));\n      int32_t2 v__14 = int32_t2((cse_var_3)+(2*0), (cse_var_3)+(2*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_1) = ((float2(((float*)conv1d_ncw_local)[v_.s0],((float*)conv1d_ncw_local)[v_.s1])) + ((float2(((float*)data_1)[v__14.s0],((float*)data_1)[v__14.s1])) * ((float2)(((float*)kernel_1)[cse_var_13], ((float*)kernel_1)[cse_var_13]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_29) = ((float2(((float*)conv1d_ncw_local)[v__2.s0],((float*)conv1d_ncw_local)[v__2.s1])) + ((float2(((float*)data_1)[v__14.s0],((float*)data_1)[v__14.s1])) * ((float2)(((float*)kernel_1)[cse_var_10], ((float*)kernel_1)[cse_var_10]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_30) = ((float2(((float*)conv1d_ncw_local)[v__3.s0],((float*)conv1d_ncw_local)[v__3.s1])) + ((float2(((float*)data_1)[v__14.s0],((float*)data_1)[v__14.s1])) * ((float2)(((float*)kernel_1)[cse_var_14], ((float*)kernel_1)[cse_var_14]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_22) = ((float2(((float*)conv1d_ncw_local)[v__4.s0],((float*)conv1d_ncw_local)[v__4.s1])) + ((float2(((float*)data_1)[v__14.s0],((float*)data_1)[v__14.s1])) * ((float2)(((float*)kernel_1)[cse_var_17], ((float*)kernel_1)[cse_var_17]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_23) = ((float2(((float*)conv1d_ncw_local)[v__5.s0],((float*)conv1d_ncw_local)[v__5.s1])) + ((float2(((float*)data_1)[v__14.s0],((float*)data_1)[v__14.s1])) * ((float2)(((float*)kernel_1)[cse_var_20], ((float*)kernel_1)[cse_var_20]))));\n      int32_t2 v__15 = int32_t2((cse_var_6)+(2*0), (cse_var_6)+(2*1));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_24) = ((float2(((float*)conv1d_ncw_local)[v__6.s0],((float*)conv1d_ncw_local)[v__6.s1])) + ((float2(((float*)data_1)[v__15.s0],((float*)data_1)[v__15.s1])) * ((float2)(((float*)kernel_1)[cse_var_13], ((float*)kernel_1)[cse_var_13]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_25) = ((float2(((float*)conv1d_ncw_local)[v__8.s0],((float*)conv1d_ncw_local)[v__8.s1])) + ((float2(((float*)data_1)[v__15.s0],((float*)data_1)[v__15.s1])) * ((float2)(((float*)kernel_1)[cse_var_10], ((float*)kernel_1)[cse_var_10]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_26) = ((float2(((float*)conv1d_ncw_local)[v__9.s0],((float*)conv1d_ncw_local)[v__9.s1])) + ((float2(((float*)data_1)[v__15.s0],((float*)data_1)[v__15.s1])) * ((float2)(((float*)kernel_1)[cse_var_14], ((float*)kernel_1)[cse_var_14]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_27) = ((float2(((float*)conv1d_ncw_local)[v__10.s0],((float*)conv1d_ncw_local)[v__10.s1])) + ((float2(((float*)data_1)[v__15.s0],((float*)data_1)[v__15.s1])) * ((float2)(((float*)kernel_1)[cse_var_17], ((float*)kernel_1)[cse_var_17]))));\n      *(float2*)(((float*)conv1d_ncw_local) + cse_var_28) = ((float2(((float*)conv1d_ncw_local)[v__11.s0],((float*)conv1d_ncw_local)[v__11.s1])) + ((float2(((float*)data_1)[v__15.s0],((float*)data_1)[v__15.s1])) * ((float2)(((float*)kernel_1)[cse_var_20], ((float*)kernel_1)[cse_var_20]))));\n    }\n  }\n  for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    for (int32_t ff_inner = 0; ff_inner < 5; ++ff_inner) {\n      int32_t cse_var_32 = ((nn_inner * 20) + (ff_inner * 4));\n      int32_t4 v__16 = int32_t4((cse_var_32)+(1*0), (cse_var_32)+(1*1), (cse_var_32)+(1*2), (cse_var_32)+(1*3));\n      *(float4*)(((float*)conv1d_ncw_1) + cse_var_32) = (float4(((float*)conv1d_ncw_local)[v__16.s0],((float*)conv1d_ncw_local)[v__16.s1],((float*)conv1d_ncw_local)[v__16.s2],((float*)conv1d_ncw_local)[v__16.s3]));\n    }\n  }\n  if (TVMBackendFreeWorkspace(1, dev_id, conv1d_ncw_local) != 0) {\n    return -1;\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[8];\n  __shared__ float pad_temp_shared[54];\n  __shared__ float kernel_shared[45];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[4] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  conv1d_ncw_local[5] = 0.000000e+00f;\n  conv1d_ncw_local[2] = 0.000000e+00f;\n  conv1d_ncw_local[6] = 0.000000e+00f;\n  conv1d_ncw_local[3] = 0.000000e+00f;\n  conv1d_ncw_local[7] = 0.000000e+00f;\n  pad_temp_shared[((int)threadIdx.x)] = data[((int)threadIdx.x)];\n  pad_temp_shared[(((int)threadIdx.x) + 5)] = data[((((((int)threadIdx.x) + 5) / 9) * 10) + ((((int)threadIdx.x) + 5) % 9))];\n  pad_temp_shared[(((int)threadIdx.x) + 10)] = data[(((((((int)threadIdx.x) + 10) / 9) * 10) + ((int)threadIdx.x)) + 1)];\n  pad_temp_shared[(((int)threadIdx.x) + 15)] = data[((((((int)threadIdx.x) + 15) / 9) * 10) + ((((int)threadIdx.x) + 6) % 9))];\n  pad_temp_shared[(((int)threadIdx.x) + 20)] = data[(((((((int)threadIdx.x) + 20) / 9) * 10) + ((int)threadIdx.x)) + 2)];\n  pad_temp_shared[(((int)threadIdx.x) + 25)] = data[((((((int)threadIdx.x) + 25) / 9) * 10) + ((((int)threadIdx.x) + 7) % 9))];\n  pad_temp_shared[(((int)threadIdx.x) + 30)] = data[(((((((int)threadIdx.x) + 30) / 9) * 10) + ((int)threadIdx.x)) + 3)];\n  pad_temp_shared[(((int)threadIdx.x) + 35)] = data[((((((int)threadIdx.x) + 35) / 9) * 10) + ((((int)threadIdx.x) + 8) % 9))];\n  pad_temp_shared[(((int)threadIdx.x) + 40)] = data[(((((((int)threadIdx.x) + 40) / 9) * 10) + ((int)threadIdx.x)) + 4)];\n  pad_temp_shared[(((int)threadIdx.x) + 45)] = data[(((int)threadIdx.x) + 50)];\n  if (((int)threadIdx.x) < 4) {\n    pad_temp_shared[(((int)threadIdx.x) + 50)] = data[(((((((int)threadIdx.x) + 50) / 9) * 10) + ((int)threadIdx.x)) + 5)];\n  }\n  kernel_shared[((int)threadIdx.x)] = kernel[((int)threadIdx.x)];\n  kernel_shared[(((int)threadIdx.x) + 5)] = kernel[(((int)threadIdx.x) + 5)];\n  kernel_shared[(((int)threadIdx.x) + 10)] = kernel[(((int)threadIdx.x) + 10)];\n  kernel_shared[(((int)threadIdx.x) + 15)] = kernel[(((int)threadIdx.x) + 15)];\n  kernel_shared[(((int)threadIdx.x) + 20)] = kernel[(((int)threadIdx.x) + 20)];\n  kernel_shared[(((int)threadIdx.x) + 25)] = kernel[(((int)threadIdx.x) + 25)];\n  kernel_shared[(((int)threadIdx.x) + 30)] = kernel[(((int)threadIdx.x) + 30)];\n  kernel_shared[(((int)threadIdx.x) + 35)] = kernel[(((int)threadIdx.x) + 35)];\n  kernel_shared[(((int)threadIdx.x) + 40)] = kernel[(((int)threadIdx.x) + 40)];\n  __syncthreads();\n  for (int rc_outer_inner = 0; rc_outer_inner < 3; ++rc_outer_inner) {\n    for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((rc_outer_inner * 9) + ry_inner)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[4] = (conv1d_ncw_local[4] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 4)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 2)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[5] = (conv1d_ncw_local[5] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 6)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[2] = (conv1d_ncw_local[2] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 27)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[6] = (conv1d_ncw_local[6] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 31)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[3] = (conv1d_ncw_local[3] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 29)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n      conv1d_ncw_local[7] = (conv1d_ncw_local[7] + (pad_temp_shared[(((rc_outer_inner * 9) + ry_inner) + 33)] * kernel_shared[(((((int)threadIdx.x) * 9) + (rc_outer_inner * 3)) + ry_inner)]));\n    }\n  }\n  for (int nn_inner = 0; nn_inner < 2; ++nn_inner) {\n    for (int yy_inner = 0; yy_inner < 2; ++yy_inner) {\n      conv1d_ncw[(((nn_inner * 20) + (((int)threadIdx.x) * 4)) + yy_inner)] = conv1d_ncw_local[((nn_inner * 2) + yy_inner)];\n      conv1d_ncw[((((nn_inner * 20) + (((int)threadIdx.x) * 4)) + yy_inner) + 2)] = conv1d_ncw_local[(((nn_inner * 2) + yy_inner) + 4)];\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 3, 10), \"float32\"), kernel: T.Buffer((5, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 5, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        conv1d_ncw_local = T.allocate([40], \"float32\", \"local\")\n        conv1d_ncw_local_1 = T.Buffer((40,), data=conv1d_ncw_local, scope=\"local\")\n        for yy_c_outer_outer_inner in range(2):\n            cse_var_1: T.int32 = yy_c_outer_outer_inner * 2\n            conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 4:cse_var_1 + 4 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 8:cse_var_1 + 8 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 12:cse_var_1 + 12 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 16:cse_var_1 + 16 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 20:cse_var_1 + 20 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 24:cse_var_1 + 24 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 28:cse_var_1 + 28 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 32:cse_var_1 + 32 + 2] = T.Broadcast(T.float32(0), 2)\n            conv1d_ncw_local_1[cse_var_1 + 36:cse_var_1 + 36 + 2] = T.Broadcast(T.float32(0), 2)\n            for rc_inner in range(3):\n                cse_var_31: T.int32 = rc_inner * 3\n                cse_var_30: T.int32 = cse_var_1 + 8\n                cse_var_29: T.int32 = cse_var_1 + 4\n                cse_var_28: T.int32 = cse_var_1 + 36\n                cse_var_27: T.int32 = cse_var_1 + 32\n                cse_var_26: T.int32 = cse_var_1 + 28\n                cse_var_25: T.int32 = cse_var_1 + 24\n                cse_var_24: T.int32 = cse_var_1 + 20\n                cse_var_23: T.int32 = cse_var_1 + 16\n                cse_var_22: T.int32 = cse_var_1 + 12\n                cse_var_21: T.int32 = cse_var_31 + 9\n                cse_var_20: T.int32 = cse_var_31 + 38\n                cse_var_19: T.int32 = cse_var_31 + 37\n                cse_var_18: T.int32 = cse_var_31 + 36\n                cse_var_17: T.int32 = cse_var_31 + 29\n                cse_var_16: T.int32 = cse_var_31 + 28\n                cse_var_15: T.int32 = cse_var_31 + 27\n                cse_var_14: T.int32 = cse_var_31 + 20\n                cse_var_13: T.int32 = cse_var_31 + 2\n                cse_var_12: T.int32 = cse_var_31 + 19\n                cse_var_11: T.int32 = cse_var_31 + 18\n                cse_var_10: T.int32 = cse_var_31 + 11\n                cse_var_9: T.int32 = cse_var_31 + 10\n                cse_var_8: T.int32 = cse_var_31 + 1\n                cse_var_7: T.int32 = rc_inner * 10 + yy_c_outer_outer_inner * 4\n                cse_var_6: T.int32 = cse_var_7 + 32\n                cse_var_5: T.int32 = cse_var_7 + 31\n                cse_var_4: T.int32 = cse_var_7 + 30\n                cse_var_3: T.int32 = cse_var_7 + 2\n                cse_var_2: T.int32 = cse_var_7 + 1\n                data_1 = T.Buffer((60,), data=data.data)\n                kernel_1 = T.Buffer((45,), data=kernel.data)\n                conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] = conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] + data_1[cse_var_7:cse_var_7 + 4:2] * T.Broadcast(kernel_1[cse_var_31], 2)\n                conv1d_ncw_local_1[cse_var_29:cse_var_29 + 2] = conv1d_ncw_local_1[cse_var_29:cse_var_29 + 2] + data_1[cse_var_7:cse_var_7 + 4:2] * T.Broadcast(kernel_1[cse_var_21], 2)\n                conv1d_ncw_local_1[cse_var_30:cse_var_30 + 2] = conv1d_ncw_local_1[cse_var_30:cse_var_30 + 2] + data_1[cse_var_7:cse_var_7 + 4:2] * T.Broadcast(kernel_1[cse_var_11], 2)\n                conv1d_ncw_local_1[cse_var_22:cse_var_22 + 2] = conv1d_ncw_local_1[cse_var_22:cse_var_22 + 2] + data_1[cse_var_7:cse_var_7 + 4:2] * T.Broadcast(kernel_1[cse_var_15], 2)\n                conv1d_ncw_local_1[cse_var_23:cse_var_23 + 2] = conv1d_ncw_local_1[cse_var_23:cse_var_23 + 2] + data_1[cse_var_7:cse_var_7 + 4:2] * T.Broadcast(kernel_1[cse_var_18], 2)\n                conv1d_ncw_local_1[cse_var_24:cse_var_24 + 2] = conv1d_ncw_local_1[cse_var_24:cse_var_24 + 2] + data_1[cse_var_4:cse_var_4 + 4:2] * T.Broadcast(kernel_1[cse_var_31], 2)\n                conv1d_ncw_local_1[cse_var_25:cse_var_25 + 2] = conv1d_ncw_local_1[cse_var_25:cse_var_25 + 2] + data_1[cse_var_4:cse_var_4 + 4:2] * T.Broadcast(kernel_1[cse_var_21], 2)\n                conv1d_ncw_local_1[cse_var_26:cse_var_26 + 2] = conv1d_ncw_local_1[cse_var_26:cse_var_26 + 2] + data_1[cse_var_4:cse_var_4 + 4:2] * T.Broadcast(kernel_1[cse_var_11], 2)\n                conv1d_ncw_local_1[cse_var_27:cse_var_27 + 2] = conv1d_ncw_local_1[cse_var_27:cse_var_27 + 2] + data_1[cse_var_4:cse_var_4 + 4:2] * T.Broadcast(kernel_1[cse_var_15], 2)\n                conv1d_ncw_local_1[cse_var_28:cse_var_28 + 2] = conv1d_ncw_local_1[cse_var_28:cse_var_28 + 2] + data_1[cse_var_4:cse_var_4 + 4:2] * T.Broadcast(kernel_1[cse_var_18], 2)\n                conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] = conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] + data_1[cse_var_2:cse_var_2 + 4:2] * T.Broadcast(kernel_1[cse_var_8], 2)\n                conv1d_ncw_local_1[cse_var_29:cse_var_29 + 2] = conv1d_ncw_local_1[cse_var_29:cse_var_29 + 2] + data_1[cse_var_2:cse_var_2 + 4:2] * T.Broadcast(kernel_1[cse_var_9], 2)\n                conv1d_ncw_local_1[cse_var_30:cse_var_30 + 2] = conv1d_ncw_local_1[cse_var_30:cse_var_30 + 2] + data_1[cse_var_2:cse_var_2 + 4:2] * T.Broadcast(kernel_1[cse_var_12], 2)\n                conv1d_ncw_local_1[cse_var_22:cse_var_22 + 2] = conv1d_ncw_local_1[cse_var_22:cse_var_22 + 2] + data_1[cse_var_2:cse_var_2 + 4:2] * T.Broadcast(kernel_1[cse_var_16], 2)\n                conv1d_ncw_local_1[cse_var_23:cse_var_23 + 2] = conv1d_ncw_local_1[cse_var_23:cse_var_23 + 2] + data_1[cse_var_2:cse_var_2 + 4:2] * T.Broadcast(kernel_1[cse_var_19], 2)\n                conv1d_ncw_local_1[cse_var_24:cse_var_24 + 2] = conv1d_ncw_local_1[cse_var_24:cse_var_24 + 2] + data_1[cse_var_5:cse_var_5 + 4:2] * T.Broadcast(kernel_1[cse_var_8], 2)\n                conv1d_ncw_local_1[cse_var_25:cse_var_25 + 2] = conv1d_ncw_local_1[cse_var_25:cse_var_25 + 2] + data_1[cse_var_5:cse_var_5 + 4:2] * T.Broadcast(kernel_1[cse_var_9], 2)\n                conv1d_ncw_local_1[cse_var_26:cse_var_26 + 2] = conv1d_ncw_local_1[cse_var_26:cse_var_26 + 2] + data_1[cse_var_5:cse_var_5 + 4:2] * T.Broadcast(kernel_1[cse_var_12], 2)\n                conv1d_ncw_local_1[cse_var_27:cse_var_27 + 2] = conv1d_ncw_local_1[cse_var_27:cse_var_27 + 2] + data_1[cse_var_5:cse_var_5 + 4:2] * T.Broadcast(kernel_1[cse_var_16], 2)\n                conv1d_ncw_local_1[cse_var_28:cse_var_28 + 2] = conv1d_ncw_local_1[cse_var_28:cse_var_28 + 2] + data_1[cse_var_5:cse_var_5 + 4:2] * T.Broadcast(kernel_1[cse_var_19], 2)\n                conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] = conv1d_ncw_local_1[cse_var_1:cse_var_1 + 2] + data_1[cse_var_3:cse_var_3 + 4:2] * T.Broadcast(kernel_1[cse_var_13], 2)\n                conv1d_ncw_local_1[cse_var_29:cse_var_29 + 2] = conv1d_ncw_local_1[cse_var_29:cse_var_29 + 2] + data_1[cse_var_3:cse_var_3 + 4:2] * T.Broadcast(kernel_1[cse_var_10], 2)\n                conv1d_ncw_local_1[cse_var_30:cse_var_30 + 2] = conv1d_ncw_local_1[cse_var_30:cse_var_30 + 2] + data_1[cse_var_3:cse_var_3 + 4:2] * T.Broadcast(kernel_1[cse_var_14], 2)\n                conv1d_ncw_local_1[cse_var_22:cse_var_22 + 2] = conv1d_ncw_local_1[cse_var_22:cse_var_22 + 2] + data_1[cse_var_3:cse_var_3 + 4:2] * T.Broadcast(kernel_1[cse_var_17], 2)\n                conv1d_ncw_local_1[cse_var_23:cse_var_23 + 2] = conv1d_ncw_local_1[cse_var_23:cse_var_23 + 2] + data_1[cse_var_3:cse_var_3 + 4:2] * T.Broadcast(kernel_1[cse_var_20], 2)\n                conv1d_ncw_local_1[cse_var_24:cse_var_24 + 2] = conv1d_ncw_local_1[cse_var_24:cse_var_24 + 2] + data_1[cse_var_6:cse_var_6 + 4:2] * T.Broadcast(kernel_1[cse_var_13], 2)\n                conv1d_ncw_local_1[cse_var_25:cse_var_25 + 2] = conv1d_ncw_local_1[cse_var_25:cse_var_25 + 2] + data_1[cse_var_6:cse_var_6 + 4:2] * T.Broadcast(kernel_1[cse_var_10], 2)\n                conv1d_ncw_local_1[cse_var_26:cse_var_26 + 2] = conv1d_ncw_local_1[cse_var_26:cse_var_26 + 2] + data_1[cse_var_6:cse_var_6 + 4:2] * T.Broadcast(kernel_1[cse_var_14], 2)\n                conv1d_ncw_local_1[cse_var_27:cse_var_27 + 2] = conv1d_ncw_local_1[cse_var_27:cse_var_27 + 2] + data_1[cse_var_6:cse_var_6 + 4:2] * T.Broadcast(kernel_1[cse_var_17], 2)\n                conv1d_ncw_local_1[cse_var_28:cse_var_28 + 2] = conv1d_ncw_local_1[cse_var_28:cse_var_28 + 2] + data_1[cse_var_6:cse_var_6 + 4:2] * T.Broadcast(kernel_1[cse_var_20], 2)\n        for nn_inner, ff_inner in T.grid(2, 5):\n            cse_var_32: T.int32 = nn_inner * 20 + ff_inner * 4\n            conv1d_ncw_1 = T.Buffer((40,), data=conv1d_ncw.data)\n            conv1d_ncw_1[cse_var_32:cse_var_32 + 4] = conv1d_ncw_local_1[cse_var_32:cse_var_32 + 4]", "op_args": [2, 3, 1, 71, 29, 10, 2, [1, 2], [1, 1]]}{"op_name": "abs", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[22];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 22; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2080; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 22; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 22) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 22; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1430; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 13, 11, 16), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([22], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((22,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(22):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2080, 22):\n            data_1 = T.Buffer((45760,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 22 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(22):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [20, 13, 11, 16]}{"op_name": "cos", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 28800; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = cosf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 6, 15, 20), \"float32\"), compute: T.Buffer((16, 6, 15, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(28800):\n            compute_1 = T.Buffer((28800,), data=compute.data)\n            data_1 = T.Buffer((28800,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [16, 6, 15, 20]}{"op_name": "atan", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1188; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 8) + i3_s);\n      ((float*)compute_1)[cse_var_1] = atanf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 6, 11, 8), \"float32\"), compute: T.Buffer((18, 6, 11, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            for i3_s in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3_s\n                compute_1 = T.Buffer((9504,), data=compute.data)\n                data_1 = T.Buffer((9504,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [18, 6, 11, 8]}{"op_name": "asin_cos", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 48) + (i2 * 4)) + i3);\n        ((float*)compute_1)[cse_var_1] = cosf(asinf(((float*)data_1)[cse_var_1]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = __cosf(asinf(data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 5, 12, 4), \"float32\"), compute: T.Buffer((7, 5, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(35):\n            for i2, i3 in T.grid(12, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 4 + i3\n                compute_1 = T.Buffer((1680,), data=compute.data)\n                data_1 = T.Buffer((1680,), data=data.data)\n                compute_1[cse_var_1] = T.cos(T.asin(data_1[cse_var_1]))", "op_args": [7, 5, 12, 4]}{"op_name": "sum", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[34];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 34; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 65; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 34);\n    data_red_rf[0] = (data_red_rf[0] + ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] + ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] + ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] + ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] + ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] + ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] + ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] + ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] + ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] + ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] + ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] + ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] + ((float*)data_1)[(cse_var_1 + 12)]);\n    data_red_rf[13] = (data_red_rf[13] + ((float*)data_1)[(cse_var_1 + 13)]);\n    data_red_rf[14] = (data_red_rf[14] + ((float*)data_1)[(cse_var_1 + 14)]);\n    data_red_rf[15] = (data_red_rf[15] + ((float*)data_1)[(cse_var_1 + 15)]);\n    data_red_rf[16] = (data_red_rf[16] + ((float*)data_1)[(cse_var_1 + 16)]);\n    data_red_rf[17] = (data_red_rf[17] + ((float*)data_1)[(cse_var_1 + 17)]);\n    data_red_rf[18] = (data_red_rf[18] + ((float*)data_1)[(cse_var_1 + 18)]);\n    data_red_rf[19] = (data_red_rf[19] + ((float*)data_1)[(cse_var_1 + 19)]);\n    data_red_rf[20] = (data_red_rf[20] + ((float*)data_1)[(cse_var_1 + 20)]);\n    data_red_rf[21] = (data_red_rf[21] + ((float*)data_1)[(cse_var_1 + 21)]);\n    data_red_rf[22] = (data_red_rf[22] + ((float*)data_1)[(cse_var_1 + 22)]);\n    data_red_rf[23] = (data_red_rf[23] + ((float*)data_1)[(cse_var_1 + 23)]);\n    data_red_rf[24] = (data_red_rf[24] + ((float*)data_1)[(cse_var_1 + 24)]);\n    data_red_rf[25] = (data_red_rf[25] + ((float*)data_1)[(cse_var_1 + 25)]);\n    data_red_rf[26] = (data_red_rf[26] + ((float*)data_1)[(cse_var_1 + 26)]);\n    data_red_rf[27] = (data_red_rf[27] + ((float*)data_1)[(cse_var_1 + 27)]);\n    data_red_rf[28] = (data_red_rf[28] + ((float*)data_1)[(cse_var_1 + 28)]);\n    data_red_rf[29] = (data_red_rf[29] + ((float*)data_1)[(cse_var_1 + 29)]);\n    data_red_rf[30] = (data_red_rf[30] + ((float*)data_1)[(cse_var_1 + 30)]);\n    data_red_rf[31] = (data_red_rf[31] + ((float*)data_1)[(cse_var_1 + 31)]);\n    data_red_rf[32] = (data_red_rf[32] + ((float*)data_1)[(cse_var_1 + 32)]);\n    data_red_rf[33] = (data_red_rf[33] + ((float*)data_1)[(cse_var_1 + 33)]);\n  }\n  ((float*)data_red_1)[0] = 0.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[12]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[13]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[14]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[15]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[16]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[17]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[18]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[19]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[20]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[21]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[22]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[23]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[24]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[25]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[26]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[27]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[28]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[29]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[30]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[31]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[32]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] + data_red_rf[33]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 70; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + (((int)threadIdx.x) >> 1)) < 1105) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 1, 17, 13), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([34], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((34,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(34):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(65):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 34\n            data_1 = T.Buffer((2210,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] + data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] + data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] + data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] + data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] + data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] + data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] + data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] + data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] + data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] + data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] + data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] + data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] + data_1[cse_var_1 + 12]\n            data_red_rf_1[13] = data_red_rf_1[13] + data_1[cse_var_1 + 13]\n            data_red_rf_1[14] = data_red_rf_1[14] + data_1[cse_var_1 + 14]\n            data_red_rf_1[15] = data_red_rf_1[15] + data_1[cse_var_1 + 15]\n            data_red_rf_1[16] = data_red_rf_1[16] + data_1[cse_var_1 + 16]\n            data_red_rf_1[17] = data_red_rf_1[17] + data_1[cse_var_1 + 17]\n            data_red_rf_1[18] = data_red_rf_1[18] + data_1[cse_var_1 + 18]\n            data_red_rf_1[19] = data_red_rf_1[19] + data_1[cse_var_1 + 19]\n            data_red_rf_1[20] = data_red_rf_1[20] + data_1[cse_var_1 + 20]\n            data_red_rf_1[21] = data_red_rf_1[21] + data_1[cse_var_1 + 21]\n            data_red_rf_1[22] = data_red_rf_1[22] + data_1[cse_var_1 + 22]\n            data_red_rf_1[23] = data_red_rf_1[23] + data_1[cse_var_1 + 23]\n            data_red_rf_1[24] = data_red_rf_1[24] + data_1[cse_var_1 + 24]\n            data_red_rf_1[25] = data_red_rf_1[25] + data_1[cse_var_1 + 25]\n            data_red_rf_1[26] = data_red_rf_1[26] + data_1[cse_var_1 + 26]\n            data_red_rf_1[27] = data_red_rf_1[27] + data_1[cse_var_1 + 27]\n            data_red_rf_1[28] = data_red_rf_1[28] + data_1[cse_var_1 + 28]\n            data_red_rf_1[29] = data_red_rf_1[29] + data_1[cse_var_1 + 29]\n            data_red_rf_1[30] = data_red_rf_1[30] + data_1[cse_var_1 + 30]\n            data_red_rf_1[31] = data_red_rf_1[31] + data_1[cse_var_1 + 31]\n            data_red_rf_1[32] = data_red_rf_1[32] + data_1[cse_var_1 + 32]\n            data_red_rf_1[33] = data_red_rf_1[33] + data_1[cse_var_1 + 33]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[12]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[13]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[14]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[15]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[16]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[17]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[18]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[19]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[20]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[21]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[22]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[23]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[24]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[25]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[26]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[27]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[28]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[29]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[30]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[31]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[32]\n        data_red_1[0] = data_red_1[0] + data_red_rf_1[33]", "op_args": [10, 1, 17, 13]}{"op_name": "cosh", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float coshf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        for (int32_t i3 = 0; i3 < 8; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 1920) + (i1 * 160)) + (i2 * 8)) + i3);\n          ((float*)compute_1)[cse_var_1] = coshf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 20, 8), \"float32\"), compute: T.Buffer((3, 12, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(12, 20, 8):\n                cse_var_1: T.int32 = i0 * 1920 + i1 * 160 + i2 * 8 + i3\n                compute_1 = T.Buffer((5760,), data=compute.data)\n                data_1 = T.Buffer((5760,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [3, 12, 20, 8]}{"op_name": "acos", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float acosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n          int32_t cse_var_1 = ((((i0 * 270) + (i1 * 45)) + (i2 * 3)) + i3_s);\n          ((float*)compute_1)[cse_var_1] = acosf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 6, 15, 3), \"float32\"), compute: T.Buffer((15, 6, 15, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(15):\n            for i1, i2, i3_s in T.grid(6, 15, 3):\n                cse_var_1: T.int32 = i0 * 270 + i1 * 45 + i2 * 3 + i3_s\n                compute_1 = T.Buffer((4050,), data=compute.data)\n                data_1 = T.Buffer((4050,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [15, 6, 15, 3]}{"op_name": "asin", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 5; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 560) + (i1 * 80)) + (i2 * 5)) + i3);\n          ((float*)compute_1)[cse_var_1] = asinf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 7, 16, 5), \"float32\"), compute: T.Buffer((7, 7, 16, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3 in T.grid(7, 16, 5):\n                cse_var_1: T.int32 = i0 * 560 + i1 * 80 + i2 * 5 + i3\n                compute_1 = T.Buffer((3920,), data=compute.data)\n                data_1 = T.Buffer((3920,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [7, 7, 16, 5]}{"op_name": "asinh", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float asinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n        int32_t cse_var_1 = (((i0_i1_fused * 48) + (i2 * 4)) + i3_s);\n        ((float*)compute_1)[cse_var_1] = asinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 12, 4), \"float32\"), compute: T.Buffer((5, 16, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(80):\n            for i2, i3_s in T.grid(12, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 4 + i3_s\n                compute_1 = T.Buffer((3840,), data=compute.data)\n                data_1 = T.Buffer((3840,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [5, 16, 12, 4]}{"op_name": "atanh", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float atanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1071; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 19) + i3);\n      ((float*)compute_1)[cse_var_1] = atanhf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 9, 7, 19), \"float32\"), compute: T.Buffer((17, 9, 7, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1071):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((20349,), data=compute.data)\n                data_1 = T.Buffer((20349,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [17, 9, 7, 19]}{"op_name": "ceil", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float ceilf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 10) + i3);\n      ((float*)compute_1)[cse_var_1] = ceilf(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 10, 2, 10), \"float32\"), compute: T.Buffer((3, 10, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3\n                compute_1 = T.Buffer((600,), data=compute.data)\n                data_1 = T.Buffer((600,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [3, 10, 2, 10]}{"op_name": "erf", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float erff(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i3 = 0; i3 < 14; ++i3) {\n        int32_t cse_var_1 = (((i0 * 196) + (i1 * 14)) + i3);\n        ((float*)compute_1)[cse_var_1] = erff(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 14, 1, 14), \"float32\"), compute: T.Buffer((17, 14, 1, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            for i1, i3 in T.grid(14, 14):\n                cse_var_1: T.int32 = i0 * 196 + i1 * 14 + i3\n                compute_1 = T.Buffer((3332,), data=compute.data)\n                data_1 = T.Buffer((3332,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [17, 14, 1, 14]}{"op_name": "exp", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 16; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 2448) + (i1 * 144)) + (i2 * 16)) + i3);\n          ((float*)compute_1)[cse_var_1] = expf(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 17, 9, 16), \"float32\"), compute: T.Buffer((3, 17, 9, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(17, 9, 16):\n                cse_var_1: T.int32 = i0 * 2448 + i1 * 144 + i2 * 16 + i3\n                compute_1 = T.Buffer((7344,), data=compute.data)\n                data_1 = T.Buffer((7344,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])", "op_args": [3, 17, 9, 16]}{"op_name": "fast_erf", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_erf_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_erf = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_erf_1 = (((DLTensor*)T_fast_erf)[0].data);\n  void* default_function_T_fast_erf_shape = (((DLTensor*)T_fast_erf)[0].shape);\n  void* default_function_T_fast_erf_strides = (((DLTensor*)T_fast_erf)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_erf_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1008; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n      int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused * 17) + ax3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      float v__1 = (v_) < (4.000000e+00f) ? (v_) : (4.000000e+00f);\n      ((float*)T_fast_erf_1)[cse_var_1] = ((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * (((((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f)) * ((v__1) > (-4.000000e+00f) ? (v__1) : (-4.000000e+00f))) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 4, 14, 17), \"float32\"), T_fast_erf: T.Buffer((18, 4, 14, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1008):\n            for ax3 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 17 + ax3\n                T_fast_erf_1 = T.Buffer((17136,), data=T_fast_erf.data)\n                data_1 = T.Buffer((17136,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [18, 4, 14, 17]}{"op_name": "fast_exp", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_exp_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_exp = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_exp_1 = (((DLTensor*)T_fast_exp)[0].data);\n  void* default_function_T_fast_exp_shape = (((DLTensor*)T_fast_exp)[0].shape);\n  void* default_function_T_fast_exp_strides = (((DLTensor*)T_fast_exp)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_exp_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 75; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      for (int32_t ax3_s = 0; ax3_s < 14; ++ax3_s) {\n        int32_t cse_var_1 = (((ax0_ax1_fused * 224) + (ax2 * 14)) + ax3_s);\n          float v_ = ((float*)data_1)[cse_var_1];\n          float v__1 = (v_) < (8.837627e+01f) ? (v_) : (8.837627e+01f);\n          int32_t v__2 = ((int32_t)(floorf(((((v__1) > (-8.837626e+01f) ? (v__1) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        float v__3 = ((float*)data_1)[cse_var_1];\n        float v__4 = (v__3) < (8.837627e+01f) ? (v__3) : (8.837627e+01f);\n        float v__5 = ((float*)data_1)[cse_var_1];\n        float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n        float v__7 = ((float*)data_1)[cse_var_1];\n        float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n        float v__9 = ((float*)data_1)[cse_var_1];\n        float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n        float v__11 = ((float*)data_1)[cse_var_1];\n        float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n        float v__13 = ((float*)data_1)[cse_var_1];\n        float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n        float v__15 = ((float*)data_1)[cse_var_1];\n        float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n        float v__17 = ((float*)data_1)[cse_var_1];\n        float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n        float v__19 = ((float*)data_1)[cse_var_1];\n        float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n        float v__21 = ((float*)data_1)[cse_var_1];\n        float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n        float v__23 = ((float*)data_1)[cse_var_1];\n        float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n        float v__25 = ((float*)data_1)[cse_var_1];\n        float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n        float v__27 = ((float*)data_1)[cse_var_1];\n        float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n        float v__29 = ((float*)data_1)[cse_var_1];\n        float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n        float v__31 = ((float*)data_1)[cse_var_1];\n        float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n        float v__33 = ((float*)data_1)[cse_var_1];\n        float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n        float v__35 = (*(float *)(&(v__2))) * ((((((((((((((1.987569e-04f * (((v__4) > (-8.837626e+01f) ? (v__4) : (-8.837626e+01f)) - (floorf(((((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) - (floorf(((((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) - (floorf(((((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) - (floorf(((((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) - (floorf(((((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) - (floorf(((((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) - (floorf(((((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) - (floorf(((((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n        float v__36 = ((float*)data_1)[cse_var_1];\n        ((float*)T_fast_exp_1)[cse_var_1] = ((v__35) > (v__36) ? (v__35) : (v__36));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 15, 16, 14), \"float32\"), T_fast_exp: T.Buffer((5, 15, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(75):\n            for ax2, ax3_s in T.grid(16, 14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 224 + ax2 * 14 + ax3_s\n                T_fast_exp_1 = T.Buffer((16800,), data=T_fast_exp.data)\n                data_1 = T.Buffer((16800,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [5, 15, 16, 14]}{"op_name": "fast_tanh", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_fast_tanh_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_fast_tanh = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_fast_tanh_1 = (((DLTensor*)T_fast_tanh)[0].data);\n  void* default_function_T_fast_tanh_shape = (((DLTensor*)T_fast_tanh)[0].shape);\n  void* default_function_T_fast_tanh_strides = (((DLTensor*)T_fast_tanh)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_fast_tanh_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1995; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float v_ = ((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n    float v__1 = (9.000000e+00f) < (v_) ? (9.000000e+00f) : (v_);\n    ((float*)T_fast_tanh_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * (((((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1)) * ((-9.000000e+00f) > (v__1) ? (-9.000000e+00f) : (v__1))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 3, 7, 5), \"float32\"), T_fast_tanh: T.Buffer((19, 3, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1995):\n            T_fast_tanh_1 = T.Buffer((1995,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((1995,), data=data.data)\n            T_fast_tanh_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [19, 3, 7, 5]}{"op_name": "flip", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_reverse_sequence_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_reverse_sequence = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_reverse_sequence_1 = (((DLTensor*)T_reverse_sequence)[0].data);\n  void* default_function_T_reverse_sequence_shape = (((DLTensor*)T_reverse_sequence)[0].shape);\n  void* default_function_T_reverse_sequence_strides = (((DLTensor*)T_reverse_sequence)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_reverse_sequence_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 952; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n      ((float*)T_reverse_sequence_1)[((ax0_ax1_fused_ax2_fused * 17) + ax3)] = ((float*)data_1)[(((((ax0_ax1_fused_ax2_fused % 56) * 17) + ax3) + 15232) - ((ax0_ax1_fused_ax2_fused / 56) * 952))];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 1)) < 8092) {\n    T_reverse_sequence[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 952) + 15232) - ((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 1)) / 476) * 952))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 8, 7, 17), \"float32\"), T_reverse_sequence: T.Buffer((17, 8, 7, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(952):\n            for ax3 in range(17):\n                T_reverse_sequence_1 = T.Buffer((16184,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((16184,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 17 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 56 * 17 + ax3 + 15232 - ax0_ax1_fused_ax2_fused // 56 * 952]", "op_args": [17, 8, 7, 17]}{"op_name": "floor", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 20; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      for (int32_t i3 = 0; i3 < 7; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 35) + (i2 * 7)) + i3);\n        ((float*)compute_1)[cse_var_1] = floorf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 175) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 5, 5, 7), \"float32\"), compute: T.Buffer((4, 5, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(20):\n            for i2, i3 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0_i1_fused * 35 + i2 * 7 + i3\n                compute_1 = T.Buffer((700,), data=compute.data)\n                data_1 = T.Buffer((700,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])", "op_args": [4, 5, 5, 7]}{"op_name": "isnan", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 900; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 19) + i3);\n      ((int8_t*)compute_1)[cse_var_1] = ((int8_t)(((float*)data_1)[cse_var_1] != ((float*)data_1)[cse_var_1]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 15, 10, 19), \"float32\"), compute: T.Buffer((6, 15, 10, 19), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((17100,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((17100,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [6, 15, 10, 19]}{"op_name": "adaptive_pool_max", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1280; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < ((((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 3) + 3) % 8) == 0) ? (((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 3) + 3) >> 3) : ((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 3) + 3) >> 3) + 1)) - ((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 3) >> 3)); ++rv0) {\n      for (int32_t rv1 = 0; rv1 < (((((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 2) + 2) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 9) + 9) >> 2) : (((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 9) + 9) >> 2) + 1)) - (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 18) >> 3)); ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n        float v__1 = ((float*)data_1)[((((((ax0_ax1_fused_ax2_fused_ax3_fused >> 6) * 54) + (((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 3) >> 3) * 18)) + (rv0 * 18)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 18) >> 3)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 3) + 3) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 3) : (((((((int)threadIdx.x) >> 3) * 3) + 3) >> 3) + 1)) - (((((int)threadIdx.x) >> 3) * 3) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 2) + 2) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 2) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 2) + 1)) - (((((int)threadIdx.x) & 7) * 18) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 54) + ((((((int)threadIdx.x) >> 3) * 3) >> 3) * 18)) + (rv0 * 18)) + (((((int)threadIdx.x) & 7) * 18) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 5, 3, 18), \"float32\"), adaptive_pool_max: T.Buffer((4, 5, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1280):\n            adaptive_pool_max_1 = T.Buffer((1280,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(T.Let(T.Let(T.Let(T.Select(cse_var_3 % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 8, where={cse_var_1: cse_var_3 // 8}), where={cse_var_3: cse_var_2 + 3}), where={cse_var_2: ax0_ax1_fused_ax2_fused_ax3_fused % 64 // 8 * 3}), T.Let(T.Let(T.Select((cse_var_5 * 2 + 2) % 8 == 0, cse_var_4, cse_var_4 + 1) - cse_var_5 * 18 // 8, where={cse_var_4: (cse_var_5 * 9 + 9) // 4}), where={cse_var_5: ax0_ax1_fused_ax2_fused_ax3_fused % 8})):\n                cse_var_2 = T.int32()\n                cse_var_3 = T.int32()\n                cse_var_1 = T.int32()\n                cse_var_5 = T.int32()\n                cse_var_4 = T.int32()\n                data_1 = T.Buffer((1080,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 64 * 54 + ax0_ax1_fused_ax2_fused_ax3_fused % 64 // 8 * 3 // 8 * 18 + rv0 * 18 + ax0_ax1_fused_ax2_fused_ax3_fused % 8 * 18 // 8 + rv1])", "op_args": [4, 5, 3, 18]}{"op_name": "log", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 17; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 3; ++i3) {\n        int32_t cse_var_1 = (((i1 * 12) + (i2 * 3)) + i3);\n        ((float*)compute_1)[cse_var_1] = logf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 51) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 17, 4, 3), \"float32\"), compute: T.Buffer((1, 17, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(17, 4, 3):\n            cse_var_1: T.int32 = i1 * 12 + i2 * 3 + i3\n            compute_1 = T.Buffer((204,), data=compute.data)\n            data_1 = T.Buffer((204,), data=data.data)\n            compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [1, 17, 4, 3]}{"op_name": "add", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_data_strides_1 == NULL)) {\n  }\n  if (!(default_function_T_add_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 3240; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_add_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = (((float*)data_2)[ax0_ax1_fused_ax2_fused_ax3_fused] + ((float*)data_3)[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 3, 15), \"float32\"), data_1: T.Buffer((6, 12, 3, 15), \"float32\"), T_add: T.Buffer((6, 12, 3, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(3240):\n            T_add_1 = T.Buffer((3240,), data=T_add.data)\n            data_2 = T.Buffer((3240,), data=data.data)\n            data_3 = T.Buffer((3240,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_2[ax0_ax1_fused_ax2_fused_ax3_fused] + data_3[ax0_ax1_fused_ax2_fused_ax3_fused]", "op_args": [6, 12, 3, 15]}{"op_name": "log10", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log10f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 20) + i3);\n      ((float*)compute_1)[cse_var_1] = log10f(((float*)data_1)[cse_var_1]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12, 20, 20), \"float32\"), compute: T.Buffer((4, 12, 20, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((19200,), data=compute.data)\n                data_1 = T.Buffer((19200,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [4, 12, 20, 20]}{"op_name": "log2", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float log2f(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          int32_t cse_var_1 = ((((i0 * 2475) + (i1 * 165)) + (i2 * 15)) + i3);\n          ((float*)compute_1)[cse_var_1] = log2f(((float*)data_1)[cse_var_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 15, 11, 15), \"float32\"), compute: T.Buffer((5, 15, 11, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(5):\n            for i1, i2, i3 in T.grid(15, 11, 15):\n                cse_var_1: T.int32 = i0 * 2475 + i1 * 165 + i2 * 15 + i3\n                compute_1 = T.Buffer((12375,), data=compute.data)\n                data_1 = T.Buffer((12375,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [5, 15, 11, 15]}{"op_name": "adaptive_pool_avg", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  }\n  void* adaptive_pool_sum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)19456, 2, 32);\n  if (adaptive_pool_sum == NULL) {\n    return -1;\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 76; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        ((float*)adaptive_pool_sum)[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = 0.000000e+00f;\n        for (int32_t rv0 = 0; rv0 < ((((ax2 + 1) % 8) == 0) ? ((ax2 + 1) >> 3) : (((ax2 + 1) >> 3) + 1)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 5) + 5) % 8) == 0) ? (((ax3 * 13) + 13) >> 3) : ((((ax3 * 13) + 13) >> 3) + 1)) - ((ax3 * 13) >> 3)); ++rv1) {\n            int32_t cse_var_3 = (((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3);\n            ((float*)adaptive_pool_sum)[cse_var_3] = (((float*)adaptive_pool_sum)[cse_var_3] + ((float*)data_1)[((((ax0_ax1_fused * 13) + (rv0 * 13)) + ((ax3 * 13) >> 3)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 4864; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    int32_t cse_var_7 = (ax0_ax1_fused_ax2_fused_ax3_fused & 63);\n    int32_t cse_var_6 = (ax0_ax1_fused_ax2_fused_ax3_fused & 7);\n    int32_t cse_var_8 = (cse_var_6 * 13);\n    int32_t cse_var_5 = ((cse_var_7 + 8) >> 6);\n    int32_t cse_var_4 = ((cse_var_8 + 13) >> 3);\n    ((float*)adaptive_pool_avg_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = (((float*)adaptive_pool_sum)[ax0_ax1_fused_ax2_fused_ax3_fused] / (((float)(((((cse_var_7 >> 3) + 1) % 8) == 0) ? cse_var_5 : (cse_var_5 + 1))) * ((float)((((((cse_var_6 * 5) + 5) % 8) == 0) ? cse_var_4 : (cse_var_4 + 1)) - (cse_var_8 >> 3)))));\n  }\n  if (TVMBackendFreeWorkspace(1, dev_id, adaptive_pool_sum) != 0) {\n    return -1;\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / (((float)(((((((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) >> 3)) + 1) % 8) == 0) ? ((((((int)threadIdx.x) + 8) >> 5) + (((int)blockIdx.x) & 1)) >> 1) : (((((((int)threadIdx.x) + 8) >> 5) + (((int)blockIdx.x) & 1)) >> 1) + 1))) * ((float)(((((((((int)threadIdx.x) & 7) * 5) + 5) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 13) + 13) >> 3) : (((((((int)threadIdx.x) & 7) * 13) + 13) >> 3) + 1)) - (((((int)threadIdx.x) & 7) * 13) >> 3)))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < (((((((int)blockIdx.x) & 7) + 1) % 8) == 0) ? (((((int)blockIdx.x) & 7) + 1) >> 3) : ((((((int)blockIdx.x) & 7) + 1) >> 3) + 1)); ++rv0) {\n    for (int rv1 = 0; rv1 < ((((((((int)threadIdx.x) * 5) + 5) % 8) == 0) ? (((((int)threadIdx.x) * 13) + 13) >> 3) : ((((((int)threadIdx.x) * 13) + 13) >> 3) + 1)) - ((((int)threadIdx.x) * 13) >> 3)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) >> 3) * 13) + (rv0 * 13)) + ((((int)threadIdx.x) * 13) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 19, 1, 13), \"float32\"), adaptive_pool_avg: T.Buffer((4, 19, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        adaptive_pool_sum = T.allocate([4864], \"float32\", \"global\")\n        adaptive_pool_sum_1 = T.Buffer((4864,), data=adaptive_pool_sum)\n        for ax0_ax1_fused in T.parallel(76):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_sum_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(0)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 + 1) % 8 == 0, cse_var_1, cse_var_1 + 1), where={cse_var_1: (ax2 + 1) // 8}), T.Let(T.Select((ax3 * 5 + 5) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 13 // 8, where={cse_var_2: (ax3 * 13 + 13) // 8})):\n                    cse_var_1 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((988,), data=data.data)\n                    adaptive_pool_sum_1[cse_var_3] = adaptive_pool_sum_1[cse_var_3] + data_1[ax0_ax1_fused * 13 + rv0 * 13 + ax3 * 13 // 8 + rv1]\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(4864):\n            cse_var_7: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 64\n            cse_var_6: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 8\n            cse_var_8: T.int32 = cse_var_6 * 13\n            cse_var_5: T.int32 = (cse_var_7 + 8) // 64\n            cse_var_4: T.int32 = (cse_var_8 + 13) // 8\n            adaptive_pool_avg_1 = T.Buffer((4864,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused_ax3_fused] = adaptive_pool_sum_1[ax0_ax1_fused_ax2_fused_ax3_fused] / (T.Cast(\"float32\", T.Select((cse_var_7 // 8 + 1) % 8 == 0, cse_var_5, cse_var_5 + 1)) * T.Cast(\"float32\", T.Select((cse_var_6 * 5 + 5) % 8 == 0, cse_var_4, cse_var_4 + 1) - cse_var_8 // 8))", "op_args": [4, 19, 1, 13]}{"op_name": "max", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[60];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 60; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 60; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 60) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) > (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = -3.402823e+38f;\n  float v__2 = ((float*)data_red_1)[0];\n  float v__3 = data_red_rf[0];\n  ((float*)data_red_1)[0] = ((v__2) > (v__3) ? (v__2) : (v__3));\n  float v__4 = ((float*)data_red_1)[0];\n  float v__5 = data_red_rf[1];\n  ((float*)data_red_1)[0] = ((v__4) > (v__5) ? (v__4) : (v__5));\n  float v__6 = ((float*)data_red_1)[0];\n  float v__7 = data_red_rf[2];\n  ((float*)data_red_1)[0] = ((v__6) > (v__7) ? (v__6) : (v__7));\n  float v__8 = ((float*)data_red_1)[0];\n  float v__9 = data_red_rf[3];\n  ((float*)data_red_1)[0] = ((v__8) > (v__9) ? (v__8) : (v__9));\n  float v__10 = ((float*)data_red_1)[0];\n  float v__11 = data_red_rf[4];\n  ((float*)data_red_1)[0] = ((v__10) > (v__11) ? (v__10) : (v__11));\n  float v__12 = ((float*)data_red_1)[0];\n  float v__13 = data_red_rf[5];\n  ((float*)data_red_1)[0] = ((v__12) > (v__13) ? (v__12) : (v__13));\n  float v__14 = ((float*)data_red_1)[0];\n  float v__15 = data_red_rf[6];\n  ((float*)data_red_1)[0] = ((v__14) > (v__15) ? (v__14) : (v__15));\n  float v__16 = ((float*)data_red_1)[0];\n  float v__17 = data_red_rf[7];\n  ((float*)data_red_1)[0] = ((v__16) > (v__17) ? (v__16) : (v__17));\n  float v__18 = ((float*)data_red_1)[0];\n  float v__19 = data_red_rf[8];\n  ((float*)data_red_1)[0] = ((v__18) > (v__19) ? (v__18) : (v__19));\n  float v__20 = ((float*)data_red_1)[0];\n  float v__21 = data_red_rf[9];\n  ((float*)data_red_1)[0] = ((v__20) > (v__21) ? (v__20) : (v__21));\n  float v__22 = ((float*)data_red_1)[0];\n  float v__23 = data_red_rf[10];\n  ((float*)data_red_1)[0] = ((v__22) > (v__23) ? (v__22) : (v__23));\n  float v__24 = ((float*)data_red_1)[0];\n  float v__25 = data_red_rf[11];\n  ((float*)data_red_1)[0] = ((v__24) > (v__25) ? (v__24) : (v__25));\n  float v__26 = ((float*)data_red_1)[0];\n  float v__27 = data_red_rf[12];\n  ((float*)data_red_1)[0] = ((v__26) > (v__27) ? (v__26) : (v__27));\n  float v__28 = ((float*)data_red_1)[0];\n  float v__29 = data_red_rf[13];\n  ((float*)data_red_1)[0] = ((v__28) > (v__29) ? (v__28) : (v__29));\n  float v__30 = ((float*)data_red_1)[0];\n  float v__31 = data_red_rf[14];\n  ((float*)data_red_1)[0] = ((v__30) > (v__31) ? (v__30) : (v__31));\n  float v__32 = ((float*)data_red_1)[0];\n  float v__33 = data_red_rf[15];\n  ((float*)data_red_1)[0] = ((v__32) > (v__33) ? (v__32) : (v__33));\n  float v__34 = ((float*)data_red_1)[0];\n  float v__35 = data_red_rf[16];\n  ((float*)data_red_1)[0] = ((v__34) > (v__35) ? (v__34) : (v__35));\n  float v__36 = ((float*)data_red_1)[0];\n  float v__37 = data_red_rf[17];\n  ((float*)data_red_1)[0] = ((v__36) > (v__37) ? (v__36) : (v__37));\n  float v__38 = ((float*)data_red_1)[0];\n  float v__39 = data_red_rf[18];\n  ((float*)data_red_1)[0] = ((v__38) > (v__39) ? (v__38) : (v__39));\n  float v__40 = ((float*)data_red_1)[0];\n  float v__41 = data_red_rf[19];\n  ((float*)data_red_1)[0] = ((v__40) > (v__41) ? (v__40) : (v__41));\n  float v__42 = ((float*)data_red_1)[0];\n  float v__43 = data_red_rf[20];\n  ((float*)data_red_1)[0] = ((v__42) > (v__43) ? (v__42) : (v__43));\n  float v__44 = ((float*)data_red_1)[0];\n  float v__45 = data_red_rf[21];\n  ((float*)data_red_1)[0] = ((v__44) > (v__45) ? (v__44) : (v__45));\n  float v__46 = ((float*)data_red_1)[0];\n  float v__47 = data_red_rf[22];\n  ((float*)data_red_1)[0] = ((v__46) > (v__47) ? (v__46) : (v__47));\n  float v__48 = ((float*)data_red_1)[0];\n  float v__49 = data_red_rf[23];\n  ((float*)data_red_1)[0] = ((v__48) > (v__49) ? (v__48) : (v__49));\n  float v__50 = ((float*)data_red_1)[0];\n  float v__51 = data_red_rf[24];\n  ((float*)data_red_1)[0] = ((v__50) > (v__51) ? (v__50) : (v__51));\n  float v__52 = ((float*)data_red_1)[0];\n  float v__53 = data_red_rf[25];\n  ((float*)data_red_1)[0] = ((v__52) > (v__53) ? (v__52) : (v__53));\n  float v__54 = ((float*)data_red_1)[0];\n  float v__55 = data_red_rf[26];\n  ((float*)data_red_1)[0] = ((v__54) > (v__55) ? (v__54) : (v__55));\n  float v__56 = ((float*)data_red_1)[0];\n  float v__57 = data_red_rf[27];\n  ((float*)data_red_1)[0] = ((v__56) > (v__57) ? (v__56) : (v__57));\n  float v__58 = ((float*)data_red_1)[0];\n  float v__59 = data_red_rf[28];\n  ((float*)data_red_1)[0] = ((v__58) > (v__59) ? (v__58) : (v__59));\n  float v__60 = ((float*)data_red_1)[0];\n  float v__61 = data_red_rf[29];\n  ((float*)data_red_1)[0] = ((v__60) > (v__61) ? (v__60) : (v__61));\n  float v__62 = ((float*)data_red_1)[0];\n  float v__63 = data_red_rf[30];\n  ((float*)data_red_1)[0] = ((v__62) > (v__63) ? (v__62) : (v__63));\n  float v__64 = ((float*)data_red_1)[0];\n  float v__65 = data_red_rf[31];\n  ((float*)data_red_1)[0] = ((v__64) > (v__65) ? (v__64) : (v__65));\n  float v__66 = ((float*)data_red_1)[0];\n  float v__67 = data_red_rf[32];\n  ((float*)data_red_1)[0] = ((v__66) > (v__67) ? (v__66) : (v__67));\n  float v__68 = ((float*)data_red_1)[0];\n  float v__69 = data_red_rf[33];\n  ((float*)data_red_1)[0] = ((v__68) > (v__69) ? (v__68) : (v__69));\n  float v__70 = ((float*)data_red_1)[0];\n  float v__71 = data_red_rf[34];\n  ((float*)data_red_1)[0] = ((v__70) > (v__71) ? (v__70) : (v__71));\n  float v__72 = ((float*)data_red_1)[0];\n  float v__73 = data_red_rf[35];\n  ((float*)data_red_1)[0] = ((v__72) > (v__73) ? (v__72) : (v__73));\n  float v__74 = ((float*)data_red_1)[0];\n  float v__75 = data_red_rf[36];\n  ((float*)data_red_1)[0] = ((v__74) > (v__75) ? (v__74) : (v__75));\n  float v__76 = ((float*)data_red_1)[0];\n  float v__77 = data_red_rf[37];\n  ((float*)data_red_1)[0] = ((v__76) > (v__77) ? (v__76) : (v__77));\n  float v__78 = ((float*)data_red_1)[0];\n  float v__79 = data_red_rf[38];\n  ((float*)data_red_1)[0] = ((v__78) > (v__79) ? (v__78) : (v__79));\n  float v__80 = ((float*)data_red_1)[0];\n  float v__81 = data_red_rf[39];\n  ((float*)data_red_1)[0] = ((v__80) > (v__81) ? (v__80) : (v__81));\n  float v__82 = ((float*)data_red_1)[0];\n  float v__83 = data_red_rf[40];\n  ((float*)data_red_1)[0] = ((v__82) > (v__83) ? (v__82) : (v__83));\n  float v__84 = ((float*)data_red_1)[0];\n  float v__85 = data_red_rf[41];\n  ((float*)data_red_1)[0] = ((v__84) > (v__85) ? (v__84) : (v__85));\n  float v__86 = ((float*)data_red_1)[0];\n  float v__87 = data_red_rf[42];\n  ((float*)data_red_1)[0] = ((v__86) > (v__87) ? (v__86) : (v__87));\n  float v__88 = ((float*)data_red_1)[0];\n  float v__89 = data_red_rf[43];\n  ((float*)data_red_1)[0] = ((v__88) > (v__89) ? (v__88) : (v__89));\n  float v__90 = ((float*)data_red_1)[0];\n  float v__91 = data_red_rf[44];\n  ((float*)data_red_1)[0] = ((v__90) > (v__91) ? (v__90) : (v__91));\n  float v__92 = ((float*)data_red_1)[0];\n  float v__93 = data_red_rf[45];\n  ((float*)data_red_1)[0] = ((v__92) > (v__93) ? (v__92) : (v__93));\n  float v__94 = ((float*)data_red_1)[0];\n  float v__95 = data_red_rf[46];\n  ((float*)data_red_1)[0] = ((v__94) > (v__95) ? (v__94) : (v__95));\n  float v__96 = ((float*)data_red_1)[0];\n  float v__97 = data_red_rf[47];\n  ((float*)data_red_1)[0] = ((v__96) > (v__97) ? (v__96) : (v__97));\n  float v__98 = ((float*)data_red_1)[0];\n  float v__99 = data_red_rf[48];\n  ((float*)data_red_1)[0] = ((v__98) > (v__99) ? (v__98) : (v__99));\n  float v__100 = ((float*)data_red_1)[0];\n  float v__101 = data_red_rf[49];\n  ((float*)data_red_1)[0] = ((v__100) > (v__101) ? (v__100) : (v__101));\n  float v__102 = ((float*)data_red_1)[0];\n  float v__103 = data_red_rf[50];\n  ((float*)data_red_1)[0] = ((v__102) > (v__103) ? (v__102) : (v__103));\n  float v__104 = ((float*)data_red_1)[0];\n  float v__105 = data_red_rf[51];\n  ((float*)data_red_1)[0] = ((v__104) > (v__105) ? (v__104) : (v__105));\n  float v__106 = ((float*)data_red_1)[0];\n  float v__107 = data_red_rf[52];\n  ((float*)data_red_1)[0] = ((v__106) > (v__107) ? (v__106) : (v__107));\n  float v__108 = ((float*)data_red_1)[0];\n  float v__109 = data_red_rf[53];\n  ((float*)data_red_1)[0] = ((v__108) > (v__109) ? (v__108) : (v__109));\n  float v__110 = ((float*)data_red_1)[0];\n  float v__111 = data_red_rf[54];\n  ((float*)data_red_1)[0] = ((v__110) > (v__111) ? (v__110) : (v__111));\n  float v__112 = ((float*)data_red_1)[0];\n  float v__113 = data_red_rf[55];\n  ((float*)data_red_1)[0] = ((v__112) > (v__113) ? (v__112) : (v__113));\n  float v__114 = ((float*)data_red_1)[0];\n  float v__115 = data_red_rf[56];\n  ((float*)data_red_1)[0] = ((v__114) > (v__115) ? (v__114) : (v__115));\n  float v__116 = ((float*)data_red_1)[0];\n  float v__117 = data_red_rf[57];\n  ((float*)data_red_1)[0] = ((v__116) > (v__117) ? (v__116) : (v__117));\n  float v__118 = ((float*)data_red_1)[0];\n  float v__119 = data_red_rf[58];\n  ((float*)data_red_1)[0] = ((v__118) > (v__119) ? (v__118) : (v__119));\n  float v__120 = ((float*)data_red_1)[0];\n  float v__121 = data_red_rf[59];\n  ((float*)data_red_1)[0] = ((v__120) > (v__121) ? (v__120) : (v__121));\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 113; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 225) {\n      normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 5, 18), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([60], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((60,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(60):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(60, 60):\n            data_1 = T.Buffer((3600,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 60 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[0])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[1])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[2])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[3])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[4])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[5])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[6])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[7])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[8])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[9])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[10])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[11])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[12])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[13])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[14])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[15])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[16])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[17])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[18])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[19])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[20])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[21])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[22])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[23])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[24])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[25])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[26])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[27])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[28])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[29])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[30])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[31])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[32])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[33])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[34])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[35])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[36])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[37])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[38])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[39])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[40])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[41])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[42])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[43])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[44])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[45])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[46])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[47])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[48])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[49])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[50])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[51])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[52])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[53])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[54])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[55])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[56])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[57])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[58])\n        data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[59])", "op_args": [2, 20, 5, 18]}{"op_name": "min", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[33];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 33; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 196; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 33; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      float v_ = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner];\n      float v__1 = ((float*)data_1)[((k0_k1_fused_k2_fused_k3_fused_outer * 33) + k0_k1_fused_k2_fused_k3_fused_inner)];\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = ((v_) < (v__1) ? (v_) : (v__1));\n    }\n  }\n  ((float*)data_red_1)[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 33; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    float v__2 = ((float*)data_red_1)[0];\n    float v__3 = data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v];\n    ((float*)data_red_1)[0] = ((v__2) < (v__3) ? (v__2) : (v__3));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 203; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 8) + (((int)threadIdx.x) >> 2)) < 1617) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 7, 12, 11), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([33], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((33,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(33):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(196, 33):\n            data_1 = T.Buffer((6468,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 33 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(33):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [7, 7, 12, 11]}{"op_name": "fast_softmax", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    float T_softmax_maxelem[84];\n    float T_softmax_expsum[6];\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        T_softmax_maxelem[((i1 * 6) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 2; ++k) {\n          int32_t cse_var_1 = ((i1 * 6) + i2);\n          float v_ = T_softmax_maxelem[cse_var_1];\n          float v__1 = ((float*)data_1)[((((i0 * 168) + (i1 * 12)) + (i2 * 2)) + k)];\n          T_softmax_maxelem[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 14; ++i1_1) {\n      int32_t cse_var_19 = (i1_1 * 6);\n      int32_t cse_var_18 = (cse_var_19 + 5);\n      int32_t cse_var_17 = (cse_var_19 + 4);\n      int32_t cse_var_16 = (cse_var_19 + 3);\n      int32_t cse_var_15 = (cse_var_19 + 2);\n      int32_t cse_var_14 = (cse_var_19 + 1);\n      int32_t cse_var_13 = ((i0 * 168) + (i1_1 * 12));\n      int32_t cse_var_12 = (cse_var_13 + 9);\n      int32_t cse_var_11 = (cse_var_13 + 8);\n      int32_t cse_var_10 = (cse_var_13 + 7);\n      int32_t cse_var_9 = (cse_var_13 + 6);\n      int32_t cse_var_8 = (cse_var_13 + 5);\n      int32_t cse_var_7 = (cse_var_13 + 4);\n      int32_t cse_var_6 = (cse_var_13 + 3);\n      int32_t cse_var_5 = (cse_var_13 + 2);\n      int32_t cse_var_4 = (cse_var_13 + 11);\n      int32_t cse_var_3 = (cse_var_13 + 10);\n      int32_t cse_var_2 = (cse_var_13 + 1);\n      T_softmax_expsum[0] = 0.000000e+00f;\n        float v__2 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n        float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n        int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__5 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n      float v__7 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n      float v__9 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n      float v__11 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n      float v__13 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n      float v__15 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n      float v__17 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n      float v__19 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n      float v__21 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n      float v__23 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n      float v__25 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n      float v__27 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n      float v__29 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n      float v__31 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n      float v__33 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n      float v__35 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n      float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__38 = ((float*)data_1)[cse_var_13] - T_softmax_maxelem[cse_var_19];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n        float v__39 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n        float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n        int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__42 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n      float v__44 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n      float v__46 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n      float v__48 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n      float v__50 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n      float v__52 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n      float v__54 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n      float v__56 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n      float v__58 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n      float v__60 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n      float v__62 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n      float v__64 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n      float v__66 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n      float v__68 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n      float v__70 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n      float v__72 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n      float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__75 = ((float*)data_1)[cse_var_2] - T_softmax_maxelem[cse_var_19];\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + ((v__74) > (v__75) ? (v__74) : (v__75)));\n      T_softmax_expsum[1] = 0.000000e+00f;\n        float v__76 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n        float v__77 = (v__76) < (8.837627e+01f) ? (v__76) : (8.837627e+01f);\n        int32_t v__78 = ((int32_t)(floorf(((((v__77) > (-8.837626e+01f) ? (v__77) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__79 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__80 = (v__79) < (8.837627e+01f) ? (v__79) : (8.837627e+01f);\n      float v__81 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__82 = (v__81) < (8.837627e+01f) ? (v__81) : (8.837627e+01f);\n      float v__83 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__84 = (v__83) < (8.837627e+01f) ? (v__83) : (8.837627e+01f);\n      float v__85 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__86 = (v__85) < (8.837627e+01f) ? (v__85) : (8.837627e+01f);\n      float v__87 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__88 = (v__87) < (8.837627e+01f) ? (v__87) : (8.837627e+01f);\n      float v__89 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__90 = (v__89) < (8.837627e+01f) ? (v__89) : (8.837627e+01f);\n      float v__91 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__92 = (v__91) < (8.837627e+01f) ? (v__91) : (8.837627e+01f);\n      float v__93 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__94 = (v__93) < (8.837627e+01f) ? (v__93) : (8.837627e+01f);\n      float v__95 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__96 = (v__95) < (8.837627e+01f) ? (v__95) : (8.837627e+01f);\n      float v__97 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__98 = (v__97) < (8.837627e+01f) ? (v__97) : (8.837627e+01f);\n      float v__99 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__100 = (v__99) < (8.837627e+01f) ? (v__99) : (8.837627e+01f);\n      float v__101 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__102 = (v__101) < (8.837627e+01f) ? (v__101) : (8.837627e+01f);\n      float v__103 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__104 = (v__103) < (8.837627e+01f) ? (v__103) : (8.837627e+01f);\n      float v__105 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__106 = (v__105) < (8.837627e+01f) ? (v__105) : (8.837627e+01f);\n      float v__107 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__108 = (v__107) < (8.837627e+01f) ? (v__107) : (8.837627e+01f);\n      float v__109 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      float v__110 = (v__109) < (8.837627e+01f) ? (v__109) : (8.837627e+01f);\n      float v__111 = (*(float *)(&(v__78))) * ((((((((((((((1.987569e-04f * (((v__80) > (-8.837626e+01f) ? (v__80) : (-8.837626e+01f)) - (floorf(((((v__82) > (-8.837626e+01f) ? (v__82) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__84) > (-8.837626e+01f) ? (v__84) : (-8.837626e+01f)) - (floorf(((((v__86) > (-8.837626e+01f) ? (v__86) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__88) > (-8.837626e+01f) ? (v__88) : (-8.837626e+01f)) - (floorf(((((v__90) > (-8.837626e+01f) ? (v__90) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__92) > (-8.837626e+01f) ? (v__92) : (-8.837626e+01f)) - (floorf(((((v__94) > (-8.837626e+01f) ? (v__94) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__96) > (-8.837626e+01f) ? (v__96) : (-8.837626e+01f)) - (floorf(((((v__98) > (-8.837626e+01f) ? (v__98) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__100) > (-8.837626e+01f) ? (v__100) : (-8.837626e+01f)) - (floorf(((((v__102) > (-8.837626e+01f) ? (v__102) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__104) > (-8.837626e+01f) ? (v__104) : (-8.837626e+01f)) - (floorf(((((v__106) > (-8.837626e+01f) ? (v__106) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__108) > (-8.837626e+01f) ? (v__108) : (-8.837626e+01f)) - (floorf(((((v__110) > (-8.837626e+01f) ? (v__110) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__112 = ((float*)data_1)[cse_var_5] - T_softmax_maxelem[cse_var_14];\n      T_softmax_expsum[1] = (T_softmax_expsum[1] + ((v__111) > (v__112) ? (v__111) : (v__112)));\n        float v__113 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n        float v__114 = (v__113) < (8.837627e+01f) ? (v__113) : (8.837627e+01f);\n        int32_t v__115 = ((int32_t)(floorf(((((v__114) > (-8.837626e+01f) ? (v__114) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__116 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__117 = (v__116) < (8.837627e+01f) ? (v__116) : (8.837627e+01f);\n      float v__118 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__119 = (v__118) < (8.837627e+01f) ? (v__118) : (8.837627e+01f);\n      float v__120 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__121 = (v__120) < (8.837627e+01f) ? (v__120) : (8.837627e+01f);\n      float v__122 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__123 = (v__122) < (8.837627e+01f) ? (v__122) : (8.837627e+01f);\n      float v__124 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__125 = (v__124) < (8.837627e+01f) ? (v__124) : (8.837627e+01f);\n      float v__126 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__127 = (v__126) < (8.837627e+01f) ? (v__126) : (8.837627e+01f);\n      float v__128 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__129 = (v__128) < (8.837627e+01f) ? (v__128) : (8.837627e+01f);\n      float v__130 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__131 = (v__130) < (8.837627e+01f) ? (v__130) : (8.837627e+01f);\n      float v__132 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__133 = (v__132) < (8.837627e+01f) ? (v__132) : (8.837627e+01f);\n      float v__134 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__135 = (v__134) < (8.837627e+01f) ? (v__134) : (8.837627e+01f);\n      float v__136 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__137 = (v__136) < (8.837627e+01f) ? (v__136) : (8.837627e+01f);\n      float v__138 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__139 = (v__138) < (8.837627e+01f) ? (v__138) : (8.837627e+01f);\n      float v__140 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__141 = (v__140) < (8.837627e+01f) ? (v__140) : (8.837627e+01f);\n      float v__142 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__143 = (v__142) < (8.837627e+01f) ? (v__142) : (8.837627e+01f);\n      float v__144 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__145 = (v__144) < (8.837627e+01f) ? (v__144) : (8.837627e+01f);\n      float v__146 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      float v__147 = (v__146) < (8.837627e+01f) ? (v__146) : (8.837627e+01f);\n      float v__148 = (*(float *)(&(v__115))) * ((((((((((((((1.987569e-04f * (((v__117) > (-8.837626e+01f) ? (v__117) : (-8.837626e+01f)) - (floorf(((((v__119) > (-8.837626e+01f) ? (v__119) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__121) > (-8.837626e+01f) ? (v__121) : (-8.837626e+01f)) - (floorf(((((v__123) > (-8.837626e+01f) ? (v__123) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__125) > (-8.837626e+01f) ? (v__125) : (-8.837626e+01f)) - (floorf(((((v__127) > (-8.837626e+01f) ? (v__127) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__129) > (-8.837626e+01f) ? (v__129) : (-8.837626e+01f)) - (floorf(((((v__131) > (-8.837626e+01f) ? (v__131) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__133) > (-8.837626e+01f) ? (v__133) : (-8.837626e+01f)) - (floorf(((((v__135) > (-8.837626e+01f) ? (v__135) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__137) > (-8.837626e+01f) ? (v__137) : (-8.837626e+01f)) - (floorf(((((v__139) > (-8.837626e+01f) ? (v__139) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__141) > (-8.837626e+01f) ? (v__141) : (-8.837626e+01f)) - (floorf(((((v__143) > (-8.837626e+01f) ? (v__143) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__145) > (-8.837626e+01f) ? (v__145) : (-8.837626e+01f)) - (floorf(((((v__147) > (-8.837626e+01f) ? (v__147) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__149 = ((float*)data_1)[cse_var_6] - T_softmax_maxelem[cse_var_14];\n      T_softmax_expsum[1] = (T_softmax_expsum[1] + ((v__148) > (v__149) ? (v__148) : (v__149)));\n      T_softmax_expsum[2] = 0.000000e+00f;\n        float v__150 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n        float v__151 = (v__150) < (8.837627e+01f) ? (v__150) : (8.837627e+01f);\n        int32_t v__152 = ((int32_t)(floorf(((((v__151) > (-8.837626e+01f) ? (v__151) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__153 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__154 = (v__153) < (8.837627e+01f) ? (v__153) : (8.837627e+01f);\n      float v__155 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__156 = (v__155) < (8.837627e+01f) ? (v__155) : (8.837627e+01f);\n      float v__157 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__158 = (v__157) < (8.837627e+01f) ? (v__157) : (8.837627e+01f);\n      float v__159 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__160 = (v__159) < (8.837627e+01f) ? (v__159) : (8.837627e+01f);\n      float v__161 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__162 = (v__161) < (8.837627e+01f) ? (v__161) : (8.837627e+01f);\n      float v__163 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__164 = (v__163) < (8.837627e+01f) ? (v__163) : (8.837627e+01f);\n      float v__165 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__166 = (v__165) < (8.837627e+01f) ? (v__165) : (8.837627e+01f);\n      float v__167 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__168 = (v__167) < (8.837627e+01f) ? (v__167) : (8.837627e+01f);\n      float v__169 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__170 = (v__169) < (8.837627e+01f) ? (v__169) : (8.837627e+01f);\n      float v__171 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__172 = (v__171) < (8.837627e+01f) ? (v__171) : (8.837627e+01f);\n      float v__173 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__174 = (v__173) < (8.837627e+01f) ? (v__173) : (8.837627e+01f);\n      float v__175 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__176 = (v__175) < (8.837627e+01f) ? (v__175) : (8.837627e+01f);\n      float v__177 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__178 = (v__177) < (8.837627e+01f) ? (v__177) : (8.837627e+01f);\n      float v__179 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__180 = (v__179) < (8.837627e+01f) ? (v__179) : (8.837627e+01f);\n      float v__181 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__182 = (v__181) < (8.837627e+01f) ? (v__181) : (8.837627e+01f);\n      float v__183 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      float v__184 = (v__183) < (8.837627e+01f) ? (v__183) : (8.837627e+01f);\n      float v__185 = (*(float *)(&(v__152))) * ((((((((((((((1.987569e-04f * (((v__154) > (-8.837626e+01f) ? (v__154) : (-8.837626e+01f)) - (floorf(((((v__156) > (-8.837626e+01f) ? (v__156) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__158) > (-8.837626e+01f) ? (v__158) : (-8.837626e+01f)) - (floorf(((((v__160) > (-8.837626e+01f) ? (v__160) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__162) > (-8.837626e+01f) ? (v__162) : (-8.837626e+01f)) - (floorf(((((v__164) > (-8.837626e+01f) ? (v__164) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__166) > (-8.837626e+01f) ? (v__166) : (-8.837626e+01f)) - (floorf(((((v__168) > (-8.837626e+01f) ? (v__168) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__170) > (-8.837626e+01f) ? (v__170) : (-8.837626e+01f)) - (floorf(((((v__172) > (-8.837626e+01f) ? (v__172) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__174) > (-8.837626e+01f) ? (v__174) : (-8.837626e+01f)) - (floorf(((((v__176) > (-8.837626e+01f) ? (v__176) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__178) > (-8.837626e+01f) ? (v__178) : (-8.837626e+01f)) - (floorf(((((v__180) > (-8.837626e+01f) ? (v__180) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__182) > (-8.837626e+01f) ? (v__182) : (-8.837626e+01f)) - (floorf(((((v__184) > (-8.837626e+01f) ? (v__184) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__186 = ((float*)data_1)[cse_var_7] - T_softmax_maxelem[cse_var_15];\n      T_softmax_expsum[2] = (T_softmax_expsum[2] + ((v__185) > (v__186) ? (v__185) : (v__186)));\n        float v__187 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n        float v__188 = (v__187) < (8.837627e+01f) ? (v__187) : (8.837627e+01f);\n        int32_t v__189 = ((int32_t)(floorf(((((v__188) > (-8.837626e+01f) ? (v__188) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__190 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__191 = (v__190) < (8.837627e+01f) ? (v__190) : (8.837627e+01f);\n      float v__192 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__193 = (v__192) < (8.837627e+01f) ? (v__192) : (8.837627e+01f);\n      float v__194 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__195 = (v__194) < (8.837627e+01f) ? (v__194) : (8.837627e+01f);\n      float v__196 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__197 = (v__196) < (8.837627e+01f) ? (v__196) : (8.837627e+01f);\n      float v__198 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__199 = (v__198) < (8.837627e+01f) ? (v__198) : (8.837627e+01f);\n      float v__200 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__201 = (v__200) < (8.837627e+01f) ? (v__200) : (8.837627e+01f);\n      float v__202 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__203 = (v__202) < (8.837627e+01f) ? (v__202) : (8.837627e+01f);\n      float v__204 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__205 = (v__204) < (8.837627e+01f) ? (v__204) : (8.837627e+01f);\n      float v__206 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__207 = (v__206) < (8.837627e+01f) ? (v__206) : (8.837627e+01f);\n      float v__208 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__209 = (v__208) < (8.837627e+01f) ? (v__208) : (8.837627e+01f);\n      float v__210 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__211 = (v__210) < (8.837627e+01f) ? (v__210) : (8.837627e+01f);\n      float v__212 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__213 = (v__212) < (8.837627e+01f) ? (v__212) : (8.837627e+01f);\n      float v__214 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__215 = (v__214) < (8.837627e+01f) ? (v__214) : (8.837627e+01f);\n      float v__216 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__217 = (v__216) < (8.837627e+01f) ? (v__216) : (8.837627e+01f);\n      float v__218 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__219 = (v__218) < (8.837627e+01f) ? (v__218) : (8.837627e+01f);\n      float v__220 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      float v__221 = (v__220) < (8.837627e+01f) ? (v__220) : (8.837627e+01f);\n      float v__222 = (*(float *)(&(v__189))) * ((((((((((((((1.987569e-04f * (((v__191) > (-8.837626e+01f) ? (v__191) : (-8.837626e+01f)) - (floorf(((((v__193) > (-8.837626e+01f) ? (v__193) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__195) > (-8.837626e+01f) ? (v__195) : (-8.837626e+01f)) - (floorf(((((v__197) > (-8.837626e+01f) ? (v__197) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__199) > (-8.837626e+01f) ? (v__199) : (-8.837626e+01f)) - (floorf(((((v__201) > (-8.837626e+01f) ? (v__201) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__203) > (-8.837626e+01f) ? (v__203) : (-8.837626e+01f)) - (floorf(((((v__205) > (-8.837626e+01f) ? (v__205) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__207) > (-8.837626e+01f) ? (v__207) : (-8.837626e+01f)) - (floorf(((((v__209) > (-8.837626e+01f) ? (v__209) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__211) > (-8.837626e+01f) ? (v__211) : (-8.837626e+01f)) - (floorf(((((v__213) > (-8.837626e+01f) ? (v__213) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__215) > (-8.837626e+01f) ? (v__215) : (-8.837626e+01f)) - (floorf(((((v__217) > (-8.837626e+01f) ? (v__217) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__219) > (-8.837626e+01f) ? (v__219) : (-8.837626e+01f)) - (floorf(((((v__221) > (-8.837626e+01f) ? (v__221) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__223 = ((float*)data_1)[cse_var_8] - T_softmax_maxelem[cse_var_15];\n      T_softmax_expsum[2] = (T_softmax_expsum[2] + ((v__222) > (v__223) ? (v__222) : (v__223)));\n      T_softmax_expsum[3] = 0.000000e+00f;\n        float v__224 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n        float v__225 = (v__224) < (8.837627e+01f) ? (v__224) : (8.837627e+01f);\n        int32_t v__226 = ((int32_t)(floorf(((((v__225) > (-8.837626e+01f) ? (v__225) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__227 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__228 = (v__227) < (8.837627e+01f) ? (v__227) : (8.837627e+01f);\n      float v__229 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__230 = (v__229) < (8.837627e+01f) ? (v__229) : (8.837627e+01f);\n      float v__231 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__232 = (v__231) < (8.837627e+01f) ? (v__231) : (8.837627e+01f);\n      float v__233 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__234 = (v__233) < (8.837627e+01f) ? (v__233) : (8.837627e+01f);\n      float v__235 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__236 = (v__235) < (8.837627e+01f) ? (v__235) : (8.837627e+01f);\n      float v__237 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__238 = (v__237) < (8.837627e+01f) ? (v__237) : (8.837627e+01f);\n      float v__239 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__240 = (v__239) < (8.837627e+01f) ? (v__239) : (8.837627e+01f);\n      float v__241 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__242 = (v__241) < (8.837627e+01f) ? (v__241) : (8.837627e+01f);\n      float v__243 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__244 = (v__243) < (8.837627e+01f) ? (v__243) : (8.837627e+01f);\n      float v__245 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__246 = (v__245) < (8.837627e+01f) ? (v__245) : (8.837627e+01f);\n      float v__247 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__248 = (v__247) < (8.837627e+01f) ? (v__247) : (8.837627e+01f);\n      float v__249 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__250 = (v__249) < (8.837627e+01f) ? (v__249) : (8.837627e+01f);\n      float v__251 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__252 = (v__251) < (8.837627e+01f) ? (v__251) : (8.837627e+01f);\n      float v__253 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__254 = (v__253) < (8.837627e+01f) ? (v__253) : (8.837627e+01f);\n      float v__255 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__256 = (v__255) < (8.837627e+01f) ? (v__255) : (8.837627e+01f);\n      float v__257 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      float v__258 = (v__257) < (8.837627e+01f) ? (v__257) : (8.837627e+01f);\n      float v__259 = (*(float *)(&(v__226))) * ((((((((((((((1.987569e-04f * (((v__228) > (-8.837626e+01f) ? (v__228) : (-8.837626e+01f)) - (floorf(((((v__230) > (-8.837626e+01f) ? (v__230) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__232) > (-8.837626e+01f) ? (v__232) : (-8.837626e+01f)) - (floorf(((((v__234) > (-8.837626e+01f) ? (v__234) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__236) > (-8.837626e+01f) ? (v__236) : (-8.837626e+01f)) - (floorf(((((v__238) > (-8.837626e+01f) ? (v__238) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__240) > (-8.837626e+01f) ? (v__240) : (-8.837626e+01f)) - (floorf(((((v__242) > (-8.837626e+01f) ? (v__242) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__244) > (-8.837626e+01f) ? (v__244) : (-8.837626e+01f)) - (floorf(((((v__246) > (-8.837626e+01f) ? (v__246) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__248) > (-8.837626e+01f) ? (v__248) : (-8.837626e+01f)) - (floorf(((((v__250) > (-8.837626e+01f) ? (v__250) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__252) > (-8.837626e+01f) ? (v__252) : (-8.837626e+01f)) - (floorf(((((v__254) > (-8.837626e+01f) ? (v__254) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__256) > (-8.837626e+01f) ? (v__256) : (-8.837626e+01f)) - (floorf(((((v__258) > (-8.837626e+01f) ? (v__258) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__260 = ((float*)data_1)[cse_var_9] - T_softmax_maxelem[cse_var_16];\n      T_softmax_expsum[3] = (T_softmax_expsum[3] + ((v__259) > (v__260) ? (v__259) : (v__260)));\n        float v__261 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n        float v__262 = (v__261) < (8.837627e+01f) ? (v__261) : (8.837627e+01f);\n        int32_t v__263 = ((int32_t)(floorf(((((v__262) > (-8.837626e+01f) ? (v__262) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__264 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__265 = (v__264) < (8.837627e+01f) ? (v__264) : (8.837627e+01f);\n      float v__266 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__267 = (v__266) < (8.837627e+01f) ? (v__266) : (8.837627e+01f);\n      float v__268 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__269 = (v__268) < (8.837627e+01f) ? (v__268) : (8.837627e+01f);\n      float v__270 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__271 = (v__270) < (8.837627e+01f) ? (v__270) : (8.837627e+01f);\n      float v__272 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__273 = (v__272) < (8.837627e+01f) ? (v__272) : (8.837627e+01f);\n      float v__274 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__275 = (v__274) < (8.837627e+01f) ? (v__274) : (8.837627e+01f);\n      float v__276 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__277 = (v__276) < (8.837627e+01f) ? (v__276) : (8.837627e+01f);\n      float v__278 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__279 = (v__278) < (8.837627e+01f) ? (v__278) : (8.837627e+01f);\n      float v__280 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__281 = (v__280) < (8.837627e+01f) ? (v__280) : (8.837627e+01f);\n      float v__282 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__283 = (v__282) < (8.837627e+01f) ? (v__282) : (8.837627e+01f);\n      float v__284 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__285 = (v__284) < (8.837627e+01f) ? (v__284) : (8.837627e+01f);\n      float v__286 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__287 = (v__286) < (8.837627e+01f) ? (v__286) : (8.837627e+01f);\n      float v__288 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__289 = (v__288) < (8.837627e+01f) ? (v__288) : (8.837627e+01f);\n      float v__290 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__291 = (v__290) < (8.837627e+01f) ? (v__290) : (8.837627e+01f);\n      float v__292 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__293 = (v__292) < (8.837627e+01f) ? (v__292) : (8.837627e+01f);\n      float v__294 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      float v__295 = (v__294) < (8.837627e+01f) ? (v__294) : (8.837627e+01f);\n      float v__296 = (*(float *)(&(v__263))) * ((((((((((((((1.987569e-04f * (((v__265) > (-8.837626e+01f) ? (v__265) : (-8.837626e+01f)) - (floorf(((((v__267) > (-8.837626e+01f) ? (v__267) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__269) > (-8.837626e+01f) ? (v__269) : (-8.837626e+01f)) - (floorf(((((v__271) > (-8.837626e+01f) ? (v__271) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__273) > (-8.837626e+01f) ? (v__273) : (-8.837626e+01f)) - (floorf(((((v__275) > (-8.837626e+01f) ? (v__275) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__277) > (-8.837626e+01f) ? (v__277) : (-8.837626e+01f)) - (floorf(((((v__279) > (-8.837626e+01f) ? (v__279) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__281) > (-8.837626e+01f) ? (v__281) : (-8.837626e+01f)) - (floorf(((((v__283) > (-8.837626e+01f) ? (v__283) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__285) > (-8.837626e+01f) ? (v__285) : (-8.837626e+01f)) - (floorf(((((v__287) > (-8.837626e+01f) ? (v__287) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__289) > (-8.837626e+01f) ? (v__289) : (-8.837626e+01f)) - (floorf(((((v__291) > (-8.837626e+01f) ? (v__291) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__293) > (-8.837626e+01f) ? (v__293) : (-8.837626e+01f)) - (floorf(((((v__295) > (-8.837626e+01f) ? (v__295) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__297 = ((float*)data_1)[cse_var_10] - T_softmax_maxelem[cse_var_16];\n      T_softmax_expsum[3] = (T_softmax_expsum[3] + ((v__296) > (v__297) ? (v__296) : (v__297)));\n      T_softmax_expsum[4] = 0.000000e+00f;\n        float v__298 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n        float v__299 = (v__298) < (8.837627e+01f) ? (v__298) : (8.837627e+01f);\n        int32_t v__300 = ((int32_t)(floorf(((((v__299) > (-8.837626e+01f) ? (v__299) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__301 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__302 = (v__301) < (8.837627e+01f) ? (v__301) : (8.837627e+01f);\n      float v__303 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__304 = (v__303) < (8.837627e+01f) ? (v__303) : (8.837627e+01f);\n      float v__305 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__306 = (v__305) < (8.837627e+01f) ? (v__305) : (8.837627e+01f);\n      float v__307 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__308 = (v__307) < (8.837627e+01f) ? (v__307) : (8.837627e+01f);\n      float v__309 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__310 = (v__309) < (8.837627e+01f) ? (v__309) : (8.837627e+01f);\n      float v__311 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__312 = (v__311) < (8.837627e+01f) ? (v__311) : (8.837627e+01f);\n      float v__313 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__314 = (v__313) < (8.837627e+01f) ? (v__313) : (8.837627e+01f);\n      float v__315 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__316 = (v__315) < (8.837627e+01f) ? (v__315) : (8.837627e+01f);\n      float v__317 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__318 = (v__317) < (8.837627e+01f) ? (v__317) : (8.837627e+01f);\n      float v__319 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__320 = (v__319) < (8.837627e+01f) ? (v__319) : (8.837627e+01f);\n      float v__321 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__322 = (v__321) < (8.837627e+01f) ? (v__321) : (8.837627e+01f);\n      float v__323 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__324 = (v__323) < (8.837627e+01f) ? (v__323) : (8.837627e+01f);\n      float v__325 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__326 = (v__325) < (8.837627e+01f) ? (v__325) : (8.837627e+01f);\n      float v__327 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__328 = (v__327) < (8.837627e+01f) ? (v__327) : (8.837627e+01f);\n      float v__329 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__330 = (v__329) < (8.837627e+01f) ? (v__329) : (8.837627e+01f);\n      float v__331 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      float v__332 = (v__331) < (8.837627e+01f) ? (v__331) : (8.837627e+01f);\n      float v__333 = (*(float *)(&(v__300))) * ((((((((((((((1.987569e-04f * (((v__302) > (-8.837626e+01f) ? (v__302) : (-8.837626e+01f)) - (floorf(((((v__304) > (-8.837626e+01f) ? (v__304) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__306) > (-8.837626e+01f) ? (v__306) : (-8.837626e+01f)) - (floorf(((((v__308) > (-8.837626e+01f) ? (v__308) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__310) > (-8.837626e+01f) ? (v__310) : (-8.837626e+01f)) - (floorf(((((v__312) > (-8.837626e+01f) ? (v__312) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__314) > (-8.837626e+01f) ? (v__314) : (-8.837626e+01f)) - (floorf(((((v__316) > (-8.837626e+01f) ? (v__316) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__318) > (-8.837626e+01f) ? (v__318) : (-8.837626e+01f)) - (floorf(((((v__320) > (-8.837626e+01f) ? (v__320) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__322) > (-8.837626e+01f) ? (v__322) : (-8.837626e+01f)) - (floorf(((((v__324) > (-8.837626e+01f) ? (v__324) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__326) > (-8.837626e+01f) ? (v__326) : (-8.837626e+01f)) - (floorf(((((v__328) > (-8.837626e+01f) ? (v__328) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__330) > (-8.837626e+01f) ? (v__330) : (-8.837626e+01f)) - (floorf(((((v__332) > (-8.837626e+01f) ? (v__332) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__334 = ((float*)data_1)[cse_var_11] - T_softmax_maxelem[cse_var_17];\n      T_softmax_expsum[4] = (T_softmax_expsum[4] + ((v__333) > (v__334) ? (v__333) : (v__334)));\n        float v__335 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n        float v__336 = (v__335) < (8.837627e+01f) ? (v__335) : (8.837627e+01f);\n        int32_t v__337 = ((int32_t)(floorf(((((v__336) > (-8.837626e+01f) ? (v__336) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__338 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__339 = (v__338) < (8.837627e+01f) ? (v__338) : (8.837627e+01f);\n      float v__340 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__341 = (v__340) < (8.837627e+01f) ? (v__340) : (8.837627e+01f);\n      float v__342 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__343 = (v__342) < (8.837627e+01f) ? (v__342) : (8.837627e+01f);\n      float v__344 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__345 = (v__344) < (8.837627e+01f) ? (v__344) : (8.837627e+01f);\n      float v__346 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__347 = (v__346) < (8.837627e+01f) ? (v__346) : (8.837627e+01f);\n      float v__348 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__349 = (v__348) < (8.837627e+01f) ? (v__348) : (8.837627e+01f);\n      float v__350 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__351 = (v__350) < (8.837627e+01f) ? (v__350) : (8.837627e+01f);\n      float v__352 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__353 = (v__352) < (8.837627e+01f) ? (v__352) : (8.837627e+01f);\n      float v__354 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__355 = (v__354) < (8.837627e+01f) ? (v__354) : (8.837627e+01f);\n      float v__356 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__357 = (v__356) < (8.837627e+01f) ? (v__356) : (8.837627e+01f);\n      float v__358 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__359 = (v__358) < (8.837627e+01f) ? (v__358) : (8.837627e+01f);\n      float v__360 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__361 = (v__360) < (8.837627e+01f) ? (v__360) : (8.837627e+01f);\n      float v__362 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__363 = (v__362) < (8.837627e+01f) ? (v__362) : (8.837627e+01f);\n      float v__364 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__365 = (v__364) < (8.837627e+01f) ? (v__364) : (8.837627e+01f);\n      float v__366 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__367 = (v__366) < (8.837627e+01f) ? (v__366) : (8.837627e+01f);\n      float v__368 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      float v__369 = (v__368) < (8.837627e+01f) ? (v__368) : (8.837627e+01f);\n      float v__370 = (*(float *)(&(v__337))) * ((((((((((((((1.987569e-04f * (((v__339) > (-8.837626e+01f) ? (v__339) : (-8.837626e+01f)) - (floorf(((((v__341) > (-8.837626e+01f) ? (v__341) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__343) > (-8.837626e+01f) ? (v__343) : (-8.837626e+01f)) - (floorf(((((v__345) > (-8.837626e+01f) ? (v__345) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__347) > (-8.837626e+01f) ? (v__347) : (-8.837626e+01f)) - (floorf(((((v__349) > (-8.837626e+01f) ? (v__349) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__351) > (-8.837626e+01f) ? (v__351) : (-8.837626e+01f)) - (floorf(((((v__353) > (-8.837626e+01f) ? (v__353) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__355) > (-8.837626e+01f) ? (v__355) : (-8.837626e+01f)) - (floorf(((((v__357) > (-8.837626e+01f) ? (v__357) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__359) > (-8.837626e+01f) ? (v__359) : (-8.837626e+01f)) - (floorf(((((v__361) > (-8.837626e+01f) ? (v__361) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__363) > (-8.837626e+01f) ? (v__363) : (-8.837626e+01f)) - (floorf(((((v__365) > (-8.837626e+01f) ? (v__365) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__367) > (-8.837626e+01f) ? (v__367) : (-8.837626e+01f)) - (floorf(((((v__369) > (-8.837626e+01f) ? (v__369) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__371 = ((float*)data_1)[cse_var_12] - T_softmax_maxelem[cse_var_17];\n      T_softmax_expsum[4] = (T_softmax_expsum[4] + ((v__370) > (v__371) ? (v__370) : (v__371)));\n      T_softmax_expsum[5] = 0.000000e+00f;\n        float v__372 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n        float v__373 = (v__372) < (8.837627e+01f) ? (v__372) : (8.837627e+01f);\n        int32_t v__374 = ((int32_t)(floorf(((((v__373) > (-8.837626e+01f) ? (v__373) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__375 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__376 = (v__375) < (8.837627e+01f) ? (v__375) : (8.837627e+01f);\n      float v__377 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__378 = (v__377) < (8.837627e+01f) ? (v__377) : (8.837627e+01f);\n      float v__379 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__380 = (v__379) < (8.837627e+01f) ? (v__379) : (8.837627e+01f);\n      float v__381 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__382 = (v__381) < (8.837627e+01f) ? (v__381) : (8.837627e+01f);\n      float v__383 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__384 = (v__383) < (8.837627e+01f) ? (v__383) : (8.837627e+01f);\n      float v__385 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__386 = (v__385) < (8.837627e+01f) ? (v__385) : (8.837627e+01f);\n      float v__387 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__388 = (v__387) < (8.837627e+01f) ? (v__387) : (8.837627e+01f);\n      float v__389 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__390 = (v__389) < (8.837627e+01f) ? (v__389) : (8.837627e+01f);\n      float v__391 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__392 = (v__391) < (8.837627e+01f) ? (v__391) : (8.837627e+01f);\n      float v__393 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__394 = (v__393) < (8.837627e+01f) ? (v__393) : (8.837627e+01f);\n      float v__395 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__396 = (v__395) < (8.837627e+01f) ? (v__395) : (8.837627e+01f);\n      float v__397 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__398 = (v__397) < (8.837627e+01f) ? (v__397) : (8.837627e+01f);\n      float v__399 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__400 = (v__399) < (8.837627e+01f) ? (v__399) : (8.837627e+01f);\n      float v__401 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__402 = (v__401) < (8.837627e+01f) ? (v__401) : (8.837627e+01f);\n      float v__403 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__404 = (v__403) < (8.837627e+01f) ? (v__403) : (8.837627e+01f);\n      float v__405 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      float v__406 = (v__405) < (8.837627e+01f) ? (v__405) : (8.837627e+01f);\n      float v__407 = (*(float *)(&(v__374))) * ((((((((((((((1.987569e-04f * (((v__376) > (-8.837626e+01f) ? (v__376) : (-8.837626e+01f)) - (floorf(((((v__378) > (-8.837626e+01f) ? (v__378) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__380) > (-8.837626e+01f) ? (v__380) : (-8.837626e+01f)) - (floorf(((((v__382) > (-8.837626e+01f) ? (v__382) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__384) > (-8.837626e+01f) ? (v__384) : (-8.837626e+01f)) - (floorf(((((v__386) > (-8.837626e+01f) ? (v__386) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__388) > (-8.837626e+01f) ? (v__388) : (-8.837626e+01f)) - (floorf(((((v__390) > (-8.837626e+01f) ? (v__390) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__392) > (-8.837626e+01f) ? (v__392) : (-8.837626e+01f)) - (floorf(((((v__394) > (-8.837626e+01f) ? (v__394) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__396) > (-8.837626e+01f) ? (v__396) : (-8.837626e+01f)) - (floorf(((((v__398) > (-8.837626e+01f) ? (v__398) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__400) > (-8.837626e+01f) ? (v__400) : (-8.837626e+01f)) - (floorf(((((v__402) > (-8.837626e+01f) ? (v__402) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__404) > (-8.837626e+01f) ? (v__404) : (-8.837626e+01f)) - (floorf(((((v__406) > (-8.837626e+01f) ? (v__406) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__408 = ((float*)data_1)[cse_var_3] - T_softmax_maxelem[cse_var_18];\n      T_softmax_expsum[5] = (T_softmax_expsum[5] + ((v__407) > (v__408) ? (v__407) : (v__408)));\n        float v__409 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n        float v__410 = (v__409) < (8.837627e+01f) ? (v__409) : (8.837627e+01f);\n        int32_t v__411 = ((int32_t)(floorf(((((v__410) > (-8.837626e+01f) ? (v__410) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      float v__412 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__413 = (v__412) < (8.837627e+01f) ? (v__412) : (8.837627e+01f);\n      float v__414 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__415 = (v__414) < (8.837627e+01f) ? (v__414) : (8.837627e+01f);\n      float v__416 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__417 = (v__416) < (8.837627e+01f) ? (v__416) : (8.837627e+01f);\n      float v__418 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__419 = (v__418) < (8.837627e+01f) ? (v__418) : (8.837627e+01f);\n      float v__420 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__421 = (v__420) < (8.837627e+01f) ? (v__420) : (8.837627e+01f);\n      float v__422 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__423 = (v__422) < (8.837627e+01f) ? (v__422) : (8.837627e+01f);\n      float v__424 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__425 = (v__424) < (8.837627e+01f) ? (v__424) : (8.837627e+01f);\n      float v__426 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__427 = (v__426) < (8.837627e+01f) ? (v__426) : (8.837627e+01f);\n      float v__428 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__429 = (v__428) < (8.837627e+01f) ? (v__428) : (8.837627e+01f);\n      float v__430 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__431 = (v__430) < (8.837627e+01f) ? (v__430) : (8.837627e+01f);\n      float v__432 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__433 = (v__432) < (8.837627e+01f) ? (v__432) : (8.837627e+01f);\n      float v__434 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__435 = (v__434) < (8.837627e+01f) ? (v__434) : (8.837627e+01f);\n      float v__436 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__437 = (v__436) < (8.837627e+01f) ? (v__436) : (8.837627e+01f);\n      float v__438 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__439 = (v__438) < (8.837627e+01f) ? (v__438) : (8.837627e+01f);\n      float v__440 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__441 = (v__440) < (8.837627e+01f) ? (v__440) : (8.837627e+01f);\n      float v__442 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      float v__443 = (v__442) < (8.837627e+01f) ? (v__442) : (8.837627e+01f);\n      float v__444 = (*(float *)(&(v__411))) * ((((((((((((((1.987569e-04f * (((v__413) > (-8.837626e+01f) ? (v__413) : (-8.837626e+01f)) - (floorf(((((v__415) > (-8.837626e+01f) ? (v__415) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__417) > (-8.837626e+01f) ? (v__417) : (-8.837626e+01f)) - (floorf(((((v__419) > (-8.837626e+01f) ? (v__419) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__421) > (-8.837626e+01f) ? (v__421) : (-8.837626e+01f)) - (floorf(((((v__423) > (-8.837626e+01f) ? (v__423) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__425) > (-8.837626e+01f) ? (v__425) : (-8.837626e+01f)) - (floorf(((((v__427) > (-8.837626e+01f) ? (v__427) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__429) > (-8.837626e+01f) ? (v__429) : (-8.837626e+01f)) - (floorf(((((v__431) > (-8.837626e+01f) ? (v__431) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__433) > (-8.837626e+01f) ? (v__433) : (-8.837626e+01f)) - (floorf(((((v__435) > (-8.837626e+01f) ? (v__435) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__437) > (-8.837626e+01f) ? (v__437) : (-8.837626e+01f)) - (floorf(((((v__439) > (-8.837626e+01f) ? (v__439) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__441) > (-8.837626e+01f) ? (v__441) : (-8.837626e+01f)) - (floorf(((((v__443) > (-8.837626e+01f) ? (v__443) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n      float v__445 = ((float*)data_1)[cse_var_4] - T_softmax_maxelem[cse_var_18];\n      T_softmax_expsum[5] = (T_softmax_expsum[5] + ((v__444) > (v__445) ? (v__444) : (v__445)));\n      for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n        for (int32_t i3_s = 0; i3_s < 2; ++i3_s) {\n          int32_t cse_var_21 = (cse_var_19 + i2_1);\n          int32_t cse_var_20 = ((cse_var_13 + (i2_1 * 2)) + i3_s);\n            float v__446 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n            float v__447 = (v__446) < (8.837627e+01f) ? (v__446) : (8.837627e+01f);\n            int32_t v__448 = ((int32_t)(floorf(((((v__447) > (-8.837626e+01f) ? (v__447) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          float v__449 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__450 = (v__449) < (8.837627e+01f) ? (v__449) : (8.837627e+01f);\n          float v__451 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__452 = (v__451) < (8.837627e+01f) ? (v__451) : (8.837627e+01f);\n          float v__453 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__454 = (v__453) < (8.837627e+01f) ? (v__453) : (8.837627e+01f);\n          float v__455 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__456 = (v__455) < (8.837627e+01f) ? (v__455) : (8.837627e+01f);\n          float v__457 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__458 = (v__457) < (8.837627e+01f) ? (v__457) : (8.837627e+01f);\n          float v__459 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__460 = (v__459) < (8.837627e+01f) ? (v__459) : (8.837627e+01f);\n          float v__461 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__462 = (v__461) < (8.837627e+01f) ? (v__461) : (8.837627e+01f);\n          float v__463 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__464 = (v__463) < (8.837627e+01f) ? (v__463) : (8.837627e+01f);\n          float v__465 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__466 = (v__465) < (8.837627e+01f) ? (v__465) : (8.837627e+01f);\n          float v__467 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__468 = (v__467) < (8.837627e+01f) ? (v__467) : (8.837627e+01f);\n          float v__469 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__470 = (v__469) < (8.837627e+01f) ? (v__469) : (8.837627e+01f);\n          float v__471 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__472 = (v__471) < (8.837627e+01f) ? (v__471) : (8.837627e+01f);\n          float v__473 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__474 = (v__473) < (8.837627e+01f) ? (v__473) : (8.837627e+01f);\n          float v__475 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__476 = (v__475) < (8.837627e+01f) ? (v__475) : (8.837627e+01f);\n          float v__477 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__478 = (v__477) < (8.837627e+01f) ? (v__477) : (8.837627e+01f);\n          float v__479 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          float v__480 = (v__479) < (8.837627e+01f) ? (v__479) : (8.837627e+01f);\n          float v__481 = (*(float *)(&(v__448))) * ((((((((((((((1.987569e-04f * (((v__450) > (-8.837626e+01f) ? (v__450) : (-8.837626e+01f)) - (floorf(((((v__452) > (-8.837626e+01f) ? (v__452) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__454) > (-8.837626e+01f) ? (v__454) : (-8.837626e+01f)) - (floorf(((((v__456) > (-8.837626e+01f) ? (v__456) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__458) > (-8.837626e+01f) ? (v__458) : (-8.837626e+01f)) - (floorf(((((v__460) > (-8.837626e+01f) ? (v__460) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__462) > (-8.837626e+01f) ? (v__462) : (-8.837626e+01f)) - (floorf(((((v__464) > (-8.837626e+01f) ? (v__464) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__466) > (-8.837626e+01f) ? (v__466) : (-8.837626e+01f)) - (floorf(((((v__468) > (-8.837626e+01f) ? (v__468) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__470) > (-8.837626e+01f) ? (v__470) : (-8.837626e+01f)) - (floorf(((((v__472) > (-8.837626e+01f) ? (v__472) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__474) > (-8.837626e+01f) ? (v__474) : (-8.837626e+01f)) - (floorf(((((v__476) > (-8.837626e+01f) ? (v__476) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__478) > (-8.837626e+01f) ? (v__478) : (-8.837626e+01f)) - (floorf(((((v__480) > (-8.837626e+01f) ? (v__480) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n          float v__482 = ((float*)data_1)[cse_var_20] - T_softmax_maxelem[cse_var_21];\n          ((float*)T_softmax_norm_1)[cse_var_20] = (((v__481) > (v__482) ? (v__481) : (v__482)) / T_softmax_expsum[i2_1]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 21) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))])) / T_softmax_expsum[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 2; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 2)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 2; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 14, 6, 2), \"float32\"), T_softmax_norm: T.Buffer((2, 14, 6, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(2):\n            T_softmax_maxelem = T.allocate([84], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([6], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((84,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((336,), data=data.data)\n            for i1, i2 in T.grid(14, 6):\n                T_softmax_maxelem_1[i1 * 6 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(2):\n                    cse_var_1: T.int32 = i1 * 6 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 168 + i1 * 12 + i2 * 2 + k])\n            for i1 in range(14):\n                cse_var_19: T.int32 = i1 * 6\n                cse_var_18: T.int32 = cse_var_19 + 5\n                cse_var_17: T.int32 = cse_var_19 + 4\n                cse_var_16: T.int32 = cse_var_19 + 3\n                cse_var_15: T.int32 = cse_var_19 + 2\n                cse_var_14: T.int32 = cse_var_19 + 1\n                cse_var_13: T.int32 = i0 * 168 + i1 * 12\n                cse_var_12: T.int32 = cse_var_13 + 9\n                cse_var_11: T.int32 = cse_var_13 + 8\n                cse_var_10: T.int32 = cse_var_13 + 7\n                cse_var_9: T.int32 = cse_var_13 + 6\n                cse_var_8: T.int32 = cse_var_13 + 5\n                cse_var_7: T.int32 = cse_var_13 + 4\n                cse_var_6: T.int32 = cse_var_13 + 3\n                cse_var_5: T.int32 = cse_var_13 + 2\n                cse_var_4: T.int32 = cse_var_13 + 11\n                cse_var_3: T.int32 = cse_var_13 + 10\n                cse_var_2: T.int32 = cse_var_13 + 1\n                T_softmax_expsum_1 = T.Buffer((6,), data=T_softmax_expsum, align=16)\n                T_softmax_expsum_1[0] = T.float32(0)\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_13] - T_softmax_maxelem_1[cse_var_19])\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_19])\n                T_softmax_expsum_1[1] = T.float32(0)\n                T_softmax_expsum_1[1] = T_softmax_expsum_1[1] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_5] - T_softmax_maxelem_1[cse_var_14])\n                T_softmax_expsum_1[1] = T_softmax_expsum_1[1] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_6] - T_softmax_maxelem_1[cse_var_14])\n                T_softmax_expsum_1[2] = T.float32(0)\n                T_softmax_expsum_1[2] = T_softmax_expsum_1[2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_7] - T_softmax_maxelem_1[cse_var_15])\n                T_softmax_expsum_1[2] = T_softmax_expsum_1[2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_8] - T_softmax_maxelem_1[cse_var_15])\n                T_softmax_expsum_1[3] = T.float32(0)\n                T_softmax_expsum_1[3] = T_softmax_expsum_1[3] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_9] - T_softmax_maxelem_1[cse_var_16])\n                T_softmax_expsum_1[3] = T_softmax_expsum_1[3] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_10] - T_softmax_maxelem_1[cse_var_16])\n                T_softmax_expsum_1[4] = T.float32(0)\n                T_softmax_expsum_1[4] = T_softmax_expsum_1[4] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_11] - T_softmax_maxelem_1[cse_var_17])\n                T_softmax_expsum_1[4] = T_softmax_expsum_1[4] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_12] - T_softmax_maxelem_1[cse_var_17])\n                T_softmax_expsum_1[5] = T.float32(0)\n                T_softmax_expsum_1[5] = T_softmax_expsum_1[5] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_18])\n                T_softmax_expsum_1[5] = T_softmax_expsum_1[5] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_18])\n                for i2, i3_s in T.grid(6, 2):\n                    cse_var_21: T.int32 = cse_var_19 + i2\n                    cse_var_20: T.int32 = cse_var_13 + i2 * 2 + i3_s\n                    T_softmax_norm_1 = T.Buffer((336,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_20] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_20] - T_softmax_maxelem_1[cse_var_21]) / T_softmax_expsum_1[i2]", "op_args": [2, 14, 6, 2]}{"op_name": "negative", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 15120; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = (((float*)data_1)[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 6, 14, 18), \"float32\"), compute: T.Buffer((10, 6, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(15120):\n            compute_1 = T.Buffer((15120,), data=compute.data)\n            data_1 = T.Buffer((15120,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [10, 6, 14, 18]}{"op_name": "batch_to_space_nd", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    void* T_reshape = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)9216, 2, 32);\n    if (T_reshape == NULL) {\n      return -1;\n    }\n    void* T_transpose = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)9216, 2, 32);\n    if (T_transpose == NULL) {\n      return -1;\n    }\n    float8 T_reshape_1[1];\n    for (int32_t ax0_1 = 0; ax0_1 < 2; ++ax0_1) {\n      for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n        for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 6; ++ax4) {\n            ((float8*)T_reshape)[((((ax0_1 * 144) + (ax1 * 72)) + (ax3 * 6)) + ax4)] = *(float8*)(((float*)data_1) + (((((ax0_1 * 3456) + (ax1 * 1728)) + (ax0 * 576)) + (ax3 * 48)) + (ax4 * 8)));\n          }\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3_1 = 0; ax3_1 < 6; ++ax3_1) {\n          for (int32_t ax4_1 = 0; ax4_1 < 2; ++ax4_1) {\n            ((float8*)T_transpose)[((((ax1_1 * 24) + (ax2 * 12)) + (ax3_1 * 2)) + ax4_1)] = ((float8*)T_reshape)[((((ax2 * 144) + (ax4_1 * 72)) + (ax1_1 * 6)) + ax3_1)];\n          }\n        }\n      }\n    }\n    for (int32_t ax1_2 = 0; ax1_2 < 24; ++ax1_2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 12; ++ax2_1) {\n        T_reshape_1[0] = ((float8*)T_transpose)[((ax1_2 * 12) + ax2_1)];\n        *(float8*)(((float*)T_strided_slice_1) + (((ax0 * 2304) + (ax1_2 * 96)) + (ax2_1 * 8))) = T_reshape_1[0];\n      }\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, T_transpose) != 0) {\n      return -1;\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, T_reshape) != 0) {\n      return -1;\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) & 3) >> 1) * 3456) + (((((int)threadIdx.x) & 15) >> 3) * 1728)) + ((((int)blockIdx.x) >> 2) * 48)) + ((((int)blockIdx.x) & 1) * 24)) + ((((int)threadIdx.x) >> 4) * 8)) + (((int)threadIdx.x) & 7))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 12, 6, 8), \"float32\"), T_strided_slice: T.Buffer((3, 24, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            T_reshape = T.allocate([288], \"float32x8\", \"global\")\n            T_transpose = T.allocate([288], \"float32x8\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32x8\", \"global\")\n            T_reshape_2 = T.Buffer((288,), \"float32x8\", data=T_reshape)\n            for ax0_1, ax1, ax3, ax4 in T.grid(2, 2, 12, 6):\n                data_1 = T.Buffer((8640,), data=data.data)\n                T_reshape_2[ax0_1 * 144 + ax1 * 72 + ax3 * 6 + ax4] = data_1[ax0_1 * 3456 + ax1 * 1728 + ax0 * 576 + ax3 * 48 + ax4 * 8:ax0_1 * 3456 + ax1 * 1728 + ax0 * 576 + ax3 * 48 + ax4 * 8 + 8]\n            T_transpose_1 = T.Buffer((288,), \"float32x8\", data=T_transpose)\n            for ax1, ax2, ax3, ax4 in T.grid(12, 2, 6, 2):\n                T_transpose_1[ax1 * 24 + ax2 * 12 + ax3 * 2 + ax4] = T_reshape_2[ax2 * 144 + ax4 * 72 + ax1 * 6 + ax3]\n            for ax1, ax2 in T.grid(24, 12):\n                T_reshape_3 = T.Buffer((1,), \"float32x8\", data=T_reshape_1, align=32)\n                T_reshape_3[0] = T_transpose_1[ax1 * 12 + ax2]\n                T_strided_slice_1 = T.Buffer((6912,), data=T_strided_slice.data)\n                T_strided_slice_1[ax0 * 2304 + ax1 * 96 + ax2 * 8:ax0 * 2304 + ax1 * 96 + ax2 * 8 + 8] = T_reshape_3[0]", "op_args": [15, 12, 6, 8]}{"op_name": "global_pool_max", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_max_1 = (((DLTensor*)adaptive_pool_max)[0].data);\n  void* default_function_adaptive_pool_max_shape = (((DLTensor*)adaptive_pool_max)[0].shape);\n  void* default_function_adaptive_pool_max_strides = (((DLTensor*)adaptive_pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 64; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 15; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 17; ++rv1) {\n        float v_ = ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused];\n        float v__1 = ((float*)data_1)[(((ax0_ax1_fused_ax2_fused_ax3_fused * 255) + (rv0 * 17)) + rv1)];\n        ((float*)adaptive_pool_max_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((v_) > (v__1) ? (v_) : (v__1));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 15; ++rv0) {\n    for (int rv1 = 0; rv1 < 17; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 2040) + (((int)threadIdx.x) * 255)) + (rv0 * 17)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 4, 15, 17), \"float32\"), adaptive_pool_max: T.Buffer((16, 4, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(64):\n            adaptive_pool_max_1 = T.Buffer((64,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(15, 17):\n                data_1 = T.Buffer((16320,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 255 + rv0 * 17 + rv1])", "op_args": [16, 4, 15, 17]}{"op_name": "prod", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_red_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_red = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_red_1 = (((DLTensor*)data_red)[0].data);\n  void* default_function_data_red_shape = (((DLTensor*)data_red)[0].shape);\n  void* default_function_data_red_strides = (((DLTensor*)data_red)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  float data_red_rf[13];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 13; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 3978; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    int32_t cse_var_1 = (k0_k1_fused_k2_fused_k3_fused_outer * 13);\n    data_red_rf[0] = (data_red_rf[0] * ((float*)data_1)[cse_var_1]);\n    data_red_rf[1] = (data_red_rf[1] * ((float*)data_1)[(cse_var_1 + 1)]);\n    data_red_rf[2] = (data_red_rf[2] * ((float*)data_1)[(cse_var_1 + 2)]);\n    data_red_rf[3] = (data_red_rf[3] * ((float*)data_1)[(cse_var_1 + 3)]);\n    data_red_rf[4] = (data_red_rf[4] * ((float*)data_1)[(cse_var_1 + 4)]);\n    data_red_rf[5] = (data_red_rf[5] * ((float*)data_1)[(cse_var_1 + 5)]);\n    data_red_rf[6] = (data_red_rf[6] * ((float*)data_1)[(cse_var_1 + 6)]);\n    data_red_rf[7] = (data_red_rf[7] * ((float*)data_1)[(cse_var_1 + 7)]);\n    data_red_rf[8] = (data_red_rf[8] * ((float*)data_1)[(cse_var_1 + 8)]);\n    data_red_rf[9] = (data_red_rf[9] * ((float*)data_1)[(cse_var_1 + 9)]);\n    data_red_rf[10] = (data_red_rf[10] * ((float*)data_1)[(cse_var_1 + 10)]);\n    data_red_rf[11] = (data_red_rf[11] * ((float*)data_1)[(cse_var_1 + 11)]);\n    data_red_rf[12] = (data_red_rf[12] * ((float*)data_1)[(cse_var_1 + 12)]);\n  }\n  ((float*)data_red_1)[0] = 1.000000e+00f;\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[0]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[1]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[2]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[3]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[4]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[5]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[6]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[7]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[8]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[9]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[10]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[11]);\n  ((float*)data_red_1)[0] = (((float*)data_red_1)[0] * data_red_rf[12]);\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 700)\n#define __shfl_sync(mask, var, lane, width) \\\n        __shfl((var), (lane), (width))\n\n#define __shfl_down_sync(mask, var, offset, width) \\\n        __shfl_down((var), (offset), (width))\n\n#define __shfl_up_sync(mask, var, offset, width) \\\n        __shfl_up((var), (offset), (width))\n#endif\n\n\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1617; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + (((int)threadIdx.x) >> 1)) < 25857) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 18, 17, 13), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([13], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((13,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(13):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer in range(3978):\n            cse_var_1: T.int32 = k0_k1_fused_k2_fused_k3_fused_outer * 13\n            data_1 = T.Buffer((51714,), data=data.data)\n            data_red_rf_1[0] = data_red_rf_1[0] * data_1[cse_var_1]\n            data_red_rf_1[1] = data_red_rf_1[1] * data_1[cse_var_1 + 1]\n            data_red_rf_1[2] = data_red_rf_1[2] * data_1[cse_var_1 + 2]\n            data_red_rf_1[3] = data_red_rf_1[3] * data_1[cse_var_1 + 3]\n            data_red_rf_1[4] = data_red_rf_1[4] * data_1[cse_var_1 + 4]\n            data_red_rf_1[5] = data_red_rf_1[5] * data_1[cse_var_1 + 5]\n            data_red_rf_1[6] = data_red_rf_1[6] * data_1[cse_var_1 + 6]\n            data_red_rf_1[7] = data_red_rf_1[7] * data_1[cse_var_1 + 7]\n            data_red_rf_1[8] = data_red_rf_1[8] * data_1[cse_var_1 + 8]\n            data_red_rf_1[9] = data_red_rf_1[9] * data_1[cse_var_1 + 9]\n            data_red_rf_1[10] = data_red_rf_1[10] * data_1[cse_var_1 + 10]\n            data_red_rf_1[11] = data_red_rf_1[11] * data_1[cse_var_1 + 11]\n            data_red_rf_1[12] = data_red_rf_1[12] * data_1[cse_var_1 + 12]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[0]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[1]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[2]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[3]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[4]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[5]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[6]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[7]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[8]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[9]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[10]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[11]\n        data_red_1[0] = data_red_1[0] * data_red_rf_1[12]", "op_args": [13, 18, 17, 13]}{"op_name": "global_pool_avg", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t adaptive_pool_avg_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* adaptive_pool_avg = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* adaptive_pool_avg_1 = (((DLTensor*)adaptive_pool_avg)[0].data);\n  void* default_function_adaptive_pool_avg_shape = (((DLTensor*)adaptive_pool_avg)[0].shape);\n  void* default_function_adaptive_pool_avg_strides = (((DLTensor*)adaptive_pool_avg)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_adaptive_pool_avg_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      int32_t cse_var_1 = ((ax0 * 420) + (ax1 * 60));\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[cse_var_1]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 1)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 2)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 3)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 4)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 5)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 6)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 7)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 8)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 9)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 10)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 11)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 12)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 13)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 14)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 15)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 16)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 17)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 18)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 19)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 20)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 21)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 22)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 23)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 24)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 25)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 26)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 27)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 28)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 29)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 30)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 31)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 32)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 33)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 34)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 35)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 36)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 37)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 38)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 39)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 40)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 41)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 42)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 43)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 44)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 45)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 46)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 47)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 48)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 49)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 50)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 51)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 52)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 53)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 54)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 55)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 56)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 57)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 58)]);\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + ((float*)data_1)[(cse_var_1 + 59)]);\n      ((float*)adaptive_pool_avg_1)[((ax0 * 7) + ax1)] = (adaptive_pool_sum[0] * 1.666667e-02f);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 1.666667e-02f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 20; ++rv1) {\n      adaptive_pool_sum[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] + data[(((((int)threadIdx.x) * 60) + (rv0 * 20)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 7, 3, 20), \"float32\"), adaptive_pool_avg: T.Buffer((4, 7, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax1 in range(7):\n                cse_var_1: T.int32 = ax0 * 420 + ax1 * 60\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                data_1 = T.Buffer((1680,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 1]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 2]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 3]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 4]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 5]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 6]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 7]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 8]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 9]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 10]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 11]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 12]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 13]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 14]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 15]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 16]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 17]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 18]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 19]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 20]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 21]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 22]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 23]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 24]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 25]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 26]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 27]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 28]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 29]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 30]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 31]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 32]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 33]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 34]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 35]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 36]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 37]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 38]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 39]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 40]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 41]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 42]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 43]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 44]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 45]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 46]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 47]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 48]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 49]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 50]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 51]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 52]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 53]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 54]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 55]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 56]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 57]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 58]\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[cse_var_1 + 59]\n                adaptive_pool_avg_1 = T.Buffer((28,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 7 + ax1] = adaptive_pool_sum_1[0] * T.float32(0.016666666666666666)", "op_args": [4, 7, 3, 20]}{"op_name": "round", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float roundf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3456; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = roundf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 1, 12, 16), \"float32\"), compute: T.Buffer((18, 1, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3456):\n            compute_1 = T.Buffer((3456,), data=compute.data)\n            data_1 = T.Buffer((3456,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [18, 1, 12, 16]}{"op_name": "dilate", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t DilatedInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* DilatedInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* DilatedInput_1 = (((DLTensor*)DilatedInput)[0].data);\n  void* default_function_DilatedInput_shape = (((DLTensor*)DilatedInput)[0].shape);\n  void* default_function_DilatedInput_strides = (((DLTensor*)DilatedInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_DilatedInput_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        int32_t cse_var_1 = (((i0 * 1088) + (i1 * 64)) + (i2 * 4));\n        int32_t4 v_ = int32_t4((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3));\n        *(float4*)(((float*)DilatedInput_1) + cse_var_1) = (float4(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3]));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 17, 16, 4), \"float32\"), DilatedInput: T.Buffer((8, 17, 16, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(17, 16):\n                cse_var_1: T.int32 = i0 * 1088 + i1 * 64 + i2 * 4\n                DilatedInput_1 = T.Buffer((8704,), data=DilatedInput.data)\n                data_1 = T.Buffer((8704,), data=data.data)\n                DilatedInput_1[cse_var_1:cse_var_1 + 4] = data_1[cse_var_1:cse_var_1 + 4]", "op_args": [8, 17, 16, 4]}{"op_name": "rsqrt", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 588; ++i0_i1_fused_i2_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused] = (1.000000e+00f / sqrtf(((float*)data_1)[i0_i1_fused_i2_fused]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) < 49) {\n    compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 7, 6, 1), \"float32\"), compute: T.Buffer((14, 7, 6, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(588):\n            compute_1 = T.Buffer((588,), data=compute.data)\n            data_1 = T.Buffer((588,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused] = T.rsqrt(data_1[i0_i1_fused_i2_fused])", "op_args": [14, 7, 6, 1]}{"op_name": "flatten", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i = 0; i < 20; ++i) {\n    for (int32_t j = 0; j < 2380; ++j) {\n      int32_t cse_var_1 = ((i * 2380) + j);\n      ((float*)compute_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 7, 20, 17), \"float32\"), compute: T.Buffer((20, 2380), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(20):\n            for j in range(2380):\n                cse_var_1: T.int32 = i * 2380 + j\n                compute_1 = T.Buffer((47600,), data=compute.data)\n                data_1 = T.Buffer((47600,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]", "op_args": [20, 7, 20, 17]}{"op_name": "fifo_buffer", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t buffer_code = arg_type_ids[1];\n  int32_t new_buffer_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* buffer = (((TVMValue*)args)[1].v_handle);\n  void* new_buffer = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* buffer_1 = (((DLTensor*)buffer)[0].data);\n  void* default_function_buffer_shape = (((DLTensor*)buffer)[0].shape);\n  void* default_function_buffer_strides = (((DLTensor*)buffer)[0].strides);\n  void* new_buffer_1 = (((DLTensor*)new_buffer)[0].data);\n  void* default_function_new_buffer_shape = (((DLTensor*)new_buffer)[0].shape);\n  void* default_function_new_buffer_strides = (((DLTensor*)new_buffer)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_buffer_strides == NULL)) {\n  }\n  if (!(default_function_new_buffer_strides == NULL)) {\n  }\n  for (int32_t i = 0; i < 20; ++i) {\n    for (int32_t j = 0; j < 2; ++j) {\n      for (int32_t k = 0; k < 20; ++k) {\n        for (int32_t l = 0; l < 4; ++l) {\n          int32_t cse_var_1 = ((((i * 160) + (j * 80)) + (k * 4)) + l);\n          ((float*)new_buffer_1)[cse_var_1] = ((float*)data_1)[cse_var_1];\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 2, 20, 4), \"float32\"), buffer: T.Buffer((20, 2, 20, 4), \"float32\"), new_buffer: T.Buffer((20, 2, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(20):\n            for j, k, l in T.grid(2, 20, 4):\n                cse_var_1: T.int32 = i * 160 + j * 80 + k * 4 + l\n                new_buffer_1 = T.Buffer((3200,), data=new_buffer.data)\n                data_1 = T.Buffer((3200,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [20, 2, 20, 4]}{"op_name": "shape", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_shape_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_shape = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_shape_1 = (((DLTensor*)T_shape)[0].data);\n  void* default_function_T_shape_shape = (((DLTensor*)T_shape)[0].shape);\n  void* default_function_T_shape_strides = (((DLTensor*)T_shape)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_shape_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    ((int32_t*)T_shape_1)[ax0] = ((ax0 == 3) ? 1 : ((ax0 == 2) ? 2 : ((ax0 == 1) ? 15 : 16)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape);\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 1 : ((((int)threadIdx.x) == 2) ? 2 : ((((int)threadIdx.x) == 1) ? 15 : 16)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 15, 2, 1), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 1, T.if_then_else(ax0 == 2, 2, T.if_then_else(ax0 == 1, 15, 16)))", "op_args": [16, 15, 2, 1]}{"op_name": "sigmoid", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 198) + (i2 * 18)) + i3);\n        ((float*)compute_1)[cse_var_1] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - ((float*)data_1)[cse_var_1]))));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 14) + (((int)threadIdx.x) >> 2)) < 1485) {\n    compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 2, 11, 18), \"float32\"), compute: T.Buffer((15, 2, 11, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(30):\n            for i2, i3 in T.grid(11, 18):\n                cse_var_1: T.int32 = i0_i1_fused * 198 + i2 * 18 + i3\n                compute_1 = T.Buffer((5940,), data=compute.data)\n                data_1 = T.Buffer((5940,), data=data.data)\n                compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])", "op_args": [15, 2, 11, 18]}{"op_name": "concatenate", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_a_code = arg_type_ids[0];\n  int32_t data_b_code = arg_type_ids[1];\n  int32_t T_concat_code = arg_type_ids[2];\n  void* data_a = (((TVMValue*)args)[0].v_handle);\n  void* data_b = (((TVMValue*)args)[1].v_handle);\n  void* T_concat = (((TVMValue*)args)[2].v_handle);\n  void* data_a_1 = (((DLTensor*)data_a)[0].data);\n  void* default_function_data_a_shape = (((DLTensor*)data_a)[0].shape);\n  void* default_function_data_a_strides = (((DLTensor*)data_a)[0].strides);\n  int32_t dev_id = (((DLTensor*)data_a)[0].device.device_id);\n  void* data_b_1 = (((DLTensor*)data_b)[0].data);\n  void* default_function_data_b_shape = (((DLTensor*)data_b)[0].shape);\n  void* default_function_data_b_strides = (((DLTensor*)data_b)[0].strides);\n  void* T_concat_1 = (((DLTensor*)T_concat)[0].data);\n  void* default_function_T_concat_shape = (((DLTensor*)T_concat)[0].shape);\n  void* default_function_T_concat_strides = (((DLTensor*)T_concat)[0].strides);\n  if (!(default_function_data_a_strides == NULL)) {\n  }\n  if (!(default_function_data_b_strides == NULL)) {\n  }\n  if (!(default_function_T_concat_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 32; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        int32_t cse_var_1 = (((ax0 * 80) + (ax1 * 8)) + ax2);\n        ((float*)T_concat_1)[cse_var_1] = ((16 <= ax0) ? ((float*)data_b_1)[(cse_var_1 - 1280)] : ((float*)data_a_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_concat, float* __restrict__ data_a, float* __restrict__ data_b) {\n  T_concat[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((80 <= ((int)blockIdx.x)) ? data_b[(((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) - 1280)] : data_a[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data_a: T.Buffer((16, 10, 8), \"float32\"), data_b: T.Buffer((16, 10, 8), \"float32\"), T_concat: T.Buffer((32, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(32):\n            for ax1, ax2 in T.grid(10, 8):\n                cse_var_1: T.int32 = ax0 * 80 + ax1 * 8 + ax2\n                T_concat_1 = T.Buffer((2560,), data=T_concat.data)\n                data_b_1 = T.Buffer((1280,), data=data_b.data)\n                data_a_1 = T.Buffer((1280,), data=data_a.data)\n                T_concat_1[cse_var_1] = T.if_then_else(16 <= ax0, data_b_1[cse_var_1 - 1280], data_a_1[cse_var_1])", "op_args": [16, 10, 8, 12]}{"op_name": "sign", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_sign_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_sign = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_sign_1 = (((DLTensor*)T_sign)[0].data);\n  void* default_function_T_sign_shape = (((DLTensor*)T_sign)[0].shape);\n  void* default_function_T_sign_strides = (((DLTensor*)T_sign)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_sign_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 153; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      int32_t cse_var_1 = ((ax0_ax1_fused * 224) + (ax2 * 16));\n      int32_t16 v_ = int32_t16((cse_var_1)+(1*0), (cse_var_1)+(1*1), (cse_var_1)+(1*2), (cse_var_1)+(1*3), (cse_var_1)+(1*4), (cse_var_1)+(1*5), (cse_var_1)+(1*6), (cse_var_1)+(1*7), (cse_var_1)+(1*8), (cse_var_1)+(1*9), (cse_var_1)+(1*10), (cse_var_1)+(1*11), (cse_var_1)+(1*12), (cse_var_1)+(1*13), (cse_var_1)+(1*14), (cse_var_1)+(1*15));\n      *(float16*)(((float*)T_sign_1) + cse_var_1) = ((((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f)) < (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]))) ? ((float16)(1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f, 1.000000e+00f)) : (((float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf])) < ((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))) ? ((float16)(-1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f, -1.000000e+00f)) : ((float16)(0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f, 0.000000e+00f))));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 9, 14, 16), \"float32\"), T_sign: T.Buffer((17, 9, 14, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(153):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 224 + ax2 * 16\n                T_sign_1 = T.Buffer((34272,), data=T_sign.data)\n                data_1 = T.Buffer((34272,), data=data.data)\n                T_sign_1[cse_var_1:cse_var_1 + 16] = T.Select(T.Broadcast(T.float32(0), 16) < data_1[cse_var_1:cse_var_1 + 16], T.Broadcast(T.float32(1), 16), T.Select(data_1[cse_var_1:cse_var_1 + 16] < T.Broadcast(T.float32(0), 16), T.Broadcast(T.float32(-1), 16), T.Broadcast(T.float32(0), 16)))", "op_args": [17, 9, 14, 16]}{"op_name": "depth_to_space", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t depth_to_space_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* depth_to_space = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* depth_to_space_1 = (((DLTensor*)depth_to_space)[0].data);\n  void* default_function_depth_to_space_shape = (((DLTensor*)depth_to_space)[0].shape);\n  void* default_function_depth_to_space_strides = (((DLTensor*)depth_to_space)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_depth_to_space_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      for (int32_t i3 = 0; i3 < 34; ++i3) {\n        ((float*)depth_to_space_1)[(((i0_i1_fused * 340) + (i2 * 34)) + i3)] = ((float*)data_1)[(((((((i0_i1_fused >> 2) * 1615) + ((i2 % 2) * 680)) + ((i3 % 2) * 340)) + ((i0_i1_fused & 3) * 85)) + ((i2 / 2) * 17)) + (i3 / 2))];\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) / 85) * 1615) + ((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 170) / 17) % 2) * 680)) + (((((((int)blockIdx.x) * 30) + ((int)threadIdx.x)) % 34) % 2) * 340)) + (((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) % 340) / 85) * 85)) + ((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 170) / 17) / 2) * 17)) + ((((((int)blockIdx.x) * 30) + ((int)threadIdx.x)) % 34) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 19, 5, 17), \"float32\"), depth_to_space: T.Buffer((12, 4, 10, 34), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(10, 34):\n                depth_to_space_1 = T.Buffer((16320,), data=depth_to_space.data)\n                data_1 = T.Buffer((19380,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 340 + i2 * 34 + i3] = data_1[i0_i1_fused // 4 * 1615 + T.truncmod(i2, 2) * 680 + T.truncmod(i3, 2) * 340 + i0_i1_fused % 4 * 85 + T.Div(i2, 2) * 17 + T.Div(i3, 2)]", "op_args": [12, 19, 5, 17]}{"op_name": "leaky_relu", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 12; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 12) + i3);\n      ((float*)compute_1)[cse_var_1] = ((0.000000e+00f < ((float*)data_1)[cse_var_1]) ? ((float*)data_1)[cse_var_1] : (((float*)data_1)[cse_var_1] * 5.000000e-01f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 16, 2, 12), \"float32\"), compute: T.Buffer((12, 16, 2, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            for i3 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3\n                compute_1 = T.Buffer((4608,), data=compute.data)\n                data_1 = T.Buffer((4608,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [12, 16, 2, 12]}{"op_name": "sin", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        int32_t cse_var_1 = (((i1 * 342) + (i2 * 18)) + i3);\n        ((float*)compute_1)[cse_var_1] = sinf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 2, 19, 18), \"float32\"), compute: T.Buffer((1, 2, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(2, 19, 18):\n            cse_var_1: T.int32 = i1 * 342 + i2 * 18 + i3\n            compute_1 = T.Buffer((684,), data=compute.data)\n            data_1 = T.Buffer((684,), data=data.data)\n            compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [1, 2, 19, 18]}{"op_name": "log_softmax", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float logf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)7488, 2, 32);\n  if (T_softmax_maxelem == NULL) {\n    return -1;\n  }\n  void* compute_2 = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)3744, 2, 32);\n  if (compute_2 == NULL) {\n    return -1;\n  }\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        ((float*)T_softmax_maxelem)[(((i0 * 234) + (i1 * 18)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 13; ++k) {\n          int32_t cse_var_1 = (((i0 * 234) + (i1 * 18)) + i2);\n          float v_ = ((float*)T_softmax_maxelem)[cse_var_1];\n          float v__1 = ((float*)data_1)[((((i0 * 3042) + (i1 * 234)) + (i2 * 13)) + k)];\n          ((float*)T_softmax_maxelem)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        }\n      }\n    }\n  }\n  for (int32_t i0_outer_outer_inner = 0; i0_outer_outer_inner < 2; ++i0_outer_outer_inner) {\n    for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n      for (int32_t i1_1 = 0; i1_1 < 13; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n          ((float*)compute_2)[(((i0_1 * 234) + (i1_1 * 18)) + i2_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 13; ++k_1) {\n            int32_t cse_var_4 = (i0_1 * 234);\n            int32_t cse_var_3 = (i1_1 * 18);\n            int32_t cse_var_2 = ((cse_var_4 + cse_var_3) + i2_1);\n            ((float*)compute_2)[cse_var_2] = (((float*)compute_2)[cse_var_2] + expf((((float*)data_1)[(((((i0_outer_outer_inner * 12168) + (i0_1 * 3042)) + (i1_1 * 234)) + (i2_1 * 13)) + k_1)] - ((float*)T_softmax_maxelem)[((((i0_outer_outer_inner * 936) + cse_var_4) + cse_var_3) + i2_1)])));\n          }\n        }\n      }\n    }\n    for (int32_t i1_outer_outer_inner = 0; i1_outer_outer_inner < 13; ++i1_outer_outer_inner) {\n      for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 13; ++i3_outer_outer_inner) {\n        for (int32_t i0_outer_inner = 0; i0_outer_inner < 4; ++i0_outer_inner) {\n          for (int32_t i2_outer_inner = 0; i2_outer_inner < 18; ++i2_outer_inner) {\n            int32_t cse_var_7 = (i0_outer_inner * 234);\n            int32_t cse_var_6 = (i1_outer_outer_inner * 18);\n            int32_t cse_var_5 = (((((i0_outer_outer_inner * 12168) + (i0_outer_inner * 3042)) + (i1_outer_outer_inner * 234)) + (i2_outer_inner * 13)) + i3_outer_outer_inner);\n            ((float*)compute_1)[cse_var_5] = ((((float*)data_1)[cse_var_5] - ((float*)T_softmax_maxelem)[((((i0_outer_outer_inner * 936) + cse_var_7) + cse_var_6) + i2_outer_inner)]) - logf(((float*)compute_2)[((cse_var_7 + cse_var_6) + i2_outer_inner)]));\n          }\n        }\n      }\n    }\n  }\n  if (TVMBackendFreeWorkspace(1, dev_id, compute_2) != 0) {\n    return -1;\n  }\n  if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n    return -1;\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 1521) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 13)]) - __logf(compute_1[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 13)]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 117) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 13; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 117) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 416) + (((int)threadIdx.x) * 13)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 13; ++k) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 208) + (((int)threadIdx.x) * 13)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 13, 18, 13), \"float32\"), compute: T.Buffer((8, 13, 18, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([1872], \"float32\", \"global\")\n        compute_1 = T.allocate([936], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((1872,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((24336,), data=data.data)\n        for i0, i1, i2 in T.grid(8, 13, 18):\n            T_softmax_maxelem_1[i0 * 234 + i1 * 18 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(13):\n                cse_var_1: T.int32 = i0 * 234 + i1 * 18 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 3042 + i1 * 234 + i2 * 13 + k])\n        for i0_outer_outer_inner in range(2):\n            compute_2 = T.Buffer((936,), data=compute_1)\n            for i0, i1, i2 in T.grid(4, 13, 18):\n                compute_2[i0 * 234 + i1 * 18 + i2] = T.float32(0)\n                for k in range(13):\n                    cse_var_4: T.int32 = i0 * 234\n                    cse_var_3: T.int32 = i1 * 18\n                    cse_var_2: T.int32 = cse_var_4 + cse_var_3 + i2\n                    compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0_outer_outer_inner * 12168 + i0 * 3042 + i1 * 234 + i2 * 13 + k] - T_softmax_maxelem_1[i0_outer_outer_inner * 936 + cse_var_4 + cse_var_3 + i2])\n            for i1_outer_outer_inner, i3_outer_outer_inner, i0_outer_inner, i2_outer_inner in T.grid(13, 13, 4, 18):\n                cse_var_7: T.int32 = i0_outer_inner * 234\n                cse_var_6: T.int32 = i1_outer_outer_inner * 18\n                cse_var_5: T.int32 = i0_outer_outer_inner * 12168 + i0_outer_inner * 3042 + i1_outer_outer_inner * 234 + i2_outer_inner * 13 + i3_outer_outer_inner\n                compute_3 = T.Buffer((24336,), data=compute.data)\n                compute_3[cse_var_5] = data_1[cse_var_5] - T_softmax_maxelem_1[i0_outer_outer_inner * 936 + cse_var_7 + cse_var_6 + i2_outer_inner] - T.log(compute_2[cse_var_7 + cse_var_6 + i2_outer_inner])", "op_args": [8, 13, 18, 13]}{"op_name": "sinh", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sinhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      for (int32_t i3 = 0; i3 < 9; ++i3) {\n        int32_t cse_var_1 = (((i0 * 117) + (i2 * 9)) + i3);\n        ((float*)compute_1)[cse_var_1] = sinhf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 1, 13, 9), \"float32\"), compute: T.Buffer((2, 1, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(2):\n            for i2, i3 in T.grid(13, 9):\n                cse_var_1: T.int32 = i0 * 117 + i2 * 9 + i3\n                compute_1 = T.Buffer((234,), data=compute.data)\n                data_1 = T.Buffer((234,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])", "op_args": [2, 1, 13, 9]}{"op_name": "sqrt", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3420; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = sqrtf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 855) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 5, 19, 18), \"float32\"), compute: T.Buffer((2, 5, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3420):\n            compute_1 = T.Buffer((3420,), data=compute.data)\n            data_1 = T.Buffer((3420,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [2, 5, 19, 18]}{"op_name": "lrn", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float powf(float, float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_divide_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_divide = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_divide_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 63; ++ax0_ax1_fused) {\n    float tensor[2];\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      int32_t cse_var_2 = ((ax0_ax1_fused * 24) + (ax2 * 2));\n      int32_t cse_var_1 = (cse_var_2 + 1);\n      tensor[0] = 0.000000e+00f;\n      tensor[0] = (tensor[0] + (((float*)data_1)[cse_var_2] * ((float*)data_1)[cse_var_2]));\n      tensor[1] = 0.000000e+00f;\n      tensor[1] = (tensor[1] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n      for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n        int32_t cse_var_3 = (cse_var_2 + ax3);\n        ((float*)T_divide_1)[cse_var_3] = (((float*)data_1)[cse_var_3] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3])), 7.500000e-01f));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 189) {\n    T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])), 7.500000e-01f));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 9, 12, 2), \"float32\"), T_divide: T.Buffer((7, 9, 12, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(63):\n            tensor = T.allocate([2], \"float32\", \"global\")\n            for ax2 in range(12):\n                cse_var_2: T.int32 = ax0_ax1_fused * 24 + ax2 * 2\n                cse_var_1: T.int32 = cse_var_2 + 1\n                tensor_1 = T.Buffer((2,), data=tensor, align=8)\n                tensor_1[0] = T.float32(0)\n                data_1 = T.Buffer((1512,), data=data.data)\n                tensor_1[0] = tensor_1[0] + data_1[cse_var_2] * data_1[cse_var_2]\n                tensor_1[1] = T.float32(0)\n                tensor_1[1] = tensor_1[1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(2):\n                    cse_var_3: T.int32 = cse_var_2 + ax3\n                    T_divide_1 = T.Buffer((1512,), data=T_divide.data)\n                    T_divide_1[cse_var_3] = data_1[cse_var_3] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))", "op_args": [7, 9, 12, 2]}{"op_name": "tan", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        int32_t cse_var_1 = (((i0_i1_fused * 60) + (i2 * 12)) + i3);\n        ((float*)compute_1)[cse_var_1] = tanf(((float*)data_1)[cse_var_1]);\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 10, 5, 12), \"float32\"), compute: T.Buffer((14, 10, 5, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(140):\n            for i2, i3 in T.grid(5, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 60 + i2 * 12 + i3\n                compute_1 = T.Buffer((8400,), data=compute.data)\n                data_1 = T.Buffer((8400,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [14, 10, 5, 12]}{"op_name": "mirror_pad", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t MirrorPadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* MirrorPadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* MirrorPadInput_1 = (((DLTensor*)MirrorPadInput)[0].data);\n  void* default_function_MirrorPadInput_shape = (((DLTensor*)MirrorPadInput)[0].shape);\n  void* default_function_MirrorPadInput_strides = (((DLTensor*)MirrorPadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_MirrorPadInput_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 255; ++i0_i1_fused) {\n    int32_t cse_var_2 = (i0_i1_fused / 17);\n    int32_t cse_var_1 = (i0_i1_fused % 17);\n    ((float*)MirrorPadInput_1)[i0_i1_fused] = ((float*)data_1)[((((221 <= i0_i1_fused) ? (24 - cse_var_2) : ((i0_i1_fused < 17) ? 0 : (cse_var_2 - 1))) * 14) + ((cse_var_1 == 16) ? 13 : ((cse_var_1 < 2) ? (1 - cse_var_1) : (cse_var_1 - 2))))];\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 21) + (((int)threadIdx.x) / 3)) < 85) {\n    MirrorPadInput[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = data[((((221 <= ((((int)blockIdx.x) * 63) + ((int)threadIdx.x))) ? (24 - (((((int)blockIdx.x) * 63) + ((int)threadIdx.x)) / 17)) : ((((((int)blockIdx.x) * 63) + ((int)threadIdx.x)) < 17) ? 0 : ((((((int)blockIdx.x) * 63) + ((int)threadIdx.x)) / 17) - 1))) * 14) + (((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 17) == 16) ? (29 - (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 17)) : (((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 17) < 2) ? (1 - (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 17)) : ((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 17) - 2))))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 14), \"float32\"), MirrorPadInput: T.Buffer((15, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(255):\n            cse_var_2: T.int32 = i0_i1_fused // 17\n            cse_var_1: T.int32 = i0_i1_fused % 17\n            MirrorPadInput_1 = T.Buffer((255,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((168,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(221 <= i0_i1_fused, 24 - cse_var_2, T.if_then_else(i0_i1_fused < 17, 0, cse_var_2 - 1)) * 14 + T.if_then_else(cse_var_1 == 16, 13, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]", "op_args": [6, 6, 12, 14]}{"op_name": "tanh", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float tanhf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 85500; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = tanhf(((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]);\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 15, 20, 19), \"float32\"), compute: T.Buffer((15, 15, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(85500):\n            compute_1 = T.Buffer((85500,), data=compute.data)\n            data_1 = T.Buffer((85500,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tanh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [15, 15, 20, 19]}{"op_name": "pad", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t PadInput_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* PadInput = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* PadInput_1 = (((DLTensor*)PadInput)[0].data);\n  void* default_function_PadInput_shape = (((DLTensor*)PadInput)[0].shape);\n  void* default_function_PadInput_strides = (((DLTensor*)PadInput)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_PadInput_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      ((float*)PadInput_1)[((i0 * 20) + i1)] = (((((1 <= i0) && (i0 < 17)) && (2 <= i1)) && (i1 < 19)) ? ((float*)data_1)[(((i0 * 17) + i1) - 19)] : 0.000000e+00f);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 95) {\n    PadInput[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (((((5 <= ((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2))) && (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 85)) && (1 <= (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 1)) % 10))) && ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 20) < 19)) ? data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 5) * 17) + (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 20)) - 19)] : 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 17), \"float32\"), PadInput: T.Buffer((19, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(19):\n            for i1 in range(20):\n                PadInput_1 = T.Buffer((380,), data=PadInput.data)\n                data_1 = T.Buffer((272,), data=data.data)\n                PadInput_1[i0 * 20 + i1] = T.if_then_else(1 <= i0 and i0 < 17 and 2 <= i1 and i1 < 19, data_1[i0 * 17 + i1 - 19], T.float32(0))", "op_args": [19, 19, 16, 17]}{"op_name": "pool1d", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 10; ++ax0_ax1_fused) {\n    float pad_temp[11];\n    for (int32_t ax1_ax2_fused_s = 0; ax1_ax2_fused_s < 11; ++ax1_ax2_fused_s) {\n      pad_temp[ax1_ax2_fused_s] = (((1 <= ax1_ax2_fused_s) && (ax1_ax2_fused_s < 10)) ? ((float*)data_1)[(((ax0_ax1_fused * 9) + ax1_ax2_fused_s) - 1)] : -3.402823e+38f);\n    }\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      ((float*)pool_max_1)[((ax0_ax1_fused * 5) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        int32_t cse_var_1 = ((ax0_ax1_fused * 5) + ax2);\n        float v_ = ((float*)pool_max_1)[cse_var_1];\n        float v__1 = pad_temp[((ax2 * 2) + rv0)];\n        ((float*)pool_max_1)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 1)) < 25) {\n    pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 1)) < 25) {\n      pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], (((1 <= ((((((int)blockIdx.x) + ((int)threadIdx.x)) % 5) * 2) + rv0)) && (((rv0 >> 1) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 5)) < 5)) ? data[(((((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) / 5) * 9) + (((((int)blockIdx.x) + ((int)threadIdx.x)) % 5) * 2)) + rv0) - 1)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 1, 9), \"float32\"), pool_max: T.Buffer((10, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(10):\n            pad_temp = T.allocate([11], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((11,), data=pad_temp, align=32)\n            for ax1_ax2_fused_s in range(11):\n                data_1 = T.Buffer((90,), data=data.data)\n                pad_temp_1[ax1_ax2_fused_s] = T.if_then_else(1 <= ax1_ax2_fused_s and ax1_ax2_fused_s < 10, data_1[ax0_ax1_fused * 9 + ax1_ax2_fused_s - 1], T.float32(-3.4028234663852886e+38))\n            for ax2 in range(5):\n                pool_max_1 = T.Buffer((50,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 5 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 2 + rv0])", "op_args": [10, 1, 11, 9]}{"op_name": "pool2d", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[9];\n    for (int32_t ax3 = 0; ax3 < 30; ++ax3) {\n      int32_t cse_var_6 = (ax0_ax1_fused_ax2_fused % 3);\n      bool cse_var_5 = (1 <= ax3);\n      bool cse_var_4 = (cse_var_6 < 2);\n      bool cse_var_3 = (1 <= cse_var_6);\n      int32_t cse_var_2 = ((ax0_ax1_fused_ax2_fused * 30) + ax3);\n      int32_t cse_var_1 = ((((ax0_ax1_fused_ax2_fused / 3) * 300) + (cse_var_6 * 120)) + (ax3 * 2));\n      pad_temp[0] = ((cse_var_3 && cse_var_5) ? ((float*)data_1)[(cse_var_1 - 61)] : -3.402823e+38f);\n      pad_temp[1] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 60)] : -3.402823e+38f);\n      pad_temp[2] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 59)] : -3.402823e+38f);\n      pad_temp[3] = (cse_var_5 ? ((float*)data_1)[(cse_var_1 - 1)] : -3.402823e+38f);\n      pad_temp[4] = ((float*)data_1)[cse_var_1];\n      pad_temp[5] = ((float*)data_1)[(cse_var_1 + 1)];\n      pad_temp[6] = ((cse_var_4 && cse_var_5) ? ((float*)data_1)[(cse_var_1 + 59)] : -3.402823e+38f);\n      pad_temp[7] = (cse_var_4 ? ((float*)data_1)[(cse_var_1 + 60)] : -3.402823e+38f);\n      pad_temp[8] = (cse_var_4 ? ((float*)data_1)[(cse_var_1 + 61)] : -3.402823e+38f);\n      ((float*)pool_max_1)[cse_var_2] = -3.402823e+38f;\n      float v_ = ((float*)pool_max_1)[cse_var_2];\n      float v__1 = pad_temp[0];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__1) ? (v_) : (v__1));\n      float v__2 = pad_temp[1];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__2) ? (v_) : (v__2));\n      float v__3 = pad_temp[2];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__3) ? (v_) : (v__3));\n      float v__4 = pad_temp[3];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__4) ? (v_) : (v__4));\n      float v__5 = pad_temp[4];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__5) ? (v_) : (v__5));\n      float v__6 = pad_temp[5];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__6) ? (v_) : (v__6));\n      float v__7 = pad_temp[6];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__7) ? (v_) : (v__7));\n      float v__8 = pad_temp[7];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__8) ? (v_) : (v__8));\n      float v__9 = pad_temp[8];\n      ((float*)pool_max_1)[cse_var_2] = ((v_) > (v__9) ? (v_) : (v__9));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 5) * 3) + (((int)threadIdx.x) / 6)) / 5) * 2) + rv0)) && ((((((((int)blockIdx.x) % 5) * 3) + (((int)threadIdx.x) / 6)) / 5) + (rv0 >> 1)) < 3)) && (1 <= (((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 30) * 2) + rv1))) ? data[(((((((((int)blockIdx.x) / 5) * 300) + (((((((int)blockIdx.x) % 5) * 3) + (((int)threadIdx.x) / 6)) / 5) * 120)) + (rv0 * 60)) + ((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 30) * 2)) + rv1) - 61)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 5, 60), \"float32\"), pool_max: T.Buffer((10, 8, 3, 30), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax3 in range(30):\n                cse_var_6: T.int32 = ax0_ax1_fused_ax2_fused % 3\n                cse_var_5: T.bool = 1 <= ax3\n                cse_var_4: T.bool = cse_var_6 < 2\n                cse_var_3: T.bool = 1 <= cse_var_6\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 30 + ax3\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused // 3 * 300 + cse_var_6 * 120 + ax3 * 2\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                data_1 = T.Buffer((24000,), data=data.data)\n                pad_temp_1[0] = T.if_then_else(cse_var_3 and cse_var_5, data_1[cse_var_1 - 61], T.float32(-3.4028234663852886e+38))\n                pad_temp_1[1] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 60], T.float32(-3.4028234663852886e+38))\n                pad_temp_1[2] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 59], T.float32(-3.4028234663852886e+38))\n                pad_temp_1[3] = T.if_then_else(cse_var_5, data_1[cse_var_1 - 1], T.float32(-3.4028234663852886e+38))\n                pad_temp_1[4] = data_1[cse_var_1]\n                pad_temp_1[5] = data_1[cse_var_1 + 1]\n                pad_temp_1[6] = T.if_then_else(cse_var_4 and cse_var_5, data_1[cse_var_1 + 59], T.float32(-3.4028234663852886e+38))\n                pad_temp_1[7] = T.if_then_else(cse_var_4, data_1[cse_var_1 + 60], T.float32(-3.4028234663852886e+38))\n                pad_temp_1[8] = T.if_then_else(cse_var_4, data_1[cse_var_1 + 61], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((7200,), data=pool_max.data)\n                pool_max_1[cse_var_2] = T.float32(-3.4028234663852886e+38)\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[0])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[1])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[2])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[3])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[4])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[5])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[6])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[7])\n                pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[8])", "op_args": [10, 8, 5, 20]}{"op_name": "pool3d", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t pool_max_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* pool_max = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* pool_max_1 = (((DLTensor*)pool_max)[0].data);\n  void* default_function_pool_max_shape = (((DLTensor*)pool_max)[0].shape);\n  void* default_function_pool_max_strides = (((DLTensor*)pool_max)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_pool_max_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 450; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[27];\n    for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n      for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n        for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n          bool cse_var_6 = (1 <= ax4);\n          int32_t cse_var_5 = (ax2 * 9);\n          bool cse_var_4 = (1 <= (((ax0_ax1_fused_ax2_fused % 6) * 2) + ax2));\n          bool cse_var_3 = (cse_var_4 && cse_var_6);\n          bool cse_var_2 = (cse_var_4 && (1 <= ax3));\n          int32_t cse_var_1 = ((((ax0_ax1_fused_ax2_fused * 640) + (ax2 * 320)) + (ax3 * 32)) + (ax4 * 2));\n          pad_temp[cse_var_5] = ((cse_var_2 && cse_var_6) ? ((float*)data_1)[(cse_var_1 - 337)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 1)] = (cse_var_2 ? ((float*)data_1)[(cse_var_1 - 336)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 2)] = (cse_var_2 ? ((float*)data_1)[(cse_var_1 - 335)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 3)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 321)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 4)] = (cse_var_4 ? ((float*)data_1)[(cse_var_1 - 320)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 5)] = (cse_var_4 ? ((float*)data_1)[(cse_var_1 - 319)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 6)] = (cse_var_3 ? ((float*)data_1)[(cse_var_1 - 305)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 7)] = (cse_var_4 ? ((float*)data_1)[(cse_var_1 - 304)] : -3.402823e+38f);\n          pad_temp[(cse_var_5 + 8)] = (cse_var_4 ? ((float*)data_1)[(cse_var_1 - 303)] : -3.402823e+38f);\n        }\n        ((float*)pool_max_1)[(((ax0_ax1_fused_ax2_fused * 80) + (ax3 * 8)) + ax4)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          int32_t cse_var_8 = (rv0 * 9);\n          int32_t cse_var_7 = (((ax0_ax1_fused_ax2_fused * 80) + (ax3 * 8)) + ax4);\n          float v_ = ((float*)pool_max_1)[cse_var_7];\n          float v__1 = pad_temp[cse_var_8];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__1) ? (v_) : (v__1));\n          float v__2 = pad_temp[(cse_var_8 + 1)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__2) ? (v_) : (v__2));\n          float v__3 = pad_temp[(cse_var_8 + 2)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__3) ? (v_) : (v__3));\n          float v__4 = pad_temp[(cse_var_8 + 3)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__4) ? (v_) : (v__4));\n          float v__5 = pad_temp[(cse_var_8 + 4)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__5) ? (v_) : (v__5));\n          float v__6 = pad_temp[(cse_var_8 + 5)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__6) ? (v_) : (v__6));\n          float v__7 = pad_temp[(cse_var_8 + 6)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__7) ? (v_) : (v__7));\n          float v__8 = pad_temp[(cse_var_8 + 7)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__8) ? (v_) : (v__8));\n          float v__9 = pad_temp[(cse_var_8 + 8)];\n          ((float*)pool_max_1)[cse_var_7] = ((v_) > (v__9) ? (v_) : (v__9));\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) & 15) * 3) + (((int)threadIdx.x) / 10)) >> 3) * 2) + rv0)) && (1 <= ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 40) >> 2) * 2) + rv1))) && (1 <= (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) >> 3) * 640) + (rv0 * 320)) + (((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 40) >> 2) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 337)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 15, 12, 20, 16), \"float32\"), pool_max: T.Buffer((5, 15, 6, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(450):\n            pad_temp = T.allocate([27], \"float32\", \"global\")\n            for ax3, ax4 in T.grid(10, 8):\n                pad_temp_1 = T.Buffer((27,), data=pad_temp)\n                for ax2 in range(3):\n                    cse_var_6: T.bool = 1 <= ax4\n                    cse_var_5: T.int32 = ax2 * 9\n                    cse_var_4: T.bool = 1 <= ax0_ax1_fused_ax2_fused % 6 * 2 + ax2\n                    cse_var_3: T.bool = cse_var_4 and cse_var_6\n                    cse_var_2: T.bool = cse_var_4 and 1 <= ax3\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 640 + ax2 * 320 + ax3 * 32 + ax4 * 2\n                    data_1 = T.Buffer((288000,), data=data.data)\n                    pad_temp_1[cse_var_5] = T.if_then_else(cse_var_2 and cse_var_6, data_1[cse_var_1 - 337], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 1] = T.if_then_else(cse_var_2, data_1[cse_var_1 - 336], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 2] = T.if_then_else(cse_var_2, data_1[cse_var_1 - 335], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 3] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 321], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 4] = T.if_then_else(cse_var_4, data_1[cse_var_1 - 320], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 5] = T.if_then_else(cse_var_4, data_1[cse_var_1 - 319], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 6] = T.if_then_else(cse_var_3, data_1[cse_var_1 - 305], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 7] = T.if_then_else(cse_var_4, data_1[cse_var_1 - 304], T.float32(-3.4028234663852886e+38))\n                    pad_temp_1[cse_var_5 + 8] = T.if_then_else(cse_var_4, data_1[cse_var_1 - 303], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((36000,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 80 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_8: T.int32 = rv0 * 9\n                    cse_var_7: T.int32 = ax0_ax1_fused_ax2_fused * 80 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 1])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 2])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 3])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 4])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 5])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 6])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 7])\n                    pool_max_1[cse_var_7] = T.max(pool_max_1[cse_var_7], pad_temp_1[cse_var_8 + 8])", "op_args": [5, 15, 12, 20]}{"op_name": "relu", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t compute_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* compute = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 702; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      int32_t cse_var_1 = ((i0_i1_fused_i2_fused * 19) + i3);\n      float v_ = ((float*)data_1)[cse_var_1];\n      ((float*)compute_1)[cse_var_1] = ((v_) > (0.000000e+00f) ? (v_) : (0.000000e+00f));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 13, 18, 19), \"float32\"), compute: T.Buffer((3, 13, 18, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(702):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((13338,), data=compute.data)\n                data_1 = T.Buffer((13338,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))", "op_args": [3, 13, 18, 19]}{"op_name": "scale_shift_nchw", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_Scale_strides == NULL)) {\n  }\n  if (!(default_function_Shift_strides == NULL)) {\n  }\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  }\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused < 2352; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 12; ++i_outer_inner) {\n      int32_t cse_var_2 = ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused % 336) / 28);\n      int32_t cse_var_1 = ((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused / 336) * 4032) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused & 1) * 2016)) + (cse_var_2 * 168)) + (i_outer_inner * 14)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused % 28) >> 1));\n      ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 252) / 21)]) + Shift[((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 252) / 21)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 12, 12, 14), \"float32\"), Scale: T.Buffer((12,), \"float32\"), Shift: T.Buffer((12,), \"float32\"), ScaleShift: T.Buffer((14, 12, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused in T.parallel(2352):\n            for i_outer_inner in range(12):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused % 336 // 28\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused // 336 * 4032 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused % 2 * 2016 + cse_var_2 * 168 + i_outer_inner * 14 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused % 28 // 2\n                ScaleShift_1 = T.Buffer((28224,), data=ScaleShift.data)\n                data_1 = T.Buffer((28224,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [14, 12, 12, 14]}{"op_name": "prelu", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* compute_1 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_Scale_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3230; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_1)[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < ((float*)data_1)[i0_i1_fused_i2_fused_i3_fused]) ? ((float*)data_1)[i0_i1_fused_i2_fused_i3_fused] : (((float*)data_1)[i0_i1_fused_i2_fused_i3_fused] * ((float*)Scale_1)[(i0_i1_fused_i2_fused_i3_fused % 19)]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) % 19)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 17, 1, 19), \"float32\"), Scale: T.Buffer((19,), \"float32\"), compute: T.Buffer((10, 17, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3230):\n            compute_1 = T.Buffer((3230,), data=compute.data)\n            data_1 = T.Buffer((3230,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 19])", "op_args": [10, 17, 1, 19]}{"op_name": "scale_shift_nchwc", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t Scale_code = arg_type_ids[1];\n  int32_t Shift_code = arg_type_ids[2];\n  int32_t ScaleShift_code = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* Scale = (((TVMValue*)args)[1].v_handle);\n  void* Shift = (((TVMValue*)args)[2].v_handle);\n  void* ScaleShift = (((TVMValue*)args)[3].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* Scale_1 = (((DLTensor*)Scale)[0].data);\n  void* default_function_Scale_shape = (((DLTensor*)Scale)[0].shape);\n  void* default_function_Scale_strides = (((DLTensor*)Scale)[0].strides);\n  void* Shift_1 = (((DLTensor*)Shift)[0].data);\n  void* default_function_Shift_shape = (((DLTensor*)Shift)[0].shape);\n  void* default_function_Shift_strides = (((DLTensor*)Shift)[0].strides);\n  void* ScaleShift_1 = (((DLTensor*)ScaleShift)[0].data);\n  void* default_function_ScaleShift_shape = (((DLTensor*)ScaleShift)[0].shape);\n  void* default_function_ScaleShift_strides = (((DLTensor*)ScaleShift)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_Scale_strides == NULL)) {\n  }\n  if (!(default_function_Shift_strides == NULL)) {\n  }\n  if (!(default_function_ScaleShift_strides == NULL)) {\n  }\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused < 1088; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 6; ++j_outer_inner) {\n      for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n          int32_t cse_var_3 = ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 34) >> 1);\n          int32_t cse_var_2 = (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 544) * 17) + cse_var_3);\n          int32_t cse_var_1 = ((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 13056) + (b_inner * 6528)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 68) * 408)) + (i_inner * 204)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 68) / 34) * 102)) + (j_outer_inner * 17)) + cse_var_3);\n          ((float*)ScaleShift_1)[cse_var_1] = ((((float*)data_1)[cse_var_1] * ((float*)Scale_1)[cse_var_2]) + ((float*)Shift_1)[cse_var_2]);\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 136) / 68) * 17) + (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 17))]) + Shift[((((((int)blockIdx.x) % 136) / 68) * 17) + (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 17))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 2, 16, 12, 17), \"float32\"), Scale: T.Buffer((2, 17), \"float32\"), Shift: T.Buffer((2, 17), \"float32\"), ScaleShift: T.Buffer((4, 2, 16, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused in T.parallel(1088):\n            for j_outer_inner, b_inner, i_inner in T.grid(6, 2, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 34 // 2\n                cse_var_2: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 544 * 17 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 2 * 13056 + b_inner * 6528 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 68 * 408 + i_inner * 204 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 68 // 34 * 102 + j_outer_inner * 17 + cse_var_3\n                ScaleShift_1 = T.Buffer((26112,), data=ScaleShift.data)\n                data_1 = T.Buffer((26112,), data=data.data)\n                Scale_1 = T.Buffer((34,), data=Scale.data)\n                Shift_1 = T.Buffer((34,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [4, 17, 16, 12]}{"op_name": "softmax", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float expf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    void* T_softmax_exp = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)16128, 2, 32);\n    if (T_softmax_exp == NULL) {\n      return -1;\n    }\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[14];\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        int32_t cse_var_1 = (((i0 * 4032) + (i1 * 252)) + (i2 * 18));\n        T_softmax_maxelem[0] = -3.402823e+38f;\n        float v_ = T_softmax_maxelem[0];\n        float v__1 = ((float*)data_1)[cse_var_1];\n        T_softmax_maxelem[0] = ((v_) > (v__1) ? (v_) : (v__1));\n        float v__2 = ((float*)data_1)[(cse_var_1 + 1)];\n        T_softmax_maxelem[0] = ((v_) > (v__2) ? (v_) : (v__2));\n        float v__3 = ((float*)data_1)[(cse_var_1 + 2)];\n        T_softmax_maxelem[0] = ((v_) > (v__3) ? (v_) : (v__3));\n        float v__4 = ((float*)data_1)[(cse_var_1 + 3)];\n        T_softmax_maxelem[0] = ((v_) > (v__4) ? (v_) : (v__4));\n        float v__5 = ((float*)data_1)[(cse_var_1 + 4)];\n        T_softmax_maxelem[0] = ((v_) > (v__5) ? (v_) : (v__5));\n        float v__6 = ((float*)data_1)[(cse_var_1 + 5)];\n        T_softmax_maxelem[0] = ((v_) > (v__6) ? (v_) : (v__6));\n        float v__7 = ((float*)data_1)[(cse_var_1 + 6)];\n        T_softmax_maxelem[0] = ((v_) > (v__7) ? (v_) : (v__7));\n        float v__8 = ((float*)data_1)[(cse_var_1 + 7)];\n        T_softmax_maxelem[0] = ((v_) > (v__8) ? (v_) : (v__8));\n        float v__9 = ((float*)data_1)[(cse_var_1 + 8)];\n        T_softmax_maxelem[0] = ((v_) > (v__9) ? (v_) : (v__9));\n        float v__10 = ((float*)data_1)[(cse_var_1 + 9)];\n        T_softmax_maxelem[0] = ((v_) > (v__10) ? (v_) : (v__10));\n        float v__11 = ((float*)data_1)[(cse_var_1 + 10)];\n        T_softmax_maxelem[0] = ((v_) > (v__11) ? (v_) : (v__11));\n        float v__12 = ((float*)data_1)[(cse_var_1 + 11)];\n        T_softmax_maxelem[0] = ((v_) > (v__12) ? (v_) : (v__12));\n        float v__13 = ((float*)data_1)[(cse_var_1 + 12)];\n        T_softmax_maxelem[0] = ((v_) > (v__13) ? (v_) : (v__13));\n        float v__14 = ((float*)data_1)[(cse_var_1 + 13)];\n        T_softmax_maxelem[0] = ((v_) > (v__14) ? (v_) : (v__14));\n        float v__15 = ((float*)data_1)[(cse_var_1 + 14)];\n        T_softmax_maxelem[0] = ((v_) > (v__15) ? (v_) : (v__15));\n        float v__16 = ((float*)data_1)[(cse_var_1 + 15)];\n        T_softmax_maxelem[0] = ((v_) > (v__16) ? (v_) : (v__16));\n        float v__17 = ((float*)data_1)[(cse_var_1 + 16)];\n        T_softmax_maxelem[0] = ((v_) > (v__17) ? (v_) : (v__17));\n        float v__18 = ((float*)data_1)[(cse_var_1 + 17)];\n        T_softmax_maxelem[0] = ((v_) > (v__18) ? (v_) : (v__18));\n        for (int32_t i3 = 0; i3 < 18; ++i3) {\n          ((float*)T_softmax_exp)[(((i1 * 252) + (i2 * 18)) + i3)] = expf((((float*)data_1)[(cse_var_1 + i3)] - T_softmax_maxelem[0]));\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 16; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        int32_t cse_var_2 = ((i1_1 * 252) + (i2_1 * 18));\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[cse_var_2]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 1)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 2)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 3)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 4)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 5)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 6)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 7)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 8)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 9)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 10)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 11)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 12)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 13)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 14)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 15)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 16)]);\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + ((float*)T_softmax_exp)[(cse_var_2 + 17)]);\n      }\n      for (int32_t i2_2 = 0; i2_2 < 14; ++i2_2) {\n        for (int32_t i3_1 = 0; i3_1 < 18; ++i3_1) {\n          int32_t cse_var_4 = (i1_1 * 252);\n          int32_t cse_var_3 = (i2_2 * 18);\n          ((float*)T_softmax_norm_1)[((((i0 * 4032) + cse_var_4) + cse_var_3) + i3_1)] = (((float*)T_softmax_exp)[((cse_var_4 + cse_var_3) + i3_1)] / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_exp) != 0) {\n      return -1;\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 18; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) / 9)])) / T_softmax_expsum[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) / 9)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 18; ++k) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 16, 14, 18), \"float32\"), T_softmax_norm: T.Buffer((18, 16, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            T_softmax_exp = T.allocate([4032], \"float32\", \"global\")\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([14], \"float32\", \"global\")\n            T_softmax_exp_1 = T.Buffer((4032,), data=T_softmax_exp)\n            for i1, i2 in T.grid(16, 14):\n                cse_var_1: T.int32 = i0 * 4032 + i1 * 252 + i2 * 18\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((72576,), data=data.data)\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 1])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 2])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 3])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 4])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 5])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 6])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 7])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 8])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 9])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 10])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 11])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 12])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 13])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 14])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 15])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 16])\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1 + 17])\n                for i3 in range(18):\n                    T_softmax_exp_1[i1 * 252 + i2 * 18 + i3] = T.exp(data_1[cse_var_1 + i3] - T_softmax_maxelem_1[0])\n            for i1 in range(16):\n                T_softmax_expsum_1 = T.Buffer((14,), data=T_softmax_expsum, align=32)\n                for i2 in range(14):\n                    cse_var_2: T.int32 = i1 * 252 + i2 * 18\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 1]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 2]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 3]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 4]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 5]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 6]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 7]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 8]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 9]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 10]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 11]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 12]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 13]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 14]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 15]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 16]\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T_softmax_exp_1[cse_var_2 + 17]\n                for i2, i3 in T.grid(14, 18):\n                    cse_var_4: T.int32 = i1 * 252\n                    cse_var_3: T.int32 = i2 * 18\n                    T_softmax_norm_1 = T.Buffer((72576,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[i0 * 4032 + cse_var_4 + cse_var_3 + i3] = T_softmax_exp_1[cse_var_4 + cse_var_3 + i3] / T_softmax_expsum_1[i2]", "op_args": [18, 16, 14, 18]}{"op_name": "softmax_common", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float floorf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t T_softmax_norm_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* T_softmax_norm = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* T_softmax_norm_1 = (((DLTensor*)T_softmax_norm)[0].data);\n  void* default_function_T_softmax_norm_shape = (((DLTensor*)T_softmax_norm)[0].shape);\n  void* default_function_T_softmax_norm_strides = (((DLTensor*)T_softmax_norm)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_T_softmax_norm_strides == NULL)) {\n  }\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    void* T_softmax_maxelem = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1224, 2, 32);\n    if (T_softmax_maxelem == NULL) {\n      return -1;\n    }\n    void* T_softmax_expsum = TVMBackendAllocWorkspace(1, dev_id, (uint64_t)1224, 2, 32);\n    if (T_softmax_expsum == NULL) {\n      return -1;\n    }\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        ((float*)T_softmax_maxelem)[((i1 * 17) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 14; ++k) {\n          int32_t cse_var_1 = ((i1 * 17) + i2);\n          float v_ = ((float*)T_softmax_maxelem)[cse_var_1];\n          float v__1 = ((float*)data_1)[((((i0 * 4284) + (i1 * 238)) + (i2 * 14)) + k)];\n          ((float*)T_softmax_maxelem)[cse_var_1] = ((v_) > (v__1) ? (v_) : (v__1));\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 18; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n        ((float*)T_softmax_expsum)[((i1_1 * 17) + i2_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 14; ++k_1) {\n          int32_t cse_var_3 = ((i1_1 * 17) + i2_1);\n          int32_t cse_var_2 = ((((i0 * 4284) + (i1_1 * 238)) + (i2_1 * 14)) + k_1);\n            float v__2 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n            float v__3 = (v__2) < (8.837627e+01f) ? (v__2) : (8.837627e+01f);\n            int32_t v__4 = ((int32_t)(floorf(((((v__3) > (-8.837626e+01f) ? (v__3) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          float v__5 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__6 = (v__5) < (8.837627e+01f) ? (v__5) : (8.837627e+01f);\n          float v__7 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__8 = (v__7) < (8.837627e+01f) ? (v__7) : (8.837627e+01f);\n          float v__9 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__10 = (v__9) < (8.837627e+01f) ? (v__9) : (8.837627e+01f);\n          float v__11 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__12 = (v__11) < (8.837627e+01f) ? (v__11) : (8.837627e+01f);\n          float v__13 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__14 = (v__13) < (8.837627e+01f) ? (v__13) : (8.837627e+01f);\n          float v__15 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__16 = (v__15) < (8.837627e+01f) ? (v__15) : (8.837627e+01f);\n          float v__17 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__18 = (v__17) < (8.837627e+01f) ? (v__17) : (8.837627e+01f);\n          float v__19 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__20 = (v__19) < (8.837627e+01f) ? (v__19) : (8.837627e+01f);\n          float v__21 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__22 = (v__21) < (8.837627e+01f) ? (v__21) : (8.837627e+01f);\n          float v__23 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__24 = (v__23) < (8.837627e+01f) ? (v__23) : (8.837627e+01f);\n          float v__25 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__26 = (v__25) < (8.837627e+01f) ? (v__25) : (8.837627e+01f);\n          float v__27 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__28 = (v__27) < (8.837627e+01f) ? (v__27) : (8.837627e+01f);\n          float v__29 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__30 = (v__29) < (8.837627e+01f) ? (v__29) : (8.837627e+01f);\n          float v__31 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__32 = (v__31) < (8.837627e+01f) ? (v__31) : (8.837627e+01f);\n          float v__33 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__34 = (v__33) < (8.837627e+01f) ? (v__33) : (8.837627e+01f);\n          float v__35 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          float v__36 = (v__35) < (8.837627e+01f) ? (v__35) : (8.837627e+01f);\n          float v__37 = (*(float *)(&(v__4))) * ((((((((((((((1.987569e-04f * (((v__6) > (-8.837626e+01f) ? (v__6) : (-8.837626e+01f)) - (floorf(((((v__8) > (-8.837626e+01f) ? (v__8) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__10) > (-8.837626e+01f) ? (v__10) : (-8.837626e+01f)) - (floorf(((((v__12) > (-8.837626e+01f) ? (v__12) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__14) > (-8.837626e+01f) ? (v__14) : (-8.837626e+01f)) - (floorf(((((v__16) > (-8.837626e+01f) ? (v__16) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__18) > (-8.837626e+01f) ? (v__18) : (-8.837626e+01f)) - (floorf(((((v__20) > (-8.837626e+01f) ? (v__20) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__22) > (-8.837626e+01f) ? (v__22) : (-8.837626e+01f)) - (floorf(((((v__24) > (-8.837626e+01f) ? (v__24) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__26) > (-8.837626e+01f) ? (v__26) : (-8.837626e+01f)) - (floorf(((((v__28) > (-8.837626e+01f) ? (v__28) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__30) > (-8.837626e+01f) ? (v__30) : (-8.837626e+01f)) - (floorf(((((v__32) > (-8.837626e+01f) ? (v__32) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__34) > (-8.837626e+01f) ? (v__34) : (-8.837626e+01f)) - (floorf(((((v__36) > (-8.837626e+01f) ? (v__36) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n          float v__38 = ((float*)data_1)[cse_var_2] - ((float*)T_softmax_maxelem)[cse_var_3];\n          ((float*)T_softmax_expsum)[cse_var_3] = (((float*)T_softmax_expsum)[cse_var_3] + ((v__37) > (v__38) ? (v__37) : (v__38)));\n        }\n      }\n    }\n    for (int32_t i1_2 = 0; i1_2 < 18; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 17; ++i2_2) {\n        for (int32_t i3 = 0; i3 < 14; ++i3) {\n          int32_t cse_var_5 = ((i1_2 * 17) + i2_2);\n          int32_t cse_var_4 = ((((i0 * 4284) + (i1_2 * 238)) + (i2_2 * 14)) + i3);\n            float v__39 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n            float v__40 = (v__39) < (8.837627e+01f) ? (v__39) : (8.837627e+01f);\n            int32_t v__41 = ((int32_t)(floorf(((((v__40) > (-8.837626e+01f) ? (v__40) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          float v__42 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__43 = (v__42) < (8.837627e+01f) ? (v__42) : (8.837627e+01f);\n          float v__44 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__45 = (v__44) < (8.837627e+01f) ? (v__44) : (8.837627e+01f);\n          float v__46 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__47 = (v__46) < (8.837627e+01f) ? (v__46) : (8.837627e+01f);\n          float v__48 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__49 = (v__48) < (8.837627e+01f) ? (v__48) : (8.837627e+01f);\n          float v__50 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__51 = (v__50) < (8.837627e+01f) ? (v__50) : (8.837627e+01f);\n          float v__52 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__53 = (v__52) < (8.837627e+01f) ? (v__52) : (8.837627e+01f);\n          float v__54 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__55 = (v__54) < (8.837627e+01f) ? (v__54) : (8.837627e+01f);\n          float v__56 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__57 = (v__56) < (8.837627e+01f) ? (v__56) : (8.837627e+01f);\n          float v__58 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__59 = (v__58) < (8.837627e+01f) ? (v__58) : (8.837627e+01f);\n          float v__60 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__61 = (v__60) < (8.837627e+01f) ? (v__60) : (8.837627e+01f);\n          float v__62 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__63 = (v__62) < (8.837627e+01f) ? (v__62) : (8.837627e+01f);\n          float v__64 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__65 = (v__64) < (8.837627e+01f) ? (v__64) : (8.837627e+01f);\n          float v__66 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__67 = (v__66) < (8.837627e+01f) ? (v__66) : (8.837627e+01f);\n          float v__68 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__69 = (v__68) < (8.837627e+01f) ? (v__68) : (8.837627e+01f);\n          float v__70 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__71 = (v__70) < (8.837627e+01f) ? (v__70) : (8.837627e+01f);\n          float v__72 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          float v__73 = (v__72) < (8.837627e+01f) ? (v__72) : (8.837627e+01f);\n          float v__74 = (*(float *)(&(v__41))) * ((((((((((((((1.987569e-04f * (((v__43) > (-8.837626e+01f) ? (v__43) : (-8.837626e+01f)) - (floorf(((((v__45) > (-8.837626e+01f) ? (v__45) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (((v__47) > (-8.837626e+01f) ? (v__47) : (-8.837626e+01f)) - (floorf(((((v__49) > (-8.837626e+01f) ? (v__49) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (((v__51) > (-8.837626e+01f) ? (v__51) : (-8.837626e+01f)) - (floorf(((((v__53) > (-8.837626e+01f) ? (v__53) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (((v__55) > (-8.837626e+01f) ? (v__55) : (-8.837626e+01f)) - (floorf(((((v__57) > (-8.837626e+01f) ? (v__57) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (((v__59) > (-8.837626e+01f) ? (v__59) : (-8.837626e+01f)) - (floorf(((((v__61) > (-8.837626e+01f) ? (v__61) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (((v__63) > (-8.837626e+01f) ? (v__63) : (-8.837626e+01f)) - (floorf(((((v__65) > (-8.837626e+01f) ? (v__65) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (((v__67) > (-8.837626e+01f) ? (v__67) : (-8.837626e+01f)) - (floorf(((((v__69) > (-8.837626e+01f) ? (v__69) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (((v__71) > (-8.837626e+01f) ? (v__71) : (-8.837626e+01f)) - (floorf(((((v__73) > (-8.837626e+01f) ? (v__73) : (-8.837626e+01f)) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f);\n          float v__75 = ((float*)data_1)[cse_var_4] - ((float*)T_softmax_maxelem)[cse_var_5];\n          ((float*)T_softmax_norm_1)[cse_var_4] = (((v__74) > (v__75) ? (v__74) : (v__75)) / ((float*)T_softmax_expsum)[cse_var_5]);\n        }\n      }\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_expsum) != 0) {\n      return -1;\n    }\n    if (TVMBackendFreeWorkspace(1, dev_id, T_softmax_maxelem) != 0) {\n      return -1;\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 459) {\n    T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 14; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 459) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 224) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 459) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 14; ++k) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 459) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 448) + (((int)threadIdx.x) * 14)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 3213) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 7)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 18, 17, 14), \"float32\"), T_softmax_norm: T.Buffer((6, 18, 17, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(6):\n            T_softmax_maxelem = T.allocate([306], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([306], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((306,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((25704,), data=data.data)\n            for i1, i2 in T.grid(18, 17):\n                T_softmax_maxelem_1[i1 * 17 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(14):\n                    cse_var_1: T.int32 = i1 * 17 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 4284 + i1 * 238 + i2 * 14 + k])\n            T_softmax_expsum_1 = T.Buffer((306,), data=T_softmax_expsum)\n            for i1, i2 in T.grid(18, 17):\n                T_softmax_expsum_1[i1 * 17 + i2] = T.float32(0)\n                for k in range(14):\n                    cse_var_3: T.int32 = i1 * 17 + i2\n                    cse_var_2: T.int32 = i0 * 4284 + i1 * 238 + i2 * 14 + k\n                    T_softmax_expsum_1[cse_var_3] = T_softmax_expsum_1[cse_var_3] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n            for i1, i2, i3 in T.grid(18, 17, 14):\n                cse_var_5: T.int32 = i1 * 17 + i2\n                cse_var_4: T.int32 = i0 * 4284 + i1 * 238 + i2 * 14 + i3\n                T_softmax_norm_1 = T.Buffer((25704,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[cse_var_5]", "op_args": [6, 18, 17, 14]}{"op_name": "space_to_depth", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t space_to_depth_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* space_to_depth = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* space_to_depth_1 = (((DLTensor*)space_to_depth)[0].data);\n  void* default_function_space_to_depth_shape = (((DLTensor*)space_to_depth)[0].shape);\n  void* default_function_space_to_depth_strides = (((DLTensor*)space_to_depth)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_space_to_depth_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 792; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      int32_t cse_var_1 = (i0_i1_fused % 72);\n      int32_t12 v_ = int32_t12((((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*0), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*1), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*2), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*3), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*4), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*5), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*6), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*7), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*8), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*9), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*10), (((((((i0_i1_fused / 72) * 6912) + ((cse_var_1 % 18) * 384)) + (i2 * 48)) + ((cse_var_1 / 36) * 24)) + ((cse_var_1 % 36) / 18)))+(2*11));\n      *(float12*)(((float*)space_to_depth_1) + ((i0_i1_fused * 96) + (i2 * 12))) = (float12(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth);\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) >> 7) * 6912) + ((((((((int)blockIdx.x) & 127) * 9) + (((int)threadIdx.x) / 6)) >> 4) % 18) * 384)) + (((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) / 6)) & 15) >> 1) * 48)) + ((((((((int)blockIdx.x) & 127) * 9) + (((int)threadIdx.x) / 6)) >> 4) / 36) * 24)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 12) * 2)) + ((((((((int)blockIdx.x) & 127) * 9) + (((int)threadIdx.x) / 6)) >> 4) % 36) / 18))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 18, 16, 24), \"float32\"), space_to_depth: T.Buffer((11, 72, 8, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(792):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused % 72\n                space_to_depth_1 = T.Buffer((76032,), data=space_to_depth.data)\n                data_1 = T.Buffer((76032,), data=data.data)\n                space_to_depth_1[i0_i1_fused * 96 + i2 * 12:i0_i1_fused * 96 + i2 * 12 + 12] = data_1[i0_i1_fused // 72 * 6912 + T.truncmod(cse_var_1, 18) * 384 + i2 * 48 + T.Div(cse_var_1, 36) * 24 + T.Div(T.truncmod(cse_var_1, 36), 18):i0_i1_fused // 72 * 6912 + T.truncmod(cse_var_1, 18) * 384 + i2 * 48 + T.Div(cse_var_1, 36) * 24 + T.Div(T.truncmod(cse_var_1, 36), 18) + 24:2]", "op_args": [11, 18, 8, 12]}{"op_name": "strided_slice", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t a_code = arg_type_ids[0];\n  int32_t T_strided_slice_code = arg_type_ids[1];\n  void* a = (((TVMValue*)args)[0].v_handle);\n  void* T_strided_slice = (((TVMValue*)args)[1].v_handle);\n  void* a_1 = (((DLTensor*)a)[0].data);\n  void* default_function_a_shape = (((DLTensor*)a)[0].shape);\n  void* default_function_a_strides = (((DLTensor*)a)[0].strides);\n  int32_t dev_id = (((DLTensor*)a)[0].device.device_id);\n  void* T_strided_slice_1 = (((DLTensor*)T_strided_slice)[0].data);\n  void* default_function_T_strided_slice_shape = (((DLTensor*)T_strided_slice)[0].shape);\n  void* default_function_T_strided_slice_strides = (((DLTensor*)T_strided_slice)[0].strides);\n  if (!(default_function_a_strides == NULL)) {\n  }\n  if (!(default_function_T_strided_slice_strides == NULL)) {\n  }\n  for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      ((float*)T_strided_slice_1)[((ax1 * 6) + ax2)] = ((float*)a_1)[(((ax1 * 9) + ax2) + 84)];\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((int)threadIdx.x)] = a[((((((int)threadIdx.x) / 6) * 9) + (((int)threadIdx.x) % 6)) + 84)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((2, 7, 9), \"float32\"), T_strided_slice: T.Buffer((1, 5, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1, ax2 in T.grid(5, 6):\n            T_strided_slice_1 = T.Buffer((30,), data=T_strided_slice.data)\n            a_1 = T.Buffer((126,), data=a.data)\n            T_strided_slice_1[ax1 * 6 + ax2] = a_1[ax1 * 9 + ax2 + 84]", "op_args": [10, 2, 7, 9]}{"op_name": "unpack_NCHWc_to_nchw", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t packed_out_code = arg_type_ids[0];\n  int32_t output_unpack_code = arg_type_ids[1];\n  void* packed_out = (((TVMValue*)args)[0].v_handle);\n  void* output_unpack = (((TVMValue*)args)[1].v_handle);\n  void* packed_out_1 = (((DLTensor*)packed_out)[0].data);\n  void* default_function_packed_out_shape = (((DLTensor*)packed_out)[0].shape);\n  void* default_function_packed_out_strides = (((DLTensor*)packed_out)[0].strides);\n  int32_t dev_id = (((DLTensor*)packed_out)[0].device.device_id);\n  void* output_unpack_1 = (((DLTensor*)output_unpack)[0].data);\n  void* default_function_output_unpack_shape = (((DLTensor*)output_unpack)[0].shape);\n  void* default_function_output_unpack_strides = (((DLTensor*)output_unpack)[0].strides);\n  if (!(default_function_packed_out_strides == NULL)) {\n  }\n  if (!(default_function_output_unpack_strides == NULL)) {\n  }\n  for (int32_t n_c_fused = 0; n_c_fused < 416; ++n_c_fused) {\n    for (int32_t h = 0; h < 14; ++h) {\n      int32_t7 v_ = int32_t7((((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*0), (((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*1), (((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*2), (((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*3), (((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*4), (((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*5), (((((n_c_fused >> 1) * 196) + (h * 14)) + (n_c_fused & 1)))+(2*6));\n      *(float7*)(((float*)output_unpack_1) + ((n_c_fused * 98) + (h * 7))) = (float7(((float*)packed_out_1)[v_.s0],((float*)packed_out_1)[v_.s1],((float*)packed_out_1)[v_.s2],((float*)packed_out_1)[v_.s3],((float*)packed_out_1)[v_.s4],((float*)packed_out_1)[v_.s5],((float*)packed_out_1)[v_.s6]));\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out);\nextern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) >> 2) * 196) + ((((int)blockIdx.x) & 1) * 98)) + (((int)threadIdx.x) * 2)) + ((((int)blockIdx.x) & 3) >> 1))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((16, 13, 14, 7, 2), \"float32\"), output_unpack: T.Buffer((16, 26, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused in T.parallel(416):\n            for h in range(14):\n                output_unpack_1 = T.Buffer((40768,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((40768,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 98 + h * 7:n_c_fused * 98 + h * 7 + 7] = packed_out_1[n_c_fused // 2 * 196 + h * 14 + n_c_fused % 2:n_c_fused // 2 * 196 + h * 14 + n_c_fused % 2 + 14:2]", "op_args": [16, 13, 14, 7]}{"op_name": "upsampling", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t resize_code = arg_type_ids[1];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* resize = (((TVMValue*)args)[1].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* resize_1 = (((DLTensor*)resize)[0].data);\n  void* default_function_resize_shape = (((DLTensor*)resize)[0].shape);\n  void* default_function_resize_strides = (((DLTensor*)resize)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_resize_strides == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5040; ++i0_i1_fused_i2_fused) {\n    int32_t16 v_ = ((int32_t16)((((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)), (((i0_i1_fused_i2_fused / 56) * 224) + (((i0_i1_fused_i2_fused % 56) / 2) * 8)))) + (int32_t16((0)+(1*0), (0)+(1*1), (0)+(1*2), (0)+(1*3), (0)+(1*4), (0)+(1*5), (0)+(1*6), (0)+(1*7), (0)+(1*8), (0)+(1*9), (0)+(1*10), (0)+(1*11), (0)+(1*12), (0)+(1*13), (0)+(1*14), (0)+(1*15)) / ((int32_t16)(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)));\n    *(float16*)(((float*)resize_1) + (i0_i1_fused_i2_fused * 16)) = (float16(((float*)data_1)[v_.s0],((float*)data_1)[v_.s1],((float*)data_1)[v_.s2],((float*)data_1)[v_.s3],((float*)data_1)[v_.s4],((float*)data_1)[v_.s5],((float*)data_1)[v_.s6],((float*)data_1)[v_.s7],((float*)data_1)[v_.s8],((float*)data_1)[v_.s9],((float*)data_1)[v_.sa],((float*)data_1)[v_.sb],((float*)data_1)[v_.sc],((float*)data_1)[v_.sd],((float*)data_1)[v_.se],((float*)data_1)[v_.sf]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ resize);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 2)) / 224) * 224) + ((((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 2)) % 224) >> 2) / 2) * 8)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 15) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 5, 28, 8), \"float32\"), resize: T.Buffer((18, 5, 56, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(5040):\n            resize_1 = T.Buffer((80640,), data=resize.data)\n            data_1 = T.Buffer((20160,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused * 16:i0_i1_fused_i2_fused * 16 + 16] = data_1[T.Broadcast(i0_i1_fused_i2_fused // 56 * 224 + T.Div(i0_i1_fused_i2_fused % 56, 2) * 8, 16) + T.Div(T.Ramp(0, 1, 16), T.Broadcast(2, 16))]", "op_args": [18, 5, 14, 4]}{"op_name": "rms_norm", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t weight_code = arg_type_ids[1];\n  int32_t T_cast_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* weight = (((TVMValue*)args)[1].v_handle);\n  void* T_cast = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* weight_1 = (((DLTensor*)weight)[0].data);\n  void* default_function_weight_shape = (((DLTensor*)weight)[0].shape);\n  void* default_function_weight_strides = (((DLTensor*)weight)[0].strides);\n  void* T_cast_1 = (((DLTensor*)T_cast)[0].data);\n  void* default_function_T_cast_shape = (((DLTensor*)T_cast)[0].shape);\n  void* default_function_T_cast_strides = (((DLTensor*)T_cast)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_weight_strides == NULL)) {\n  }\n  if (!(default_function_T_cast_strides == NULL)) {\n  }\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    float T_multiply_red[3];\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n          T_multiply_red[ax2_1] = 0.000000e+00f;\n          for (int32_t k1 = 0; k1 < 10; ++k1) {\n            int32_t cse_var_1 = ((((ax0 * 150) + (k1 * 15)) + (ax2 * 3)) + ax2_1);\n            T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (((float*)data_1)[cse_var_1] * ((float*)data_1)[cse_var_1]));\n          }\n        }\n        for (int32_t ax3_s = 0; ax3_s < 3; ++ax3_s) {\n          int32_t cse_var_2 = ((((ax0 * 150) + (ax1 * 15)) + (ax2 * 3)) + ax3_s);\n          ((float*)T_cast_1)[cse_var_2] = ((((float*)data_1)[cse_var_2] * ((float*)weight_1)[ax1]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 1.000000e-01f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight);\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 375) {\n    T_cast[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 150) / 15)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 75) * 15) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 15))] * 1.000000e-01f) + 1.000000e-05f))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 10; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] + (data[(((((int)blockIdx.x) * 150) + (k1 * 15)) + ((int)threadIdx.x))] * data[(((((int)blockIdx.x) * 150) + (k1 * 15)) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 10, 5, 3), \"float32\"), weight: T.Buffer((3,), \"float32\"), T_cast: T.Buffer((5, 10, 5, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            T_multiply_red = T.allocate([3], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(10, 5):\n                T_multiply_red_1 = T.Buffer((3,), data=T_multiply_red, align=8)\n                data_1 = T.Buffer((750,), data=data.data)\n                for ax2_1 in range(3):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(10):\n                        cse_var_1: T.int32 = ax0 * 150 + k1 * 15 + ax2 * 3 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3_s in range(3):\n                    cse_var_2: T.int32 = ax0 * 150 + ax1 * 15 + ax2 * 3 + ax3_s\n                    T_cast_1 = T.Buffer((750,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax1] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.10000000000000001) + T.float32(1.0000000000000001e-05))", "op_args": [5, 10, 5, 3]}{"op_name": "batch_norm", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t gamma_code = arg_type_ids[1];\n  int32_t beta_code = arg_type_ids[2];\n  int32_t moving_mean_code = arg_type_ids[3];\n  int32_t moving_var_code = arg_type_ids[4];\n  int32_t T_divide_code = arg_type_ids[5];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* gamma = (((TVMValue*)args)[1].v_handle);\n  void* beta = (((TVMValue*)args)[2].v_handle);\n  void* moving_mean = (((TVMValue*)args)[3].v_handle);\n  void* moving_var = (((TVMValue*)args)[4].v_handle);\n  void* T_divide = (((TVMValue*)args)[5].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* gamma_1 = (((DLTensor*)gamma)[0].data);\n  void* default_function_gamma_shape = (((DLTensor*)gamma)[0].shape);\n  void* default_function_gamma_strides = (((DLTensor*)gamma)[0].strides);\n  void* beta_1 = (((DLTensor*)beta)[0].data);\n  void* default_function_beta_shape = (((DLTensor*)beta)[0].shape);\n  void* default_function_beta_strides = (((DLTensor*)beta)[0].strides);\n  void* moving_mean_1 = (((DLTensor*)moving_mean)[0].data);\n  void* default_function_moving_mean_shape = (((DLTensor*)moving_mean)[0].shape);\n  void* default_function_moving_mean_strides = (((DLTensor*)moving_mean)[0].strides);\n  void* moving_var_1 = (((DLTensor*)moving_var)[0].data);\n  void* default_function_moving_var_shape = (((DLTensor*)moving_var)[0].shape);\n  void* default_function_moving_var_strides = (((DLTensor*)moving_var)[0].strides);\n  void* T_divide_1 = (((DLTensor*)T_divide)[0].data);\n  void* default_function_T_divide_shape = (((DLTensor*)T_divide)[0].shape);\n  void* default_function_T_divide_strides = (((DLTensor*)T_divide)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_gamma_strides == NULL)) {\n  }\n  if (!(default_function_beta_strides == NULL)) {\n  }\n  if (!(default_function_moving_mean_strides == NULL)) {\n  }\n  if (!(default_function_moving_var_strides == NULL)) {\n  }\n  if (!(default_function_T_divide_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 7200; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    int32_t cse_var_1 = ((ax0_ax1_fused_ax2_fused_ax3_fused % 3600) / 240);\n    float T_reshape[1];\n    float T_reshape_1[1];\n    T_reshape[0] = ((float*)moving_mean_1)[cse_var_1];\n    T_reshape_1[0] = ((float*)moving_var_1)[cse_var_1];\n    ((float*)T_divide_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = ((((float*)data_1)[ax0_ax1_fused_ax2_fused_ax3_fused] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var);\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) % 60) >> 2)]) / sqrtf((moving_var[((((int)blockIdx.x) % 60) >> 2)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 15, 15, 16), \"float32\"), gamma: T.Buffer((15,), \"float32\"), beta: T.Buffer((15,), \"float32\"), moving_mean: T.Buffer((15,), \"float32\"), moving_var: T.Buffer((15,), \"float32\"), T_divide: T.Buffer((2, 15, 15, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(7200):\n            cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 3600 // 240\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n            T_reshape_2[0] = moving_mean[cse_var_1]\n            T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n            T_reshape_3[0] = moving_var[cse_var_1]\n            T_divide_1 = T.Buffer((7200,), data=T_divide.data)\n            data_1 = T.Buffer((7200,), data=data.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused_ax3_fused] = (data_1[ax0_ax1_fused_ax2_fused_ax3_fused] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))", "op_args": [2, 15, 15, 16]}{"op_name": "conv1d", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t kernel_code = arg_type_ids[1];\n  int32_t conv1d_ncw_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* kernel = (((TVMValue*)args)[1].v_handle);\n  void* conv1d_ncw = (((TVMValue*)args)[2].v_handle);\n  void* data_1 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* kernel_1 = (((DLTensor*)kernel)[0].data);\n  void* default_function_kernel_shape = (((DLTensor*)kernel)[0].shape);\n  void* default_function_kernel_strides = (((DLTensor*)kernel)[0].strides);\n  void* conv1d_ncw_1 = (((DLTensor*)conv1d_ncw)[0].data);\n  void* default_function_conv1d_ncw_shape = (((DLTensor*)conv1d_ncw)[0].shape);\n  void* default_function_conv1d_ncw_strides = (((DLTensor*)conv1d_ncw)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_kernel_strides == NULL)) {\n  }\n  if (!(default_function_conv1d_ncw_strides == NULL)) {\n  }\n  for (int32_t nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused = 0; nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused < 40; ++nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused) {\n    int32_t cse_var_6 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused / 20);\n    int32_t cse_var_5 = (nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 5);\n    int32_t cse_var_4 = ((nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 20) / 5);\n    int32_t cse_var_3 = (cse_var_5 * 9);\n    int32_t cse_var_2 = ((cse_var_6 * 30) + (cse_var_4 * 2));\n    int32_t cse_var_1 = (((cse_var_6 * 20) + (cse_var_5 * 4)) + cse_var_4);\n    ((float*)conv1d_ncw_1)[cse_var_1] = 0.000000e+00f;\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[cse_var_2] * ((float*)kernel_1)[cse_var_3]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 1)] * ((float*)kernel_1)[(cse_var_3 + 1)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 2)] * ((float*)kernel_1)[(cse_var_3 + 2)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 10)] * ((float*)kernel_1)[(cse_var_3 + 3)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 11)] * ((float*)kernel_1)[(cse_var_3 + 4)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 12)] * ((float*)kernel_1)[(cse_var_3 + 5)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 20)] * ((float*)kernel_1)[(cse_var_3 + 6)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 21)] * ((float*)kernel_1)[(cse_var_3 + 7)]));\n    ((float*)conv1d_ncw_1)[cse_var_1] = (((float*)conv1d_ncw_1)[cse_var_1] + (((float*)data_1)[(cse_var_2 + 22)] * ((float*)kernel_1)[(cse_var_3 + 8)]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel);\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[1];\n  __shared__ float pad_temp_shared[10];\n  __shared__ float kernel_shared[15];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  for (int rc_outer_outer = 0; rc_outer_outer < 3; ++rc_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 10) {\n      pad_temp_shared[((int)threadIdx.x)] = data[(((((((int)threadIdx.x) / 5) * 30) + (rc_outer_outer * 10)) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) % 5))];\n    }\n    if (((int)threadIdx.x) < 15) {\n      kernel_shared[((int)threadIdx.x)] = kernel[((((((int)threadIdx.x) / 3) * 9) + (rc_outer_outer * 3)) + (((int)threadIdx.x) % 3))];\n    }\n    __syncthreads();\n    for (int ry_inner = 0; ry_inner < 3; ++ry_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[((((((int)threadIdx.x) / 10) * 5) + ((((int)threadIdx.x) & 1) * 2)) + ry_inner)] * kernel_shared[((((((int)threadIdx.x) % 10) >> 1) * 3) + ry_inner)]));\n    }\n  }\n  conv1d_ncw[((((((int)threadIdx.x) >> 1) * 4) + (((int)blockIdx.x) * 2)) + (((int)threadIdx.x) & 1))] = conv1d_ncw_local[0];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 3, 10), \"float32\"), kernel: T.Buffer((5, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 5, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused in T.parallel(40):\n            cse_var_6: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused // 20\n            cse_var_5: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 5\n            cse_var_4: T.int32 = nn_outer_outer_outer_ff_outer_outer_outer_fused_yy_outer_outer_outer_fused_nn_outer_outer_inner_fused_ff_outer_outer_inner_fused_yy_outer_outer_inner_fused % 20 // 5\n            cse_var_3: T.int32 = cse_var_5 * 9\n            cse_var_2: T.int32 = cse_var_6 * 30 + cse_var_4 * 2\n            cse_var_1: T.int32 = cse_var_6 * 20 + cse_var_5 * 4 + cse_var_4\n            conv1d_ncw_1 = T.Buffer((40,), data=conv1d_ncw.data)\n            conv1d_ncw_1[cse_var_1] = T.float32(0)\n            data_1 = T.Buffer((60,), data=data.data)\n            kernel_1 = T.Buffer((45,), data=kernel.data)\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2] * kernel_1[cse_var_3]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 1] * kernel_1[cse_var_3 + 1]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 2] * kernel_1[cse_var_3 + 2]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 10] * kernel_1[cse_var_3 + 3]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 11] * kernel_1[cse_var_3 + 4]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 12] * kernel_1[cse_var_3 + 5]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 20] * kernel_1[cse_var_3 + 6]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 21] * kernel_1[cse_var_3 + 7]\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[cse_var_2 + 22] * kernel_1[cse_var_3 + 8]", "op_args": [15, 15, 2, 60, 70, 19, 9, [2, 1], [1, 1]]}{"op_name": "multi_out_op", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t compute_code = arg_type_ids[2];\n  int32_t compute_code_1 = arg_type_ids[3];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* compute = (((TVMValue*)args)[2].v_handle);\n  void* compute_1 = (((TVMValue*)args)[3].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* compute_2 = (((DLTensor*)compute)[0].data);\n  void* default_function_compute_shape = (((DLTensor*)compute)[0].shape);\n  void* default_function_compute_strides = (((DLTensor*)compute)[0].strides);\n  void* compute_3 = (((DLTensor*)compute_1)[0].data);\n  void* default_function_compute_shape_1 = (((DLTensor*)compute_1)[0].shape);\n  void* default_function_compute_strides_1 = (((DLTensor*)compute_1)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_data_strides_1 == NULL)) {\n  }\n  if (!(default_function_compute_strides == NULL)) {\n  }\n  if (!(default_function_compute_strides_1 == NULL)) {\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3120; ++i0_i1_fused_i2_fused_i3_fused) {\n    ((float*)compute_2)[i0_i1_fused_i2_fused_i3_fused] = sqrtf((((float*)data_2)[i0_i1_fused_i2_fused_i3_fused] + ((float*)data_3)[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  for (int32_t i0_i1_fused_i2_fused_i3_fused_1 = 0; i0_i1_fused_i2_fused_i3_fused_1 < 3120; ++i0_i1_fused_i2_fused_i3_fused_1) {\n    ((float*)compute_3)[i0_i1_fused_i2_fused_i3_fused_1] = cosf((((float*)data_2)[i0_i1_fused_i2_fused_i3_fused_1] + ((float*)data_3)[i0_i1_fused_i2_fused_i3_fused_1]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 20, 2, 6), \"float32\"), data_1: T.Buffer((13, 20, 2, 6), \"float32\"), compute: T.Buffer((13, 20, 2, 6), \"float32\"), compute_1: T.Buffer((13, 20, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((3120,), data=data.data)\n        data_3 = T.Buffer((3120,), data=data_1.data)\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3120):\n            compute_2 = T.Buffer((3120,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_2[i0_i1_fused_i2_fused_i3_fused] + data_3[i0_i1_fused_i2_fused_i3_fused])\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3120):\n            compute_2 = T.Buffer((3120,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_2[i0_i1_fused_i2_fused_i3_fused] + data_3[i0_i1_fused_i2_fused_i3_fused])", "op_args": [13, 20, 2, 6]}{"op_name": "combination_op", "c_code": "// tvm target: c -keys=cpu \n#define TVM_EXPORTS\n#include \"tvm/runtime/c_runtime_api.h\"\n#include \"tvm/runtime/c_backend_api.h\"\n#include <math.h>\n#include <stdbool.h>\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float sqrtf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL float cosf(float);\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t default_function(void* args, int32_t* arg_type_ids, int32_t num_args, void* out_ret_value, int32_t* out_ret_tcode, void* resource_handle) {\n  int32_t data_code = arg_type_ids[0];\n  int32_t data_code_1 = arg_type_ids[1];\n  int32_t T_add_code = arg_type_ids[2];\n  void* data = (((TVMValue*)args)[0].v_handle);\n  void* data_1 = (((TVMValue*)args)[1].v_handle);\n  void* T_add = (((TVMValue*)args)[2].v_handle);\n  void* data_2 = (((DLTensor*)data)[0].data);\n  void* default_function_data_shape = (((DLTensor*)data)[0].shape);\n  void* default_function_data_strides = (((DLTensor*)data)[0].strides);\n  int32_t dev_id = (((DLTensor*)data)[0].device.device_id);\n  void* data_3 = (((DLTensor*)data_1)[0].data);\n  void* default_function_data_shape_1 = (((DLTensor*)data_1)[0].shape);\n  void* default_function_data_strides_1 = (((DLTensor*)data_1)[0].strides);\n  void* T_add_1 = (((DLTensor*)T_add)[0].data);\n  void* default_function_T_add_shape = (((DLTensor*)T_add)[0].shape);\n  void* default_function_T_add_strides = (((DLTensor*)T_add)[0].strides);\n  if (!(default_function_data_strides == NULL)) {\n  }\n  if (!(default_function_data_strides_1 == NULL)) {\n  }\n  if (!(default_function_T_add_strides == NULL)) {\n  }\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 75140; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    ((float*)T_add_1)[ax0_ax1_fused_ax2_fused_ax3_fused] = (sqrtf(((float*)data_2)[ax0_ax1_fused_ax2_fused_ax3_fused]) + cosf(((float*)data_3)[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  }\n  return 0;\n}\n\n// CodegenC: NOTE: Auto-generated entry function\n#ifdef __cplusplus\nextern \"C\"\n#endif\nTVM_DLL int32_t __tvm_main__(void* args, int* arg_type_ids, int num_args, void* out_ret_value, int* out_ret_tcode, void* resource_handle) {\n  return default_function(args, arg_type_ids, num_args, out_ret_value, out_ret_tcode, resource_handle);\n}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1);\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 18785) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 13, 17, 20), \"float32\"), data_1: T.Buffer((17, 13, 17, 20), \"float32\"), T_add: T.Buffer((17, 13, 17, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(75140):\n            T_add_1 = T.Buffer((75140,), data=T_add.data)\n            data_2 = T.Buffer((75140,), data=data.data)\n            data_3 = T.Buffer((75140,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.sqrt(data_2[ax0_ax1_fused_ax2_fused_ax3_fused]) + T.cos(data_3[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [17, 13, 17, 20]}{"op_name": "topology_expansion", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%0 = type { double }\n%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }\n%2 = type { i32, i32 }\n%3 = type { i8, i8, i16 }\n\n@__tvm_module_ctx = linkonce dllexport local_unnamed_addr global i8* null, align 8\n@__TVMFuncCall = linkonce dllexport local_unnamed_addr global i32 (i8*, %0*, i32*, i32, %0*, i32*)* null, align 8\n@__TVMBackendGetFuncFromEnv = linkonce dllexport local_unnamed_addr global i32 (i8*, i8*, i8**)* null, align 8\n@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8\n@.str = private constant [67 x i8] c\"Assert fail: num_args == 3, default_function: num_args should be 3\\00\", align 1\n@.str.1 = private constant [130 x i8] c\"Assert fail: ph_0_code == 3 or ph_0_code == 13 or ph_0_code == 7 or ph_0_code == 4, default_function: Expect arg[0] to be pointer\\00\", align 1\n@.str.2 = private constant [130 x i8] c\"Assert fail: ph_2_code == 3 or ph_2_code == 13 or ph_2_code == 7 or ph_2_code == 4, default_function: Expect arg[1] to be pointer\\00\", align 1\n@.str.3 = private constant [134 x i8] c\"Assert fail: T_add_code == 3 or T_add_code == 13 or T_add_code == 7 or T_add_code == 4, default_function: Expect arg[2] to be pointer\\00\", align 1\n@.str.4 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_0, 0, 4, \\22int32\\22), default_function.ph_0.ndim is expected to equal 3\\00\", align 1\n@.str.5 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_0, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_0, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_0.dtype is expected to be float32\\00\", align 1\n@.str.6 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[0]) == 12, Argument default_function.ph_0.shape[0] has an unsatisfied constraint: 12 == T.Cast(\\22int32\\22, default_function_ph_0_shape[0])\\00\", align 1\n@.str.7 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[1]) == 14, Argument default_function.ph_0.shape[1] has an unsatisfied constraint: 14 == T.Cast(\\22int32\\22, default_function_ph_0_shape[1])\\00\", align 1\n@.str.8 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[2]) == 18, Argument default_function.ph_0.shape[2] has an unsatisfied constraint: 18 == T.Cast(\\22int32\\22, default_function_ph_0_shape[2])\\00\", align 1\n@.str.9 = private constant [250 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_0_strides[2]) and 18 == T.Cast(\\22int32\\22, default_function_ph_0_strides[1]) and 252 == T.Cast(\\22int32\\22, default_function_ph_0_strides[0]), default_function.ph_0.strides: expected to be compact array\\00\", align 1\n@.str.10 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22), Argument default_function.ph_0.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.11 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_0.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.12 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_2, 0, 4, \\22int32\\22), default_function.ph_2.ndim is expected to equal 3\\00\", align 1\n@.str.13 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_2, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_2, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_2, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_2.dtype is expected to be float32\\00\", align 1\n@.str.14 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_2_shape[0]) == 12, Argument default_function.ph_2.shape[0] has an unsatisfied constraint: 12 == T.Cast(\\22int32\\22, default_function_ph_2_shape[0])\\00\", align 1\n@.str.15 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_2_shape[1]) == 14, Argument default_function.ph_2.shape[1] has an unsatisfied constraint: 14 == T.Cast(\\22int32\\22, default_function_ph_2_shape[1])\\00\", align 1\n@.str.16 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_2_shape[2]) == 18, Argument default_function.ph_2.shape[2] has an unsatisfied constraint: 18 == T.Cast(\\22int32\\22, default_function_ph_2_shape[2])\\00\", align 1\n@.str.17 = private constant [250 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_2_strides[2]) and 18 == T.Cast(\\22int32\\22, default_function_ph_2_strides[1]) and 252 == T.Cast(\\22int32\\22, default_function_ph_2_strides[0]), default_function.ph_2.strides: expected to be compact array\\00\", align 1\n@.str.18 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_2, 0, 8, \\22uint64\\22), Argument default_function.ph_2.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_2, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.19 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_2, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_2.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_2, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.20 = private constant [182 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(ph_2, 0, 9, \\22int32\\22), Argument default_function.ph_2.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(ph_2, 0, 9, \\22int32\\22)\\00\", align 1\n@.str.21 = private constant [109 x i8] c\"Assert fail: 3 == T.tvm_struct_get(T_add, 0, 4, \\22int32\\22), default_function.T_add.ndim is expected to equal 3\\00\", align 1\n@.str.22 = private constant [239 x i8] c\"Assert fail: T.tvm_struct_get(T_add, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(T_add, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(T_add, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.T_add.dtype is expected to be float32\\00\", align 1\n@.str.23 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[0]) == 12, Argument default_function.T_add.shape[0] has an unsatisfied constraint: 12 == T.Cast(\\22int32\\22, default_function_T_add_shape[0])\\00\", align 1\n@.str.24 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[1]) == 14, Argument default_function.T_add.shape[1] has an unsatisfied constraint: 14 == T.Cast(\\22int32\\22, default_function_T_add_shape[1])\\00\", align 1\n@.str.25 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[2]) == 18, Argument default_function.T_add.shape[2] has an unsatisfied constraint: 18 == T.Cast(\\22int32\\22, default_function_T_add_shape[2])\\00\", align 1\n@.str.26 = private constant [254 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_T_add_strides[2]) and 18 == T.Cast(\\22int32\\22, default_function_T_add_strides[1]) and 252 == T.Cast(\\22int32\\22, default_function_T_add_strides[0]), default_function.T_add.strides: expected to be compact array\\00\", align 1\n@.str.27 = private constant [199 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(T_add, 0, 8, \\22uint64\\22), Argument default_function.T_add.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(T_add, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.28 = private constant [179 x i8] c\"Assert fail: T.tvm_struct_get(T_add, 0, 10, \\22int32\\22) == 1, Argument default_function.T_add.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(T_add, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.29 = private constant [185 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(T_add, 0, 9, \\22int32\\22), Argument default_function.T_add.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(T_add, 0, 9, \\22int32\\22)\\00\", align 1\n@.tvm_func.default_function_kernel = internal unnamed_addr global i8* null, align 8\n@.str.30 = private constant [24 x i8] c\"default_function_kernel\\00\", align 1\n@.str.31 = private constant [68 x i8] c\"Assert fail: kernel_error_code == 0, Error executing compute kernel\\00\", align 1\n@__tvm_main__ = weak dllexport local_unnamed_addr constant [17 x i8] c\"default_function\\00\", align 1\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\ndefine dllexport i32 @default_function(i8* noalias nocapture readonly %args, i32* noalias nocapture readonly %arg_type_ids, i32 %num_args, i8* noalias nocapture readnone %out_ret_value, i32* noalias nocapture readnone %out_ret_tcode, i8* noalias nocapture readnone %resource_handle) local_unnamed_addr #0 !dbg !5 {\nentry:\n  call void @llvm.dbg.value(metadata i8* %args, metadata !12, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %arg_type_ids, metadata !13, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32 %num_args, metadata !14, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %out_ret_value, metadata !15, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %out_ret_tcode, metadata !16, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %resource_handle, metadata !17, metadata !DIExpression()), !dbg !18\n  %stack_value_void_ptr81 = alloca [4 x %0], align 8, !dbg !18\n  %stack_tcode82 = alloca [4 x i32], align 4, !dbg !18\n  %stack_tcode82.sub = getelementptr inbounds [4 x i32], [4 x i32]* %stack_tcode82, i64 0, i64 0\n  %stack_value = bitcast [4 x %0]* %stack_value_void_ptr81 to i8*, !dbg !18\n  %0 = icmp eq i32 %num_args, 3, !dbg !18\n  br i1 %0, label %assert_end, label %assert_fail, !dbg !18, !prof !19\n\nassert_fail:                                      ; preds = %entry\n  %1 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %1(i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end:                                       ; preds = %entry\n  %ph_0.code = load i32, i32* %arg_type_ids, align 4, !dbg !18, !tbaa !23\n  %2 = getelementptr inbounds i32, i32* %arg_type_ids, i64 1, !dbg !18\n  %ph_2.code = load i32, i32* %2, align 4, !dbg !18, !tbaa !34\n  %3 = getelementptr inbounds i32, i32* %arg_type_ids, i64 2, !dbg !18\n  %T_add.code = load i32, i32* %3, align 4, !dbg !18, !tbaa !36\n  %4 = bitcast i8* %args to %1**, !dbg !18\n  %ph_083 = load %1*, %1** %4, align 8, !dbg !18\n  %5 = getelementptr inbounds i8, i8* %args, i64 8, !dbg !18\n  %6 = bitcast i8* %5 to %1**, !dbg !18\n  %ph_284 = load %1*, %1** %6, align 8, !dbg !18\n  %7 = getelementptr inbounds i8, i8* %args, i64 16, !dbg !18\n  %8 = bitcast i8* %7 to %1**, !dbg !18\n  %T_add85 = load %1*, %1** %8, align 8, !dbg !18\n  %9 = bitcast %1* %ph_083 to float**, !dbg !18\n  %ph_0_void_ptr86 = load float*, float** %9, align 8, !dbg !18\n  %ptrint = ptrtoint float* %ph_0_void_ptr86 to i64, !dbg !18\n  %maskedptr = and i64 %ptrint, 63, !dbg !18\n  %maskcond = icmp eq i64 %maskedptr, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond), !dbg !18\n  %10 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 4, !dbg !18\n  %default_function.ph_0.shape = load i64*, i64** %10, align 8, !dbg !18\n  %11 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 5, !dbg !18\n  %default_function.ph_0.strides = load i64*, i64** %11, align 8, !dbg !18\n  %12 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 1, !dbg !18\n  %dev_id = load i32, i32* %12, align 4, !dbg !18\n  %13 = bitcast %1* %ph_284 to float**, !dbg !18\n  %ph_2_void_ptr87 = load float*, float** %13, align 8, !dbg !18\n  %ptrint3 = ptrtoint float* %ph_2_void_ptr87 to i64, !dbg !18\n  %maskedptr4 = and i64 %ptrint3, 63, !dbg !18\n  %maskcond5 = icmp eq i64 %maskedptr4, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond5), !dbg !18\n  %14 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 4, !dbg !18\n  %default_function.ph_2.shape = load i64*, i64** %14, align 8, !dbg !18\n  %15 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 5, !dbg !18\n  %default_function.ph_2.strides = load i64*, i64** %15, align 8, !dbg !18\n  %16 = bitcast %1* %T_add85 to float**, !dbg !18\n  %T_add_void_ptr88 = load float*, float** %16, align 8, !dbg !18\n  %ptrint7 = ptrtoint float* %T_add_void_ptr88 to i64, !dbg !18\n  %maskedptr8 = and i64 %ptrint7, 63, !dbg !18\n  %maskcond9 = icmp eq i64 %maskedptr8, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond9), !dbg !18\n  %17 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 4, !dbg !18\n  %default_function.T_add.shape = load i64*, i64** %17, align 8, !dbg !18\n  %18 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 5, !dbg !18\n  %default_function.T_add.strides = load i64*, i64** %18, align 8, !dbg !18\n  switch i32 %ph_0.code, label %assert_fail10 [\n    i32 13, label %assert_end11\n    i32 7, label %assert_end11\n    i32 4, label %assert_end11\n    i32 3, label %assert_end11\n  ], !dbg !18\n\nassert_fail10:                                    ; preds = %assert_end\n  %19 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %19(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.1, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end11:                                     ; preds = %assert_end, %assert_end, %assert_end, %assert_end\n  switch i32 %ph_2.code, label %assert_fail12 [\n    i32 13, label %assert_end13\n    i32 7, label %assert_end13\n    i32 4, label %assert_end13\n    i32 3, label %assert_end13\n  ], !dbg !18\n\nassert_fail12:                                    ; preds = %assert_end11\n  %20 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %20(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.2, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end13:                                     ; preds = %assert_end11, %assert_end11, %assert_end11, %assert_end11\n  switch i32 %T_add.code, label %assert_fail14 [\n    i32 13, label %assert_end15\n    i32 7, label %assert_end15\n    i32 4, label %assert_end15\n    i32 3, label %assert_end15\n  ], !dbg !18\n\nassert_fail14:                                    ; preds = %assert_end13\n  %21 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %21(i8* getelementptr inbounds ([134 x i8], [134 x i8]* @.str.3, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end15:                                     ; preds = %assert_end13, %assert_end13, %assert_end13, %assert_end13\n  %22 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 2, !dbg !18\n  %23 = load i32, i32* %22, align 4, !dbg !18\n  %24 = icmp eq i32 %23, 3, !dbg !18\n  br i1 %24, label %assert_end19, label %assert_fail16, !dbg !18, !prof !19\n\nassert_fail16:                                    ; preds = %assert_end15\n  %25 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %25(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.4, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end19:                                     ; preds = %assert_end15\n  %26 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 2, !dbg !18\n  %27 = load i16, i16* %26, align 2, !dbg !18\n  %28 = icmp eq i16 %27, 1, !dbg !18\n  %29 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 1, !dbg !18\n  %30 = load i8, i8* %29, align 1, !dbg !18\n  %31 = icmp eq i8 %30, 32, !dbg !18\n  %32 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 0, !dbg !18\n  %33 = load i8, i8* %32, align 1, !dbg !18\n  %34 = icmp eq i8 %33, 2, !dbg !18\n  %35 = and i1 %31, %34, !dbg !18\n  %36 = and i1 %28, %35, !dbg !18\n  br i1 %36, label %assert_end21, label %assert_fail20, !dbg !18, !prof !19\n\nassert_fail20:                                    ; preds = %assert_end19\n  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %37(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.5, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end21:                                     ; preds = %assert_end19\n  %38 = load i64, i64* %default_function.ph_0.shape, align 8, !dbg !18, !tbaa !39\n  %39 = trunc i64 %38 to i32, !dbg !18\n  %40 = icmp eq i32 %39, 12, !dbg !18\n  br i1 %40, label %assert_end23, label %assert_fail22, !dbg !18, !prof !19\n\nassert_fail22:                                    ; preds = %assert_end21\n  %41 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %41(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.6, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end23:                                     ; preds = %assert_end21\n  %42 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 1, !dbg !18\n  %43 = load i64, i64* %42, align 8, !dbg !18, !tbaa !49\n  %44 = trunc i64 %43 to i32, !dbg !18\n  %45 = icmp eq i32 %44, 14, !dbg !18\n  br i1 %45, label %assert_end25, label %assert_fail24, !dbg !18, !prof !19\n\nassert_fail24:                                    ; preds = %assert_end23\n  %46 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %46(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.7, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end25:                                     ; preds = %assert_end23\n  %47 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 2, !dbg !18\n  %48 = load i64, i64* %47, align 8, !dbg !18, !tbaa !51\n  %49 = trunc i64 %48 to i32, !dbg !18\n  %50 = icmp eq i32 %49, 18, !dbg !18\n  br i1 %50, label %assert_end27, label %assert_fail26, !dbg !18, !prof !19\n\nassert_fail26:                                    ; preds = %assert_end25\n  %51 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %51(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.8, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end27:                                     ; preds = %assert_end25\n  %52 = icmp eq i64* %default_function.ph_0.strides, null, !dbg !18\n  br i1 %52, label %if_end, label %if_then, !dbg !18, !prof !54\n\nif_then:                                          ; preds = %assert_end27\n  %53 = load i64, i64* %default_function.ph_0.strides, align 8, !dbg !18, !tbaa !55\n  %54 = trunc i64 %53 to i32, !dbg !18\n  %55 = icmp eq i32 %54, 252, !dbg !18\n  %56 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 1, !dbg !18\n  %57 = load i64, i64* %56, align 8, !dbg !18, !tbaa !65\n  %58 = trunc i64 %57 to i32, !dbg !18\n  %59 = icmp eq i32 %58, 18, !dbg !18\n  %60 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 2, !dbg !18\n  %61 = load i64, i64* %60, align 8, !dbg !18, !tbaa !67\n  %62 = trunc i64 %61 to i32, !dbg !18\n  %63 = icmp eq i32 %62, 1, !dbg !18\n  %64 = and i1 %59, %63, !dbg !18\n  %65 = and i1 %55, %64, !dbg !18\n  br i1 %65, label %if_end, label %assert_fail28, !dbg !18, !prof !19\n\nif_end:                                           ; preds = %assert_end27, %if_then\n  %66 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 6, !dbg !18\n  %67 = load i64, i64* %66, align 8, !dbg !18\n  %68 = icmp eq i64 %67, 0, !dbg !18\n  br i1 %68, label %assert_end31, label %assert_fail30, !dbg !18, !prof !19\n\nassert_fail28:                                    ; preds = %if_then\n  %69 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %69(i8* getelementptr inbounds ([250 x i8], [250 x i8]* @.str.9, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail30:                                    ; preds = %if_end\n  %70 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %70(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.10, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end31:                                     ; preds = %if_end\n  %71 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 0, !dbg !18\n  %72 = load i32, i32* %71, align 4, !dbg !18\n  %73 = icmp eq i32 %72, 1, !dbg !18\n  br i1 %73, label %assert_end33, label %assert_fail32, !dbg !18, !prof !19\n\nassert_fail32:                                    ; preds = %assert_end31\n  %74 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %74(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.11, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end33:                                     ; preds = %assert_end31\n  %75 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 2, !dbg !18\n  %76 = load i32, i32* %75, align 4, !dbg !18\n  %77 = icmp eq i32 %76, 3, !dbg !18\n  br i1 %77, label %assert_end37, label %assert_fail34, !dbg !18, !prof !19\n\nassert_fail34:                                    ; preds = %assert_end33\n  %78 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %78(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.12, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end37:                                     ; preds = %assert_end33\n  %79 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 3, i32 2, !dbg !18\n  %80 = load i16, i16* %79, align 2, !dbg !18\n  %81 = icmp eq i16 %80, 1, !dbg !18\n  %82 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 3, i32 1, !dbg !18\n  %83 = load i8, i8* %82, align 1, !dbg !18\n  %84 = icmp eq i8 %83, 32, !dbg !18\n  %85 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 3, i32 0, !dbg !18\n  %86 = load i8, i8* %85, align 1, !dbg !18\n  %87 = icmp eq i8 %86, 2, !dbg !18\n  %88 = and i1 %84, %87, !dbg !18\n  %89 = and i1 %81, %88, !dbg !18\n  br i1 %89, label %assert_end39, label %assert_fail38, !dbg !18, !prof !19\n\nassert_fail38:                                    ; preds = %assert_end37\n  %90 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %90(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.13, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end39:                                     ; preds = %assert_end37\n  %91 = load i64, i64* %default_function.ph_2.shape, align 8, !dbg !18, !tbaa !70\n  %92 = trunc i64 %91 to i32, !dbg !18\n  %93 = icmp eq i32 %92, 12, !dbg !18\n  br i1 %93, label %assert_end41, label %assert_fail40, !dbg !18, !prof !19\n\nassert_fail40:                                    ; preds = %assert_end39\n  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %94(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.14, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end41:                                     ; preds = %assert_end39\n  %95 = getelementptr inbounds i64, i64* %default_function.ph_2.shape, i64 1, !dbg !18\n  %96 = load i64, i64* %95, align 8, !dbg !18, !tbaa !80\n  %97 = trunc i64 %96 to i32, !dbg !18\n  %98 = icmp eq i32 %97, 14, !dbg !18\n  br i1 %98, label %assert_end43, label %assert_fail42, !dbg !18, !prof !19\n\nassert_fail42:                                    ; preds = %assert_end41\n  %99 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %99(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.15, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end43:                                     ; preds = %assert_end41\n  %100 = getelementptr inbounds i64, i64* %default_function.ph_2.shape, i64 2, !dbg !18\n  %101 = load i64, i64* %100, align 8, !dbg !18, !tbaa !82\n  %102 = trunc i64 %101 to i32, !dbg !18\n  %103 = icmp eq i32 %102, 18, !dbg !18\n  br i1 %103, label %assert_end45, label %assert_fail44, !dbg !18, !prof !19\n\nassert_fail44:                                    ; preds = %assert_end43\n  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %104(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.16, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end45:                                     ; preds = %assert_end43\n  %105 = icmp eq i64* %default_function.ph_2.strides, null, !dbg !18\n  br i1 %105, label %if_end47, label %if_then46, !dbg !18, !prof !54\n\nif_then46:                                        ; preds = %assert_end45\n  %106 = load i64, i64* %default_function.ph_2.strides, align 8, !dbg !18, !tbaa !85\n  %107 = trunc i64 %106 to i32, !dbg !18\n  %108 = icmp eq i32 %107, 252, !dbg !18\n  %109 = getelementptr inbounds i64, i64* %default_function.ph_2.strides, i64 1, !dbg !18\n  %110 = load i64, i64* %109, align 8, !dbg !18, !tbaa !95\n  %111 = trunc i64 %110 to i32, !dbg !18\n  %112 = icmp eq i32 %111, 18, !dbg !18\n  %113 = getelementptr inbounds i64, i64* %default_function.ph_2.strides, i64 2, !dbg !18\n  %114 = load i64, i64* %113, align 8, !dbg !18, !tbaa !97\n  %115 = trunc i64 %114 to i32, !dbg !18\n  %116 = icmp eq i32 %115, 1, !dbg !18\n  %117 = and i1 %112, %116, !dbg !18\n  %118 = and i1 %108, %117, !dbg !18\n  br i1 %118, label %if_end47, label %assert_fail48, !dbg !18, !prof !19\n\nif_end47:                                         ; preds = %assert_end45, %if_then46\n  %119 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 6, !dbg !18\n  %120 = load i64, i64* %119, align 8, !dbg !18\n  %121 = icmp eq i64 %120, 0, !dbg !18\n  br i1 %121, label %assert_end51, label %assert_fail50, !dbg !18, !prof !19\n\nassert_fail48:                                    ; preds = %if_then46\n  %122 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %122(i8* getelementptr inbounds ([250 x i8], [250 x i8]* @.str.17, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail50:                                    ; preds = %if_end47\n  %123 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %123(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.18, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end51:                                     ; preds = %if_end47\n  %124 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 1, i32 0, !dbg !18\n  %125 = load i32, i32* %124, align 4, !dbg !18\n  %126 = icmp eq i32 %125, 1, !dbg !18\n  br i1 %126, label %assert_end53, label %assert_fail52, !dbg !18, !prof !19\n\nassert_fail52:                                    ; preds = %assert_end51\n  %127 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %127(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.19, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end53:                                     ; preds = %assert_end51\n  %128 = getelementptr inbounds %1, %1* %ph_284, i64 0, i32 1, i32 1, !dbg !18\n  %129 = load i32, i32* %128, align 4, !dbg !18\n  %130 = icmp eq i32 %dev_id, %129, !dbg !18\n  br i1 %130, label %assert_end55, label %assert_fail54, !dbg !18, !prof !19\n\nassert_fail54:                                    ; preds = %assert_end53\n  %131 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %131(i8* getelementptr inbounds ([182 x i8], [182 x i8]* @.str.20, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end55:                                     ; preds = %assert_end53\n  %132 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 2, !dbg !18\n  %133 = load i32, i32* %132, align 4, !dbg !18\n  %134 = icmp eq i32 %133, 3, !dbg !18\n  br i1 %134, label %assert_end59, label %assert_fail56, !dbg !18, !prof !19\n\nassert_fail56:                                    ; preds = %assert_end55\n  %135 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %135(i8* getelementptr inbounds ([109 x i8], [109 x i8]* @.str.21, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end59:                                     ; preds = %assert_end55\n  %136 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 3, i32 2, !dbg !18\n  %137 = load i16, i16* %136, align 2, !dbg !18\n  %138 = icmp eq i16 %137, 1, !dbg !18\n  %139 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 3, i32 1, !dbg !18\n  %140 = load i8, i8* %139, align 1, !dbg !18\n  %141 = icmp eq i8 %140, 32, !dbg !18\n  %142 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 3, i32 0, !dbg !18\n  %143 = load i8, i8* %142, align 1, !dbg !18\n  %144 = icmp eq i8 %143, 2, !dbg !18\n  %145 = and i1 %141, %144, !dbg !18\n  %146 = and i1 %138, %145, !dbg !18\n  br i1 %146, label %assert_end61, label %assert_fail60, !dbg !18, !prof !19\n\nassert_fail60:                                    ; preds = %assert_end59\n  %147 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %147(i8* getelementptr inbounds ([239 x i8], [239 x i8]* @.str.22, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end61:                                     ; preds = %assert_end59\n  %148 = load i64, i64* %default_function.T_add.shape, align 8, !dbg !18, !tbaa !100\n  %149 = trunc i64 %148 to i32, !dbg !18\n  %150 = icmp eq i32 %149, 12, !dbg !18\n  br i1 %150, label %assert_end63, label %assert_fail62, !dbg !18, !prof !19\n\nassert_fail62:                                    ; preds = %assert_end61\n  %151 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %151(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.23, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end63:                                     ; preds = %assert_end61\n  %152 = getelementptr inbounds i64, i64* %default_function.T_add.shape, i64 1, !dbg !18\n  %153 = load i64, i64* %152, align 8, !dbg !18, !tbaa !110\n  %154 = trunc i64 %153 to i32, !dbg !18\n  %155 = icmp eq i32 %154, 14, !dbg !18\n  br i1 %155, label %assert_end65, label %assert_fail64, !dbg !18, !prof !19\n\nassert_fail64:                                    ; preds = %assert_end63\n  %156 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %156(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.24, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end65:                                     ; preds = %assert_end63\n  %157 = getelementptr inbounds i64, i64* %default_function.T_add.shape, i64 2, !dbg !18\n  %158 = load i64, i64* %157, align 8, !dbg !18, !tbaa !112\n  %159 = trunc i64 %158 to i32, !dbg !18\n  %160 = icmp eq i32 %159, 18, !dbg !18\n  br i1 %160, label %assert_end67, label %assert_fail66, !dbg !18, !prof !19\n\nassert_fail66:                                    ; preds = %assert_end65\n  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %161(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.25, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end67:                                     ; preds = %assert_end65\n  %162 = icmp eq i64* %default_function.T_add.strides, null, !dbg !18\n  br i1 %162, label %if_end69, label %if_then68, !dbg !18, !prof !54\n\nif_then68:                                        ; preds = %assert_end67\n  %163 = load i64, i64* %default_function.T_add.strides, align 8, !dbg !18, !tbaa !115\n  %164 = trunc i64 %163 to i32, !dbg !18\n  %165 = icmp eq i32 %164, 252, !dbg !18\n  %166 = getelementptr inbounds i64, i64* %default_function.T_add.strides, i64 1, !dbg !18\n  %167 = load i64, i64* %166, align 8, !dbg !18, !tbaa !125\n  %168 = trunc i64 %167 to i32, !dbg !18\n  %169 = icmp eq i32 %168, 18, !dbg !18\n  %170 = getelementptr inbounds i64, i64* %default_function.T_add.strides, i64 2, !dbg !18\n  %171 = load i64, i64* %170, align 8, !dbg !18, !tbaa !127\n  %172 = trunc i64 %171 to i32, !dbg !18\n  %173 = icmp eq i32 %172, 1, !dbg !18\n  %174 = and i1 %169, %173, !dbg !18\n  %175 = and i1 %165, %174, !dbg !18\n  br i1 %175, label %if_end69, label %assert_fail70, !dbg !18, !prof !19\n\nif_end69:                                         ; preds = %assert_end67, %if_then68\n  %176 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 6, !dbg !18\n  %177 = load i64, i64* %176, align 8, !dbg !18\n  %178 = icmp eq i64 %177, 0, !dbg !18\n  br i1 %178, label %assert_end73, label %assert_fail72, !dbg !18, !prof !19\n\nassert_fail70:                                    ; preds = %if_then68\n  %179 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %179(i8* getelementptr inbounds ([254 x i8], [254 x i8]* @.str.26, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail72:                                    ; preds = %if_end69\n  %180 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %180(i8* getelementptr inbounds ([199 x i8], [199 x i8]* @.str.27, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end73:                                     ; preds = %if_end69\n  %181 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 1, i32 0, !dbg !18\n  %182 = load i32, i32* %181, align 4, !dbg !18\n  %183 = icmp eq i32 %182, 1, !dbg !18\n  br i1 %183, label %assert_end75, label %assert_fail74, !dbg !18, !prof !19\n\nassert_fail74:                                    ; preds = %assert_end73\n  %184 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %184(i8* getelementptr inbounds ([179 x i8], [179 x i8]* @.str.28, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end75:                                     ; preds = %assert_end73\n  %185 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 1, i32 1, !dbg !18\n  %186 = load i32, i32* %185, align 4, !dbg !18\n  %187 = icmp eq i32 %dev_id, %186, !dbg !18\n  br i1 %187, label %assert_end77, label %assert_fail76, !dbg !18, !prof !19\n\nassert_fail76:                                    ; preds = %assert_end75\n  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %188(i8* getelementptr inbounds ([185 x i8], [185 x i8]* @.str.29, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end77:                                     ; preds = %assert_end75\n  %189 = call fastcc i32 @default_function_compute_(i8* nonnull %stack_value, float* %T_add_void_ptr88, i32* nonnull %stack_tcode82.sub, float* %ph_0_void_ptr86, float* %ph_2_void_ptr87), !dbg !18\n  ret i32 %189, !dbg !18\n}\n\n; Function Attrs: nounwind willreturn\ndeclare void @llvm.assume(i1) #1\n\n; Function Attrs: noinline\ndefine internal fastcc i32 @default_function_compute_(i8* noalias %0, float* noalias align 64 %1, i32* noalias %2, float* noalias align 64 %3, float* noalias align 64 %4) unnamed_addr #2 {\nentry:\n  %5 = alloca i8*, align 8\n  %6 = bitcast i8* %0 to %0*\n  %7 = bitcast i8* %0 to float**\n  store float* %1, float** %7, align 8\n  store i32 3, i32* %2, align 4, !tbaa !130\n  %8 = getelementptr inbounds i8, i8* %0, i64 8\n  %9 = bitcast i8* %8 to float**\n  store float* %3, float** %9, align 8\n  %10 = getelementptr inbounds i32, i32* %2, i64 1\n  store i32 3, i32* %10, align 4, !tbaa !141\n  %11 = getelementptr inbounds i8, i8* %0, i64 16\n  %12 = bitcast i8* %11 to float**\n  store float* %4, float** %12, align 8\n  %13 = getelementptr inbounds i32, i32* %2, i64 2\n  store i32 3, i32* %13, align 4, !tbaa !143\n  %14 = getelementptr inbounds i8, i8* %0, i64 24\n  %15 = bitcast i8* %14 to %0*\n  %16 = getelementptr inbounds i32, i32* %2, i64 3\n  %17 = load i32 (i8*, %0*, i32*, i32, %0*, i32*)*, i32 (i8*, %0*, i32*, i32, %0*, i32*)** @__TVMFuncCall, align 8, !tbaa !20\n  %18 = load i8*, i8** @.tvm_func.default_function_kernel, align 8\n  %19 = icmp eq i8* %18, null\n  br i1 %19, label %handle_init, label %handle_init_end, !prof !54\n\nhandle_init:                                      ; preds = %entry\n  %20 = load i8*, i8** @__tvm_module_ctx, align 8, !tbaa !20\n  %21 = load i32 (i8*, i8*, i8**)*, i32 (i8*, i8*, i8**)** @__TVMBackendGetFuncFromEnv, align 8, !tbaa !20\n  %22 = call i32 %21(i8* %20, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.30, i64 0, i64 0), i8** nonnull %5)\n  %23 = icmp eq i32 %22, 0\n  br i1 %23, label %call_end, label %call_fail, !prof !19\n\nhandle_init_end:                                  ; preds = %entry, %call_end\n  %24 = phi i8* [ %18, %entry ], [ %27, %call_end ]\n  %25 = call i32 %17(i8* %24, %0* %6, i32* nonnull %2, i32 3, %0* nonnull %15, i32* nonnull %16)\n  %26 = icmp eq i32 %25, 0\n  br i1 %26, label %call_end2, label %call_fail, !prof !19\n\ncall_fail:                                        ; preds = %call_end2, %handle_init_end, %handle_init\n  %merge = phi i32 [ %22, %handle_init ], [ %25, %handle_init_end ], [ 0, %call_end2 ]\n  ret i32 %merge\n\ncall_end:                                         ; preds = %handle_init\n  %27 = load i8*, i8** %5, align 8\n  store i8* %27, i8** @.tvm_func.default_function_kernel, align 8\n  br label %handle_init_end\n\ncall_end2:                                        ; preds = %handle_init_end\n  %28 = bitcast i8* %14 to i64*\n  %29 = load i64, i64* %28, align 8\n  %kernel_error_code = trunc i64 %29 to i32\n  %30 = icmp eq i32 %kernel_error_code, 0\n  br i1 %30, label %call_fail, label %assert_fail, !prof !19\n\nassert_fail:                                      ; preds = %call_end2\n  %31 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !20\n  call void %31(i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.31, i64 0, i64 0))\n  ret i32 -1\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\n; Function Attrs: nounwind readnone speculatable willreturn\ndeclare void @llvm.dbg.value(metadata, metadata, metadata) #4\n\nattributes #0 = { \"target-cpu\"=\"generic\" }\nattributes #1 = { nounwind willreturn }\nattributes #2 = { noinline \"target-cpu\"=\"generic\" }\nattributes #3 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\nattributes #4 = { nounwind readnone speculatable willreturn }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n!5 = distinct !DISubprogram(name: \"main.tir\", scope: !1, file: !1, type: !6, spFlags: DISPFlagLocalToUnit | DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !11)\n!6 = !DISubroutineType(types: !7)\n!7 = !{!8, !9, !10, !8, !9, !10, !9}\n!8 = !DIBasicType(name: \"int32\", size: 32, encoding: DW_ATE_signed)\n!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null)\n!10 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8)\n!11 = !{!12, !13, !14, !15, !16, !17}\n!12 = !DILocalVariable(name: \"arg1\", arg: 1, scope: !5, file: !1, type: !9)\n!13 = !DILocalVariable(name: \"arg2\", arg: 2, scope: !5, file: !1, type: !10)\n!14 = !DILocalVariable(name: \"arg3\", arg: 3, scope: !5, file: !1, type: !8)\n!15 = !DILocalVariable(name: \"arg4\", arg: 4, scope: !5, file: !1, type: !9)\n!16 = !DILocalVariable(name: \"arg5\", arg: 5, scope: !5, file: !1, type: !10)\n!17 = !DILocalVariable(name: \"arg6\", arg: 6, scope: !5, file: !1, type: !9)\n!18 = !DILocation(line: 0, scope: !5)\n!19 = !{!\"branch_weights\", i32 1048576, i32 1}\n!20 = !{!21, !21, i64 0}\n!21 = !{!\"ctx_ptr\", !22, i64 0}\n!22 = !{!\"tvm-tbaa\"}\n!23 = !{!24, !24, i64 0}\n!24 = !{!\"0x48bac00.w4.b0\", !25, i64 0}\n!25 = !{!\"0x48bac00.w8.b0\", !26, i64 0}\n!26 = !{!\"0x48bac00.w16.b0\", !27, i64 0}\n!27 = !{!\"0x48bac00.w32.b0\", !28, i64 0}\n!28 = !{!\"0x48bac00.w64.b0\", !29, i64 0}\n!29 = !{!\"0x48bac00.w128.b0\", !30, i64 0}\n!30 = !{!\"0x48bac00.w256.b0\", !31, i64 0}\n!31 = !{!\"0x48bac00.w512.b0\", !32, i64 0}\n!32 = !{!\"0x48bac00.w1024.b0\", !33, i64 0}\n!33 = !{!\"0x48bac00\", !22, i64 0}\n!34 = !{!35, !35, i64 0}\n!35 = !{!\"0x48bac00.w4.b4\", !25, i64 0}\n!36 = !{!37, !37, i64 0}\n!37 = !{!\"0x48bac00.w4.b8\", !38, i64 0}\n!38 = !{!\"0x48bac00.w8.b8\", !26, i64 0}\n!39 = !{!40, !40, i64 0}\n!40 = !{!\"0x48a2350.w8.b0\", !41, i64 0}\n!41 = !{!\"0x48a2350.w16.b0\", !42, i64 0}\n!42 = !{!\"0x48a2350.w32.b0\", !43, i64 0}\n!43 = !{!\"0x48a2350.w64.b0\", !44, i64 0}\n!44 = !{!\"0x48a2350.w128.b0\", !45, i64 0}\n!45 = !{!\"0x48a2350.w256.b0\", !46, i64 0}\n!46 = !{!\"0x48a2350.w512.b0\", !47, i64 0}\n!47 = !{!\"0x48a2350.w1024.b0\", !48, i64 0}\n!48 = !{!\"0x48a2350\", !22, i64 0}\n!49 = !{!50, !50, i64 0}\n!50 = !{!\"0x48a2350.w8.b8\", !41, i64 0}\n!51 = !{!52, !52, i64 0}\n!52 = !{!\"0x48a2350.w8.b16\", !53, i64 0}\n!53 = !{!\"0x48a2350.w16.b16\", !42, i64 0}\n!54 = !{!\"branch_weights\", i32 1, i32 1048576}\n!55 = !{!56, !56, i64 0}\n!56 = !{!\"0x48bbca0.w8.b0\", !57, i64 0}\n!57 = !{!\"0x48bbca0.w16.b0\", !58, i64 0}\n!58 = !{!\"0x48bbca0.w32.b0\", !59, i64 0}\n!59 = !{!\"0x48bbca0.w64.b0\", !60, i64 0}\n!60 = !{!\"0x48bbca0.w128.b0\", !61, i64 0}\n!61 = !{!\"0x48bbca0.w256.b0\", !62, i64 0}\n!62 = !{!\"0x48bbca0.w512.b0\", !63, i64 0}\n!63 = !{!\"0x48bbca0.w1024.b0\", !64, i64 0}\n!64 = !{!\"0x48bbca0\", !22, i64 0}\n!65 = !{!66, !66, i64 0}\n!66 = !{!\"0x48bbca0.w8.b8\", !57, i64 0}\n!67 = !{!68, !68, i64 0}\n!68 = !{!\"0x48bbca0.w8.b16\", !69, i64 0}\n!69 = !{!\"0x48bbca0.w16.b16\", !58, i64 0}\n!70 = !{!71, !71, i64 0}\n!71 = !{!\"0x47e66b0.w8.b0\", !72, i64 0}\n!72 = !{!\"0x47e66b0.w16.b0\", !73, i64 0}\n!73 = !{!\"0x47e66b0.w32.b0\", !74, i64 0}\n!74 = !{!\"0x47e66b0.w64.b0\", !75, i64 0}\n!75 = !{!\"0x47e66b0.w128.b0\", !76, i64 0}\n!76 = !{!\"0x47e66b0.w256.b0\", !77, i64 0}\n!77 = !{!\"0x47e66b0.w512.b0\", !78, i64 0}\n!78 = !{!\"0x47e66b0.w1024.b0\", !79, i64 0}\n!79 = !{!\"0x47e66b0\", !22, i64 0}\n!80 = !{!81, !81, i64 0}\n!81 = !{!\"0x47e66b0.w8.b8\", !72, i64 0}\n!82 = !{!83, !83, i64 0}\n!83 = !{!\"0x47e66b0.w8.b16\", !84, i64 0}\n!84 = !{!\"0x47e66b0.w16.b16\", !73, i64 0}\n!85 = !{!86, !86, i64 0}\n!86 = !{!\"0x48e97f0.w8.b0\", !87, i64 0}\n!87 = !{!\"0x48e97f0.w16.b0\", !88, i64 0}\n!88 = !{!\"0x48e97f0.w32.b0\", !89, i64 0}\n!89 = !{!\"0x48e97f0.w64.b0\", !90, i64 0}\n!90 = !{!\"0x48e97f0.w128.b0\", !91, i64 0}\n!91 = !{!\"0x48e97f0.w256.b0\", !92, i64 0}\n!92 = !{!\"0x48e97f0.w512.b0\", !93, i64 0}\n!93 = !{!\"0x48e97f0.w1024.b0\", !94, i64 0}\n!94 = !{!\"0x48e97f0\", !22, i64 0}\n!95 = !{!96, !96, i64 0}\n!96 = !{!\"0x48e97f0.w8.b8\", !87, i64 0}\n!97 = !{!98, !98, i64 0}\n!98 = !{!\"0x48e97f0.w8.b16\", !99, i64 0}\n!99 = !{!\"0x48e97f0.w16.b16\", !88, i64 0}\n!100 = !{!101, !101, i64 0}\n!101 = !{!\"0x489afe0.w8.b0\", !102, i64 0}\n!102 = !{!\"0x489afe0.w16.b0\", !103, i64 0}\n!103 = !{!\"0x489afe0.w32.b0\", !104, i64 0}\n!104 = !{!\"0x489afe0.w64.b0\", !105, i64 0}\n!105 = !{!\"0x489afe0.w128.b0\", !106, i64 0}\n!106 = !{!\"0x489afe0.w256.b0\", !107, i64 0}\n!107 = !{!\"0x489afe0.w512.b0\", !108, i64 0}\n!108 = !{!\"0x489afe0.w1024.b0\", !109, i64 0}\n!109 = !{!\"0x489afe0\", !22, i64 0}\n!110 = !{!111, !111, i64 0}\n!111 = !{!\"0x489afe0.w8.b8\", !102, i64 0}\n!112 = !{!113, !113, i64 0}\n!113 = !{!\"0x489afe0.w8.b16\", !114, i64 0}\n!114 = !{!\"0x489afe0.w16.b16\", !103, i64 0}\n!115 = !{!116, !116, i64 0}\n!116 = !{!\"0x489c0e0.w8.b0\", !117, i64 0}\n!117 = !{!\"0x489c0e0.w16.b0\", !118, i64 0}\n!118 = !{!\"0x489c0e0.w32.b0\", !119, i64 0}\n!119 = !{!\"0x489c0e0.w64.b0\", !120, i64 0}\n!120 = !{!\"0x489c0e0.w128.b0\", !121, i64 0}\n!121 = !{!\"0x489c0e0.w256.b0\", !122, i64 0}\n!122 = !{!\"0x489c0e0.w512.b0\", !123, i64 0}\n!123 = !{!\"0x489c0e0.w1024.b0\", !124, i64 0}\n!124 = !{!\"0x489c0e0\", !22, i64 0}\n!125 = !{!126, !126, i64 0}\n!126 = !{!\"0x489c0e0.w8.b8\", !117, i64 0}\n!127 = !{!128, !128, i64 0}\n!128 = !{!\"0x489c0e0.w8.b16\", !129, i64 0}\n!129 = !{!\"0x489c0e0.w16.b16\", !118, i64 0}\n!130 = !{!131, !131, i64 0}\n!131 = !{!\"0x4382230.w4.b0\", !132, i64 0}\n!132 = !{!\"0x4382230.w8.b0\", !133, i64 0}\n!133 = !{!\"0x4382230.w16.b0\", !134, i64 0}\n!134 = !{!\"0x4382230.w32.b0\", !135, i64 0}\n!135 = !{!\"0x4382230.w64.b0\", !136, i64 0}\n!136 = !{!\"0x4382230.w128.b0\", !137, i64 0}\n!137 = !{!\"0x4382230.w256.b0\", !138, i64 0}\n!138 = !{!\"0x4382230.w512.b0\", !139, i64 0}\n!139 = !{!\"0x4382230.w1024.b0\", !140, i64 0}\n!140 = !{!\"0x4382230\", !22, i64 0}\n!141 = !{!142, !142, i64 0}\n!142 = !{!\"0x4382230.w4.b4\", !132, i64 0}\n!143 = !{!144, !144, i64 0}\n!144 = !{!\"0x4382230.w4.b8\", !145, i64 0}\n!145 = !{!\"0x4382230.w8.b8\", !133, i64 0}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_2);\nextern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_2) {\n  T_add[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] + ph_2[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 18), \"float32\"), ph_2: T.Buffer((12, 14, 18), \"float32\"), T_add: T.Buffer((12, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3024):\n            T_add_1 = T.Buffer((3024,), data=T_add.data)\n            ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n            ph_2_1 = T.Buffer((3024,), data=ph_2.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_2_1[ax0_ax1_fused_ax2_fused]", "op_args": []}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* ph_0, float* ph_2);\nvoid default_function_kernel(float* T_add, float* ph_0, float* ph_2) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3024; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_2[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_2);\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_2) {\n  T_add[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] + ph_2[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 18), \"float32\"), ph_2: T.Buffer((12, 14, 18), \"float32\"), T_add: T.Buffer((12, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3024):\n            T_add_1 = T.Buffer((3024,), data=T_add.data)\n            ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n            ph_2_1 = T.Buffer((3024,), data=ph_2.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_2_1[ax0_ax1_fused_ax2_fused]", "op_args": []}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3024; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 18), \"float32\"), compute: T.Buffer((12, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3024):\n            compute_1 = T.Buffer((3024,), data=compute.data)\n            ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": []}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* ph_0, float* ph_2) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3024; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_2[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_2) {\n  T_add[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] + ph_2[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 18), \"float32\"), ph_2: T.Buffer((12, 14, 18), \"float32\"), T_add: T.Buffer((12, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3024):\n            T_add_1 = T.Buffer((3024,), data=T_add.data)\n            ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n            ph_2_1 = T.Buffer((3024,), data=ph_2.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_2_1[ax0_ax1_fused_ax2_fused]", "op_args": []}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* ph_0, float* ph_2, float* ph_4) {\n  float auto_scheduler_layout_transform[3024];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3024; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_2[ax0_ax1_fused_ax2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 14; ++ax6) {\n        for (int32_t ax7 = 0; ax7 < 9; ++ax7) {\n          for (int32_t ax8 = 0; ax8 < 6; ++ax8) {\n            auto_scheduler_layout_transform[(((((ax4 * 1512) + (ax5 * 756)) + (ax6 * 54)) + (ax7 * 6)) + ax8)] = ph_4[(((((ax5 * 1512) + (ax8 * 252)) + (ax4 * 126)) + (ax7 * 14)) + ax6)];\n          }\n        }\n      }\n    }\n  }\n  for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 14; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 6; ++b_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 14; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 1176) + (b_inner_init * 196)) + (i_inner_init * 14)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 14; ++j_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 9; ++k_inner) {\n          for (int32_t b_inner = 0; b_inner < 6; ++b_inner) {\n            for (int32_t i_inner = 0; i_inner < 14; ++i_inner) {\n              T_batch_matmul_NN[((((b_outer_inner * 1176) + (b_inner * 196)) + (i_inner * 14)) + j_outer_inner)] = (T_batch_matmul_NN[((((b_outer_inner * 1176) + (b_inner * 196)) + (i_inner * 14)) + j_outer_inner)] + (ph_2[(((((b_outer_inner * 1512) + (b_inner * 252)) + (i_inner * 18)) + (k_outer * 9)) + k_inner)] * auto_scheduler_layout_transform[(((((k_outer * 1512) + (b_outer_inner * 756)) + (j_outer_inner * 54)) + (k_inner * 6)) + b_inner)]));\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(49) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_2, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float ph_4_shared[126];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n    for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n      T_batch_matmul_NN_local[((i_c_outer_inner_init * 2) + j_c_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 7) + (((int)threadIdx.x) / 7)) < 9) {\n        *(float2*)(ph_4_shared + ((ax0_ax1_fused_ax2_fused_outer_outer * 98) + (((int)threadIdx.x) * 2))) = *(float2*)(ph_4 + ((((((int)blockIdx.x) * 252) + (k_outer_outer * 126)) + (ax0_ax1_fused_ax2_fused_outer_outer * 98)) + (((int)threadIdx.x) * 2)));\n      }\n    }\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 9; ++k_outer_inner) {\n      for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n        for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n          T_batch_matmul_NN_local[((i_c_outer_inner * 2) + j_c_inner)] = (T_batch_matmul_NN_local[((i_c_outer_inner * 2) + j_c_inner)] + (ph_2[(((((((int)blockIdx.x) * 252) + ((((int)threadIdx.x) / 7) * 36)) + (i_c_outer_inner * 18)) + (k_outer_outer * 9)) + k_outer_inner)] * ph_4_shared[(((k_outer_inner * 14) + ((((int)threadIdx.x) % 7) * 2)) + j_c_inner)]));\n        }\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 2; ++i_inner) {\n    for (int j_inner = 0; j_inner < 2; ++j_inner) {\n      T_batch_matmul_NN[(((((((int)blockIdx.x) * 196) + ((((int)threadIdx.x) / 7) * 28)) + (i_inner * 14)) + ((((int)threadIdx.x) % 7) * 2)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 2) + j_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_2) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 189) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_2[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 18), \"float32\"), ph_2: T.Buffer((12, 14, 18), \"float32\"), ph_4: T.Buffer((12, 18, 14), \"float32\"), T_add: T.Buffer((12, 14, 18), \"float32\"), T_batch_matmul_NN: T.Buffer((12, 14, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([3024], \"float32\", \"global\")\n        ph_2_1 = T.Buffer((3024,), data=ph_2.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3024):\n            T_add_1 = T.Buffer((3024,), data=T_add.data)\n            ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_2_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((3024,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax6, ax7, ax8 in T.grid(2, 2, 14, 9, 6):\n            ph_4_1 = T.Buffer((3024,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[ax4 * 1512 + ax5 * 756 + ax6 * 54 + ax7 * 6 + ax8] = ph_4_1[ax5 * 1512 + ax8 * 252 + ax4 * 126 + ax7 * 14 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((2352,), data=T_batch_matmul_NN.data)\n        for b_outer_inner_init, j_outer_inner_init, b_inner_init, i_inner_init in T.grid(2, 14, 6, 14):\n            T_batch_matmul_NN_1[b_outer_inner_init * 1176 + b_inner_init * 196 + i_inner_init * 14 + j_outer_inner_init] = T.float32(0)\n        for k_outer, b_outer_inner, j_outer_inner, k_inner, b_inner, i_inner in T.grid(2, 2, 14, 9, 6, 14):\n            cse_var_1: T.int32 = b_outer_inner * 1176 + b_inner * 196 + i_inner * 14 + j_outer_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_2_1[b_outer_inner * 1512 + b_inner * 252 + i_inner * 18 + k_outer * 9 + k_inner] * auto_scheduler_layout_transform_1[k_outer * 1512 + b_outer_inner * 756 + j_outer_inner * 54 + k_inner * 6 + b_inner]", "op_args": []}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = sinf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 18), \"float32\"), compute: T.Buffer((12, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(168):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_1 = T.Buffer((3024,), data=compute.data)\n                ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n                compute_1[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": []}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* ph_2, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1040; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_2[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_2, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 969) {\n    T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_2[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_2: T.Buffer((13, 10, 8), \"float32\"), ph_3: T.Buffer((13, 10, 8), \"float32\"), T_add: T.Buffer((13, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1040):\n            T_add_1 = T.Buffer((1040,), data=T_add.data)\n            ph_2_1 = T.Buffer((1040,), data=ph_2.data)\n            ph_3_1 = T.Buffer((1040,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_2_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]", "op_args": []}{"op_name": "topology_expansion", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #0 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #0 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\nattributes #0 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3);\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((int)threadIdx.x)] = (ph_0[((int)threadIdx.x)] + ph_3[((int)threadIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 12, 10), \"float32\"), ph_3: T.Buffer((1, 12, 10), \"float32\"), T_add: T.Buffer((1, 12, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_add_1 = T.Buffer((120,), data=T_add.data)\n                ph_0_1 = T.Buffer((120,), data=ph_0.data)\n                ph_3_1 = T.Buffer((120,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]", "op_args": []}{"op_name": "topology_expansion", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%0 = type { double }\n%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }\n%2 = type { i32, i32 }\n%3 = type { i8, i8, i16 }\n\n@__tvm_module_ctx = linkonce dllexport local_unnamed_addr global i8* null, align 8\n@__TVMFuncCall = linkonce dllexport local_unnamed_addr global i32 (i8*, %0*, i32*, i32, %0*, i32*)* null, align 8\n@__TVMBackendGetFuncFromEnv = linkonce dllexport local_unnamed_addr global i32 (i8*, i8*, i8**)* null, align 8\n@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8\n@.str = private constant [67 x i8] c\"Assert fail: num_args == 3, default_function: num_args should be 3\\00\", align 1\n@.str.1 = private constant [130 x i8] c\"Assert fail: ph_0_code == 3 or ph_0_code == 13 or ph_0_code == 7 or ph_0_code == 4, default_function: Expect arg[0] to be pointer\\00\", align 1\n@.str.2 = private constant [130 x i8] c\"Assert fail: ph_3_code == 3 or ph_3_code == 13 or ph_3_code == 7 or ph_3_code == 4, default_function: Expect arg[1] to be pointer\\00\", align 1\n@.str.3 = private constant [134 x i8] c\"Assert fail: T_add_code == 3 or T_add_code == 13 or T_add_code == 7 or T_add_code == 4, default_function: Expect arg[2] to be pointer\\00\", align 1\n@.str.4 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_0, 0, 4, \\22int32\\22), default_function.ph_0.ndim is expected to equal 3\\00\", align 1\n@.str.5 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_0, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_0, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_0.dtype is expected to be float32\\00\", align 1\n@.str.6 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[0]) == 10, Argument default_function.ph_0.shape[0] has an unsatisfied constraint: 10 == T.Cast(\\22int32\\22, default_function_ph_0_shape[0])\\00\", align 1\n@.str.7 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[1]) == 1, Argument default_function.ph_0.shape[1] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_ph_0_shape[1])\\00\", align 1\n@.str.8 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[2]) == 12, Argument default_function.ph_0.shape[2] has an unsatisfied constraint: 12 == T.Cast(\\22int32\\22, default_function_ph_0_shape[2])\\00\", align 1\n@.str.9 = private constant [189 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_0_strides[2]) and 12 == T.Cast(\\22int32\\22, default_function_ph_0_strides[0]), default_function.ph_0.strides: expected to be compact array\\00\", align 1\n@.str.10 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22), Argument default_function.ph_0.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.11 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_0.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.12 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_3, 0, 4, \\22int32\\22), default_function.ph_3.ndim is expected to equal 3\\00\", align 1\n@.str.13 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_3, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_3, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_3, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_3.dtype is expected to be float32\\00\", align 1\n@.str.14 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_3_shape[0]) == 10, Argument default_function.ph_3.shape[0] has an unsatisfied constraint: 10 == T.Cast(\\22int32\\22, default_function_ph_3_shape[0])\\00\", align 1\n@.str.15 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_3_shape[1]) == 1, Argument default_function.ph_3.shape[1] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_ph_3_shape[1])\\00\", align 1\n@.str.16 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_3_shape[2]) == 12, Argument default_function.ph_3.shape[2] has an unsatisfied constraint: 12 == T.Cast(\\22int32\\22, default_function_ph_3_shape[2])\\00\", align 1\n@.str.17 = private constant [189 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_3_strides[2]) and 12 == T.Cast(\\22int32\\22, default_function_ph_3_strides[0]), default_function.ph_3.strides: expected to be compact array\\00\", align 1\n@.str.18 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_3, 0, 8, \\22uint64\\22), Argument default_function.ph_3.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_3, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.19 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_3, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_3.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_3, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.20 = private constant [182 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(ph_3, 0, 9, \\22int32\\22), Argument default_function.ph_3.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(ph_3, 0, 9, \\22int32\\22)\\00\", align 1\n@.str.21 = private constant [109 x i8] c\"Assert fail: 3 == T.tvm_struct_get(T_add, 0, 4, \\22int32\\22), default_function.T_add.ndim is expected to equal 3\\00\", align 1\n@.str.22 = private constant [239 x i8] c\"Assert fail: T.tvm_struct_get(T_add, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(T_add, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(T_add, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.T_add.dtype is expected to be float32\\00\", align 1\n@.str.23 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[0]) == 10, Argument default_function.T_add.shape[0] has an unsatisfied constraint: 10 == T.Cast(\\22int32\\22, default_function_T_add_shape[0])\\00\", align 1\n@.str.24 = private constant [194 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[1]) == 1, Argument default_function.T_add.shape[1] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_T_add_shape[1])\\00\", align 1\n@.str.25 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[2]) == 12, Argument default_function.T_add.shape[2] has an unsatisfied constraint: 12 == T.Cast(\\22int32\\22, default_function_T_add_shape[2])\\00\", align 1\n@.str.26 = private constant [192 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_T_add_strides[2]) and 12 == T.Cast(\\22int32\\22, default_function_T_add_strides[0]), default_function.T_add.strides: expected to be compact array\\00\", align 1\n@.str.27 = private constant [199 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(T_add, 0, 8, \\22uint64\\22), Argument default_function.T_add.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(T_add, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.28 = private constant [179 x i8] c\"Assert fail: T.tvm_struct_get(T_add, 0, 10, \\22int32\\22) == 1, Argument default_function.T_add.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(T_add, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.29 = private constant [185 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(T_add, 0, 9, \\22int32\\22), Argument default_function.T_add.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(T_add, 0, 9, \\22int32\\22)\\00\", align 1\n@.tvm_func.default_function_kernel = internal unnamed_addr global i8* null, align 8\n@.str.30 = private constant [24 x i8] c\"default_function_kernel\\00\", align 1\n@.str.31 = private constant [68 x i8] c\"Assert fail: kernel_error_code == 0, Error executing compute kernel\\00\", align 1\n@__tvm_main__ = weak dllexport local_unnamed_addr constant [17 x i8] c\"default_function\\00\", align 1\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\ndefine dllexport i32 @default_function(i8* noalias nocapture readonly %args, i32* noalias nocapture readonly %arg_type_ids, i32 %num_args, i8* noalias nocapture readnone %out_ret_value, i32* noalias nocapture readnone %out_ret_tcode, i8* noalias nocapture readnone %resource_handle) local_unnamed_addr #0 !dbg !5 {\nentry:\n  call void @llvm.dbg.value(metadata i8* %args, metadata !12, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %arg_type_ids, metadata !13, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32 %num_args, metadata !14, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %out_ret_value, metadata !15, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %out_ret_tcode, metadata !16, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %resource_handle, metadata !17, metadata !DIExpression()), !dbg !18\n  %stack_value_void_ptr81 = alloca [4 x %0], align 8, !dbg !18\n  %stack_tcode82 = alloca [4 x i32], align 4, !dbg !18\n  %stack_tcode82.sub = getelementptr inbounds [4 x i32], [4 x i32]* %stack_tcode82, i64 0, i64 0\n  %stack_value = bitcast [4 x %0]* %stack_value_void_ptr81 to i8*, !dbg !18\n  %0 = icmp eq i32 %num_args, 3, !dbg !18\n  br i1 %0, label %assert_end, label %assert_fail, !dbg !18, !prof !19\n\nassert_fail:                                      ; preds = %entry\n  %1 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %1(i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end:                                       ; preds = %entry\n  %ph_0.code = load i32, i32* %arg_type_ids, align 4, !dbg !18, !tbaa !23\n  %2 = getelementptr inbounds i32, i32* %arg_type_ids, i64 1, !dbg !18\n  %ph_3.code = load i32, i32* %2, align 4, !dbg !18, !tbaa !34\n  %3 = getelementptr inbounds i32, i32* %arg_type_ids, i64 2, !dbg !18\n  %T_add.code = load i32, i32* %3, align 4, !dbg !18, !tbaa !36\n  %4 = bitcast i8* %args to %1**, !dbg !18\n  %ph_083 = load %1*, %1** %4, align 8, !dbg !18\n  %5 = getelementptr inbounds i8, i8* %args, i64 8, !dbg !18\n  %6 = bitcast i8* %5 to %1**, !dbg !18\n  %ph_384 = load %1*, %1** %6, align 8, !dbg !18\n  %7 = getelementptr inbounds i8, i8* %args, i64 16, !dbg !18\n  %8 = bitcast i8* %7 to %1**, !dbg !18\n  %T_add85 = load %1*, %1** %8, align 8, !dbg !18\n  %9 = bitcast %1* %ph_083 to float**, !dbg !18\n  %ph_0_void_ptr86 = load float*, float** %9, align 8, !dbg !18\n  %ptrint = ptrtoint float* %ph_0_void_ptr86 to i64, !dbg !18\n  %maskedptr = and i64 %ptrint, 63, !dbg !18\n  %maskcond = icmp eq i64 %maskedptr, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond), !dbg !18\n  %10 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 4, !dbg !18\n  %default_function.ph_0.shape = load i64*, i64** %10, align 8, !dbg !18\n  %11 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 5, !dbg !18\n  %default_function.ph_0.strides = load i64*, i64** %11, align 8, !dbg !18\n  %12 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 1, !dbg !18\n  %dev_id = load i32, i32* %12, align 4, !dbg !18\n  %13 = bitcast %1* %ph_384 to float**, !dbg !18\n  %ph_3_void_ptr87 = load float*, float** %13, align 8, !dbg !18\n  %ptrint3 = ptrtoint float* %ph_3_void_ptr87 to i64, !dbg !18\n  %maskedptr4 = and i64 %ptrint3, 63, !dbg !18\n  %maskcond5 = icmp eq i64 %maskedptr4, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond5), !dbg !18\n  %14 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 4, !dbg !18\n  %default_function.ph_3.shape = load i64*, i64** %14, align 8, !dbg !18\n  %15 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 5, !dbg !18\n  %default_function.ph_3.strides = load i64*, i64** %15, align 8, !dbg !18\n  %16 = bitcast %1* %T_add85 to float**, !dbg !18\n  %T_add_void_ptr88 = load float*, float** %16, align 8, !dbg !18\n  %ptrint7 = ptrtoint float* %T_add_void_ptr88 to i64, !dbg !18\n  %maskedptr8 = and i64 %ptrint7, 63, !dbg !18\n  %maskcond9 = icmp eq i64 %maskedptr8, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond9), !dbg !18\n  %17 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 4, !dbg !18\n  %default_function.T_add.shape = load i64*, i64** %17, align 8, !dbg !18\n  %18 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 5, !dbg !18\n  %default_function.T_add.strides = load i64*, i64** %18, align 8, !dbg !18\n  switch i32 %ph_0.code, label %assert_fail10 [\n    i32 13, label %assert_end11\n    i32 7, label %assert_end11\n    i32 4, label %assert_end11\n    i32 3, label %assert_end11\n  ], !dbg !18\n\nassert_fail10:                                    ; preds = %assert_end\n  %19 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %19(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.1, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end11:                                     ; preds = %assert_end, %assert_end, %assert_end, %assert_end\n  switch i32 %ph_3.code, label %assert_fail12 [\n    i32 13, label %assert_end13\n    i32 7, label %assert_end13\n    i32 4, label %assert_end13\n    i32 3, label %assert_end13\n  ], !dbg !18\n\nassert_fail12:                                    ; preds = %assert_end11\n  %20 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %20(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.2, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end13:                                     ; preds = %assert_end11, %assert_end11, %assert_end11, %assert_end11\n  switch i32 %T_add.code, label %assert_fail14 [\n    i32 13, label %assert_end15\n    i32 7, label %assert_end15\n    i32 4, label %assert_end15\n    i32 3, label %assert_end15\n  ], !dbg !18\n\nassert_fail14:                                    ; preds = %assert_end13\n  %21 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %21(i8* getelementptr inbounds ([134 x i8], [134 x i8]* @.str.3, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end15:                                     ; preds = %assert_end13, %assert_end13, %assert_end13, %assert_end13\n  %22 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 2, !dbg !18\n  %23 = load i32, i32* %22, align 4, !dbg !18\n  %24 = icmp eq i32 %23, 3, !dbg !18\n  br i1 %24, label %assert_end19, label %assert_fail16, !dbg !18, !prof !19\n\nassert_fail16:                                    ; preds = %assert_end15\n  %25 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %25(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.4, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end19:                                     ; preds = %assert_end15\n  %26 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 2, !dbg !18\n  %27 = load i16, i16* %26, align 2, !dbg !18\n  %28 = icmp eq i16 %27, 1, !dbg !18\n  %29 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 1, !dbg !18\n  %30 = load i8, i8* %29, align 1, !dbg !18\n  %31 = icmp eq i8 %30, 32, !dbg !18\n  %32 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 0, !dbg !18\n  %33 = load i8, i8* %32, align 1, !dbg !18\n  %34 = icmp eq i8 %33, 2, !dbg !18\n  %35 = and i1 %31, %34, !dbg !18\n  %36 = and i1 %28, %35, !dbg !18\n  br i1 %36, label %assert_end21, label %assert_fail20, !dbg !18, !prof !19\n\nassert_fail20:                                    ; preds = %assert_end19\n  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %37(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.5, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end21:                                     ; preds = %assert_end19\n  %38 = load i64, i64* %default_function.ph_0.shape, align 8, !dbg !18, !tbaa !39\n  %39 = trunc i64 %38 to i32, !dbg !18\n  %40 = icmp eq i32 %39, 10, !dbg !18\n  br i1 %40, label %assert_end23, label %assert_fail22, !dbg !18, !prof !19\n\nassert_fail22:                                    ; preds = %assert_end21\n  %41 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %41(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.6, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end23:                                     ; preds = %assert_end21\n  %42 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 1, !dbg !18\n  %43 = load i64, i64* %42, align 8, !dbg !18, !tbaa !49\n  %44 = trunc i64 %43 to i32, !dbg !18\n  %45 = icmp eq i32 %44, 1, !dbg !18\n  br i1 %45, label %assert_end25, label %assert_fail24, !dbg !18, !prof !19\n\nassert_fail24:                                    ; preds = %assert_end23\n  %46 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %46(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.7, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end25:                                     ; preds = %assert_end23\n  %47 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 2, !dbg !18\n  %48 = load i64, i64* %47, align 8, !dbg !18, !tbaa !51\n  %49 = trunc i64 %48 to i32, !dbg !18\n  %50 = icmp eq i32 %49, 12, !dbg !18\n  br i1 %50, label %assert_end27, label %assert_fail26, !dbg !18, !prof !19\n\nassert_fail26:                                    ; preds = %assert_end25\n  %51 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %51(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.8, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end27:                                     ; preds = %assert_end25\n  %52 = icmp eq i64* %default_function.ph_0.strides, null, !dbg !18\n  br i1 %52, label %if_end, label %if_then, !dbg !18, !prof !54\n\nif_then:                                          ; preds = %assert_end27\n  %53 = load i64, i64* %default_function.ph_0.strides, align 8, !dbg !18, !tbaa !55\n  %54 = trunc i64 %53 to i32, !dbg !18\n  %55 = icmp eq i32 %54, 12, !dbg !18\n  %56 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 2, !dbg !18\n  %57 = load i64, i64* %56, align 8, !dbg !18, !tbaa !65\n  %58 = trunc i64 %57 to i32, !dbg !18\n  %59 = icmp eq i32 %58, 1, !dbg !18\n  %60 = and i1 %55, %59, !dbg !18\n  br i1 %60, label %if_end, label %assert_fail28, !dbg !18, !prof !19\n\nif_end:                                           ; preds = %assert_end27, %if_then\n  %61 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 6, !dbg !18\n  %62 = load i64, i64* %61, align 8, !dbg !18\n  %63 = icmp eq i64 %62, 0, !dbg !18\n  br i1 %63, label %assert_end31, label %assert_fail30, !dbg !18, !prof !19\n\nassert_fail28:                                    ; preds = %if_then\n  %64 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %64(i8* getelementptr inbounds ([189 x i8], [189 x i8]* @.str.9, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail30:                                    ; preds = %if_end\n  %65 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %65(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.10, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end31:                                     ; preds = %if_end\n  %66 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 0, !dbg !18\n  %67 = load i32, i32* %66, align 4, !dbg !18\n  %68 = icmp eq i32 %67, 1, !dbg !18\n  br i1 %68, label %assert_end33, label %assert_fail32, !dbg !18, !prof !19\n\nassert_fail32:                                    ; preds = %assert_end31\n  %69 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %69(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.11, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end33:                                     ; preds = %assert_end31\n  %70 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 2, !dbg !18\n  %71 = load i32, i32* %70, align 4, !dbg !18\n  %72 = icmp eq i32 %71, 3, !dbg !18\n  br i1 %72, label %assert_end37, label %assert_fail34, !dbg !18, !prof !19\n\nassert_fail34:                                    ; preds = %assert_end33\n  %73 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %73(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.12, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end37:                                     ; preds = %assert_end33\n  %74 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 3, i32 2, !dbg !18\n  %75 = load i16, i16* %74, align 2, !dbg !18\n  %76 = icmp eq i16 %75, 1, !dbg !18\n  %77 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 3, i32 1, !dbg !18\n  %78 = load i8, i8* %77, align 1, !dbg !18\n  %79 = icmp eq i8 %78, 32, !dbg !18\n  %80 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 3, i32 0, !dbg !18\n  %81 = load i8, i8* %80, align 1, !dbg !18\n  %82 = icmp eq i8 %81, 2, !dbg !18\n  %83 = and i1 %79, %82, !dbg !18\n  %84 = and i1 %76, %83, !dbg !18\n  br i1 %84, label %assert_end39, label %assert_fail38, !dbg !18, !prof !19\n\nassert_fail38:                                    ; preds = %assert_end37\n  %85 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %85(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.13, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end39:                                     ; preds = %assert_end37\n  %86 = load i64, i64* %default_function.ph_3.shape, align 8, !dbg !18, !tbaa !68\n  %87 = trunc i64 %86 to i32, !dbg !18\n  %88 = icmp eq i32 %87, 10, !dbg !18\n  br i1 %88, label %assert_end41, label %assert_fail40, !dbg !18, !prof !19\n\nassert_fail40:                                    ; preds = %assert_end39\n  %89 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %89(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.14, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end41:                                     ; preds = %assert_end39\n  %90 = getelementptr inbounds i64, i64* %default_function.ph_3.shape, i64 1, !dbg !18\n  %91 = load i64, i64* %90, align 8, !dbg !18, !tbaa !78\n  %92 = trunc i64 %91 to i32, !dbg !18\n  %93 = icmp eq i32 %92, 1, !dbg !18\n  br i1 %93, label %assert_end43, label %assert_fail42, !dbg !18, !prof !19\n\nassert_fail42:                                    ; preds = %assert_end41\n  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %94(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.15, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end43:                                     ; preds = %assert_end41\n  %95 = getelementptr inbounds i64, i64* %default_function.ph_3.shape, i64 2, !dbg !18\n  %96 = load i64, i64* %95, align 8, !dbg !18, !tbaa !80\n  %97 = trunc i64 %96 to i32, !dbg !18\n  %98 = icmp eq i32 %97, 12, !dbg !18\n  br i1 %98, label %assert_end45, label %assert_fail44, !dbg !18, !prof !19\n\nassert_fail44:                                    ; preds = %assert_end43\n  %99 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %99(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.16, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end45:                                     ; preds = %assert_end43\n  %100 = icmp eq i64* %default_function.ph_3.strides, null, !dbg !18\n  br i1 %100, label %if_end47, label %if_then46, !dbg !18, !prof !54\n\nif_then46:                                        ; preds = %assert_end45\n  %101 = load i64, i64* %default_function.ph_3.strides, align 8, !dbg !18, !tbaa !83\n  %102 = trunc i64 %101 to i32, !dbg !18\n  %103 = icmp eq i32 %102, 12, !dbg !18\n  %104 = getelementptr inbounds i64, i64* %default_function.ph_3.strides, i64 2, !dbg !18\n  %105 = load i64, i64* %104, align 8, !dbg !18, !tbaa !93\n  %106 = trunc i64 %105 to i32, !dbg !18\n  %107 = icmp eq i32 %106, 1, !dbg !18\n  %108 = and i1 %103, %107, !dbg !18\n  br i1 %108, label %if_end47, label %assert_fail48, !dbg !18, !prof !19\n\nif_end47:                                         ; preds = %assert_end45, %if_then46\n  %109 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 6, !dbg !18\n  %110 = load i64, i64* %109, align 8, !dbg !18\n  %111 = icmp eq i64 %110, 0, !dbg !18\n  br i1 %111, label %assert_end51, label %assert_fail50, !dbg !18, !prof !19\n\nassert_fail48:                                    ; preds = %if_then46\n  %112 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %112(i8* getelementptr inbounds ([189 x i8], [189 x i8]* @.str.17, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail50:                                    ; preds = %if_end47\n  %113 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %113(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.18, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end51:                                     ; preds = %if_end47\n  %114 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 1, i32 0, !dbg !18\n  %115 = load i32, i32* %114, align 4, !dbg !18\n  %116 = icmp eq i32 %115, 1, !dbg !18\n  br i1 %116, label %assert_end53, label %assert_fail52, !dbg !18, !prof !19\n\nassert_fail52:                                    ; preds = %assert_end51\n  %117 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %117(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.19, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end53:                                     ; preds = %assert_end51\n  %118 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 1, i32 1, !dbg !18\n  %119 = load i32, i32* %118, align 4, !dbg !18\n  %120 = icmp eq i32 %dev_id, %119, !dbg !18\n  br i1 %120, label %assert_end55, label %assert_fail54, !dbg !18, !prof !19\n\nassert_fail54:                                    ; preds = %assert_end53\n  %121 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %121(i8* getelementptr inbounds ([182 x i8], [182 x i8]* @.str.20, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end55:                                     ; preds = %assert_end53\n  %122 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 2, !dbg !18\n  %123 = load i32, i32* %122, align 4, !dbg !18\n  %124 = icmp eq i32 %123, 3, !dbg !18\n  br i1 %124, label %assert_end59, label %assert_fail56, !dbg !18, !prof !19\n\nassert_fail56:                                    ; preds = %assert_end55\n  %125 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %125(i8* getelementptr inbounds ([109 x i8], [109 x i8]* @.str.21, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end59:                                     ; preds = %assert_end55\n  %126 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 3, i32 2, !dbg !18\n  %127 = load i16, i16* %126, align 2, !dbg !18\n  %128 = icmp eq i16 %127, 1, !dbg !18\n  %129 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 3, i32 1, !dbg !18\n  %130 = load i8, i8* %129, align 1, !dbg !18\n  %131 = icmp eq i8 %130, 32, !dbg !18\n  %132 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 3, i32 0, !dbg !18\n  %133 = load i8, i8* %132, align 1, !dbg !18\n  %134 = icmp eq i8 %133, 2, !dbg !18\n  %135 = and i1 %131, %134, !dbg !18\n  %136 = and i1 %128, %135, !dbg !18\n  br i1 %136, label %assert_end61, label %assert_fail60, !dbg !18, !prof !19\n\nassert_fail60:                                    ; preds = %assert_end59\n  %137 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %137(i8* getelementptr inbounds ([239 x i8], [239 x i8]* @.str.22, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end61:                                     ; preds = %assert_end59\n  %138 = load i64, i64* %default_function.T_add.shape, align 8, !dbg !18, !tbaa !96\n  %139 = trunc i64 %138 to i32, !dbg !18\n  %140 = icmp eq i32 %139, 10, !dbg !18\n  br i1 %140, label %assert_end63, label %assert_fail62, !dbg !18, !prof !19\n\nassert_fail62:                                    ; preds = %assert_end61\n  %141 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %141(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.23, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end63:                                     ; preds = %assert_end61\n  %142 = getelementptr inbounds i64, i64* %default_function.T_add.shape, i64 1, !dbg !18\n  %143 = load i64, i64* %142, align 8, !dbg !18, !tbaa !106\n  %144 = trunc i64 %143 to i32, !dbg !18\n  %145 = icmp eq i32 %144, 1, !dbg !18\n  br i1 %145, label %assert_end65, label %assert_fail64, !dbg !18, !prof !19\n\nassert_fail64:                                    ; preds = %assert_end63\n  %146 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %146(i8* getelementptr inbounds ([194 x i8], [194 x i8]* @.str.24, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end65:                                     ; preds = %assert_end63\n  %147 = getelementptr inbounds i64, i64* %default_function.T_add.shape, i64 2, !dbg !18\n  %148 = load i64, i64* %147, align 8, !dbg !18, !tbaa !108\n  %149 = trunc i64 %148 to i32, !dbg !18\n  %150 = icmp eq i32 %149, 12, !dbg !18\n  br i1 %150, label %assert_end67, label %assert_fail66, !dbg !18, !prof !19\n\nassert_fail66:                                    ; preds = %assert_end65\n  %151 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %151(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.25, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end67:                                     ; preds = %assert_end65\n  %152 = icmp eq i64* %default_function.T_add.strides, null, !dbg !18\n  br i1 %152, label %if_end69, label %if_then68, !dbg !18, !prof !54\n\nif_then68:                                        ; preds = %assert_end67\n  %153 = load i64, i64* %default_function.T_add.strides, align 8, !dbg !18, !tbaa !111\n  %154 = trunc i64 %153 to i32, !dbg !18\n  %155 = icmp eq i32 %154, 12, !dbg !18\n  %156 = getelementptr inbounds i64, i64* %default_function.T_add.strides, i64 2, !dbg !18\n  %157 = load i64, i64* %156, align 8, !dbg !18, !tbaa !121\n  %158 = trunc i64 %157 to i32, !dbg !18\n  %159 = icmp eq i32 %158, 1, !dbg !18\n  %160 = and i1 %155, %159, !dbg !18\n  br i1 %160, label %if_end69, label %assert_fail70, !dbg !18, !prof !19\n\nif_end69:                                         ; preds = %assert_end67, %if_then68\n  %161 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 6, !dbg !18\n  %162 = load i64, i64* %161, align 8, !dbg !18\n  %163 = icmp eq i64 %162, 0, !dbg !18\n  br i1 %163, label %assert_end73, label %assert_fail72, !dbg !18, !prof !19\n\nassert_fail70:                                    ; preds = %if_then68\n  %164 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %164(i8* getelementptr inbounds ([192 x i8], [192 x i8]* @.str.26, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail72:                                    ; preds = %if_end69\n  %165 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %165(i8* getelementptr inbounds ([199 x i8], [199 x i8]* @.str.27, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end73:                                     ; preds = %if_end69\n  %166 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 1, i32 0, !dbg !18\n  %167 = load i32, i32* %166, align 4, !dbg !18\n  %168 = icmp eq i32 %167, 1, !dbg !18\n  br i1 %168, label %assert_end75, label %assert_fail74, !dbg !18, !prof !19\n\nassert_fail74:                                    ; preds = %assert_end73\n  %169 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %169(i8* getelementptr inbounds ([179 x i8], [179 x i8]* @.str.28, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end75:                                     ; preds = %assert_end73\n  %170 = getelementptr inbounds %1, %1* %T_add85, i64 0, i32 1, i32 1, !dbg !18\n  %171 = load i32, i32* %170, align 4, !dbg !18\n  %172 = icmp eq i32 %dev_id, %171, !dbg !18\n  br i1 %172, label %assert_end77, label %assert_fail76, !dbg !18, !prof !19\n\nassert_fail76:                                    ; preds = %assert_end75\n  %173 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %173(i8* getelementptr inbounds ([185 x i8], [185 x i8]* @.str.29, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end77:                                     ; preds = %assert_end75\n  %174 = call fastcc i32 @default_function_compute_(i8* nonnull %stack_value, float* %T_add_void_ptr88, i32* nonnull %stack_tcode82.sub, float* %ph_0_void_ptr86, float* %ph_3_void_ptr87), !dbg !18\n  ret i32 %174, !dbg !18\n}\n\n; Function Attrs: nounwind willreturn\ndeclare void @llvm.assume(i1) #1\n\n; Function Attrs: noinline\ndefine internal fastcc i32 @default_function_compute_(i8* noalias %0, float* noalias align 64 %1, i32* noalias %2, float* noalias align 64 %3, float* noalias align 64 %4) unnamed_addr #2 {\nentry:\n  %5 = alloca i8*, align 8\n  %6 = bitcast i8* %0 to %0*\n  %7 = bitcast i8* %0 to float**\n  store float* %1, float** %7, align 8\n  store i32 3, i32* %2, align 4, !tbaa !124\n  %8 = getelementptr inbounds i8, i8* %0, i64 8\n  %9 = bitcast i8* %8 to float**\n  store float* %3, float** %9, align 8\n  %10 = getelementptr inbounds i32, i32* %2, i64 1\n  store i32 3, i32* %10, align 4, !tbaa !135\n  %11 = getelementptr inbounds i8, i8* %0, i64 16\n  %12 = bitcast i8* %11 to float**\n  store float* %4, float** %12, align 8\n  %13 = getelementptr inbounds i32, i32* %2, i64 2\n  store i32 3, i32* %13, align 4, !tbaa !137\n  %14 = getelementptr inbounds i8, i8* %0, i64 24\n  %15 = bitcast i8* %14 to %0*\n  %16 = getelementptr inbounds i32, i32* %2, i64 3\n  %17 = load i32 (i8*, %0*, i32*, i32, %0*, i32*)*, i32 (i8*, %0*, i32*, i32, %0*, i32*)** @__TVMFuncCall, align 8, !tbaa !20\n  %18 = load i8*, i8** @.tvm_func.default_function_kernel, align 8\n  %19 = icmp eq i8* %18, null\n  br i1 %19, label %handle_init, label %handle_init_end, !prof !54\n\nhandle_init:                                      ; preds = %entry\n  %20 = load i8*, i8** @__tvm_module_ctx, align 8, !tbaa !20\n  %21 = load i32 (i8*, i8*, i8**)*, i32 (i8*, i8*, i8**)** @__TVMBackendGetFuncFromEnv, align 8, !tbaa !20\n  %22 = call i32 %21(i8* %20, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.30, i64 0, i64 0), i8** nonnull %5)\n  %23 = icmp eq i32 %22, 0\n  br i1 %23, label %call_end, label %call_fail, !prof !19\n\nhandle_init_end:                                  ; preds = %entry, %call_end\n  %24 = phi i8* [ %18, %entry ], [ %27, %call_end ]\n  %25 = call i32 %17(i8* %24, %0* %6, i32* nonnull %2, i32 3, %0* nonnull %15, i32* nonnull %16)\n  %26 = icmp eq i32 %25, 0\n  br i1 %26, label %call_end2, label %call_fail, !prof !19\n\ncall_fail:                                        ; preds = %call_end2, %handle_init_end, %handle_init\n  %merge = phi i32 [ %22, %handle_init ], [ %25, %handle_init_end ], [ 0, %call_end2 ]\n  ret i32 %merge\n\ncall_end:                                         ; preds = %handle_init\n  %27 = load i8*, i8** %5, align 8\n  store i8* %27, i8** @.tvm_func.default_function_kernel, align 8\n  br label %handle_init_end\n\ncall_end2:                                        ; preds = %handle_init_end\n  %28 = bitcast i8* %14 to i64*\n  %29 = load i64, i64* %28, align 8\n  %kernel_error_code = trunc i64 %29 to i32\n  %30 = icmp eq i32 %kernel_error_code, 0\n  br i1 %30, label %call_fail, label %assert_fail, !prof !19\n\nassert_fail:                                      ; preds = %call_end2\n  %31 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !20\n  call void %31(i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.31, i64 0, i64 0))\n  ret i32 -1\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\n; Function Attrs: nounwind readnone speculatable willreturn\ndeclare void @llvm.dbg.value(metadata, metadata, metadata) #4\n\nattributes #0 = { \"target-cpu\"=\"generic\" }\nattributes #1 = { nounwind willreturn }\nattributes #2 = { noinline \"target-cpu\"=\"generic\" }\nattributes #3 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\nattributes #4 = { nounwind readnone speculatable willreturn }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n!5 = distinct !DISubprogram(name: \"main.tir\", scope: !1, file: !1, type: !6, spFlags: DISPFlagLocalToUnit | DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !11)\n!6 = !DISubroutineType(types: !7)\n!7 = !{!8, !9, !10, !8, !9, !10, !9}\n!8 = !DIBasicType(name: \"int32\", size: 32, encoding: DW_ATE_signed)\n!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null)\n!10 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8)\n!11 = !{!12, !13, !14, !15, !16, !17}\n!12 = !DILocalVariable(name: \"arg1\", arg: 1, scope: !5, file: !1, type: !9)\n!13 = !DILocalVariable(name: \"arg2\", arg: 2, scope: !5, file: !1, type: !10)\n!14 = !DILocalVariable(name: \"arg3\", arg: 3, scope: !5, file: !1, type: !8)\n!15 = !DILocalVariable(name: \"arg4\", arg: 4, scope: !5, file: !1, type: !9)\n!16 = !DILocalVariable(name: \"arg5\", arg: 5, scope: !5, file: !1, type: !10)\n!17 = !DILocalVariable(name: \"arg6\", arg: 6, scope: !5, file: !1, type: !9)\n!18 = !DILocation(line: 0, scope: !5)\n!19 = !{!\"branch_weights\", i32 1048576, i32 1}\n!20 = !{!21, !21, i64 0}\n!21 = !{!\"ctx_ptr\", !22, i64 0}\n!22 = !{!\"tvm-tbaa\"}\n!23 = !{!24, !24, i64 0}\n!24 = !{!\"0x4b1c810.w4.b0\", !25, i64 0}\n!25 = !{!\"0x4b1c810.w8.b0\", !26, i64 0}\n!26 = !{!\"0x4b1c810.w16.b0\", !27, i64 0}\n!27 = !{!\"0x4b1c810.w32.b0\", !28, i64 0}\n!28 = !{!\"0x4b1c810.w64.b0\", !29, i64 0}\n!29 = !{!\"0x4b1c810.w128.b0\", !30, i64 0}\n!30 = !{!\"0x4b1c810.w256.b0\", !31, i64 0}\n!31 = !{!\"0x4b1c810.w512.b0\", !32, i64 0}\n!32 = !{!\"0x4b1c810.w1024.b0\", !33, i64 0}\n!33 = !{!\"0x4b1c810\", !22, i64 0}\n!34 = !{!35, !35, i64 0}\n!35 = !{!\"0x4b1c810.w4.b4\", !25, i64 0}\n!36 = !{!37, !37, i64 0}\n!37 = !{!\"0x4b1c810.w4.b8\", !38, i64 0}\n!38 = !{!\"0x4b1c810.w8.b8\", !26, i64 0}\n!39 = !{!40, !40, i64 0}\n!40 = !{!\"0x4bd0ba0.w8.b0\", !41, i64 0}\n!41 = !{!\"0x4bd0ba0.w16.b0\", !42, i64 0}\n!42 = !{!\"0x4bd0ba0.w32.b0\", !43, i64 0}\n!43 = !{!\"0x4bd0ba0.w64.b0\", !44, i64 0}\n!44 = !{!\"0x4bd0ba0.w128.b0\", !45, i64 0}\n!45 = !{!\"0x4bd0ba0.w256.b0\", !46, i64 0}\n!46 = !{!\"0x4bd0ba0.w512.b0\", !47, i64 0}\n!47 = !{!\"0x4bd0ba0.w1024.b0\", !48, i64 0}\n!48 = !{!\"0x4bd0ba0\", !22, i64 0}\n!49 = !{!50, !50, i64 0}\n!50 = !{!\"0x4bd0ba0.w8.b8\", !41, i64 0}\n!51 = !{!52, !52, i64 0}\n!52 = !{!\"0x4bd0ba0.w8.b16\", !53, i64 0}\n!53 = !{!\"0x4bd0ba0.w16.b16\", !42, i64 0}\n!54 = !{!\"branch_weights\", i32 1, i32 1048576}\n!55 = !{!56, !56, i64 0}\n!56 = !{!\"0x4bd3db0.w8.b0\", !57, i64 0}\n!57 = !{!\"0x4bd3db0.w16.b0\", !58, i64 0}\n!58 = !{!\"0x4bd3db0.w32.b0\", !59, i64 0}\n!59 = !{!\"0x4bd3db0.w64.b0\", !60, i64 0}\n!60 = !{!\"0x4bd3db0.w128.b0\", !61, i64 0}\n!61 = !{!\"0x4bd3db0.w256.b0\", !62, i64 0}\n!62 = !{!\"0x4bd3db0.w512.b0\", !63, i64 0}\n!63 = !{!\"0x4bd3db0.w1024.b0\", !64, i64 0}\n!64 = !{!\"0x4bd3db0\", !22, i64 0}\n!65 = !{!66, !66, i64 0}\n!66 = !{!\"0x4bd3db0.w8.b16\", !67, i64 0}\n!67 = !{!\"0x4bd3db0.w16.b16\", !58, i64 0}\n!68 = !{!69, !69, i64 0}\n!69 = !{!\"0x4b24de0.w8.b0\", !70, i64 0}\n!70 = !{!\"0x4b24de0.w16.b0\", !71, i64 0}\n!71 = !{!\"0x4b24de0.w32.b0\", !72, i64 0}\n!72 = !{!\"0x4b24de0.w64.b0\", !73, i64 0}\n!73 = !{!\"0x4b24de0.w128.b0\", !74, i64 0}\n!74 = !{!\"0x4b24de0.w256.b0\", !75, i64 0}\n!75 = !{!\"0x4b24de0.w512.b0\", !76, i64 0}\n!76 = !{!\"0x4b24de0.w1024.b0\", !77, i64 0}\n!77 = !{!\"0x4b24de0\", !22, i64 0}\n!78 = !{!79, !79, i64 0}\n!79 = !{!\"0x4b24de0.w8.b8\", !70, i64 0}\n!80 = !{!81, !81, i64 0}\n!81 = !{!\"0x4b24de0.w8.b16\", !82, i64 0}\n!82 = !{!\"0x4b24de0.w16.b16\", !71, i64 0}\n!83 = !{!84, !84, i64 0}\n!84 = !{!\"0x4669920.w8.b0\", !85, i64 0}\n!85 = !{!\"0x4669920.w16.b0\", !86, i64 0}\n!86 = !{!\"0x4669920.w32.b0\", !87, i64 0}\n!87 = !{!\"0x4669920.w64.b0\", !88, i64 0}\n!88 = !{!\"0x4669920.w128.b0\", !89, i64 0}\n!89 = !{!\"0x4669920.w256.b0\", !90, i64 0}\n!90 = !{!\"0x4669920.w512.b0\", !91, i64 0}\n!91 = !{!\"0x4669920.w1024.b0\", !92, i64 0}\n!92 = !{!\"0x4669920\", !22, i64 0}\n!93 = !{!94, !94, i64 0}\n!94 = !{!\"0x4669920.w8.b16\", !95, i64 0}\n!95 = !{!\"0x4669920.w16.b16\", !86, i64 0}\n!96 = !{!97, !97, i64 0}\n!97 = !{!\"0x4cad550.w8.b0\", !98, i64 0}\n!98 = !{!\"0x4cad550.w16.b0\", !99, i64 0}\n!99 = !{!\"0x4cad550.w32.b0\", !100, i64 0}\n!100 = !{!\"0x4cad550.w64.b0\", !101, i64 0}\n!101 = !{!\"0x4cad550.w128.b0\", !102, i64 0}\n!102 = !{!\"0x4cad550.w256.b0\", !103, i64 0}\n!103 = !{!\"0x4cad550.w512.b0\", !104, i64 0}\n!104 = !{!\"0x4cad550.w1024.b0\", !105, i64 0}\n!105 = !{!\"0x4cad550\", !22, i64 0}\n!106 = !{!107, !107, i64 0}\n!107 = !{!\"0x4cad550.w8.b8\", !98, i64 0}\n!108 = !{!109, !109, i64 0}\n!109 = !{!\"0x4cad550.w8.b16\", !110, i64 0}\n!110 = !{!\"0x4cad550.w16.b16\", !99, i64 0}\n!111 = !{!112, !112, i64 0}\n!112 = !{!\"0x4cae640.w8.b0\", !113, i64 0}\n!113 = !{!\"0x4cae640.w16.b0\", !114, i64 0}\n!114 = !{!\"0x4cae640.w32.b0\", !115, i64 0}\n!115 = !{!\"0x4cae640.w64.b0\", !116, i64 0}\n!116 = !{!\"0x4cae640.w128.b0\", !117, i64 0}\n!117 = !{!\"0x4cae640.w256.b0\", !118, i64 0}\n!118 = !{!\"0x4cae640.w512.b0\", !119, i64 0}\n!119 = !{!\"0x4cae640.w1024.b0\", !120, i64 0}\n!120 = !{!\"0x4cae640\", !22, i64 0}\n!121 = !{!122, !122, i64 0}\n!122 = !{!\"0x4cae640.w8.b16\", !123, i64 0}\n!123 = !{!\"0x4cae640.w16.b16\", !114, i64 0}\n!124 = !{!125, !125, i64 0}\n!125 = !{!\"0x4bcb930.w4.b0\", !126, i64 0}\n!126 = !{!\"0x4bcb930.w8.b0\", !127, i64 0}\n!127 = !{!\"0x4bcb930.w16.b0\", !128, i64 0}\n!128 = !{!\"0x4bcb930.w32.b0\", !129, i64 0}\n!129 = !{!\"0x4bcb930.w64.b0\", !130, i64 0}\n!130 = !{!\"0x4bcb930.w128.b0\", !131, i64 0}\n!131 = !{!\"0x4bcb930.w256.b0\", !132, i64 0}\n!132 = !{!\"0x4bcb930.w512.b0\", !133, i64 0}\n!133 = !{!\"0x4bcb930.w1024.b0\", !134, i64 0}\n!134 = !{!\"0x4bcb930\", !22, i64 0}\n!135 = !{!136, !136, i64 0}\n!136 = !{!\"0x4bcb930.w4.b4\", !126, i64 0}\n!137 = !{!138, !138, i64 0}\n!138 = !{!\"0x4bcb930.w4.b8\", !139, i64 0}\n!139 = !{!\"0x4bcb930.w8.b8\", !127, i64 0}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3);\nextern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 1, 12), \"float32\"), ph_3: T.Buffer((10, 1, 12), \"float32\"), T_add: T.Buffer((10, 1, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(10):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0 * 12 + ax2\n                T_add_1 = T.Buffer((120,), data=T_add.data)\n                ph_0_1 = T.Buffer((120,), data=ph_0.data)\n                ph_3_1 = T.Buffer((120,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]", "op_args": []}{"op_name": "topology_expansion", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%0 = type { double }\n%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }\n%2 = type { i32, i32 }\n%3 = type { i8, i8, i16 }\n\n@__tvm_module_ctx = linkonce dllexport local_unnamed_addr global i8* null, align 8\n@__TVMFuncCall = linkonce dllexport local_unnamed_addr global i32 (i8*, %0*, i32*, i32, %0*, i32*)* null, align 8\n@__TVMBackendGetFuncFromEnv = linkonce dllexport local_unnamed_addr global i32 (i8*, i8*, i8**)* null, align 8\n@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8\n@.str = private constant [67 x i8] c\"Assert fail: num_args == 3, default_function: num_args should be 3\\00\", align 1\n@.str.1 = private constant [130 x i8] c\"Assert fail: ph_0_code == 3 or ph_0_code == 13 or ph_0_code == 7 or ph_0_code == 4, default_function: Expect arg[0] to be pointer\\00\", align 1\n@.str.2 = private constant [130 x i8] c\"Assert fail: ph_3_code == 3 or ph_3_code == 13 or ph_3_code == 7 or ph_3_code == 4, default_function: Expect arg[1] to be pointer\\00\", align 1\n@.str.3 = private constant [182 x i8] c\"Assert fail: T_batch_matmul_NN_code == 3 or T_batch_matmul_NN_code == 13 or T_batch_matmul_NN_code == 7 or T_batch_matmul_NN_code == 4, default_function: Expect arg[2] to be pointer\\00\", align 1\n@.str.4 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_0, 0, 4, \\22int32\\22), default_function.ph_0.ndim is expected to equal 3\\00\", align 1\n@.str.5 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_0, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_0, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_0.dtype is expected to be float32\\00\", align 1\n@.str.6 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[0]) == 1, Argument default_function.ph_0.shape[0] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_ph_0_shape[0])\\00\", align 1\n@.str.7 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[1]) == 2, Argument default_function.ph_0.shape[1] has an unsatisfied constraint: 2 == T.Cast(\\22int32\\22, default_function_ph_0_shape[1])\\00\", align 1\n@.str.8 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[2]) == 1, Argument default_function.ph_0.shape[2] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_ph_0_shape[2])\\00\", align 1\n@.str.9 = private constant [129 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_0_strides[1]), default_function.ph_0.strides: expected to be compact array\\00\", align 1\n@.str.10 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22), Argument default_function.ph_0.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.11 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_0.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.12 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_3, 0, 4, \\22int32\\22), default_function.ph_3.ndim is expected to equal 3\\00\", align 1\n@.str.13 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_3, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_3, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_3, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_3.dtype is expected to be float32\\00\", align 1\n@.str.14 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_3_shape[0]) == 1, Argument default_function.ph_3.shape[0] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_ph_3_shape[0])\\00\", align 1\n@.str.15 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_3_shape[1]) == 1, Argument default_function.ph_3.shape[1] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_ph_3_shape[1])\\00\", align 1\n@.str.16 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_3_shape[2]) == 9, Argument default_function.ph_3.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_ph_3_shape[2])\\00\", align 1\n@.str.17 = private constant [129 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_3_strides[2]), default_function.ph_3.strides: expected to be compact array\\00\", align 1\n@.str.18 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_3, 0, 8, \\22uint64\\22), Argument default_function.ph_3.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_3, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.19 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_3, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_3.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_3, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.20 = private constant [182 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(ph_3, 0, 9, \\22int32\\22), Argument default_function.ph_3.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(ph_3, 0, 9, \\22int32\\22)\\00\", align 1\n@.str.21 = private constant [133 x i8] c\"Assert fail: 3 == T.tvm_struct_get(T_batch_matmul_NN, 0, 4, \\22int32\\22), default_function.T_batch_matmul_NN.ndim is expected to equal 3\\00\", align 1\n@.str.22 = private constant [287 x i8] c\"Assert fail: T.tvm_struct_get(T_batch_matmul_NN, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(T_batch_matmul_NN, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(T_batch_matmul_NN, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.T_batch_matmul_NN.dtype is expected to be float32\\00\", align 1\n@.str.23 = private constant [230 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_shape[0]) == 1, Argument default_function.T_batch_matmul_NN.shape[0] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_shape[0])\\00\", align 1\n@.str.24 = private constant [230 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_shape[1]) == 2, Argument default_function.T_batch_matmul_NN.shape[1] has an unsatisfied constraint: 2 == T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_shape[1])\\00\", align 1\n@.str.25 = private constant [230 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_shape[2]) == 9, Argument default_function.T_batch_matmul_NN.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_shape[2])\\00\", align 1\n@.str.26 = private constant [227 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_strides[2]) and 9 == T.Cast(\\22int32\\22, default_function_T_batch_matmul_NN_strides[1]), default_function.T_batch_matmul_NN.strides: expected to be compact array\\00\", align 1\n@.str.27 = private constant [235 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(T_batch_matmul_NN, 0, 8, \\22uint64\\22), Argument default_function.T_batch_matmul_NN.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(T_batch_matmul_NN, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.28 = private constant [215 x i8] c\"Assert fail: T.tvm_struct_get(T_batch_matmul_NN, 0, 10, \\22int32\\22) == 1, Argument default_function.T_batch_matmul_NN.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(T_batch_matmul_NN, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.29 = private constant [221 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(T_batch_matmul_NN, 0, 9, \\22int32\\22), Argument default_function.T_batch_matmul_NN.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(T_batch_matmul_NN, 0, 9, \\22int32\\22)\\00\", align 1\n@.tvm_func.default_function_kernel = internal unnamed_addr global i8* null, align 8\n@.str.30 = private constant [24 x i8] c\"default_function_kernel\\00\", align 1\n@.str.31 = private constant [68 x i8] c\"Assert fail: kernel_error_code == 0, Error executing compute kernel\\00\", align 1\n@__tvm_main__ = weak dllexport local_unnamed_addr constant [17 x i8] c\"default_function\\00\", align 1\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\ndefine dllexport i32 @default_function(i8* noalias nocapture readonly %args, i32* noalias nocapture readonly %arg_type_ids, i32 %num_args, i8* noalias nocapture readnone %out_ret_value, i32* noalias nocapture readnone %out_ret_tcode, i8* noalias nocapture readnone %resource_handle) local_unnamed_addr #0 !dbg !5 {\nentry:\n  call void @llvm.dbg.value(metadata i8* %args, metadata !12, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %arg_type_ids, metadata !13, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32 %num_args, metadata !14, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %out_ret_value, metadata !15, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %out_ret_tcode, metadata !16, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %resource_handle, metadata !17, metadata !DIExpression()), !dbg !18\n  %stack_value_void_ptr81 = alloca [4 x %0], align 8, !dbg !18\n  %stack_tcode82 = alloca [4 x i32], align 4, !dbg !18\n  %stack_tcode82.sub = getelementptr inbounds [4 x i32], [4 x i32]* %stack_tcode82, i64 0, i64 0\n  %stack_value = bitcast [4 x %0]* %stack_value_void_ptr81 to i8*, !dbg !18\n  %0 = icmp eq i32 %num_args, 3, !dbg !18\n  br i1 %0, label %assert_end, label %assert_fail, !dbg !18, !prof !19\n\nassert_fail:                                      ; preds = %entry\n  %1 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %1(i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end:                                       ; preds = %entry\n  %ph_0.code = load i32, i32* %arg_type_ids, align 4, !dbg !18, !tbaa !23\n  %2 = getelementptr inbounds i32, i32* %arg_type_ids, i64 1, !dbg !18\n  %ph_3.code = load i32, i32* %2, align 4, !dbg !18, !tbaa !34\n  %3 = getelementptr inbounds i32, i32* %arg_type_ids, i64 2, !dbg !18\n  %T_batch_matmul_NN.code = load i32, i32* %3, align 4, !dbg !18, !tbaa !36\n  %4 = bitcast i8* %args to %1**, !dbg !18\n  %ph_083 = load %1*, %1** %4, align 8, !dbg !18\n  %5 = getelementptr inbounds i8, i8* %args, i64 8, !dbg !18\n  %6 = bitcast i8* %5 to %1**, !dbg !18\n  %ph_384 = load %1*, %1** %6, align 8, !dbg !18\n  %7 = getelementptr inbounds i8, i8* %args, i64 16, !dbg !18\n  %8 = bitcast i8* %7 to %1**, !dbg !18\n  %T_batch_matmul_NN85 = load %1*, %1** %8, align 8, !dbg !18\n  %9 = bitcast %1* %ph_083 to float**, !dbg !18\n  %ph_0_void_ptr86 = load float*, float** %9, align 8, !dbg !18\n  %ptrint = ptrtoint float* %ph_0_void_ptr86 to i64, !dbg !18\n  %maskedptr = and i64 %ptrint, 63, !dbg !18\n  %maskcond = icmp eq i64 %maskedptr, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond), !dbg !18\n  %10 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 4, !dbg !18\n  %default_function.ph_0.shape = load i64*, i64** %10, align 8, !dbg !18\n  %11 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 5, !dbg !18\n  %default_function.ph_0.strides = load i64*, i64** %11, align 8, !dbg !18\n  %12 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 1, !dbg !18\n  %dev_id = load i32, i32* %12, align 4, !dbg !18\n  %13 = bitcast %1* %ph_384 to float**, !dbg !18\n  %ph_3_void_ptr87 = load float*, float** %13, align 8, !dbg !18\n  %ptrint3 = ptrtoint float* %ph_3_void_ptr87 to i64, !dbg !18\n  %maskedptr4 = and i64 %ptrint3, 63, !dbg !18\n  %maskcond5 = icmp eq i64 %maskedptr4, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond5), !dbg !18\n  %14 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 4, !dbg !18\n  %default_function.ph_3.shape = load i64*, i64** %14, align 8, !dbg !18\n  %15 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 5, !dbg !18\n  %default_function.ph_3.strides = load i64*, i64** %15, align 8, !dbg !18\n  %16 = bitcast %1* %T_batch_matmul_NN85 to float**, !dbg !18\n  %T_batch_matmul_NN_void_ptr88 = load float*, float** %16, align 8, !dbg !18\n  %ptrint7 = ptrtoint float* %T_batch_matmul_NN_void_ptr88 to i64, !dbg !18\n  %maskedptr8 = and i64 %ptrint7, 63, !dbg !18\n  %maskcond9 = icmp eq i64 %maskedptr8, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond9), !dbg !18\n  %17 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 4, !dbg !18\n  %default_function.T_batch_matmul_NN.shape = load i64*, i64** %17, align 8, !dbg !18\n  %18 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 5, !dbg !18\n  %default_function.T_batch_matmul_NN.strides = load i64*, i64** %18, align 8, !dbg !18\n  switch i32 %ph_0.code, label %assert_fail10 [\n    i32 13, label %assert_end11\n    i32 7, label %assert_end11\n    i32 4, label %assert_end11\n    i32 3, label %assert_end11\n  ], !dbg !18\n\nassert_fail10:                                    ; preds = %assert_end\n  %19 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %19(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.1, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end11:                                     ; preds = %assert_end, %assert_end, %assert_end, %assert_end\n  switch i32 %ph_3.code, label %assert_fail12 [\n    i32 13, label %assert_end13\n    i32 7, label %assert_end13\n    i32 4, label %assert_end13\n    i32 3, label %assert_end13\n  ], !dbg !18\n\nassert_fail12:                                    ; preds = %assert_end11\n  %20 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %20(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.2, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end13:                                     ; preds = %assert_end11, %assert_end11, %assert_end11, %assert_end11\n  switch i32 %T_batch_matmul_NN.code, label %assert_fail14 [\n    i32 13, label %assert_end15\n    i32 7, label %assert_end15\n    i32 4, label %assert_end15\n    i32 3, label %assert_end15\n  ], !dbg !18\n\nassert_fail14:                                    ; preds = %assert_end13\n  %21 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %21(i8* getelementptr inbounds ([182 x i8], [182 x i8]* @.str.3, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end15:                                     ; preds = %assert_end13, %assert_end13, %assert_end13, %assert_end13\n  %22 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 2, !dbg !18\n  %23 = load i32, i32* %22, align 4, !dbg !18\n  %24 = icmp eq i32 %23, 3, !dbg !18\n  br i1 %24, label %assert_end19, label %assert_fail16, !dbg !18, !prof !19\n\nassert_fail16:                                    ; preds = %assert_end15\n  %25 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %25(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.4, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end19:                                     ; preds = %assert_end15\n  %26 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 2, !dbg !18\n  %27 = load i16, i16* %26, align 2, !dbg !18\n  %28 = icmp eq i16 %27, 1, !dbg !18\n  %29 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 1, !dbg !18\n  %30 = load i8, i8* %29, align 1, !dbg !18\n  %31 = icmp eq i8 %30, 32, !dbg !18\n  %32 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 0, !dbg !18\n  %33 = load i8, i8* %32, align 1, !dbg !18\n  %34 = icmp eq i8 %33, 2, !dbg !18\n  %35 = and i1 %31, %34, !dbg !18\n  %36 = and i1 %28, %35, !dbg !18\n  br i1 %36, label %assert_end21, label %assert_fail20, !dbg !18, !prof !19\n\nassert_fail20:                                    ; preds = %assert_end19\n  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %37(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.5, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end21:                                     ; preds = %assert_end19\n  %38 = load i64, i64* %default_function.ph_0.shape, align 8, !dbg !18, !tbaa !39\n  %39 = trunc i64 %38 to i32, !dbg !18\n  %40 = icmp eq i32 %39, 1, !dbg !18\n  br i1 %40, label %assert_end23, label %assert_fail22, !dbg !18, !prof !19\n\nassert_fail22:                                    ; preds = %assert_end21\n  %41 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %41(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.6, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end23:                                     ; preds = %assert_end21\n  %42 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 1, !dbg !18\n  %43 = load i64, i64* %42, align 8, !dbg !18, !tbaa !49\n  %44 = trunc i64 %43 to i32, !dbg !18\n  %45 = icmp eq i32 %44, 2, !dbg !18\n  br i1 %45, label %assert_end25, label %assert_fail24, !dbg !18, !prof !19\n\nassert_fail24:                                    ; preds = %assert_end23\n  %46 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %46(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.7, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end25:                                     ; preds = %assert_end23\n  %47 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 2, !dbg !18\n  %48 = load i64, i64* %47, align 8, !dbg !18, !tbaa !51\n  %49 = trunc i64 %48 to i32, !dbg !18\n  %50 = icmp eq i32 %49, 1, !dbg !18\n  br i1 %50, label %assert_end27, label %assert_fail26, !dbg !18, !prof !19\n\nassert_fail26:                                    ; preds = %assert_end25\n  %51 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %51(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.8, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end27:                                     ; preds = %assert_end25\n  %52 = icmp eq i64* %default_function.ph_0.strides, null, !dbg !18\n  br i1 %52, label %if_end, label %if_then, !dbg !18, !prof !54\n\nif_then:                                          ; preds = %assert_end27\n  %53 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 1, !dbg !18\n  %54 = load i64, i64* %53, align 8, !dbg !18, !tbaa !55\n  %55 = trunc i64 %54 to i32, !dbg !18\n  %56 = icmp eq i32 %55, 1, !dbg !18\n  br i1 %56, label %if_end, label %assert_fail28, !dbg !18, !prof !19\n\nif_end:                                           ; preds = %assert_end27, %if_then\n  %57 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 6, !dbg !18\n  %58 = load i64, i64* %57, align 8, !dbg !18\n  %59 = icmp eq i64 %58, 0, !dbg !18\n  br i1 %59, label %assert_end31, label %assert_fail30, !dbg !18, !prof !19\n\nassert_fail28:                                    ; preds = %if_then\n  %60 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %60(i8* getelementptr inbounds ([129 x i8], [129 x i8]* @.str.9, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail30:                                    ; preds = %if_end\n  %61 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %61(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.10, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end31:                                     ; preds = %if_end\n  %62 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 0, !dbg !18\n  %63 = load i32, i32* %62, align 4, !dbg !18\n  %64 = icmp eq i32 %63, 1, !dbg !18\n  br i1 %64, label %assert_end33, label %assert_fail32, !dbg !18, !prof !19\n\nassert_fail32:                                    ; preds = %assert_end31\n  %65 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %65(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.11, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end33:                                     ; preds = %assert_end31\n  %66 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 2, !dbg !18\n  %67 = load i32, i32* %66, align 4, !dbg !18\n  %68 = icmp eq i32 %67, 3, !dbg !18\n  br i1 %68, label %assert_end37, label %assert_fail34, !dbg !18, !prof !19\n\nassert_fail34:                                    ; preds = %assert_end33\n  %69 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %69(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.12, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end37:                                     ; preds = %assert_end33\n  %70 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 3, i32 2, !dbg !18\n  %71 = load i16, i16* %70, align 2, !dbg !18\n  %72 = icmp eq i16 %71, 1, !dbg !18\n  %73 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 3, i32 1, !dbg !18\n  %74 = load i8, i8* %73, align 1, !dbg !18\n  %75 = icmp eq i8 %74, 32, !dbg !18\n  %76 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 3, i32 0, !dbg !18\n  %77 = load i8, i8* %76, align 1, !dbg !18\n  %78 = icmp eq i8 %77, 2, !dbg !18\n  %79 = and i1 %75, %78, !dbg !18\n  %80 = and i1 %72, %79, !dbg !18\n  br i1 %80, label %assert_end39, label %assert_fail38, !dbg !18, !prof !19\n\nassert_fail38:                                    ; preds = %assert_end37\n  %81 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %81(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.13, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end39:                                     ; preds = %assert_end37\n  %82 = load i64, i64* %default_function.ph_3.shape, align 8, !dbg !18, !tbaa !65\n  %83 = trunc i64 %82 to i32, !dbg !18\n  %84 = icmp eq i32 %83, 1, !dbg !18\n  br i1 %84, label %assert_end41, label %assert_fail40, !dbg !18, !prof !19\n\nassert_fail40:                                    ; preds = %assert_end39\n  %85 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %85(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.14, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end41:                                     ; preds = %assert_end39\n  %86 = getelementptr inbounds i64, i64* %default_function.ph_3.shape, i64 1, !dbg !18\n  %87 = load i64, i64* %86, align 8, !dbg !18, !tbaa !75\n  %88 = trunc i64 %87 to i32, !dbg !18\n  %89 = icmp eq i32 %88, 1, !dbg !18\n  br i1 %89, label %assert_end43, label %assert_fail42, !dbg !18, !prof !19\n\nassert_fail42:                                    ; preds = %assert_end41\n  %90 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %90(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.15, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end43:                                     ; preds = %assert_end41\n  %91 = getelementptr inbounds i64, i64* %default_function.ph_3.shape, i64 2, !dbg !18\n  %92 = load i64, i64* %91, align 8, !dbg !18, !tbaa !77\n  %93 = trunc i64 %92 to i32, !dbg !18\n  %94 = icmp eq i32 %93, 9, !dbg !18\n  br i1 %94, label %assert_end45, label %assert_fail44, !dbg !18, !prof !19\n\nassert_fail44:                                    ; preds = %assert_end43\n  %95 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %95(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.16, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end45:                                     ; preds = %assert_end43\n  %96 = icmp eq i64* %default_function.ph_3.strides, null, !dbg !18\n  br i1 %96, label %if_end47, label %if_then46, !dbg !18, !prof !54\n\nif_then46:                                        ; preds = %assert_end45\n  %97 = getelementptr inbounds i64, i64* %default_function.ph_3.strides, i64 2, !dbg !18\n  %98 = load i64, i64* %97, align 8, !dbg !18, !tbaa !80\n  %99 = trunc i64 %98 to i32, !dbg !18\n  %100 = icmp eq i32 %99, 1, !dbg !18\n  br i1 %100, label %if_end47, label %assert_fail48, !dbg !18, !prof !19\n\nif_end47:                                         ; preds = %assert_end45, %if_then46\n  %101 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 6, !dbg !18\n  %102 = load i64, i64* %101, align 8, !dbg !18\n  %103 = icmp eq i64 %102, 0, !dbg !18\n  br i1 %103, label %assert_end51, label %assert_fail50, !dbg !18, !prof !19\n\nassert_fail48:                                    ; preds = %if_then46\n  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %104(i8* getelementptr inbounds ([129 x i8], [129 x i8]* @.str.17, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail50:                                    ; preds = %if_end47\n  %105 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %105(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.18, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end51:                                     ; preds = %if_end47\n  %106 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 1, i32 0, !dbg !18\n  %107 = load i32, i32* %106, align 4, !dbg !18\n  %108 = icmp eq i32 %107, 1, !dbg !18\n  br i1 %108, label %assert_end53, label %assert_fail52, !dbg !18, !prof !19\n\nassert_fail52:                                    ; preds = %assert_end51\n  %109 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %109(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.19, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end53:                                     ; preds = %assert_end51\n  %110 = getelementptr inbounds %1, %1* %ph_384, i64 0, i32 1, i32 1, !dbg !18\n  %111 = load i32, i32* %110, align 4, !dbg !18\n  %112 = icmp eq i32 %dev_id, %111, !dbg !18\n  br i1 %112, label %assert_end55, label %assert_fail54, !dbg !18, !prof !19\n\nassert_fail54:                                    ; preds = %assert_end53\n  %113 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %113(i8* getelementptr inbounds ([182 x i8], [182 x i8]* @.str.20, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end55:                                     ; preds = %assert_end53\n  %114 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 2, !dbg !18\n  %115 = load i32, i32* %114, align 4, !dbg !18\n  %116 = icmp eq i32 %115, 3, !dbg !18\n  br i1 %116, label %assert_end59, label %assert_fail56, !dbg !18, !prof !19\n\nassert_fail56:                                    ; preds = %assert_end55\n  %117 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %117(i8* getelementptr inbounds ([133 x i8], [133 x i8]* @.str.21, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end59:                                     ; preds = %assert_end55\n  %118 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 3, i32 2, !dbg !18\n  %119 = load i16, i16* %118, align 2, !dbg !18\n  %120 = icmp eq i16 %119, 1, !dbg !18\n  %121 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 3, i32 1, !dbg !18\n  %122 = load i8, i8* %121, align 1, !dbg !18\n  %123 = icmp eq i8 %122, 32, !dbg !18\n  %124 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 3, i32 0, !dbg !18\n  %125 = load i8, i8* %124, align 1, !dbg !18\n  %126 = icmp eq i8 %125, 2, !dbg !18\n  %127 = and i1 %123, %126, !dbg !18\n  %128 = and i1 %120, %127, !dbg !18\n  br i1 %128, label %assert_end61, label %assert_fail60, !dbg !18, !prof !19\n\nassert_fail60:                                    ; preds = %assert_end59\n  %129 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %129(i8* getelementptr inbounds ([287 x i8], [287 x i8]* @.str.22, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end61:                                     ; preds = %assert_end59\n  %130 = load i64, i64* %default_function.T_batch_matmul_NN.shape, align 8, !dbg !18, !tbaa !90\n  %131 = trunc i64 %130 to i32, !dbg !18\n  %132 = icmp eq i32 %131, 1, !dbg !18\n  br i1 %132, label %assert_end63, label %assert_fail62, !dbg !18, !prof !19\n\nassert_fail62:                                    ; preds = %assert_end61\n  %133 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %133(i8* getelementptr inbounds ([230 x i8], [230 x i8]* @.str.23, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end63:                                     ; preds = %assert_end61\n  %134 = getelementptr inbounds i64, i64* %default_function.T_batch_matmul_NN.shape, i64 1, !dbg !18\n  %135 = load i64, i64* %134, align 8, !dbg !18, !tbaa !100\n  %136 = trunc i64 %135 to i32, !dbg !18\n  %137 = icmp eq i32 %136, 2, !dbg !18\n  br i1 %137, label %assert_end65, label %assert_fail64, !dbg !18, !prof !19\n\nassert_fail64:                                    ; preds = %assert_end63\n  %138 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %138(i8* getelementptr inbounds ([230 x i8], [230 x i8]* @.str.24, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end65:                                     ; preds = %assert_end63\n  %139 = getelementptr inbounds i64, i64* %default_function.T_batch_matmul_NN.shape, i64 2, !dbg !18\n  %140 = load i64, i64* %139, align 8, !dbg !18, !tbaa !102\n  %141 = trunc i64 %140 to i32, !dbg !18\n  %142 = icmp eq i32 %141, 9, !dbg !18\n  br i1 %142, label %assert_end67, label %assert_fail66, !dbg !18, !prof !19\n\nassert_fail66:                                    ; preds = %assert_end65\n  %143 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %143(i8* getelementptr inbounds ([230 x i8], [230 x i8]* @.str.25, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end67:                                     ; preds = %assert_end65\n  %144 = icmp eq i64* %default_function.T_batch_matmul_NN.strides, null, !dbg !18\n  br i1 %144, label %if_end69, label %if_then68, !dbg !18, !prof !54\n\nif_then68:                                        ; preds = %assert_end67\n  %145 = getelementptr inbounds i64, i64* %default_function.T_batch_matmul_NN.strides, i64 1, !dbg !18\n  %146 = load i64, i64* %145, align 8, !dbg !18, !tbaa !105\n  %147 = trunc i64 %146 to i32, !dbg !18\n  %148 = icmp eq i32 %147, 9, !dbg !18\n  %149 = getelementptr inbounds i64, i64* %default_function.T_batch_matmul_NN.strides, i64 2, !dbg !18\n  %150 = load i64, i64* %149, align 8, !dbg !18, !tbaa !115\n  %151 = trunc i64 %150 to i32, !dbg !18\n  %152 = icmp eq i32 %151, 1, !dbg !18\n  %153 = and i1 %148, %152, !dbg !18\n  br i1 %153, label %if_end69, label %assert_fail70, !dbg !18, !prof !19\n\nif_end69:                                         ; preds = %assert_end67, %if_then68\n  %154 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 6, !dbg !18\n  %155 = load i64, i64* %154, align 8, !dbg !18\n  %156 = icmp eq i64 %155, 0, !dbg !18\n  br i1 %156, label %assert_end73, label %assert_fail72, !dbg !18, !prof !19\n\nassert_fail70:                                    ; preds = %if_then68\n  %157 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %157(i8* getelementptr inbounds ([227 x i8], [227 x i8]* @.str.26, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail72:                                    ; preds = %if_end69\n  %158 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %158(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.27, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end73:                                     ; preds = %if_end69\n  %159 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 1, i32 0, !dbg !18\n  %160 = load i32, i32* %159, align 4, !dbg !18\n  %161 = icmp eq i32 %160, 1, !dbg !18\n  br i1 %161, label %assert_end75, label %assert_fail74, !dbg !18, !prof !19\n\nassert_fail74:                                    ; preds = %assert_end73\n  %162 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %162(i8* getelementptr inbounds ([215 x i8], [215 x i8]* @.str.28, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end75:                                     ; preds = %assert_end73\n  %163 = getelementptr inbounds %1, %1* %T_batch_matmul_NN85, i64 0, i32 1, i32 1, !dbg !18\n  %164 = load i32, i32* %163, align 4, !dbg !18\n  %165 = icmp eq i32 %dev_id, %164, !dbg !18\n  br i1 %165, label %assert_end77, label %assert_fail76, !dbg !18, !prof !19\n\nassert_fail76:                                    ; preds = %assert_end75\n  %166 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %166(i8* getelementptr inbounds ([221 x i8], [221 x i8]* @.str.29, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end77:                                     ; preds = %assert_end75\n  %167 = call fastcc i32 @default_function_compute_(i8* nonnull %stack_value, float* %T_batch_matmul_NN_void_ptr88, i32* nonnull %stack_tcode82.sub, float* %ph_0_void_ptr86, float* %ph_3_void_ptr87), !dbg !18\n  ret i32 %167, !dbg !18\n}\n\n; Function Attrs: nounwind willreturn\ndeclare void @llvm.assume(i1) #1\n\n; Function Attrs: noinline\ndefine internal fastcc i32 @default_function_compute_(i8* noalias %0, float* noalias align 64 %1, i32* noalias %2, float* noalias align 64 %3, float* noalias align 64 %4) unnamed_addr #2 {\nentry:\n  %5 = alloca i8*, align 8\n  %6 = bitcast i8* %0 to %0*\n  %7 = bitcast i8* %0 to float**\n  store float* %1, float** %7, align 8\n  store i32 3, i32* %2, align 4, !tbaa !118\n  %8 = getelementptr inbounds i8, i8* %0, i64 8\n  %9 = bitcast i8* %8 to float**\n  store float* %3, float** %9, align 8\n  %10 = getelementptr inbounds i32, i32* %2, i64 1\n  store i32 3, i32* %10, align 4, !tbaa !129\n  %11 = getelementptr inbounds i8, i8* %0, i64 16\n  %12 = bitcast i8* %11 to float**\n  store float* %4, float** %12, align 8\n  %13 = getelementptr inbounds i32, i32* %2, i64 2\n  store i32 3, i32* %13, align 4, !tbaa !131\n  %14 = getelementptr inbounds i8, i8* %0, i64 24\n  %15 = bitcast i8* %14 to %0*\n  %16 = getelementptr inbounds i32, i32* %2, i64 3\n  %17 = load i32 (i8*, %0*, i32*, i32, %0*, i32*)*, i32 (i8*, %0*, i32*, i32, %0*, i32*)** @__TVMFuncCall, align 8, !tbaa !20\n  %18 = load i8*, i8** @.tvm_func.default_function_kernel, align 8\n  %19 = icmp eq i8* %18, null\n  br i1 %19, label %handle_init, label %handle_init_end, !prof !54\n\nhandle_init:                                      ; preds = %entry\n  %20 = load i8*, i8** @__tvm_module_ctx, align 8, !tbaa !20\n  %21 = load i32 (i8*, i8*, i8**)*, i32 (i8*, i8*, i8**)** @__TVMBackendGetFuncFromEnv, align 8, !tbaa !20\n  %22 = call i32 %21(i8* %20, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.30, i64 0, i64 0), i8** nonnull %5)\n  %23 = icmp eq i32 %22, 0\n  br i1 %23, label %call_end, label %call_fail, !prof !19\n\nhandle_init_end:                                  ; preds = %entry, %call_end\n  %24 = phi i8* [ %18, %entry ], [ %27, %call_end ]\n  %25 = call i32 %17(i8* %24, %0* %6, i32* nonnull %2, i32 3, %0* nonnull %15, i32* nonnull %16)\n  %26 = icmp eq i32 %25, 0\n  br i1 %26, label %call_end2, label %call_fail, !prof !19\n\ncall_fail:                                        ; preds = %call_end2, %handle_init_end, %handle_init\n  %merge = phi i32 [ %22, %handle_init ], [ %25, %handle_init_end ], [ 0, %call_end2 ]\n  ret i32 %merge\n\ncall_end:                                         ; preds = %handle_init\n  %27 = load i8*, i8** %5, align 8\n  store i8* %27, i8** @.tvm_func.default_function_kernel, align 8\n  br label %handle_init_end\n\ncall_end2:                                        ; preds = %handle_init_end\n  %28 = bitcast i8* %14 to i64*\n  %29 = load i64, i64* %28, align 8\n  %kernel_error_code = trunc i64 %29 to i32\n  %30 = icmp eq i32 %kernel_error_code, 0\n  br i1 %30, label %call_fail, label %assert_fail, !prof !19\n\nassert_fail:                                      ; preds = %call_end2\n  %31 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !20\n  call void %31(i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.31, i64 0, i64 0))\n  ret i32 -1\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\n; Function Attrs: nounwind readnone speculatable willreturn\ndeclare void @llvm.dbg.value(metadata, metadata, metadata) #4\n\nattributes #0 = { \"target-cpu\"=\"generic\" }\nattributes #1 = { nounwind willreturn }\nattributes #2 = { noinline \"target-cpu\"=\"generic\" }\nattributes #3 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\nattributes #4 = { nounwind readnone speculatable willreturn }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n!5 = distinct !DISubprogram(name: \"main.tir\", scope: !1, file: !1, type: !6, spFlags: DISPFlagLocalToUnit | DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !11)\n!6 = !DISubroutineType(types: !7)\n!7 = !{!8, !9, !10, !8, !9, !10, !9}\n!8 = !DIBasicType(name: \"int32\", size: 32, encoding: DW_ATE_signed)\n!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null)\n!10 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8)\n!11 = !{!12, !13, !14, !15, !16, !17}\n!12 = !DILocalVariable(name: \"arg1\", arg: 1, scope: !5, file: !1, type: !9)\n!13 = !DILocalVariable(name: \"arg2\", arg: 2, scope: !5, file: !1, type: !10)\n!14 = !DILocalVariable(name: \"arg3\", arg: 3, scope: !5, file: !1, type: !8)\n!15 = !DILocalVariable(name: \"arg4\", arg: 4, scope: !5, file: !1, type: !9)\n!16 = !DILocalVariable(name: \"arg5\", arg: 5, scope: !5, file: !1, type: !10)\n!17 = !DILocalVariable(name: \"arg6\", arg: 6, scope: !5, file: !1, type: !9)\n!18 = !DILocation(line: 0, scope: !5)\n!19 = !{!\"branch_weights\", i32 1048576, i32 1}\n!20 = !{!21, !21, i64 0}\n!21 = !{!\"ctx_ptr\", !22, i64 0}\n!22 = !{!\"tvm-tbaa\"}\n!23 = !{!24, !24, i64 0}\n!24 = !{!\"0x3d0e740.w4.b0\", !25, i64 0}\n!25 = !{!\"0x3d0e740.w8.b0\", !26, i64 0}\n!26 = !{!\"0x3d0e740.w16.b0\", !27, i64 0}\n!27 = !{!\"0x3d0e740.w32.b0\", !28, i64 0}\n!28 = !{!\"0x3d0e740.w64.b0\", !29, i64 0}\n!29 = !{!\"0x3d0e740.w128.b0\", !30, i64 0}\n!30 = !{!\"0x3d0e740.w256.b0\", !31, i64 0}\n!31 = !{!\"0x3d0e740.w512.b0\", !32, i64 0}\n!32 = !{!\"0x3d0e740.w1024.b0\", !33, i64 0}\n!33 = !{!\"0x3d0e740\", !22, i64 0}\n!34 = !{!35, !35, i64 0}\n!35 = !{!\"0x3d0e740.w4.b4\", !25, i64 0}\n!36 = !{!37, !37, i64 0}\n!37 = !{!\"0x3d0e740.w4.b8\", !38, i64 0}\n!38 = !{!\"0x3d0e740.w8.b8\", !26, i64 0}\n!39 = !{!40, !40, i64 0}\n!40 = !{!\"0x3e7f300.w8.b0\", !41, i64 0}\n!41 = !{!\"0x3e7f300.w16.b0\", !42, i64 0}\n!42 = !{!\"0x3e7f300.w32.b0\", !43, i64 0}\n!43 = !{!\"0x3e7f300.w64.b0\", !44, i64 0}\n!44 = !{!\"0x3e7f300.w128.b0\", !45, i64 0}\n!45 = !{!\"0x3e7f300.w256.b0\", !46, i64 0}\n!46 = !{!\"0x3e7f300.w512.b0\", !47, i64 0}\n!47 = !{!\"0x3e7f300.w1024.b0\", !48, i64 0}\n!48 = !{!\"0x3e7f300\", !22, i64 0}\n!49 = !{!50, !50, i64 0}\n!50 = !{!\"0x3e7f300.w8.b8\", !41, i64 0}\n!51 = !{!52, !52, i64 0}\n!52 = !{!\"0x3e7f300.w8.b16\", !53, i64 0}\n!53 = !{!\"0x3e7f300.w16.b16\", !42, i64 0}\n!54 = !{!\"branch_weights\", i32 1, i32 1048576}\n!55 = !{!56, !56, i64 0}\n!56 = !{!\"0x4422ef0.w8.b8\", !57, i64 0}\n!57 = !{!\"0x4422ef0.w16.b0\", !58, i64 0}\n!58 = !{!\"0x4422ef0.w32.b0\", !59, i64 0}\n!59 = !{!\"0x4422ef0.w64.b0\", !60, i64 0}\n!60 = !{!\"0x4422ef0.w128.b0\", !61, i64 0}\n!61 = !{!\"0x4422ef0.w256.b0\", !62, i64 0}\n!62 = !{!\"0x4422ef0.w512.b0\", !63, i64 0}\n!63 = !{!\"0x4422ef0.w1024.b0\", !64, i64 0}\n!64 = !{!\"0x4422ef0\", !22, i64 0}\n!65 = !{!66, !66, i64 0}\n!66 = !{!\"0x43fbb20.w8.b0\", !67, i64 0}\n!67 = !{!\"0x43fbb20.w16.b0\", !68, i64 0}\n!68 = !{!\"0x43fbb20.w32.b0\", !69, i64 0}\n!69 = !{!\"0x43fbb20.w64.b0\", !70, i64 0}\n!70 = !{!\"0x43fbb20.w128.b0\", !71, i64 0}\n!71 = !{!\"0x43fbb20.w256.b0\", !72, i64 0}\n!72 = !{!\"0x43fbb20.w512.b0\", !73, i64 0}\n!73 = !{!\"0x43fbb20.w1024.b0\", !74, i64 0}\n!74 = !{!\"0x43fbb20\", !22, i64 0}\n!75 = !{!76, !76, i64 0}\n!76 = !{!\"0x43fbb20.w8.b8\", !67, i64 0}\n!77 = !{!78, !78, i64 0}\n!78 = !{!\"0x43fbb20.w8.b16\", !79, i64 0}\n!79 = !{!\"0x43fbb20.w16.b16\", !68, i64 0}\n!80 = !{!81, !81, i64 0}\n!81 = !{!\"0x44aade0.w8.b16\", !82, i64 0}\n!82 = !{!\"0x44aade0.w16.b16\", !83, i64 0}\n!83 = !{!\"0x44aade0.w32.b0\", !84, i64 0}\n!84 = !{!\"0x44aade0.w64.b0\", !85, i64 0}\n!85 = !{!\"0x44aade0.w128.b0\", !86, i64 0}\n!86 = !{!\"0x44aade0.w256.b0\", !87, i64 0}\n!87 = !{!\"0x44aade0.w512.b0\", !88, i64 0}\n!88 = !{!\"0x44aade0.w1024.b0\", !89, i64 0}\n!89 = !{!\"0x44aade0\", !22, i64 0}\n!90 = !{!91, !91, i64 0}\n!91 = !{!\"0x4438720.w8.b0\", !92, i64 0}\n!92 = !{!\"0x4438720.w16.b0\", !93, i64 0}\n!93 = !{!\"0x4438720.w32.b0\", !94, i64 0}\n!94 = !{!\"0x4438720.w64.b0\", !95, i64 0}\n!95 = !{!\"0x4438720.w128.b0\", !96, i64 0}\n!96 = !{!\"0x4438720.w256.b0\", !97, i64 0}\n!97 = !{!\"0x4438720.w512.b0\", !98, i64 0}\n!98 = !{!\"0x4438720.w1024.b0\", !99, i64 0}\n!99 = !{!\"0x4438720\", !22, i64 0}\n!100 = !{!101, !101, i64 0}\n!101 = !{!\"0x4438720.w8.b8\", !92, i64 0}\n!102 = !{!103, !103, i64 0}\n!103 = !{!\"0x4438720.w8.b16\", !104, i64 0}\n!104 = !{!\"0x4438720.w16.b16\", !93, i64 0}\n!105 = !{!106, !106, i64 0}\n!106 = !{!\"0x44adc60.w8.b8\", !107, i64 0}\n!107 = !{!\"0x44adc60.w16.b0\", !108, i64 0}\n!108 = !{!\"0x44adc60.w32.b0\", !109, i64 0}\n!109 = !{!\"0x44adc60.w64.b0\", !110, i64 0}\n!110 = !{!\"0x44adc60.w128.b0\", !111, i64 0}\n!111 = !{!\"0x44adc60.w256.b0\", !112, i64 0}\n!112 = !{!\"0x44adc60.w512.b0\", !113, i64 0}\n!113 = !{!\"0x44adc60.w1024.b0\", !114, i64 0}\n!114 = !{!\"0x44adc60\", !22, i64 0}\n!115 = !{!116, !116, i64 0}\n!116 = !{!\"0x44adc60.w8.b16\", !117, i64 0}\n!117 = !{!\"0x44adc60.w16.b16\", !108, i64 0}\n!118 = !{!119, !119, i64 0}\n!119 = !{!\"0x4423a70.w4.b0\", !120, i64 0}\n!120 = !{!\"0x4423a70.w8.b0\", !121, i64 0}\n!121 = !{!\"0x4423a70.w16.b0\", !122, i64 0}\n!122 = !{!\"0x4423a70.w32.b0\", !123, i64 0}\n!123 = !{!\"0x4423a70.w64.b0\", !124, i64 0}\n!124 = !{!\"0x4423a70.w128.b0\", !125, i64 0}\n!125 = !{!\"0x4423a70.w256.b0\", !126, i64 0}\n!126 = !{!\"0x4423a70.w512.b0\", !127, i64 0}\n!127 = !{!\"0x4423a70.w1024.b0\", !128, i64 0}\n!128 = !{!\"0x4423a70\", !22, i64 0}\n!129 = !{!130, !130, i64 0}\n!130 = !{!\"0x4423a70.w4.b4\", !120, i64 0}\n!131 = !{!132, !132, i64 0}\n!132 = !{!\"0x4423a70.w4.b8\", !133, i64 0}\n!133 = !{!\"0x4423a70.w8.b8\", !121, i64 0}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3);\nextern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float ph_0_shared[60];\n  __shared__ float ph_3_shared[20];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 2)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 7; ++k_outer_outer) {\n    __syncthreads();\n    int2 v_ = make_int2((((((((int)blockIdx.x) >> 1) * 420) + (((int)threadIdx.x) * 14)) + k_outer_outer))+(7*0), (((((((int)blockIdx.x) >> 1) * 420) + (((int)threadIdx.x) * 14)) + k_outer_outer))+(7*1));\n    *(float2*)(ph_0_shared + (((int)threadIdx.x) * 2)) = make_float2(ph_0[v_.x],ph_0[v_.y]);\n    if (((int)threadIdx.x) < 20) {\n      ph_3_shared[((int)threadIdx.x)] = ph_3[((((((((int)blockIdx.x) >> 1) * 280) + ((((int)threadIdx.x) >> 1) * 28)) + (k_outer_outer * 4)) + ((((int)blockIdx.x) & 1) * 2)) + (((int)threadIdx.x) & 1))];\n    }\n    __syncthreads();\n    for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (ph_0_shared[((((((int)threadIdx.x) / 6) * 12) + (b_c_inner * 6)) + (((int)threadIdx.x) % 6))] * ph_3_shared[(((((int)threadIdx.x) / 6) * 4) + (b_c_inner * 2))]));\n      T_batch_matmul_NN_local[(b_c_inner + 2)] = (T_batch_matmul_NN_local[(b_c_inner + 2)] + (ph_0_shared[((((((int)threadIdx.x) / 6) * 12) + (b_c_inner * 6)) + (((int)threadIdx.x) % 6))] * ph_3_shared[((((((int)threadIdx.x) / 6) * 4) + (b_c_inner * 2)) + 1)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[((((((((int)blockIdx.x) >> 1) * 240) + ((((int)threadIdx.x) / 6) * 48)) + (b_inner * 24)) + ((((int)threadIdx.x) % 6) * 4)) + ((((int)blockIdx.x) & 1) * 2))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[(((((((((int)blockIdx.x) >> 1) * 240) + ((((int)threadIdx.x) / 6) * 48)) + (b_inner * 24)) + ((((int)threadIdx.x) % 6) * 4)) + ((((int)blockIdx.x) & 1) * 2)) + 1)] = T_batch_matmul_NN_local[(b_inner + 2)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 2, 1), \"float32\"), ph_3: T.Buffer((1, 1, 9), \"float32\"), T_batch_matmul_NN: T.Buffer((1, 2, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([9], \"float32\", \"global\")\n        auto_scheduler_layout_transform_1 = T.Buffer((9,), data=auto_scheduler_layout_transform, align=32)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3):\n            for ax3 in range(3):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 3 + ax3\n                ph_3_1 = T.Buffer((9,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[cse_var_1] = ph_3_1[cse_var_1]\n        for j_outer_outer_outer in range(3):\n            T_batch_matmul_NN_1 = T.Buffer((18,), data=T_batch_matmul_NN.data)\n            for i_outer_inner_init, j_outer_inner_init in T.grid(2, 3):\n                T_batch_matmul_NN_1[i_outer_inner_init * 9 + j_outer_outer_outer * 3 + j_outer_inner_init] = T.float32(0)\n            for i_outer_inner, j_outer_inner in T.grid(2, 3):\n                cse_var_3: T.int32 = j_outer_outer_outer * 3\n                cse_var_2: T.int32 = i_outer_inner * 9 + cse_var_3 + j_outer_inner\n                ph_0_1 = T.Buffer((2,), data=ph_0.data)\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[i_outer_inner] * auto_scheduler_layout_transform_1[cse_var_3 + j_outer_inner]", "op_args": []}{"op_name": "topology_expansion", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%0 = type { double }\n%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }\n%2 = type { i32, i32 }\n%3 = type { i8, i8, i16 }\n\n@__tvm_module_ctx = linkonce dllexport local_unnamed_addr global i8* null, align 8\n@__TVMFuncCall = linkonce dllexport local_unnamed_addr global i32 (i8*, %0*, i32*, i32, %0*, i32*)* null, align 8\n@__TVMBackendGetFuncFromEnv = linkonce dllexport local_unnamed_addr global i32 (i8*, i8*, i8**)* null, align 8\n@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8\n@.str = private constant [67 x i8] c\"Assert fail: num_args == 2, default_function: num_args should be 2\\00\", align 1\n@.str.1 = private constant [130 x i8] c\"Assert fail: ph_0_code == 3 or ph_0_code == 13 or ph_0_code == 7 or ph_0_code == 4, default_function: Expect arg[0] to be pointer\\00\", align 1\n@.str.2 = private constant [142 x i8] c\"Assert fail: compute_code == 3 or compute_code == 13 or compute_code == 7 or compute_code == 4, default_function: Expect arg[1] to be pointer\\00\", align 1\n@.str.3 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_0, 0, 4, \\22int32\\22), default_function.ph_0.ndim is expected to equal 3\\00\", align 1\n@.str.4 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_0, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_0, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_0.dtype is expected to be float32\\00\", align 1\n@.str.5 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[0]) == 20, Argument default_function.ph_0.shape[0] has an unsatisfied constraint: 20 == T.Cast(\\22int32\\22, default_function_ph_0_shape[0])\\00\", align 1\n@.str.6 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[1]) == 3, Argument default_function.ph_0.shape[1] has an unsatisfied constraint: 3 == T.Cast(\\22int32\\22, default_function_ph_0_shape[1])\\00\", align 1\n@.str.7 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[2]) == 8, Argument default_function.ph_0.shape[2] has an unsatisfied constraint: 8 == T.Cast(\\22int32\\22, default_function_ph_0_shape[2])\\00\", align 1\n@.str.8 = private constant [248 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_0_strides[2]) and 8 == T.Cast(\\22int32\\22, default_function_ph_0_strides[1]) and 24 == T.Cast(\\22int32\\22, default_function_ph_0_strides[0]), default_function.ph_0.strides: expected to be compact array\\00\", align 1\n@.str.9 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22), Argument default_function.ph_0.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.10 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_0.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.11 = private constant [113 x i8] c\"Assert fail: 3 == T.tvm_struct_get(compute, 0, 4, \\22int32\\22), default_function.compute.ndim is expected to equal 3\\00\", align 1\n@.str.12 = private constant [247 x i8] c\"Assert fail: T.tvm_struct_get(compute, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(compute, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(compute, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.compute.dtype is expected to be float32\\00\", align 1\n@.str.13 = private constant [202 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[0]) == 20, Argument default_function.compute.shape[0] has an unsatisfied constraint: 20 == T.Cast(\\22int32\\22, default_function_compute_shape[0])\\00\", align 1\n@.str.14 = private constant [200 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[1]) == 3, Argument default_function.compute.shape[1] has an unsatisfied constraint: 3 == T.Cast(\\22int32\\22, default_function_compute_shape[1])\\00\", align 1\n@.str.15 = private constant [200 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[2]) == 8, Argument default_function.compute.shape[2] has an unsatisfied constraint: 8 == T.Cast(\\22int32\\22, default_function_compute_shape[2])\\00\", align 1\n@.str.16 = private constant [260 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_compute_strides[2]) and 8 == T.Cast(\\22int32\\22, default_function_compute_strides[1]) and 24 == T.Cast(\\22int32\\22, default_function_compute_strides[0]), default_function.compute.strides: expected to be compact array\\00\", align 1\n@.str.17 = private constant [205 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(compute, 0, 8, \\22uint64\\22), Argument default_function.compute.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(compute, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.18 = private constant [185 x i8] c\"Assert fail: T.tvm_struct_get(compute, 0, 10, \\22int32\\22) == 1, Argument default_function.compute.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(compute, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.19 = private constant [191 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(compute, 0, 9, \\22int32\\22), Argument default_function.compute.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(compute, 0, 9, \\22int32\\22)\\00\", align 1\n@.tvm_func.default_function_kernel = internal unnamed_addr global i8* null, align 8\n@.str.20 = private constant [24 x i8] c\"default_function_kernel\\00\", align 1\n@.str.21 = private constant [68 x i8] c\"Assert fail: kernel_error_code == 0, Error executing compute kernel\\00\", align 1\n@__tvm_main__ = weak dllexport local_unnamed_addr constant [17 x i8] c\"default_function\\00\", align 1\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\ndefine dllexport i32 @default_function(i8* noalias nocapture readonly %args, i32* noalias nocapture readonly %arg_type_ids, i32 %num_args, i8* noalias nocapture readnone %out_ret_value, i32* noalias nocapture readnone %out_ret_tcode, i8* noalias nocapture readnone %resource_handle) local_unnamed_addr #0 !dbg !5 {\nentry:\n  call void @llvm.dbg.value(metadata i8* %args, metadata !12, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %arg_type_ids, metadata !13, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32 %num_args, metadata !14, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %out_ret_value, metadata !15, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %out_ret_tcode, metadata !16, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %resource_handle, metadata !17, metadata !DIExpression()), !dbg !18\n  %stack_value_void_ptr52 = alloca [3 x %0], align 8, !dbg !18\n  %stack_tcode53 = alloca [3 x i32], align 4, !dbg !18\n  %stack_tcode53.sub = getelementptr inbounds [3 x i32], [3 x i32]* %stack_tcode53, i64 0, i64 0\n  %stack_value = bitcast [3 x %0]* %stack_value_void_ptr52 to i8*, !dbg !18\n  %0 = icmp eq i32 %num_args, 2, !dbg !18\n  br i1 %0, label %assert_end, label %assert_fail, !dbg !18, !prof !19\n\nassert_fail:                                      ; preds = %entry\n  %1 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %1(i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end:                                       ; preds = %entry\n  %ph_0.code = load i32, i32* %arg_type_ids, align 4, !dbg !18, !tbaa !23\n  %2 = getelementptr inbounds i32, i32* %arg_type_ids, i64 1, !dbg !18\n  %compute.code = load i32, i32* %2, align 4, !dbg !18, !tbaa !34\n  %3 = bitcast i8* %args to %1**, !dbg !18\n  %ph_054 = load %1*, %1** %3, align 8, !dbg !18\n  %4 = getelementptr inbounds i8, i8* %args, i64 8, !dbg !18\n  %5 = bitcast i8* %4 to %1**, !dbg !18\n  %compute55 = load %1*, %1** %5, align 8, !dbg !18\n  %6 = bitcast %1* %ph_054 to float**, !dbg !18\n  %ph_0_void_ptr56 = load float*, float** %6, align 8, !dbg !18\n  %ptrint = ptrtoint float* %ph_0_void_ptr56 to i64, !dbg !18\n  %maskedptr = and i64 %ptrint, 63, !dbg !18\n  %maskcond = icmp eq i64 %maskedptr, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond), !dbg !18\n  %7 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 4, !dbg !18\n  %default_function.ph_0.shape = load i64*, i64** %7, align 8, !dbg !18\n  %8 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 5, !dbg !18\n  %default_function.ph_0.strides = load i64*, i64** %8, align 8, !dbg !18\n  %9 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 1, i32 1, !dbg !18\n  %dev_id = load i32, i32* %9, align 4, !dbg !18\n  %10 = bitcast %1* %compute55 to float**, !dbg !18\n  %compute_void_ptr57 = load float*, float** %10, align 8, !dbg !18\n  %ptrint3 = ptrtoint float* %compute_void_ptr57 to i64, !dbg !18\n  %maskedptr4 = and i64 %ptrint3, 63, !dbg !18\n  %maskcond5 = icmp eq i64 %maskedptr4, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond5), !dbg !18\n  %11 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 4, !dbg !18\n  %default_function.compute.shape = load i64*, i64** %11, align 8, !dbg !18\n  %12 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 5, !dbg !18\n  %default_function.compute.strides = load i64*, i64** %12, align 8, !dbg !18\n  switch i32 %ph_0.code, label %assert_fail6 [\n    i32 13, label %assert_end7\n    i32 7, label %assert_end7\n    i32 4, label %assert_end7\n    i32 3, label %assert_end7\n  ], !dbg !18\n\nassert_fail6:                                     ; preds = %assert_end\n  %13 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %13(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.1, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end7:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end\n  switch i32 %compute.code, label %assert_fail8 [\n    i32 13, label %assert_end9\n    i32 7, label %assert_end9\n    i32 4, label %assert_end9\n    i32 3, label %assert_end9\n  ], !dbg !18\n\nassert_fail8:                                     ; preds = %assert_end7\n  %14 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %14(i8* getelementptr inbounds ([142 x i8], [142 x i8]* @.str.2, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end9:                                      ; preds = %assert_end7, %assert_end7, %assert_end7, %assert_end7\n  %15 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 2, !dbg !18\n  %16 = load i32, i32* %15, align 4, !dbg !18\n  %17 = icmp eq i32 %16, 3, !dbg !18\n  br i1 %17, label %assert_end13, label %assert_fail10, !dbg !18, !prof !19\n\nassert_fail10:                                    ; preds = %assert_end9\n  %18 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %18(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.3, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end13:                                     ; preds = %assert_end9\n  %19 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 3, i32 2, !dbg !18\n  %20 = load i16, i16* %19, align 2, !dbg !18\n  %21 = icmp eq i16 %20, 1, !dbg !18\n  %22 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 3, i32 1, !dbg !18\n  %23 = load i8, i8* %22, align 1, !dbg !18\n  %24 = icmp eq i8 %23, 32, !dbg !18\n  %25 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 3, i32 0, !dbg !18\n  %26 = load i8, i8* %25, align 1, !dbg !18\n  %27 = icmp eq i8 %26, 2, !dbg !18\n  %28 = and i1 %24, %27, !dbg !18\n  %29 = and i1 %21, %28, !dbg !18\n  br i1 %29, label %assert_end15, label %assert_fail14, !dbg !18, !prof !19\n\nassert_fail14:                                    ; preds = %assert_end13\n  %30 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %30(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.4, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end15:                                     ; preds = %assert_end13\n  %31 = load i64, i64* %default_function.ph_0.shape, align 8, !dbg !18, !tbaa !36\n  %32 = trunc i64 %31 to i32, !dbg !18\n  %33 = icmp eq i32 %32, 20, !dbg !18\n  br i1 %33, label %assert_end17, label %assert_fail16, !dbg !18, !prof !19\n\nassert_fail16:                                    ; preds = %assert_end15\n  %34 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %34(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.5, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end17:                                     ; preds = %assert_end15\n  %35 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 1, !dbg !18\n  %36 = load i64, i64* %35, align 8, !dbg !18, !tbaa !46\n  %37 = trunc i64 %36 to i32, !dbg !18\n  %38 = icmp eq i32 %37, 3, !dbg !18\n  br i1 %38, label %assert_end19, label %assert_fail18, !dbg !18, !prof !19\n\nassert_fail18:                                    ; preds = %assert_end17\n  %39 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %39(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.6, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end19:                                     ; preds = %assert_end17\n  %40 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 2, !dbg !18\n  %41 = load i64, i64* %40, align 8, !dbg !18, !tbaa !48\n  %42 = trunc i64 %41 to i32, !dbg !18\n  %43 = icmp eq i32 %42, 8, !dbg !18\n  br i1 %43, label %assert_end21, label %assert_fail20, !dbg !18, !prof !19\n\nassert_fail20:                                    ; preds = %assert_end19\n  %44 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %44(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.7, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end21:                                     ; preds = %assert_end19\n  %45 = icmp eq i64* %default_function.ph_0.strides, null, !dbg !18\n  br i1 %45, label %if_end, label %if_then, !dbg !18, !prof !51\n\nif_then:                                          ; preds = %assert_end21\n  %46 = load i64, i64* %default_function.ph_0.strides, align 8, !dbg !18, !tbaa !52\n  %47 = trunc i64 %46 to i32, !dbg !18\n  %48 = icmp eq i32 %47, 24, !dbg !18\n  %49 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 1, !dbg !18\n  %50 = load i64, i64* %49, align 8, !dbg !18, !tbaa !62\n  %51 = trunc i64 %50 to i32, !dbg !18\n  %52 = icmp eq i32 %51, 8, !dbg !18\n  %53 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 2, !dbg !18\n  %54 = load i64, i64* %53, align 8, !dbg !18, !tbaa !64\n  %55 = trunc i64 %54 to i32, !dbg !18\n  %56 = icmp eq i32 %55, 1, !dbg !18\n  %57 = and i1 %52, %56, !dbg !18\n  %58 = and i1 %48, %57, !dbg !18\n  br i1 %58, label %if_end, label %assert_fail22, !dbg !18, !prof !19\n\nif_end:                                           ; preds = %assert_end21, %if_then\n  %59 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 6, !dbg !18\n  %60 = load i64, i64* %59, align 8, !dbg !18\n  %61 = icmp eq i64 %60, 0, !dbg !18\n  br i1 %61, label %assert_end25, label %assert_fail24, !dbg !18, !prof !19\n\nassert_fail22:                                    ; preds = %if_then\n  %62 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %62(i8* getelementptr inbounds ([248 x i8], [248 x i8]* @.str.8, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail24:                                    ; preds = %if_end\n  %63 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %63(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.9, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end25:                                     ; preds = %if_end\n  %64 = getelementptr inbounds %1, %1* %ph_054, i64 0, i32 1, i32 0, !dbg !18\n  %65 = load i32, i32* %64, align 4, !dbg !18\n  %66 = icmp eq i32 %65, 1, !dbg !18\n  br i1 %66, label %assert_end27, label %assert_fail26, !dbg !18, !prof !19\n\nassert_fail26:                                    ; preds = %assert_end25\n  %67 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %67(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.10, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end27:                                     ; preds = %assert_end25\n  %68 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 2, !dbg !18\n  %69 = load i32, i32* %68, align 4, !dbg !18\n  %70 = icmp eq i32 %69, 3, !dbg !18\n  br i1 %70, label %assert_end31, label %assert_fail28, !dbg !18, !prof !19\n\nassert_fail28:                                    ; preds = %assert_end27\n  %71 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %71(i8* getelementptr inbounds ([113 x i8], [113 x i8]* @.str.11, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end31:                                     ; preds = %assert_end27\n  %72 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 3, i32 2, !dbg !18\n  %73 = load i16, i16* %72, align 2, !dbg !18\n  %74 = icmp eq i16 %73, 1, !dbg !18\n  %75 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 3, i32 1, !dbg !18\n  %76 = load i8, i8* %75, align 1, !dbg !18\n  %77 = icmp eq i8 %76, 32, !dbg !18\n  %78 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 3, i32 0, !dbg !18\n  %79 = load i8, i8* %78, align 1, !dbg !18\n  %80 = icmp eq i8 %79, 2, !dbg !18\n  %81 = and i1 %77, %80, !dbg !18\n  %82 = and i1 %74, %81, !dbg !18\n  br i1 %82, label %assert_end33, label %assert_fail32, !dbg !18, !prof !19\n\nassert_fail32:                                    ; preds = %assert_end31\n  %83 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %83(i8* getelementptr inbounds ([247 x i8], [247 x i8]* @.str.12, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end33:                                     ; preds = %assert_end31\n  %84 = load i64, i64* %default_function.compute.shape, align 8, !dbg !18, !tbaa !67\n  %85 = trunc i64 %84 to i32, !dbg !18\n  %86 = icmp eq i32 %85, 20, !dbg !18\n  br i1 %86, label %assert_end35, label %assert_fail34, !dbg !18, !prof !19\n\nassert_fail34:                                    ; preds = %assert_end33\n  %87 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %87(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.13, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end35:                                     ; preds = %assert_end33\n  %88 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 1, !dbg !18\n  %89 = load i64, i64* %88, align 8, !dbg !18, !tbaa !77\n  %90 = trunc i64 %89 to i32, !dbg !18\n  %91 = icmp eq i32 %90, 3, !dbg !18\n  br i1 %91, label %assert_end37, label %assert_fail36, !dbg !18, !prof !19\n\nassert_fail36:                                    ; preds = %assert_end35\n  %92 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %92(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.14, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end37:                                     ; preds = %assert_end35\n  %93 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 2, !dbg !18\n  %94 = load i64, i64* %93, align 8, !dbg !18, !tbaa !79\n  %95 = trunc i64 %94 to i32, !dbg !18\n  %96 = icmp eq i32 %95, 8, !dbg !18\n  br i1 %96, label %assert_end39, label %assert_fail38, !dbg !18, !prof !19\n\nassert_fail38:                                    ; preds = %assert_end37\n  %97 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %97(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.15, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end39:                                     ; preds = %assert_end37\n  %98 = icmp eq i64* %default_function.compute.strides, null, !dbg !18\n  br i1 %98, label %if_end41, label %if_then40, !dbg !18, !prof !51\n\nif_then40:                                        ; preds = %assert_end39\n  %99 = load i64, i64* %default_function.compute.strides, align 8, !dbg !18, !tbaa !82\n  %100 = trunc i64 %99 to i32, !dbg !18\n  %101 = icmp eq i32 %100, 24, !dbg !18\n  %102 = getelementptr inbounds i64, i64* %default_function.compute.strides, i64 1, !dbg !18\n  %103 = load i64, i64* %102, align 8, !dbg !18, !tbaa !92\n  %104 = trunc i64 %103 to i32, !dbg !18\n  %105 = icmp eq i32 %104, 8, !dbg !18\n  %106 = getelementptr inbounds i64, i64* %default_function.compute.strides, i64 2, !dbg !18\n  %107 = load i64, i64* %106, align 8, !dbg !18, !tbaa !94\n  %108 = trunc i64 %107 to i32, !dbg !18\n  %109 = icmp eq i32 %108, 1, !dbg !18\n  %110 = and i1 %105, %109, !dbg !18\n  %111 = and i1 %101, %110, !dbg !18\n  br i1 %111, label %if_end41, label %assert_fail42, !dbg !18, !prof !19\n\nif_end41:                                         ; preds = %assert_end39, %if_then40\n  %112 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 6, !dbg !18\n  %113 = load i64, i64* %112, align 8, !dbg !18\n  %114 = icmp eq i64 %113, 0, !dbg !18\n  br i1 %114, label %assert_end45, label %assert_fail44, !dbg !18, !prof !19\n\nassert_fail42:                                    ; preds = %if_then40\n  %115 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %115(i8* getelementptr inbounds ([260 x i8], [260 x i8]* @.str.16, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail44:                                    ; preds = %if_end41\n  %116 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %116(i8* getelementptr inbounds ([205 x i8], [205 x i8]* @.str.17, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end45:                                     ; preds = %if_end41\n  %117 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 1, i32 0, !dbg !18\n  %118 = load i32, i32* %117, align 4, !dbg !18\n  %119 = icmp eq i32 %118, 1, !dbg !18\n  br i1 %119, label %assert_end47, label %assert_fail46, !dbg !18, !prof !19\n\nassert_fail46:                                    ; preds = %assert_end45\n  %120 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %120(i8* getelementptr inbounds ([185 x i8], [185 x i8]* @.str.18, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end47:                                     ; preds = %assert_end45\n  %121 = getelementptr inbounds %1, %1* %compute55, i64 0, i32 1, i32 1, !dbg !18\n  %122 = load i32, i32* %121, align 4, !dbg !18\n  %123 = icmp eq i32 %dev_id, %122, !dbg !18\n  br i1 %123, label %assert_end49, label %assert_fail48, !dbg !18, !prof !19\n\nassert_fail48:                                    ; preds = %assert_end47\n  %124 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %124(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.19, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end49:                                     ; preds = %assert_end47\n  %125 = call fastcc i32 @default_function_compute_(i8* nonnull %stack_value, float* %compute_void_ptr57, i32* nonnull %stack_tcode53.sub, float* %ph_0_void_ptr56), !dbg !18\n  ret i32 %125, !dbg !18\n}\n\n; Function Attrs: nounwind willreturn\ndeclare void @llvm.assume(i1) #1\n\n; Function Attrs: noinline\ndefine internal fastcc i32 @default_function_compute_(i8* noalias %0, float* noalias align 64 %1, i32* noalias %2, float* noalias align 64 %3) unnamed_addr #2 {\nentry:\n  %4 = alloca i8*, align 8\n  %5 = bitcast i8* %0 to %0*\n  %6 = bitcast i8* %0 to float**\n  store float* %1, float** %6, align 8\n  store i32 3, i32* %2, align 4, !tbaa !97\n  %7 = getelementptr inbounds i8, i8* %0, i64 8\n  %8 = bitcast i8* %7 to float**\n  store float* %3, float** %8, align 8\n  %9 = getelementptr inbounds i32, i32* %2, i64 1\n  store i32 3, i32* %9, align 4, !tbaa !108\n  %10 = getelementptr inbounds i8, i8* %0, i64 16\n  %11 = bitcast i8* %10 to %0*\n  %12 = getelementptr inbounds i32, i32* %2, i64 2\n  %13 = load i32 (i8*, %0*, i32*, i32, %0*, i32*)*, i32 (i8*, %0*, i32*, i32, %0*, i32*)** @__TVMFuncCall, align 8, !tbaa !20\n  %14 = load i8*, i8** @.tvm_func.default_function_kernel, align 8\n  %15 = icmp eq i8* %14, null\n  br i1 %15, label %handle_init, label %handle_init_end, !prof !51\n\nhandle_init:                                      ; preds = %entry\n  %16 = load i8*, i8** @__tvm_module_ctx, align 8, !tbaa !20\n  %17 = load i32 (i8*, i8*, i8**)*, i32 (i8*, i8*, i8**)** @__TVMBackendGetFuncFromEnv, align 8, !tbaa !20\n  %18 = call i32 %17(i8* %16, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.20, i64 0, i64 0), i8** nonnull %4)\n  %19 = icmp eq i32 %18, 0\n  br i1 %19, label %call_end, label %call_fail, !prof !19\n\nhandle_init_end:                                  ; preds = %entry, %call_end\n  %20 = phi i8* [ %14, %entry ], [ %23, %call_end ]\n  %21 = call i32 %13(i8* %20, %0* %5, i32* nonnull %2, i32 2, %0* nonnull %11, i32* nonnull %12)\n  %22 = icmp eq i32 %21, 0\n  br i1 %22, label %call_end2, label %call_fail, !prof !19\n\ncall_fail:                                        ; preds = %call_end2, %handle_init_end, %handle_init\n  %merge = phi i32 [ %18, %handle_init ], [ %21, %handle_init_end ], [ 0, %call_end2 ]\n  ret i32 %merge\n\ncall_end:                                         ; preds = %handle_init\n  %23 = load i8*, i8** %4, align 8\n  store i8* %23, i8** @.tvm_func.default_function_kernel, align 8\n  br label %handle_init_end\n\ncall_end2:                                        ; preds = %handle_init_end\n  %24 = bitcast i8* %10 to i64*\n  %25 = load i64, i64* %24, align 8\n  %kernel_error_code = trunc i64 %25 to i32\n  %26 = icmp eq i32 %kernel_error_code, 0\n  br i1 %26, label %call_fail, label %assert_fail, !prof !19\n\nassert_fail:                                      ; preds = %call_end2\n  %27 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !20\n  call void %27(i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.21, i64 0, i64 0))\n  ret i32 -1\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\n; Function Attrs: nounwind readnone speculatable willreturn\ndeclare void @llvm.dbg.value(metadata, metadata, metadata) #4\n\nattributes #0 = { \"target-cpu\"=\"generic\" }\nattributes #1 = { nounwind willreturn }\nattributes #2 = { noinline \"target-cpu\"=\"generic\" }\nattributes #3 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\nattributes #4 = { nounwind readnone speculatable willreturn }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n!5 = distinct !DISubprogram(name: \"main.tir\", scope: !1, file: !1, type: !6, spFlags: DISPFlagLocalToUnit | DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !11)\n!6 = !DISubroutineType(types: !7)\n!7 = !{!8, !9, !10, !8, !9, !10, !9}\n!8 = !DIBasicType(name: \"int32\", size: 32, encoding: DW_ATE_signed)\n!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null)\n!10 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8)\n!11 = !{!12, !13, !14, !15, !16, !17}\n!12 = !DILocalVariable(name: \"arg1\", arg: 1, scope: !5, file: !1, type: !9)\n!13 = !DILocalVariable(name: \"arg2\", arg: 2, scope: !5, file: !1, type: !10)\n!14 = !DILocalVariable(name: \"arg3\", arg: 3, scope: !5, file: !1, type: !8)\n!15 = !DILocalVariable(name: \"arg4\", arg: 4, scope: !5, file: !1, type: !9)\n!16 = !DILocalVariable(name: \"arg5\", arg: 5, scope: !5, file: !1, type: !10)\n!17 = !DILocalVariable(name: \"arg6\", arg: 6, scope: !5, file: !1, type: !9)\n!18 = !DILocation(line: 0, scope: !5)\n!19 = !{!\"branch_weights\", i32 1048576, i32 1}\n!20 = !{!21, !21, i64 0}\n!21 = !{!\"ctx_ptr\", !22, i64 0}\n!22 = !{!\"tvm-tbaa\"}\n!23 = !{!24, !24, i64 0}\n!24 = !{!\"0x41b6f00.w4.b0\", !25, i64 0}\n!25 = !{!\"0x41b6f00.w8.b0\", !26, i64 0}\n!26 = !{!\"0x41b6f00.w16.b0\", !27, i64 0}\n!27 = !{!\"0x41b6f00.w32.b0\", !28, i64 0}\n!28 = !{!\"0x41b6f00.w64.b0\", !29, i64 0}\n!29 = !{!\"0x41b6f00.w128.b0\", !30, i64 0}\n!30 = !{!\"0x41b6f00.w256.b0\", !31, i64 0}\n!31 = !{!\"0x41b6f00.w512.b0\", !32, i64 0}\n!32 = !{!\"0x41b6f00.w1024.b0\", !33, i64 0}\n!33 = !{!\"0x41b6f00\", !22, i64 0}\n!34 = !{!35, !35, i64 0}\n!35 = !{!\"0x41b6f00.w4.b4\", !25, i64 0}\n!36 = !{!37, !37, i64 0}\n!37 = !{!\"0x4645e30.w8.b0\", !38, i64 0}\n!38 = !{!\"0x4645e30.w16.b0\", !39, i64 0}\n!39 = !{!\"0x4645e30.w32.b0\", !40, i64 0}\n!40 = !{!\"0x4645e30.w64.b0\", !41, i64 0}\n!41 = !{!\"0x4645e30.w128.b0\", !42, i64 0}\n!42 = !{!\"0x4645e30.w256.b0\", !43, i64 0}\n!43 = !{!\"0x4645e30.w512.b0\", !44, i64 0}\n!44 = !{!\"0x4645e30.w1024.b0\", !45, i64 0}\n!45 = !{!\"0x4645e30\", !22, i64 0}\n!46 = !{!47, !47, i64 0}\n!47 = !{!\"0x4645e30.w8.b8\", !38, i64 0}\n!48 = !{!49, !49, i64 0}\n!49 = !{!\"0x4645e30.w8.b16\", !50, i64 0}\n!50 = !{!\"0x4645e30.w16.b16\", !39, i64 0}\n!51 = !{!\"branch_weights\", i32 1, i32 1048576}\n!52 = !{!53, !53, i64 0}\n!53 = !{!\"0x473a2e0.w8.b0\", !54, i64 0}\n!54 = !{!\"0x473a2e0.w16.b0\", !55, i64 0}\n!55 = !{!\"0x473a2e0.w32.b0\", !56, i64 0}\n!56 = !{!\"0x473a2e0.w64.b0\", !57, i64 0}\n!57 = !{!\"0x473a2e0.w128.b0\", !58, i64 0}\n!58 = !{!\"0x473a2e0.w256.b0\", !59, i64 0}\n!59 = !{!\"0x473a2e0.w512.b0\", !60, i64 0}\n!60 = !{!\"0x473a2e0.w1024.b0\", !61, i64 0}\n!61 = !{!\"0x473a2e0\", !22, i64 0}\n!62 = !{!63, !63, i64 0}\n!63 = !{!\"0x473a2e0.w8.b8\", !54, i64 0}\n!64 = !{!65, !65, i64 0}\n!65 = !{!\"0x473a2e0.w8.b16\", !66, i64 0}\n!66 = !{!\"0x473a2e0.w16.b16\", !55, i64 0}\n!67 = !{!68, !68, i64 0}\n!68 = !{!\"0x435f200.w8.b0\", !69, i64 0}\n!69 = !{!\"0x435f200.w16.b0\", !70, i64 0}\n!70 = !{!\"0x435f200.w32.b0\", !71, i64 0}\n!71 = !{!\"0x435f200.w64.b0\", !72, i64 0}\n!72 = !{!\"0x435f200.w128.b0\", !73, i64 0}\n!73 = !{!\"0x435f200.w256.b0\", !74, i64 0}\n!74 = !{!\"0x435f200.w512.b0\", !75, i64 0}\n!75 = !{!\"0x435f200.w1024.b0\", !76, i64 0}\n!76 = !{!\"0x435f200\", !22, i64 0}\n!77 = !{!78, !78, i64 0}\n!78 = !{!\"0x435f200.w8.b8\", !69, i64 0}\n!79 = !{!80, !80, i64 0}\n!80 = !{!\"0x435f200.w8.b16\", !81, i64 0}\n!81 = !{!\"0x435f200.w16.b16\", !70, i64 0}\n!82 = !{!83, !83, i64 0}\n!83 = !{!\"0x473b460.w8.b0\", !84, i64 0}\n!84 = !{!\"0x473b460.w16.b0\", !85, i64 0}\n!85 = !{!\"0x473b460.w32.b0\", !86, i64 0}\n!86 = !{!\"0x473b460.w64.b0\", !87, i64 0}\n!87 = !{!\"0x473b460.w128.b0\", !88, i64 0}\n!88 = !{!\"0x473b460.w256.b0\", !89, i64 0}\n!89 = !{!\"0x473b460.w512.b0\", !90, i64 0}\n!90 = !{!\"0x473b460.w1024.b0\", !91, i64 0}\n!91 = !{!\"0x473b460\", !22, i64 0}\n!92 = !{!93, !93, i64 0}\n!93 = !{!\"0x473b460.w8.b8\", !84, i64 0}\n!94 = !{!95, !95, i64 0}\n!95 = !{!\"0x473b460.w8.b16\", !96, i64 0}\n!96 = !{!\"0x473b460.w16.b16\", !85, i64 0}\n!97 = !{!98, !98, i64 0}\n!98 = !{!\"0x463cfa0.w4.b0\", !99, i64 0}\n!99 = !{!\"0x463cfa0.w8.b0\", !100, i64 0}\n!100 = !{!\"0x463cfa0.w16.b0\", !101, i64 0}\n!101 = !{!\"0x463cfa0.w32.b0\", !102, i64 0}\n!102 = !{!\"0x463cfa0.w64.b0\", !103, i64 0}\n!103 = !{!\"0x463cfa0.w128.b0\", !104, i64 0}\n!104 = !{!\"0x463cfa0.w256.b0\", !105, i64 0}\n!105 = !{!\"0x463cfa0.w512.b0\", !106, i64 0}\n!106 = !{!\"0x463cfa0.w1024.b0\", !107, i64 0}\n!107 = !{!\"0x463cfa0\", !22, i64 0}\n!108 = !{!109, !109, i64 0}\n!109 = !{!\"0x463cfa0.w4.b4\", !99, i64 0}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0);\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 3, 8), \"float32\"), compute: T.Buffer((20, 3, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            ph_0_1 = T.Buffer((480,), data=ph_0.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] + T.cos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": []}{"op_name": "sin", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%0 = type { double }\n%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }\n%2 = type { i32, i32 }\n%3 = type { i8, i8, i16 }\n\n@__tvm_module_ctx = linkonce dllexport local_unnamed_addr global i8* null, align 8\n@__TVMFuncCall = linkonce dllexport local_unnamed_addr global i32 (i8*, %0*, i32*, i32, %0*, i32*)* null, align 8\n@__TVMBackendGetFuncFromEnv = linkonce dllexport local_unnamed_addr global i32 (i8*, i8*, i8**)* null, align 8\n@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8\n@.str = private constant [67 x i8] c\"Assert fail: num_args == 2, default_function: num_args should be 2\\00\", align 1\n@.str.1 = private constant [130 x i8] c\"Assert fail: data_code == 3 or data_code == 13 or data_code == 7 or data_code == 4, default_function: Expect arg[0] to be pointer\\00\", align 1\n@.str.2 = private constant [142 x i8] c\"Assert fail: compute_code == 3 or compute_code == 13 or compute_code == 7 or compute_code == 4, default_function: Expect arg[1] to be pointer\\00\", align 1\n@.str.3 = private constant [107 x i8] c\"Assert fail: 4 == T.tvm_struct_get(data, 0, 4, \\22int32\\22), default_function.data.ndim is expected to equal 4\\00\", align 1\n@.str.4 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(data, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(data, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(data, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.data.dtype is expected to be float32\\00\", align 1\n@.str.5 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_data_shape[0]) == 20, Argument default_function.data.shape[0] has an unsatisfied constraint: 20 == T.Cast(\\22int32\\22, default_function_data_shape[0])\\00\", align 1\n@.str.6 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_data_shape[1]) == 3, Argument default_function.data.shape[1] has an unsatisfied constraint: 3 == T.Cast(\\22int32\\22, default_function_data_shape[1])\\00\", align 1\n@.str.7 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_data_shape[2]) == 9, Argument default_function.data.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_data_shape[2])\\00\", align 1\n@.str.8 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_data_shape[3]) == 1, Argument default_function.data.shape[3] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_data_shape[3])\\00\", align 1\n@.str.9 = private constant [248 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_data_strides[2]) and 9 == T.Cast(\\22int32\\22, default_function_data_strides[1]) and 27 == T.Cast(\\22int32\\22, default_function_data_strides[0]), default_function.data.strides: expected to be compact array\\00\", align 1\n@.str.10 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(data, 0, 8, \\22uint64\\22), Argument default_function.data.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(data, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.11 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(data, 0, 10, \\22int32\\22) == 1, Argument default_function.data.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(data, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.12 = private constant [113 x i8] c\"Assert fail: 4 == T.tvm_struct_get(compute, 0, 4, \\22int32\\22), default_function.compute.ndim is expected to equal 4\\00\", align 1\n@.str.13 = private constant [247 x i8] c\"Assert fail: T.tvm_struct_get(compute, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(compute, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(compute, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.compute.dtype is expected to be float32\\00\", align 1\n@.str.14 = private constant [202 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[0]) == 20, Argument default_function.compute.shape[0] has an unsatisfied constraint: 20 == T.Cast(\\22int32\\22, default_function_compute_shape[0])\\00\", align 1\n@.str.15 = private constant [200 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[1]) == 3, Argument default_function.compute.shape[1] has an unsatisfied constraint: 3 == T.Cast(\\22int32\\22, default_function_compute_shape[1])\\00\", align 1\n@.str.16 = private constant [200 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[2]) == 9, Argument default_function.compute.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_compute_shape[2])\\00\", align 1\n@.str.17 = private constant [200 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[3]) == 1, Argument default_function.compute.shape[3] has an unsatisfied constraint: 1 == T.Cast(\\22int32\\22, default_function_compute_shape[3])\\00\", align 1\n@.str.18 = private constant [260 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_compute_strides[2]) and 9 == T.Cast(\\22int32\\22, default_function_compute_strides[1]) and 27 == T.Cast(\\22int32\\22, default_function_compute_strides[0]), default_function.compute.strides: expected to be compact array\\00\", align 1\n@.str.19 = private constant [205 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(compute, 0, 8, \\22uint64\\22), Argument default_function.compute.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(compute, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.20 = private constant [185 x i8] c\"Assert fail: T.tvm_struct_get(compute, 0, 10, \\22int32\\22) == 1, Argument default_function.compute.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(compute, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.21 = private constant [191 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(compute, 0, 9, \\22int32\\22), Argument default_function.compute.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(compute, 0, 9, \\22int32\\22)\\00\", align 1\n@.tvm_func.default_function_kernel = internal unnamed_addr global i8* null, align 8\n@.str.22 = private constant [24 x i8] c\"default_function_kernel\\00\", align 1\n@.str.23 = private constant [68 x i8] c\"Assert fail: kernel_error_code == 0, Error executing compute kernel\\00\", align 1\n@__tvm_main__ = weak dllexport local_unnamed_addr constant [17 x i8] c\"default_function\\00\", align 1\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\ndefine dllexport i32 @default_function(i8* noalias nocapture readonly %args, i32* noalias nocapture readonly %arg_type_ids, i32 %num_args, i8* noalias nocapture readnone %out_ret_value, i32* noalias nocapture readnone %out_ret_tcode, i8* noalias nocapture readnone %resource_handle) local_unnamed_addr #0 !dbg !5 {\nentry:\n  call void @llvm.dbg.value(metadata i8* %args, metadata !12, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %arg_type_ids, metadata !13, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32 %num_args, metadata !14, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %out_ret_value, metadata !15, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %out_ret_tcode, metadata !16, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %resource_handle, metadata !17, metadata !DIExpression()), !dbg !18\n  %stack_value_void_ptr56 = alloca [3 x %0], align 8, !dbg !18\n  %stack_tcode57 = alloca [3 x i32], align 4, !dbg !18\n  %stack_tcode57.sub = getelementptr inbounds [3 x i32], [3 x i32]* %stack_tcode57, i64 0, i64 0\n  %stack_value = bitcast [3 x %0]* %stack_value_void_ptr56 to i8*, !dbg !18\n  %0 = icmp eq i32 %num_args, 2, !dbg !18\n  br i1 %0, label %assert_end, label %assert_fail, !dbg !18, !prof !19\n\nassert_fail:                                      ; preds = %entry\n  %1 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %1(i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end:                                       ; preds = %entry\n  %data.code = load i32, i32* %arg_type_ids, align 4, !dbg !18, !tbaa !23\n  %2 = getelementptr inbounds i32, i32* %arg_type_ids, i64 1, !dbg !18\n  %compute.code = load i32, i32* %2, align 4, !dbg !18, !tbaa !34\n  %3 = bitcast i8* %args to %1**, !dbg !18\n  %data58 = load %1*, %1** %3, align 8, !dbg !18\n  %4 = getelementptr inbounds i8, i8* %args, i64 8, !dbg !18\n  %5 = bitcast i8* %4 to %1**, !dbg !18\n  %compute59 = load %1*, %1** %5, align 8, !dbg !18\n  %6 = bitcast %1* %data58 to float**, !dbg !18\n  %data_void_ptr60 = load float*, float** %6, align 8, !dbg !18\n  %ptrint = ptrtoint float* %data_void_ptr60 to i64, !dbg !18\n  %maskedptr = and i64 %ptrint, 63, !dbg !18\n  %maskcond = icmp eq i64 %maskedptr, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond), !dbg !18\n  %7 = getelementptr inbounds %1, %1* %data58, i64 0, i32 4, !dbg !18\n  %default_function.data.shape = load i64*, i64** %7, align 8, !dbg !18\n  %8 = getelementptr inbounds %1, %1* %data58, i64 0, i32 5, !dbg !18\n  %default_function.data.strides = load i64*, i64** %8, align 8, !dbg !18\n  %9 = getelementptr inbounds %1, %1* %data58, i64 0, i32 1, i32 1, !dbg !18\n  %dev_id = load i32, i32* %9, align 4, !dbg !18\n  %10 = bitcast %1* %compute59 to float**, !dbg !18\n  %compute_void_ptr61 = load float*, float** %10, align 8, !dbg !18\n  %ptrint3 = ptrtoint float* %compute_void_ptr61 to i64, !dbg !18\n  %maskedptr4 = and i64 %ptrint3, 63, !dbg !18\n  %maskcond5 = icmp eq i64 %maskedptr4, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond5), !dbg !18\n  %11 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 4, !dbg !18\n  %default_function.compute.shape = load i64*, i64** %11, align 8, !dbg !18\n  %12 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 5, !dbg !18\n  %default_function.compute.strides = load i64*, i64** %12, align 8, !dbg !18\n  switch i32 %data.code, label %assert_fail6 [\n    i32 13, label %assert_end7\n    i32 7, label %assert_end7\n    i32 4, label %assert_end7\n    i32 3, label %assert_end7\n  ], !dbg !18\n\nassert_fail6:                                     ; preds = %assert_end\n  %13 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %13(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.1, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end7:                                      ; preds = %assert_end, %assert_end, %assert_end, %assert_end\n  switch i32 %compute.code, label %assert_fail8 [\n    i32 13, label %assert_end9\n    i32 7, label %assert_end9\n    i32 4, label %assert_end9\n    i32 3, label %assert_end9\n  ], !dbg !18\n\nassert_fail8:                                     ; preds = %assert_end7\n  %14 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %14(i8* getelementptr inbounds ([142 x i8], [142 x i8]* @.str.2, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end9:                                      ; preds = %assert_end7, %assert_end7, %assert_end7, %assert_end7\n  %15 = getelementptr inbounds %1, %1* %data58, i64 0, i32 2, !dbg !18\n  %16 = load i32, i32* %15, align 4, !dbg !18\n  %17 = icmp eq i32 %16, 4, !dbg !18\n  br i1 %17, label %assert_end13, label %assert_fail10, !dbg !18, !prof !19\n\nassert_fail10:                                    ; preds = %assert_end9\n  %18 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %18(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.3, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end13:                                     ; preds = %assert_end9\n  %19 = getelementptr inbounds %1, %1* %data58, i64 0, i32 3, i32 2, !dbg !18\n  %20 = load i16, i16* %19, align 2, !dbg !18\n  %21 = icmp eq i16 %20, 1, !dbg !18\n  %22 = getelementptr inbounds %1, %1* %data58, i64 0, i32 3, i32 1, !dbg !18\n  %23 = load i8, i8* %22, align 1, !dbg !18\n  %24 = icmp eq i8 %23, 32, !dbg !18\n  %25 = getelementptr inbounds %1, %1* %data58, i64 0, i32 3, i32 0, !dbg !18\n  %26 = load i8, i8* %25, align 1, !dbg !18\n  %27 = icmp eq i8 %26, 2, !dbg !18\n  %28 = and i1 %24, %27, !dbg !18\n  %29 = and i1 %21, %28, !dbg !18\n  br i1 %29, label %assert_end15, label %assert_fail14, !dbg !18, !prof !19\n\nassert_fail14:                                    ; preds = %assert_end13\n  %30 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %30(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.4, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end15:                                     ; preds = %assert_end13\n  %31 = load i64, i64* %default_function.data.shape, align 8, !dbg !18, !tbaa !36\n  %32 = trunc i64 %31 to i32, !dbg !18\n  %33 = icmp eq i32 %32, 20, !dbg !18\n  br i1 %33, label %assert_end17, label %assert_fail16, !dbg !18, !prof !19\n\nassert_fail16:                                    ; preds = %assert_end15\n  %34 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %34(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.5, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end17:                                     ; preds = %assert_end15\n  %35 = getelementptr inbounds i64, i64* %default_function.data.shape, i64 1, !dbg !18\n  %36 = load i64, i64* %35, align 8, !dbg !18, !tbaa !46\n  %37 = trunc i64 %36 to i32, !dbg !18\n  %38 = icmp eq i32 %37, 3, !dbg !18\n  br i1 %38, label %assert_end19, label %assert_fail18, !dbg !18, !prof !19\n\nassert_fail18:                                    ; preds = %assert_end17\n  %39 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %39(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.6, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end19:                                     ; preds = %assert_end17\n  %40 = getelementptr inbounds i64, i64* %default_function.data.shape, i64 2, !dbg !18\n  %41 = load i64, i64* %40, align 8, !dbg !18, !tbaa !48\n  %42 = trunc i64 %41 to i32, !dbg !18\n  %43 = icmp eq i32 %42, 9, !dbg !18\n  br i1 %43, label %assert_end21, label %assert_fail20, !dbg !18, !prof !19\n\nassert_fail20:                                    ; preds = %assert_end19\n  %44 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %44(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.7, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end21:                                     ; preds = %assert_end19\n  %45 = getelementptr inbounds i64, i64* %default_function.data.shape, i64 3, !dbg !18\n  %46 = load i64, i64* %45, align 8, !dbg !18, !tbaa !51\n  %47 = trunc i64 %46 to i32, !dbg !18\n  %48 = icmp eq i32 %47, 1, !dbg !18\n  br i1 %48, label %assert_end23, label %assert_fail22, !dbg !18, !prof !19\n\nassert_fail22:                                    ; preds = %assert_end21\n  %49 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %49(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.8, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end23:                                     ; preds = %assert_end21\n  %50 = icmp eq i64* %default_function.data.strides, null, !dbg !18\n  br i1 %50, label %if_end, label %if_then, !dbg !18, !prof !53\n\nif_then:                                          ; preds = %assert_end23\n  %51 = load i64, i64* %default_function.data.strides, align 8, !dbg !18, !tbaa !54\n  %52 = trunc i64 %51 to i32, !dbg !18\n  %53 = icmp eq i32 %52, 27, !dbg !18\n  %54 = getelementptr inbounds i64, i64* %default_function.data.strides, i64 1, !dbg !18\n  %55 = load i64, i64* %54, align 8, !dbg !18, !tbaa !64\n  %56 = trunc i64 %55 to i32, !dbg !18\n  %57 = icmp eq i32 %56, 9, !dbg !18\n  %58 = getelementptr inbounds i64, i64* %default_function.data.strides, i64 2, !dbg !18\n  %59 = load i64, i64* %58, align 8, !dbg !18, !tbaa !66\n  %60 = trunc i64 %59 to i32, !dbg !18\n  %61 = icmp eq i32 %60, 1, !dbg !18\n  %62 = and i1 %57, %61, !dbg !18\n  %63 = and i1 %53, %62, !dbg !18\n  br i1 %63, label %if_end, label %assert_fail24, !dbg !18, !prof !19\n\nif_end:                                           ; preds = %assert_end23, %if_then\n  %64 = getelementptr inbounds %1, %1* %data58, i64 0, i32 6, !dbg !18\n  %65 = load i64, i64* %64, align 8, !dbg !18\n  %66 = icmp eq i64 %65, 0, !dbg !18\n  br i1 %66, label %assert_end27, label %assert_fail26, !dbg !18, !prof !19\n\nassert_fail24:                                    ; preds = %if_then\n  %67 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %67(i8* getelementptr inbounds ([248 x i8], [248 x i8]* @.str.9, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail26:                                    ; preds = %if_end\n  %68 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %68(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.10, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end27:                                     ; preds = %if_end\n  %69 = getelementptr inbounds %1, %1* %data58, i64 0, i32 1, i32 0, !dbg !18\n  %70 = load i32, i32* %69, align 4, !dbg !18\n  %71 = icmp eq i32 %70, 1, !dbg !18\n  br i1 %71, label %assert_end29, label %assert_fail28, !dbg !18, !prof !19\n\nassert_fail28:                                    ; preds = %assert_end27\n  %72 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %72(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.11, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end29:                                     ; preds = %assert_end27\n  %73 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 2, !dbg !18\n  %74 = load i32, i32* %73, align 4, !dbg !18\n  %75 = icmp eq i32 %74, 4, !dbg !18\n  br i1 %75, label %assert_end33, label %assert_fail30, !dbg !18, !prof !19\n\nassert_fail30:                                    ; preds = %assert_end29\n  %76 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %76(i8* getelementptr inbounds ([113 x i8], [113 x i8]* @.str.12, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end33:                                     ; preds = %assert_end29\n  %77 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 3, i32 2, !dbg !18\n  %78 = load i16, i16* %77, align 2, !dbg !18\n  %79 = icmp eq i16 %78, 1, !dbg !18\n  %80 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 3, i32 1, !dbg !18\n  %81 = load i8, i8* %80, align 1, !dbg !18\n  %82 = icmp eq i8 %81, 32, !dbg !18\n  %83 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 3, i32 0, !dbg !18\n  %84 = load i8, i8* %83, align 1, !dbg !18\n  %85 = icmp eq i8 %84, 2, !dbg !18\n  %86 = and i1 %82, %85, !dbg !18\n  %87 = and i1 %79, %86, !dbg !18\n  br i1 %87, label %assert_end35, label %assert_fail34, !dbg !18, !prof !19\n\nassert_fail34:                                    ; preds = %assert_end33\n  %88 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %88(i8* getelementptr inbounds ([247 x i8], [247 x i8]* @.str.13, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end35:                                     ; preds = %assert_end33\n  %89 = load i64, i64* %default_function.compute.shape, align 8, !dbg !18, !tbaa !69\n  %90 = trunc i64 %89 to i32, !dbg !18\n  %91 = icmp eq i32 %90, 20, !dbg !18\n  br i1 %91, label %assert_end37, label %assert_fail36, !dbg !18, !prof !19\n\nassert_fail36:                                    ; preds = %assert_end35\n  %92 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %92(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.14, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end37:                                     ; preds = %assert_end35\n  %93 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 1, !dbg !18\n  %94 = load i64, i64* %93, align 8, !dbg !18, !tbaa !79\n  %95 = trunc i64 %94 to i32, !dbg !18\n  %96 = icmp eq i32 %95, 3, !dbg !18\n  br i1 %96, label %assert_end39, label %assert_fail38, !dbg !18, !prof !19\n\nassert_fail38:                                    ; preds = %assert_end37\n  %97 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %97(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.15, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end39:                                     ; preds = %assert_end37\n  %98 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 2, !dbg !18\n  %99 = load i64, i64* %98, align 8, !dbg !18, !tbaa !81\n  %100 = trunc i64 %99 to i32, !dbg !18\n  %101 = icmp eq i32 %100, 9, !dbg !18\n  br i1 %101, label %assert_end41, label %assert_fail40, !dbg !18, !prof !19\n\nassert_fail40:                                    ; preds = %assert_end39\n  %102 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %102(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.16, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end41:                                     ; preds = %assert_end39\n  %103 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 3, !dbg !18\n  %104 = load i64, i64* %103, align 8, !dbg !18, !tbaa !84\n  %105 = trunc i64 %104 to i32, !dbg !18\n  %106 = icmp eq i32 %105, 1, !dbg !18\n  br i1 %106, label %assert_end43, label %assert_fail42, !dbg !18, !prof !19\n\nassert_fail42:                                    ; preds = %assert_end41\n  %107 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %107(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.17, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end43:                                     ; preds = %assert_end41\n  %108 = icmp eq i64* %default_function.compute.strides, null, !dbg !18\n  br i1 %108, label %if_end45, label %if_then44, !dbg !18, !prof !53\n\nif_then44:                                        ; preds = %assert_end43\n  %109 = load i64, i64* %default_function.compute.strides, align 8, !dbg !18, !tbaa !86\n  %110 = trunc i64 %109 to i32, !dbg !18\n  %111 = icmp eq i32 %110, 27, !dbg !18\n  %112 = getelementptr inbounds i64, i64* %default_function.compute.strides, i64 1, !dbg !18\n  %113 = load i64, i64* %112, align 8, !dbg !18, !tbaa !96\n  %114 = trunc i64 %113 to i32, !dbg !18\n  %115 = icmp eq i32 %114, 9, !dbg !18\n  %116 = getelementptr inbounds i64, i64* %default_function.compute.strides, i64 2, !dbg !18\n  %117 = load i64, i64* %116, align 8, !dbg !18, !tbaa !98\n  %118 = trunc i64 %117 to i32, !dbg !18\n  %119 = icmp eq i32 %118, 1, !dbg !18\n  %120 = and i1 %115, %119, !dbg !18\n  %121 = and i1 %111, %120, !dbg !18\n  br i1 %121, label %if_end45, label %assert_fail46, !dbg !18, !prof !19\n\nif_end45:                                         ; preds = %assert_end43, %if_then44\n  %122 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 6, !dbg !18\n  %123 = load i64, i64* %122, align 8, !dbg !18\n  %124 = icmp eq i64 %123, 0, !dbg !18\n  br i1 %124, label %assert_end49, label %assert_fail48, !dbg !18, !prof !19\n\nassert_fail46:                                    ; preds = %if_then44\n  %125 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %125(i8* getelementptr inbounds ([260 x i8], [260 x i8]* @.str.18, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail48:                                    ; preds = %if_end45\n  %126 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %126(i8* getelementptr inbounds ([205 x i8], [205 x i8]* @.str.19, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end49:                                     ; preds = %if_end45\n  %127 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 1, i32 0, !dbg !18\n  %128 = load i32, i32* %127, align 4, !dbg !18\n  %129 = icmp eq i32 %128, 1, !dbg !18\n  br i1 %129, label %assert_end51, label %assert_fail50, !dbg !18, !prof !19\n\nassert_fail50:                                    ; preds = %assert_end49\n  %130 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %130(i8* getelementptr inbounds ([185 x i8], [185 x i8]* @.str.20, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end51:                                     ; preds = %assert_end49\n  %131 = getelementptr inbounds %1, %1* %compute59, i64 0, i32 1, i32 1, !dbg !18\n  %132 = load i32, i32* %131, align 4, !dbg !18\n  %133 = icmp eq i32 %dev_id, %132, !dbg !18\n  br i1 %133, label %assert_end53, label %assert_fail52, !dbg !18, !prof !19\n\nassert_fail52:                                    ; preds = %assert_end51\n  %134 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %134(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.21, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end53:                                     ; preds = %assert_end51\n  %135 = call fastcc i32 @default_function_compute_(i8* nonnull %stack_value, float* %compute_void_ptr61, i32* nonnull %stack_tcode57.sub, float* %data_void_ptr60), !dbg !18\n  ret i32 %135, !dbg !18\n}\n\n; Function Attrs: nounwind willreturn\ndeclare void @llvm.assume(i1) #1\n\n; Function Attrs: noinline\ndefine internal fastcc i32 @default_function_compute_(i8* noalias %0, float* noalias align 64 %1, i32* noalias %2, float* noalias align 64 %3) unnamed_addr #2 {\nentry:\n  %4 = alloca i8*, align 8\n  %5 = bitcast i8* %0 to %0*\n  %6 = bitcast i8* %0 to float**\n  store float* %1, float** %6, align 8\n  store i32 3, i32* %2, align 4, !tbaa !101\n  %7 = getelementptr inbounds i8, i8* %0, i64 8\n  %8 = bitcast i8* %7 to float**\n  store float* %3, float** %8, align 8\n  %9 = getelementptr inbounds i32, i32* %2, i64 1\n  store i32 3, i32* %9, align 4, !tbaa !112\n  %10 = getelementptr inbounds i8, i8* %0, i64 16\n  %11 = bitcast i8* %10 to %0*\n  %12 = getelementptr inbounds i32, i32* %2, i64 2\n  %13 = load i32 (i8*, %0*, i32*, i32, %0*, i32*)*, i32 (i8*, %0*, i32*, i32, %0*, i32*)** @__TVMFuncCall, align 8, !tbaa !20\n  %14 = load i8*, i8** @.tvm_func.default_function_kernel, align 8\n  %15 = icmp eq i8* %14, null\n  br i1 %15, label %handle_init, label %handle_init_end, !prof !53\n\nhandle_init:                                      ; preds = %entry\n  %16 = load i8*, i8** @__tvm_module_ctx, align 8, !tbaa !20\n  %17 = load i32 (i8*, i8*, i8**)*, i32 (i8*, i8*, i8**)** @__TVMBackendGetFuncFromEnv, align 8, !tbaa !20\n  %18 = call i32 %17(i8* %16, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.22, i64 0, i64 0), i8** nonnull %4)\n  %19 = icmp eq i32 %18, 0\n  br i1 %19, label %call_end, label %call_fail, !prof !19\n\nhandle_init_end:                                  ; preds = %entry, %call_end\n  %20 = phi i8* [ %14, %entry ], [ %23, %call_end ]\n  %21 = call i32 %13(i8* %20, %0* %5, i32* nonnull %2, i32 2, %0* nonnull %11, i32* nonnull %12)\n  %22 = icmp eq i32 %21, 0\n  br i1 %22, label %call_end2, label %call_fail, !prof !19\n\ncall_fail:                                        ; preds = %call_end2, %handle_init_end, %handle_init\n  %merge = phi i32 [ %18, %handle_init ], [ %21, %handle_init_end ], [ 0, %call_end2 ]\n  ret i32 %merge\n\ncall_end:                                         ; preds = %handle_init\n  %23 = load i8*, i8** %4, align 8\n  store i8* %23, i8** @.tvm_func.default_function_kernel, align 8\n  br label %handle_init_end\n\ncall_end2:                                        ; preds = %handle_init_end\n  %24 = bitcast i8* %10 to i64*\n  %25 = load i64, i64* %24, align 8\n  %kernel_error_code = trunc i64 %25 to i32\n  %26 = icmp eq i32 %kernel_error_code, 0\n  br i1 %26, label %call_fail, label %assert_fail, !prof !19\n\nassert_fail:                                      ; preds = %call_end2\n  %27 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !20\n  call void %27(i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.23, i64 0, i64 0))\n  ret i32 -1\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\n; Function Attrs: nounwind readnone speculatable willreturn\ndeclare void @llvm.dbg.value(metadata, metadata, metadata) #4\n\nattributes #0 = { \"target-cpu\"=\"generic\" }\nattributes #1 = { nounwind willreturn }\nattributes #2 = { noinline \"target-cpu\"=\"generic\" }\nattributes #3 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\nattributes #4 = { nounwind readnone speculatable willreturn }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n!5 = distinct !DISubprogram(name: \"main.tir\", scope: !1, file: !1, type: !6, spFlags: DISPFlagLocalToUnit | DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !11)\n!6 = !DISubroutineType(types: !7)\n!7 = !{!8, !9, !10, !8, !9, !10, !9}\n!8 = !DIBasicType(name: \"int32\", size: 32, encoding: DW_ATE_signed)\n!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null)\n!10 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8)\n!11 = !{!12, !13, !14, !15, !16, !17}\n!12 = !DILocalVariable(name: \"arg1\", arg: 1, scope: !5, file: !1, type: !9)\n!13 = !DILocalVariable(name: \"arg2\", arg: 2, scope: !5, file: !1, type: !10)\n!14 = !DILocalVariable(name: \"arg3\", arg: 3, scope: !5, file: !1, type: !8)\n!15 = !DILocalVariable(name: \"arg4\", arg: 4, scope: !5, file: !1, type: !9)\n!16 = !DILocalVariable(name: \"arg5\", arg: 5, scope: !5, file: !1, type: !10)\n!17 = !DILocalVariable(name: \"arg6\", arg: 6, scope: !5, file: !1, type: !9)\n!18 = !DILocation(line: 0, scope: !5)\n!19 = !{!\"branch_weights\", i32 1048576, i32 1}\n!20 = !{!21, !21, i64 0}\n!21 = !{!\"ctx_ptr\", !22, i64 0}\n!22 = !{!\"tvm-tbaa\"}\n!23 = !{!24, !24, i64 0}\n!24 = !{!\"0x48efe20.w4.b0\", !25, i64 0}\n!25 = !{!\"0x48efe20.w8.b0\", !26, i64 0}\n!26 = !{!\"0x48efe20.w16.b0\", !27, i64 0}\n!27 = !{!\"0x48efe20.w32.b0\", !28, i64 0}\n!28 = !{!\"0x48efe20.w64.b0\", !29, i64 0}\n!29 = !{!\"0x48efe20.w128.b0\", !30, i64 0}\n!30 = !{!\"0x48efe20.w256.b0\", !31, i64 0}\n!31 = !{!\"0x48efe20.w512.b0\", !32, i64 0}\n!32 = !{!\"0x48efe20.w1024.b0\", !33, i64 0}\n!33 = !{!\"0x48efe20\", !22, i64 0}\n!34 = !{!35, !35, i64 0}\n!35 = !{!\"0x48efe20.w4.b4\", !25, i64 0}\n!36 = !{!37, !37, i64 0}\n!37 = !{!\"0x4abdaf0.w8.b0\", !38, i64 0}\n!38 = !{!\"0x4abdaf0.w16.b0\", !39, i64 0}\n!39 = !{!\"0x4abdaf0.w32.b0\", !40, i64 0}\n!40 = !{!\"0x4abdaf0.w64.b0\", !41, i64 0}\n!41 = !{!\"0x4abdaf0.w128.b0\", !42, i64 0}\n!42 = !{!\"0x4abdaf0.w256.b0\", !43, i64 0}\n!43 = !{!\"0x4abdaf0.w512.b0\", !44, i64 0}\n!44 = !{!\"0x4abdaf0.w1024.b0\", !45, i64 0}\n!45 = !{!\"0x4abdaf0\", !22, i64 0}\n!46 = !{!47, !47, i64 0}\n!47 = !{!\"0x4abdaf0.w8.b8\", !38, i64 0}\n!48 = !{!49, !49, i64 0}\n!49 = !{!\"0x4abdaf0.w8.b16\", !50, i64 0}\n!50 = !{!\"0x4abdaf0.w16.b16\", !39, i64 0}\n!51 = !{!52, !52, i64 0}\n!52 = !{!\"0x4abdaf0.w8.b24\", !50, i64 0}\n!53 = !{!\"branch_weights\", i32 1, i32 1048576}\n!54 = !{!55, !55, i64 0}\n!55 = !{!\"0x48f22b0.w8.b0\", !56, i64 0}\n!56 = !{!\"0x48f22b0.w16.b0\", !57, i64 0}\n!57 = !{!\"0x48f22b0.w32.b0\", !58, i64 0}\n!58 = !{!\"0x48f22b0.w64.b0\", !59, i64 0}\n!59 = !{!\"0x48f22b0.w128.b0\", !60, i64 0}\n!60 = !{!\"0x48f22b0.w256.b0\", !61, i64 0}\n!61 = !{!\"0x48f22b0.w512.b0\", !62, i64 0}\n!62 = !{!\"0x48f22b0.w1024.b0\", !63, i64 0}\n!63 = !{!\"0x48f22b0\", !22, i64 0}\n!64 = !{!65, !65, i64 0}\n!65 = !{!\"0x48f22b0.w8.b8\", !56, i64 0}\n!66 = !{!67, !67, i64 0}\n!67 = !{!\"0x48f22b0.w8.b16\", !68, i64 0}\n!68 = !{!\"0x48f22b0.w16.b16\", !57, i64 0}\n!69 = !{!70, !70, i64 0}\n!70 = !{!\"0x4bb15d0.w8.b0\", !71, i64 0}\n!71 = !{!\"0x4bb15d0.w16.b0\", !72, i64 0}\n!72 = !{!\"0x4bb15d0.w32.b0\", !73, i64 0}\n!73 = !{!\"0x4bb15d0.w64.b0\", !74, i64 0}\n!74 = !{!\"0x4bb15d0.w128.b0\", !75, i64 0}\n!75 = !{!\"0x4bb15d0.w256.b0\", !76, i64 0}\n!76 = !{!\"0x4bb15d0.w512.b0\", !77, i64 0}\n!77 = !{!\"0x4bb15d0.w1024.b0\", !78, i64 0}\n!78 = !{!\"0x4bb15d0\", !22, i64 0}\n!79 = !{!80, !80, i64 0}\n!80 = !{!\"0x4bb15d0.w8.b8\", !71, i64 0}\n!81 = !{!82, !82, i64 0}\n!82 = !{!\"0x4bb15d0.w8.b16\", !83, i64 0}\n!83 = !{!\"0x4bb15d0.w16.b16\", !72, i64 0}\n!84 = !{!85, !85, i64 0}\n!85 = !{!\"0x4bb15d0.w8.b24\", !83, i64 0}\n!86 = !{!87, !87, i64 0}\n!87 = !{!\"0x4a5b5e0.w8.b0\", !88, i64 0}\n!88 = !{!\"0x4a5b5e0.w16.b0\", !89, i64 0}\n!89 = !{!\"0x4a5b5e0.w32.b0\", !90, i64 0}\n!90 = !{!\"0x4a5b5e0.w64.b0\", !91, i64 0}\n!91 = !{!\"0x4a5b5e0.w128.b0\", !92, i64 0}\n!92 = !{!\"0x4a5b5e0.w256.b0\", !93, i64 0}\n!93 = !{!\"0x4a5b5e0.w512.b0\", !94, i64 0}\n!94 = !{!\"0x4a5b5e0.w1024.b0\", !95, i64 0}\n!95 = !{!\"0x4a5b5e0\", !22, i64 0}\n!96 = !{!97, !97, i64 0}\n!97 = !{!\"0x4a5b5e0.w8.b8\", !88, i64 0}\n!98 = !{!99, !99, i64 0}\n!99 = !{!\"0x4a5b5e0.w8.b16\", !100, i64 0}\n!100 = !{!\"0x4a5b5e0.w16.b16\", !89, i64 0}\n!101 = !{!102, !102, i64 0}\n!102 = !{!\"0x4557600.w4.b0\", !103, i64 0}\n!103 = !{!\"0x4557600.w8.b0\", !104, i64 0}\n!104 = !{!\"0x4557600.w16.b0\", !105, i64 0}\n!105 = !{!\"0x4557600.w32.b0\", !106, i64 0}\n!106 = !{!\"0x4557600.w64.b0\", !107, i64 0}\n!107 = !{!\"0x4557600.w128.b0\", !108, i64 0}\n!108 = !{!\"0x4557600.w256.b0\", !109, i64 0}\n!109 = !{!\"0x4557600.w512.b0\", !110, i64 0}\n!110 = !{!\"0x4557600.w1024.b0\", !111, i64 0}\n!111 = !{!\"0x4557600\", !22, i64 0}\n!112 = !{!113, !113, i64 0}\n!113 = !{!\"0x4557600.w4.b4\", !103, i64 0}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data);\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 12)) < 45) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 3, 9, 1), \"float32\"), compute: T.Buffer((20, 3, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(3, 9):\n                cse_var_1: T.int32 = i0 * 27 + i1 * 9 + i2\n                compute_1 = T.Buffer((540,), data=compute.data)\n                data_1 = T.Buffer((540,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [20, 3, 9, 1]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 132) + (i1 * 12)) + i2)] = sinf(data[(((i0 * 132) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 11, 12, 1), \"float32\"), compute: T.Buffer((12, 11, 12, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(11, 12):\n                cse_var_1: T.int32 = i0 * 132 + i1 * 12 + i2\n                compute_1 = T.Buffer((1584,), data=compute.data)\n                data_1 = T.Buffer((1584,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [12, 11, 12, 1]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 5; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i1 * 24) + (i2 * 6)) + i3)] = fabsf(data[(((i1 * 24) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 5, 4, 6), \"float32\"), compute: T.Buffer((1, 5, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(5, 4, 6):\n            cse_var_1: T.int32 = i1 * 24 + i2 * 6 + i3\n            compute_1 = T.Buffer((120,), data=compute.data)\n            data_1 = T.Buffer((120,), data=data.data)\n            compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [1, 5, 4, 6]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i3 = 0; i3 < 14; ++i3) {\n        compute[(((i0 * 168) + (i1 * 14)) + i3)] = cosf(data[(((i0 * 168) + (i1 * 14)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 1, 14), \"float32\"), compute: T.Buffer((8, 12, 1, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i3 in T.grid(12, 14):\n                cse_var_1: T.int32 = i0 * 168 + i1 * 14 + i3\n                compute_1 = T.Buffer((1344,), data=compute.data)\n                data_1 = T.Buffer((1344,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [8, 12, 1, 14]}{"op_name": "conv1d", "c_code": "void default_function_kernel(float* conv1d_ncw, float* data, float* kernel) {\n  for (int32_t ff_outer_inner_init = 0; ff_outer_inner_init < 5; ++ff_outer_inner_init) {\n    for (int32_t yy_outer_inner_init = 0; yy_outer_inner_init < 4; ++yy_outer_inner_init) {\n      for (int32_t nn_inner_init = 0; nn_inner_init < 2; ++nn_inner_init) {\n        conv1d_ncw[(((nn_inner_init * 20) + (ff_outer_inner_init * 4)) + yy_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t rc_outer = 0; rc_outer < 3; ++rc_outer) {\n    for (int32_t ry_outer = 0; ry_outer < 3; ++ry_outer) {\n      for (int32_t ff_outer_inner = 0; ff_outer_inner < 5; ++ff_outer_inner) {\n        for (int32_t yy_outer_inner = 0; yy_outer_inner < 4; ++yy_outer_inner) {\n          for (int32_t nn_inner = 0; nn_inner < 2; ++nn_inner) {\n            conv1d_ncw[(((nn_inner * 20) + (ff_outer_inner * 4)) + yy_outer_inner)] = (conv1d_ncw[(((nn_inner * 20) + (ff_outer_inner * 4)) + yy_outer_inner)] + (data[((((nn_inner * 30) + (rc_outer * 10)) + (yy_outer_inner * 2)) + ry_outer)] * kernel[(((ff_outer_inner * 9) + (rc_outer * 3)) + ry_outer)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ conv1d_ncw, float* __restrict__ data, float* __restrict__ kernel) {\n  float conv1d_ncw_local[2];\n  __shared__ float pad_temp_shared[6];\n  __shared__ float kernel_shared[3];\n  conv1d_ncw_local[0] = 0.000000e+00f;\n  conv1d_ncw_local[1] = 0.000000e+00f;\n  for (int ry_outer_outer = 0; ry_outer_outer < 3; ++ry_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 6; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      pad_temp_shared[ax0_ax1_fused_ax2_fused_outer_outer] = data[(((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((((int)blockIdx.x) & 3) * 2)) + ry_outer_outer)];\n    }\n    int3 v_ = make_int3(((((((int)blockIdx.x) >> 2) * 9) + ry_outer_outer))+(3*0), ((((((int)blockIdx.x) >> 2) * 9) + ry_outer_outer))+(3*1), ((((((int)blockIdx.x) >> 2) * 9) + ry_outer_outer))+(3*2));\n    *(float3*)(kernel_shared + 0) = make_float3(kernel[v_.x],kernel[v_.y],kernel[v_.z]);\n    __syncthreads();\n    for (int rc_inner = 0; rc_inner < 3; ++rc_inner) {\n      conv1d_ncw_local[0] = (conv1d_ncw_local[0] + (pad_temp_shared[rc_inner] * kernel_shared[rc_inner]));\n      conv1d_ncw_local[1] = (conv1d_ncw_local[1] + (pad_temp_shared[(rc_inner + 3)] * kernel_shared[rc_inner]));\n    }\n  }\n  conv1d_ncw[((int)blockIdx.x)] = conv1d_ncw_local[0];\n  conv1d_ncw[(((int)blockIdx.x) + 20)] = conv1d_ncw_local[1];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 3, 10), \"float32\"), kernel: T.Buffer((5, 3, 3), \"float32\"), conv1d_ncw: T.Buffer((2, 5, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        conv1d_ncw_1 = T.Buffer((40,), data=conv1d_ncw.data)\n        for ff_outer_inner_init, yy_outer_inner_init, nn_inner_init in T.grid(5, 4, 2):\n            conv1d_ncw_1[nn_inner_init * 20 + ff_outer_inner_init * 4 + yy_outer_inner_init] = T.float32(0)\n        for rc_outer, ry_outer, ff_outer_inner, yy_outer_inner, nn_inner in T.grid(3, 3, 5, 4, 2):\n            cse_var_1: T.int32 = nn_inner * 20 + ff_outer_inner * 4 + yy_outer_inner\n            data_1 = T.Buffer((60,), data=data.data)\n            kernel_1 = T.Buffer((45,), data=kernel.data)\n            conv1d_ncw_1[cse_var_1] = conv1d_ncw_1[cse_var_1] + data_1[nn_inner * 30 + rc_outer * 10 + yy_outer_inner * 2 + ry_outer] * kernel_1[ff_outer_inner * 9 + rc_outer * 3 + ry_outer]", "op_args": [15, 16, 6, 33, 46, 11, 16, [1, 1], [1, 1]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 3; ++i3) {\n        compute[(((i0 * 48) + (i2 * 3)) + i3)] = atanf(data[(((i0 * 48) + (i2 * 3)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 1, 16, 3), \"float32\"), compute: T.Buffer((13, 1, 16, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i2, i3 in T.grid(16, 3):\n                cse_var_1: T.int32 = i0 * 48 + i2 * 3 + i3\n                compute_1 = T.Buffer((624,), data=compute.data)\n                data_1 = T.Buffer((624,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [13, 1, 16, 3]}{"op_name": "sum", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[12];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 12; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1092; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 12; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 12) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 12; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 410; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 819) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 14, 13), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([12], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((12,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(12):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1092, 12):\n            data_1 = T.Buffer((13104,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 12 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(12):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [6, 12, 14, 13]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 154; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i0_i1_fused * 40) + (i2 * 20)) + i3)] = coshf(data[(((i0_i1_fused * 40) + (i2 * 20)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 11, 2, 20), \"float32\"), compute: T.Buffer((14, 11, 2, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(154):\n            for i2, i3 in T.grid(2, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 40 + i2 * 20 + i3\n                compute_1 = T.Buffer((6160,), data=compute.data)\n                data_1 = T.Buffer((6160,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [14, 11, 2, 20]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 182; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 2) + 2) % 8) == 0) ? (((ax2 * 5) + 5) >> 2) : ((((ax2 * 5) + 5) >> 2) + 1)) - ((ax2 * 10) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < (((((ax3 + 1) % 8) == 0) ? (((ax3 * 17) + 17) >> 3) : ((((ax3 * 17) + 17) >> 3) + 1)) - (ax3 * 2)); ++rv1) {\n            adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = max(adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)], data[(((((ax0_ax1_fused * 170) + (((ax2 * 10) >> 3) * 17)) + (rv0 * 17)) + (ax3 * 2)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 2) + 2) % 8) == 0) ? (((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) + 5) >> 2) : ((((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 5) + 5) >> 2) + 1)) - ((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 10) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < ((((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) + 1) % 8) == 0) ? ((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) * 17) + 17) >> 3) : (((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) * 17) + 17) >> 3) + 1)) - ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) * 2)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], data[((((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) >> 4) * 170) + (((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) & 15) >> 1) * 10) >> 3) * 17)) + (rv0 * 17)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) * 2)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 13, 10, 17), \"float32\"), adaptive_pool_max: T.Buffer((14, 13, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(182):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((11648,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 2 + 2) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 10 // 8, where={cse_var_1: (ax2 * 5 + 5) // 4}), T.Let(T.Select((ax3 + 1) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 2, where={cse_var_2: (ax3 * 17 + 17) // 8})):\n                    cse_var_1 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((30940,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0_ax1_fused * 170 + ax2 * 10 // 8 * 17 + rv0 * 17 + ax3 * 2 + rv1])", "op_args": [14, 13, 10, 17]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4160; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 10; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3_s)] = acosf(data[((i0_i1_fused_i2_fused * 10) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 20, 16, 10), \"float32\"), compute: T.Buffer((13, 20, 16, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4160):\n            for i3_s in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3_s\n                compute_1 = T.Buffer((41600,), data=compute.data)\n                data_1 = T.Buffer((41600,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [13, 20, 16, 10]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 3780; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_add[ax0_ax1_fused_ax2_fused_ax3_fused] = (data[ax0_ax1_fused_ax2_fused_ax3_fused] + data_1[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 3, 7, 18), \"float32\"), data_1: T.Buffer((10, 3, 7, 18), \"float32\"), T_add: T.Buffer((10, 3, 7, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(3780):\n            T_add_1 = T.Buffer((3780,), data=T_add.data)\n            data_2 = T.Buffer((3780,), data=data.data)\n            data_3 = T.Buffer((3780,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_2[ax0_ax1_fused_ax2_fused_ax3_fused] + data_3[ax0_ax1_fused_ax2_fused_ax3_fused]", "op_args": [10, 3, 7, 18]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 4200) + (i1 * 300)) + (i2 * 20)) + i3)] = asinf(data[((((i0 * 4200) + (i1 * 300)) + (i2 * 20)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 14, 15, 20), \"float32\"), compute: T.Buffer((19, 14, 15, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(19):\n            for i1, i2, i3 in T.grid(14, 15, 20):\n                cse_var_1: T.int32 = i0 * 4200 + i1 * 300 + i2 * 20 + i3\n                compute_1 = T.Buffer((79800,), data=compute.data)\n                data_1 = T.Buffer((79800,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [19, 14, 15, 20]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 15; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 8) + i3)] = asinhf(data[((i0_i1_fused_i2_fused * 8) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 5, 1, 8), \"float32\"), compute: T.Buffer((3, 5, 1, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(15):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((120,), data=compute.data)\n                data_1 = T.Buffer((120,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [3, 5, 1, 8]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n          compute[((((i0 * 1584) + (i1 * 99)) + (i2 * 9)) + i3_s)] = atanhf(data[((((i0 * 1584) + (i1 * 99)) + (i2 * 9)) + i3_s)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 16, 11, 9), \"float32\"), compute: T.Buffer((13, 16, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i1, i2, i3_s in T.grid(16, 11, 9):\n                cse_var_1: T.int32 = i0 * 1584 + i1 * 99 + i2 * 9 + i3_s\n                compute_1 = T.Buffer((20592,), data=compute.data)\n                data_1 = T.Buffer((20592,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [13, 16, 11, 9]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      for (int32_t i3 = 0; i3 < 2; ++i3) {\n        compute[(((i0_i1_fused * 34) + (i2 * 2)) + i3)] = ceilf(data[(((i0_i1_fused * 34) + (i2 * 2)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 6, 17, 2), \"float32\"), compute: T.Buffer((7, 6, 17, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(42):\n            for i2, i3 in T.grid(17, 2):\n                cse_var_1: T.int32 = i0_i1_fused * 34 + i2 * 2 + i3\n                compute_1 = T.Buffer((1428,), data=compute.data)\n                data_1 = T.Buffer((1428,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [7, 6, 17, 2]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 6; ++i3) {\n          compute[((((i0 * 204) + (i1 * 12)) + (i2 * 6)) + i3)] = erff(data[((((i0 * 204) + (i1 * 12)) + (i2 * 6)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 357) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 17, 2, 6), \"float32\"), compute: T.Buffer((14, 17, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(14):\n            for i1, i2, i3 in T.grid(17, 2, 6):\n                cse_var_1: T.int32 = i0 * 204 + i1 * 12 + i2 * 6 + i3\n                compute_1 = T.Buffer((2856,), data=compute.data)\n                data_1 = T.Buffer((2856,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [14, 17, 2, 6]}{"op_name": "exp", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        compute[(((i0_i1_fused * 143) + (i2 * 11)) + i3)] = expf(data[(((i0_i1_fused * 143) + (i2 * 11)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 3003) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 7, 13, 11), \"float32\"), compute: T.Buffer((6, 7, 13, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(42):\n            for i2, i3 in T.grid(13, 11):\n                cse_var_1: T.int32 = i0_i1_fused * 143 + i2 * 11 + i3\n                compute_1 = T.Buffer((6006,), data=compute.data)\n                data_1 = T.Buffer((6006,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])", "op_args": [6, 7, 13, 11]}{"op_name": "adaptive_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 720; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < (((((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 3) : (((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 3) + 1)) - (((ax0_ax1_fused_ax2_fused & 7) * 3) >> 3)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < (((((ax3 + 1) % 8) == 0) ? (((ax3 * 9) + 9) >> 3) : ((((ax3 * 9) + 9) >> 3) + 1)) - ax3); ++rv1) {\n          adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((((((ax0_ax1_fused_ax2_fused >> 3) * 27) + ((((ax0_ax1_fused_ax2_fused & 7) * 3) >> 3) * 9)) + (rv0 * 9)) + ax3) + rv1)]);\n        }\n      }\n      adaptive_pool_avg[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = (adaptive_pool_sum[0] / (((float)(((((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 3) : (((((ax0_ax1_fused_ax2_fused & 7) * 3) + 3) >> 3) + 1)) - (((ax0_ax1_fused_ax2_fused & 7) * 3) >> 3))) * ((float)(((((ax3 + 1) % 8) == 0) ? (((ax3 * 9) + 9) >> 3) : ((((ax3 * 9) + 9) >> 3) + 1)) - ax3))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (((float)(((((((((int)threadIdx.x) >> 3) * 3) + 3) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 3) : (((((((int)threadIdx.x) >> 3) * 3) + 3) >> 3) + 1)) - (((((int)threadIdx.x) >> 3) * 3) >> 3))) * ((float)((((((((int)threadIdx.x) & 7) + 1) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 3) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 3) + 1)) - (((int)threadIdx.x) & 7)))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 3) + 3) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 3) + 3) >> 3) : (((((((int)threadIdx.x) >> 3) * 3) + 3) >> 3) + 1)) - (((((int)threadIdx.x) >> 3) * 3) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < ((((((((int)threadIdx.x) & 7) + 1) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 3) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 3) + 1)) - (((int)threadIdx.x) & 7)); ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data[(((((((int)blockIdx.x) * 27) + ((((((int)threadIdx.x) >> 3) * 3) >> 3) * 9)) + (rv0 * 9)) + rv1) + (((int)threadIdx.x) & 7))]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 6, 3, 9), \"float32\"), adaptive_pool_avg: T.Buffer((15, 6, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(720):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(8):\n                cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused % 8 * 3\n                cse_var_4: T.int32 = cse_var_3 + 3\n                cse_var_2: T.int32 = (ax3 * 9 + 9) // 8\n                cse_var_1: T.int32 = cse_var_4 // 8\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(T.Select(cse_var_4 % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_3 // 8, T.Select((ax3 + 1) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3):\n                    data_1 = T.Buffer((2430,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused // 8 * 27 + cse_var_3 // 8 * 9 + rv0 * 9 + ax3 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((5760,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = adaptive_pool_sum_1[0] / (T.Cast(\"float32\", T.Select(cse_var_4 % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_3 // 8) * T.Cast(\"float32\", T.Select((ax3 + 1) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3))", "op_args": [15, 6, 3, 9]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 2700; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 675) {\n    T_fast_erf[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 5, 9, 6), \"float32\"), T_fast_erf: T.Buffer((10, 5, 9, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(2700):\n            T_fast_erf_1 = T.Buffer((2700,), data=T_fast_erf.data)\n            data_1 = T.Buffer((2700,), data=data.data)\n            T_fast_erf_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [10, 5, 9, 6]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    float T_softmax_maxelem[8];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        T_softmax_maxelem[i2] = -3.402823e+38f;\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((i0 * 64) + (i1 * 8)) + i2)]);\n      }\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        T_softmax_expsum[0] = 0.000000e+00f;\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1])));\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0 * 64) + (i1 * 8)) + i2_1)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0 * 64) + (i1 * 8)) + i2_1)] - T_softmax_maxelem[i2_1])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) / T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 8, 8, 1), \"float32\"), T_softmax_norm: T.Buffer((11, 8, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            T_softmax_maxelem = T.allocate([8], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1 in range(8):\n                T_softmax_maxelem_1 = T.Buffer((8,), data=T_softmax_maxelem, align=32)\n                data_1 = T.Buffer((704,), data=data.data)\n                for i2 in range(8):\n                    T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0 * 64 + i1 * 8 + i2])\n                for i2 in range(8):\n                    cse_var_1: T.int32 = i0 * 64 + i1 * 8 + i2\n                    T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                    T_softmax_expsum_1[0] = T.float32(0)\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n                    T_softmax_norm_1 = T.Buffer((704,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[0]", "op_args": [11, 8, 8, 1]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 765; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 8) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused_ax2_fused * 8) + ax3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 17, 9, 8), \"float32\"), T_fast_exp: T.Buffer((5, 17, 9, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(765):\n            for ax3 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                T_fast_exp_1 = T.Buffer((6120,), data=T_fast_exp.data)\n                data_1 = T.Buffer((6120,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [5, 17, 9, 8]}{"op_name": "batch_to_space_nd", "c_code": "void default_function_kernel(float* T_strided_slice, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float T_reshape[6840];\n    float T_reshape_1[190];\n    float T_transpose[1];\n    for (int32_t ax1 = 0; ax1 < 36; ++ax1) {\n      for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n        for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n          for (int32_t ax5 = 0; ax5 < 19; ++ax5) {\n            T_reshape_1[(((ax1_1 * 95) + (ax4 * 19)) + ax5)] = data[(((((((ax1 & 1) * 13680) + (ax1_1 * 6840)) + (ax0 * 1710)) + ((ax1 >> 1) * 95)) + (ax4 * 19)) + ax5)];\n          }\n        }\n      }\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 19; ++ax3) {\n          T_transpose[0] = T_reshape_1[((((ax2 & 1) * 95) + ((ax2 >> 1) * 19)) + ax3)];\n          T_reshape[(((ax1 * 190) + (ax2 * 19)) + ax3)] = T_transpose[0];\n        }\n      }\n    }\n    for (int32_t ax1_2 = 0; ax1_2 < 36; ++ax1_2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 19; ++ax3_1) {\n          T_strided_slice[((((ax0 * 6840) + (ax1_2 * 190)) + (ax2_1 * 19)) + ax3_1)] = T_reshape[(((ax1_2 * 190) + (ax2_1 * 19)) + ax3_1)];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ data) {\n  T_strided_slice[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) % 38) / 19) * 13680) + (((((((int)blockIdx.x) * 30) + ((int)threadIdx.x)) % 38) / 19) * 6840)) + ((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) / 38) * 95)) + (((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 95) / 19) * 19)) + (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 19))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 18, 5, 19), \"float32\"), T_strided_slice: T.Buffer((4, 36, 10, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_reshape = T.allocate([6840], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([190], \"float32\", \"global\")\n            T_transpose = T.allocate([1], \"float32\", \"global\")\n            T_reshape_3 = T.Buffer((6840,), data=T_reshape)\n            for ax1 in range(36):\n                T_reshape_2 = T.Buffer((190,), data=T_reshape_1)\n                for ax1_1, ax4, ax5 in T.grid(2, 5, 19):\n                    cse_var_1: T.int32 = ax4 * 19\n                    data_1 = T.Buffer((30780,), data=data.data)\n                    T_reshape_2[ax1_1 * 95 + cse_var_1 + ax5] = data_1[ax1 % 2 * 13680 + ax1_1 * 6840 + ax0 * 1710 + ax1 // 2 * 95 + cse_var_1 + ax5]\n                for ax2, ax3 in T.grid(10, 19):\n                    T_transpose_1 = T.Buffer((1,), data=T_transpose, align=4)\n                    T_transpose_1[0] = T_reshape_2[ax2 % 2 * 95 + ax2 // 2 * 19 + ax3]\n                    T_reshape_3[ax1 * 190 + ax2 * 19 + ax3] = T_transpose_1[0]\n            for ax1, ax2, ax3 in T.grid(36, 10, 19):\n                cse_var_3: T.int32 = ax1 * 190\n                cse_var_2: T.int32 = ax2 * 19\n                T_strided_slice_1 = T.Buffer((27360,), data=T_strided_slice.data)\n                T_strided_slice_1[ax0 * 6840 + cse_var_3 + cse_var_2 + ax3] = T_reshape_3[cse_var_3 + cse_var_2 + ax3]", "op_args": [18, 18, 5, 19]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 204; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 17) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 17) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) < 289) {\n    T_fast_tanh[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 17, 3, 17), \"float32\"), T_fast_tanh: T.Buffer((4, 17, 3, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(204):\n            for ax3 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 17 + ax3\n                T_fast_tanh_1 = T.Buffer((3468,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((3468,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [4, 17, 3, 17]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 252; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 16; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 18; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[(((ax0_ax1_fused_ax2_fused_ax3_fused * 288) + (rv0 * 18)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 16; ++rv0) {\n    for (int rv1 = 0; rv1 < 18; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 864) + (((int)threadIdx.x) * 288)) + (rv0 * 18)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 16, 18), \"float32\"), adaptive_pool_max: T.Buffer((18, 14, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(252):\n            adaptive_pool_max_1 = T.Buffer((252,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(16, 18):\n                data_1 = T.Buffer((72576,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 288 + rv0 * 18 + rv1])", "op_args": [18, 14, 16, 18]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = data[(((((ax0_ax1_fused_ax2_fused % 80) * 20) + ax3) + 24000) - ((ax0_ax1_fused_ax2_fused / 80) * 1600))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 50) * 32) + ((int)threadIdx.x)) + 24000) - ((((int)blockIdx.x) / 50) * 1600))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 10, 8, 20), \"float32\"), T_reverse_sequence: T.Buffer((16, 10, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1280):\n            for ax3 in range(20):\n                T_reverse_sequence_1 = T.Buffer((25600,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((25600,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 20 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 80 * 20 + ax3 + 24000 - ax0_ax1_fused_ax2_fused // 80 * 1600]", "op_args": [16, 10, 8, 20]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    float adaptive_pool_sum[2];\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      adaptive_pool_sum[ax1] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 4; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 19; ++rv1) {\n          adaptive_pool_sum[ax1] = (adaptive_pool_sum[ax1] + data[((((ax0 * 152) + (ax1 * 76)) + (rv0 * 19)) + rv1)]);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      adaptive_pool_avg[((ax0 * 2) + ax1_1)] = (adaptive_pool_sum[ax1_1] * 1.315789e-02f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] * 1.315789e-02f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 4; ++rv0) {\n    for (int rv1 = 0; rv1 < 19; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 608) + (((int)threadIdx.x) * 76)) + (rv0 * 19)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 2, 4, 19), \"float32\"), adaptive_pool_avg: T.Buffer((20, 2, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(20):\n            adaptive_pool_sum = T.allocate([2], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((2,), data=adaptive_pool_sum, align=8)\n            for ax1 in range(2):\n                adaptive_pool_sum_1[ax1] = T.float32(0)\n                for rv0, rv1 in T.grid(4, 19):\n                    data_1 = T.Buffer((3040,), data=data.data)\n                    adaptive_pool_sum_1[ax1] = adaptive_pool_sum_1[ax1] + data_1[ax0 * 152 + ax1 * 76 + rv0 * 19 + rv1]\n            for ax1 in range(2):\n                adaptive_pool_avg_1 = T.Buffer((40,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 2 + ax1] = adaptive_pool_sum_1[ax1] * T.float32(0.013157894736842105)", "op_args": [20, 2, 4, 19]}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 10; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 7; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 7) + i3)] = floorf(data[((i0_i1_fused_i2_fused * 7) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 1, 5, 7), \"float32\"), compute: T.Buffer((2, 1, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(10):\n            for i3 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 7 + i3\n                compute_1 = T.Buffer((70,), data=compute.data)\n                data_1 = T.Buffer((70,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])", "op_args": [2, 1, 5, 7]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 54; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      for (int32_t i3 = 0; i3 < 2; ++i3) {\n        DilatedInput[(((i0_i1_fused * 30) + (i2 * 2)) + i3)] = data[(((i0_i1_fused * 30) + (i2 * 2)) + i3)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 405) {\n    DilatedInput[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 6, 15, 2), \"float32\"), DilatedInput: T.Buffer((9, 6, 15, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(54):\n            for i2, i3 in T.grid(15, 2):\n                cse_var_1: T.int32 = i0_i1_fused * 30 + i2 * 2 + i3\n                DilatedInput_1 = T.Buffer((1620,), data=DilatedInput.data)\n                data_1 = T.Buffer((1620,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [9, 6, 15, 2]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 18; ++i3) {\n          compute[((((i0 * 3024) + (i1 * 216)) + (i2 * 18)) + i3)] = ((int8_t)(data[((((i0 * 3024) + (i1 * 216)) + (i2 * 18)) + i3)] != data[((((i0 * 3024) + (i1 * 216)) + (i2 * 18)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 14, 12, 18), \"float32\"), compute: T.Buffer((8, 14, 12, 18), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(14, 12, 18):\n                cse_var_1: T.int32 = i0 * 3024 + i1 * 216 + i2 * 18 + i3\n                compute_1 = T.Buffer((24192,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((24192,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [8, 14, 12, 18]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 5; ++i) {\n    for (int32_t j = 0; j < 96; ++j) {\n      compute[((i * 96) + j)] = data[((i * 96) + j)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 3, 4, 8), \"float32\"), compute: T.Buffer((5, 96), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(5):\n            for j in range(96):\n                cse_var_1: T.int32 = i * 96 + j\n                compute_1 = T.Buffer((480,), data=compute.data)\n                data_1 = T.Buffer((480,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]", "op_args": [5, 3, 4, 8]}{"op_name": "topology_expansion", "c_code": "; ModuleID = 'TVMMod'\nsource_filename = \"TVMMod\"\ntarget datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\ntarget triple = \"x86_64-pc-linux-gnu\"\n\n%0 = type { double }\n%1 = type { i8*, %2, i32, %3, i64*, i64*, i64 }\n%2 = type { i32, i32 }\n%3 = type { i8, i8, i16 }\n\n@__tvm_module_ctx = linkonce dllexport local_unnamed_addr global i8* null, align 8\n@__TVMFuncCall = linkonce dllexport local_unnamed_addr global i32 (i8*, %0*, i32*, i32, %0*, i32*)* null, align 8\n@__TVMBackendGetFuncFromEnv = linkonce dllexport local_unnamed_addr global i32 (i8*, i8*, i8**)* null, align 8\n@__TVMAPISetLastError = linkonce dllexport local_unnamed_addr global void (i8*)* null, align 8\n@.str = private constant [67 x i8] c\"Assert fail: num_args == 3, default_function: num_args should be 3\\00\", align 1\n@.str.1 = private constant [130 x i8] c\"Assert fail: ph_0_code == 3 or ph_0_code == 13 or ph_0_code == 7 or ph_0_code == 4, default_function: Expect arg[0] to be pointer\\00\", align 1\n@.str.2 = private constant [134 x i8] c\"Assert fail: T_add_code == 3 or T_add_code == 13 or T_add_code == 7 or T_add_code == 4, default_function: Expect arg[1] to be pointer\\00\", align 1\n@.str.3 = private constant [142 x i8] c\"Assert fail: compute_code == 3 or compute_code == 13 or compute_code == 7 or compute_code == 4, default_function: Expect arg[2] to be pointer\\00\", align 1\n@.str.4 = private constant [107 x i8] c\"Assert fail: 3 == T.tvm_struct_get(ph_0, 0, 4, \\22int32\\22), default_function.ph_0.ndim is expected to equal 3\\00\", align 1\n@.str.5 = private constant [235 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(ph_0, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(ph_0, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.ph_0.dtype is expected to be float32\\00\", align 1\n@.str.6 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[0]) == 15, Argument default_function.ph_0.shape[0] has an unsatisfied constraint: 15 == T.Cast(\\22int32\\22, default_function_ph_0_shape[0])\\00\", align 1\n@.str.7 = private constant [193 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[1]) == 19, Argument default_function.ph_0.shape[1] has an unsatisfied constraint: 19 == T.Cast(\\22int32\\22, default_function_ph_0_shape[1])\\00\", align 1\n@.str.8 = private constant [191 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_ph_0_shape[2]) == 9, Argument default_function.ph_0.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_ph_0_shape[2])\\00\", align 1\n@.str.9 = private constant [249 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_ph_0_strides[2]) and 9 == T.Cast(\\22int32\\22, default_function_ph_0_strides[1]) and 171 == T.Cast(\\22int32\\22, default_function_ph_0_strides[0]), default_function.ph_0.strides: expected to be compact array\\00\", align 1\n@.str.10 = private constant [196 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22), Argument default_function.ph_0.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(ph_0, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.11 = private constant [176 x i8] c\"Assert fail: T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22) == 1, Argument default_function.ph_0.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(ph_0, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.12 = private constant [109 x i8] c\"Assert fail: 3 == T.tvm_struct_get(T_add, 0, 4, \\22int32\\22), default_function.T_add.ndim is expected to equal 3\\00\", align 1\n@.str.13 = private constant [239 x i8] c\"Assert fail: T.tvm_struct_get(T_add, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(T_add, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(T_add, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.T_add.dtype is expected to be float32\\00\", align 1\n@.str.14 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[0]) == 15, Argument default_function.T_add.shape[0] has an unsatisfied constraint: 15 == T.Cast(\\22int32\\22, default_function_T_add_shape[0])\\00\", align 1\n@.str.15 = private constant [196 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[1]) == 19, Argument default_function.T_add.shape[1] has an unsatisfied constraint: 19 == T.Cast(\\22int32\\22, default_function_T_add_shape[1])\\00\", align 1\n@.str.16 = private constant [194 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_T_add_shape[2]) == 9, Argument default_function.T_add.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_T_add_shape[2])\\00\", align 1\n@.str.17 = private constant [253 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_T_add_strides[2]) and 9 == T.Cast(\\22int32\\22, default_function_T_add_strides[1]) and 171 == T.Cast(\\22int32\\22, default_function_T_add_strides[0]), default_function.T_add.strides: expected to be compact array\\00\", align 1\n@.str.18 = private constant [199 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(T_add, 0, 8, \\22uint64\\22), Argument default_function.T_add.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(T_add, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.19 = private constant [179 x i8] c\"Assert fail: T.tvm_struct_get(T_add, 0, 10, \\22int32\\22) == 1, Argument default_function.T_add.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(T_add, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.20 = private constant [185 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(T_add, 0, 9, \\22int32\\22), Argument default_function.T_add.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(T_add, 0, 9, \\22int32\\22)\\00\", align 1\n@.str.21 = private constant [113 x i8] c\"Assert fail: 3 == T.tvm_struct_get(compute, 0, 4, \\22int32\\22), default_function.compute.ndim is expected to equal 3\\00\", align 1\n@.str.22 = private constant [247 x i8] c\"Assert fail: T.tvm_struct_get(compute, 0, 5, \\22uint8\\22) == T.uint8(2) and T.tvm_struct_get(compute, 0, 6, \\22uint8\\22) == T.uint8(32) and T.tvm_struct_get(compute, 0, 7, \\22uint16\\22) == T.uint16(1), default_function.compute.dtype is expected to be float32\\00\", align 1\n@.str.23 = private constant [202 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[0]) == 15, Argument default_function.compute.shape[0] has an unsatisfied constraint: 15 == T.Cast(\\22int32\\22, default_function_compute_shape[0])\\00\", align 1\n@.str.24 = private constant [202 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[1]) == 19, Argument default_function.compute.shape[1] has an unsatisfied constraint: 19 == T.Cast(\\22int32\\22, default_function_compute_shape[1])\\00\", align 1\n@.str.25 = private constant [200 x i8] c\"Assert fail: T.Cast(\\22int32\\22, default_function_compute_shape[2]) == 9, Argument default_function.compute.shape[2] has an unsatisfied constraint: 9 == T.Cast(\\22int32\\22, default_function_compute_shape[2])\\00\", align 1\n@.str.26 = private constant [261 x i8] c\"Assert fail: 1 == T.Cast(\\22int32\\22, default_function_compute_strides[2]) and 9 == T.Cast(\\22int32\\22, default_function_compute_strides[1]) and 171 == T.Cast(\\22int32\\22, default_function_compute_strides[0]), default_function.compute.strides: expected to be compact array\\00\", align 1\n@.str.27 = private constant [205 x i8] c\"Assert fail: T.uint64(0) == T.tvm_struct_get(compute, 0, 8, \\22uint64\\22), Argument default_function.compute.byte_offset has an unsatisfied constraint: T.uint64(0) == T.tvm_struct_get(compute, 0, 8, \\22uint64\\22)\\00\", align 1\n@.str.28 = private constant [185 x i8] c\"Assert fail: T.tvm_struct_get(compute, 0, 10, \\22int32\\22) == 1, Argument default_function.compute.device_type has an unsatisfied constraint: 1 == T.tvm_struct_get(compute, 0, 10, \\22int32\\22)\\00\", align 1\n@.str.29 = private constant [191 x i8] c\"Assert fail: dev_id == T.tvm_struct_get(compute, 0, 9, \\22int32\\22), Argument default_function.compute.device_id has an unsatisfied constraint: dev_id == T.tvm_struct_get(compute, 0, 9, \\22int32\\22)\\00\", align 1\n@.tvm_func.default_function_kernel = internal unnamed_addr global i8* null, align 8\n@.str.30 = private constant [24 x i8] c\"default_function_kernel\\00\", align 1\n@.str.31 = private constant [68 x i8] c\"Assert fail: kernel_error_code == 0, Error executing compute kernel\\00\", align 1\n@__tvm_main__ = weak dllexport local_unnamed_addr constant [17 x i8] c\"default_function\\00\", align 1\n@llvm.global_ctors = appending global [0 x { i32, void ()*, i8* }] zeroinitializer\n\ndefine dllexport i32 @default_function(i8* noalias nocapture readonly %args, i32* noalias nocapture readonly %arg_type_ids, i32 %num_args, i8* noalias nocapture readnone %out_ret_value, i32* noalias nocapture readnone %out_ret_tcode, i8* noalias nocapture readnone %resource_handle) local_unnamed_addr #0 !dbg !5 {\nentry:\n  call void @llvm.dbg.value(metadata i8* %args, metadata !12, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %arg_type_ids, metadata !13, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32 %num_args, metadata !14, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %out_ret_value, metadata !15, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i32* %out_ret_tcode, metadata !16, metadata !DIExpression()), !dbg !18\n  call void @llvm.dbg.value(metadata i8* %resource_handle, metadata !17, metadata !DIExpression()), !dbg !18\n  %stack_value_void_ptr81 = alloca [4 x %0], align 8, !dbg !18\n  %stack_tcode82 = alloca [4 x i32], align 4, !dbg !18\n  %stack_tcode82.sub = getelementptr inbounds [4 x i32], [4 x i32]* %stack_tcode82, i64 0, i64 0\n  %stack_value = bitcast [4 x %0]* %stack_value_void_ptr81 to i8*, !dbg !18\n  %0 = icmp eq i32 %num_args, 3, !dbg !18\n  br i1 %0, label %assert_end, label %assert_fail, !dbg !18, !prof !19\n\nassert_fail:                                      ; preds = %entry\n  %1 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %1(i8* getelementptr inbounds ([67 x i8], [67 x i8]* @.str, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end:                                       ; preds = %entry\n  %ph_0.code = load i32, i32* %arg_type_ids, align 4, !dbg !18, !tbaa !23\n  %2 = getelementptr inbounds i32, i32* %arg_type_ids, i64 1, !dbg !18\n  %T_add.code = load i32, i32* %2, align 4, !dbg !18, !tbaa !34\n  %3 = getelementptr inbounds i32, i32* %arg_type_ids, i64 2, !dbg !18\n  %compute.code = load i32, i32* %3, align 4, !dbg !18, !tbaa !36\n  %4 = bitcast i8* %args to %1**, !dbg !18\n  %ph_083 = load %1*, %1** %4, align 8, !dbg !18\n  %5 = getelementptr inbounds i8, i8* %args, i64 8, !dbg !18\n  %6 = bitcast i8* %5 to %1**, !dbg !18\n  %T_add84 = load %1*, %1** %6, align 8, !dbg !18\n  %7 = getelementptr inbounds i8, i8* %args, i64 16, !dbg !18\n  %8 = bitcast i8* %7 to %1**, !dbg !18\n  %compute85 = load %1*, %1** %8, align 8, !dbg !18\n  %9 = bitcast %1* %ph_083 to float**, !dbg !18\n  %ph_0_void_ptr86 = load float*, float** %9, align 8, !dbg !18\n  %ptrint = ptrtoint float* %ph_0_void_ptr86 to i64, !dbg !18\n  %maskedptr = and i64 %ptrint, 63, !dbg !18\n  %maskcond = icmp eq i64 %maskedptr, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond), !dbg !18\n  %10 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 4, !dbg !18\n  %default_function.ph_0.shape = load i64*, i64** %10, align 8, !dbg !18\n  %11 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 5, !dbg !18\n  %default_function.ph_0.strides = load i64*, i64** %11, align 8, !dbg !18\n  %12 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 1, !dbg !18\n  %dev_id = load i32, i32* %12, align 4, !dbg !18\n  %13 = bitcast %1* %T_add84 to float**, !dbg !18\n  %T_add_void_ptr87 = load float*, float** %13, align 8, !dbg !18\n  %ptrint3 = ptrtoint float* %T_add_void_ptr87 to i64, !dbg !18\n  %maskedptr4 = and i64 %ptrint3, 63, !dbg !18\n  %maskcond5 = icmp eq i64 %maskedptr4, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond5), !dbg !18\n  %14 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 4, !dbg !18\n  %default_function.T_add.shape = load i64*, i64** %14, align 8, !dbg !18\n  %15 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 5, !dbg !18\n  %default_function.T_add.strides = load i64*, i64** %15, align 8, !dbg !18\n  %16 = bitcast %1* %compute85 to float**, !dbg !18\n  %compute_void_ptr88 = load float*, float** %16, align 8, !dbg !18\n  %ptrint7 = ptrtoint float* %compute_void_ptr88 to i64, !dbg !18\n  %maskedptr8 = and i64 %ptrint7, 63, !dbg !18\n  %maskcond9 = icmp eq i64 %maskedptr8, 0, !dbg !18\n  tail call void @llvm.assume(i1 %maskcond9), !dbg !18\n  %17 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 4, !dbg !18\n  %default_function.compute.shape = load i64*, i64** %17, align 8, !dbg !18\n  %18 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 5, !dbg !18\n  %default_function.compute.strides = load i64*, i64** %18, align 8, !dbg !18\n  switch i32 %ph_0.code, label %assert_fail10 [\n    i32 13, label %assert_end11\n    i32 7, label %assert_end11\n    i32 4, label %assert_end11\n    i32 3, label %assert_end11\n  ], !dbg !18\n\nassert_fail10:                                    ; preds = %assert_end\n  %19 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %19(i8* getelementptr inbounds ([130 x i8], [130 x i8]* @.str.1, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end11:                                     ; preds = %assert_end, %assert_end, %assert_end, %assert_end\n  switch i32 %T_add.code, label %assert_fail12 [\n    i32 13, label %assert_end13\n    i32 7, label %assert_end13\n    i32 4, label %assert_end13\n    i32 3, label %assert_end13\n  ], !dbg !18\n\nassert_fail12:                                    ; preds = %assert_end11\n  %20 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %20(i8* getelementptr inbounds ([134 x i8], [134 x i8]* @.str.2, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end13:                                     ; preds = %assert_end11, %assert_end11, %assert_end11, %assert_end11\n  switch i32 %compute.code, label %assert_fail14 [\n    i32 13, label %assert_end15\n    i32 7, label %assert_end15\n    i32 4, label %assert_end15\n    i32 3, label %assert_end15\n  ], !dbg !18\n\nassert_fail14:                                    ; preds = %assert_end13\n  %21 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %21(i8* getelementptr inbounds ([142 x i8], [142 x i8]* @.str.3, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end15:                                     ; preds = %assert_end13, %assert_end13, %assert_end13, %assert_end13\n  %22 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 2, !dbg !18\n  %23 = load i32, i32* %22, align 4, !dbg !18\n  %24 = icmp eq i32 %23, 3, !dbg !18\n  br i1 %24, label %assert_end19, label %assert_fail16, !dbg !18, !prof !19\n\nassert_fail16:                                    ; preds = %assert_end15\n  %25 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %25(i8* getelementptr inbounds ([107 x i8], [107 x i8]* @.str.4, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end19:                                     ; preds = %assert_end15\n  %26 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 2, !dbg !18\n  %27 = load i16, i16* %26, align 2, !dbg !18\n  %28 = icmp eq i16 %27, 1, !dbg !18\n  %29 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 1, !dbg !18\n  %30 = load i8, i8* %29, align 1, !dbg !18\n  %31 = icmp eq i8 %30, 32, !dbg !18\n  %32 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 3, i32 0, !dbg !18\n  %33 = load i8, i8* %32, align 1, !dbg !18\n  %34 = icmp eq i8 %33, 2, !dbg !18\n  %35 = and i1 %31, %34, !dbg !18\n  %36 = and i1 %28, %35, !dbg !18\n  br i1 %36, label %assert_end21, label %assert_fail20, !dbg !18, !prof !19\n\nassert_fail20:                                    ; preds = %assert_end19\n  %37 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %37(i8* getelementptr inbounds ([235 x i8], [235 x i8]* @.str.5, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end21:                                     ; preds = %assert_end19\n  %38 = load i64, i64* %default_function.ph_0.shape, align 8, !dbg !18, !tbaa !39\n  %39 = trunc i64 %38 to i32, !dbg !18\n  %40 = icmp eq i32 %39, 15, !dbg !18\n  br i1 %40, label %assert_end23, label %assert_fail22, !dbg !18, !prof !19\n\nassert_fail22:                                    ; preds = %assert_end21\n  %41 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %41(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.6, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end23:                                     ; preds = %assert_end21\n  %42 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 1, !dbg !18\n  %43 = load i64, i64* %42, align 8, !dbg !18, !tbaa !49\n  %44 = trunc i64 %43 to i32, !dbg !18\n  %45 = icmp eq i32 %44, 19, !dbg !18\n  br i1 %45, label %assert_end25, label %assert_fail24, !dbg !18, !prof !19\n\nassert_fail24:                                    ; preds = %assert_end23\n  %46 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %46(i8* getelementptr inbounds ([193 x i8], [193 x i8]* @.str.7, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end25:                                     ; preds = %assert_end23\n  %47 = getelementptr inbounds i64, i64* %default_function.ph_0.shape, i64 2, !dbg !18\n  %48 = load i64, i64* %47, align 8, !dbg !18, !tbaa !51\n  %49 = trunc i64 %48 to i32, !dbg !18\n  %50 = icmp eq i32 %49, 9, !dbg !18\n  br i1 %50, label %assert_end27, label %assert_fail26, !dbg !18, !prof !19\n\nassert_fail26:                                    ; preds = %assert_end25\n  %51 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %51(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.8, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end27:                                     ; preds = %assert_end25\n  %52 = icmp eq i64* %default_function.ph_0.strides, null, !dbg !18\n  br i1 %52, label %if_end, label %if_then, !dbg !18, !prof !54\n\nif_then:                                          ; preds = %assert_end27\n  %53 = load i64, i64* %default_function.ph_0.strides, align 8, !dbg !18, !tbaa !55\n  %54 = trunc i64 %53 to i32, !dbg !18\n  %55 = icmp eq i32 %54, 171, !dbg !18\n  %56 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 1, !dbg !18\n  %57 = load i64, i64* %56, align 8, !dbg !18, !tbaa !65\n  %58 = trunc i64 %57 to i32, !dbg !18\n  %59 = icmp eq i32 %58, 9, !dbg !18\n  %60 = getelementptr inbounds i64, i64* %default_function.ph_0.strides, i64 2, !dbg !18\n  %61 = load i64, i64* %60, align 8, !dbg !18, !tbaa !67\n  %62 = trunc i64 %61 to i32, !dbg !18\n  %63 = icmp eq i32 %62, 1, !dbg !18\n  %64 = and i1 %59, %63, !dbg !18\n  %65 = and i1 %55, %64, !dbg !18\n  br i1 %65, label %if_end, label %assert_fail28, !dbg !18, !prof !19\n\nif_end:                                           ; preds = %assert_end27, %if_then\n  %66 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 6, !dbg !18\n  %67 = load i64, i64* %66, align 8, !dbg !18\n  %68 = icmp eq i64 %67, 0, !dbg !18\n  br i1 %68, label %assert_end31, label %assert_fail30, !dbg !18, !prof !19\n\nassert_fail28:                                    ; preds = %if_then\n  %69 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %69(i8* getelementptr inbounds ([249 x i8], [249 x i8]* @.str.9, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail30:                                    ; preds = %if_end\n  %70 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %70(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.10, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end31:                                     ; preds = %if_end\n  %71 = getelementptr inbounds %1, %1* %ph_083, i64 0, i32 1, i32 0, !dbg !18\n  %72 = load i32, i32* %71, align 4, !dbg !18\n  %73 = icmp eq i32 %72, 1, !dbg !18\n  br i1 %73, label %assert_end33, label %assert_fail32, !dbg !18, !prof !19\n\nassert_fail32:                                    ; preds = %assert_end31\n  %74 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %74(i8* getelementptr inbounds ([176 x i8], [176 x i8]* @.str.11, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end33:                                     ; preds = %assert_end31\n  %75 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 2, !dbg !18\n  %76 = load i32, i32* %75, align 4, !dbg !18\n  %77 = icmp eq i32 %76, 3, !dbg !18\n  br i1 %77, label %assert_end37, label %assert_fail34, !dbg !18, !prof !19\n\nassert_fail34:                                    ; preds = %assert_end33\n  %78 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %78(i8* getelementptr inbounds ([109 x i8], [109 x i8]* @.str.12, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end37:                                     ; preds = %assert_end33\n  %79 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 3, i32 2, !dbg !18\n  %80 = load i16, i16* %79, align 2, !dbg !18\n  %81 = icmp eq i16 %80, 1, !dbg !18\n  %82 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 3, i32 1, !dbg !18\n  %83 = load i8, i8* %82, align 1, !dbg !18\n  %84 = icmp eq i8 %83, 32, !dbg !18\n  %85 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 3, i32 0, !dbg !18\n  %86 = load i8, i8* %85, align 1, !dbg !18\n  %87 = icmp eq i8 %86, 2, !dbg !18\n  %88 = and i1 %84, %87, !dbg !18\n  %89 = and i1 %81, %88, !dbg !18\n  br i1 %89, label %assert_end39, label %assert_fail38, !dbg !18, !prof !19\n\nassert_fail38:                                    ; preds = %assert_end37\n  %90 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %90(i8* getelementptr inbounds ([239 x i8], [239 x i8]* @.str.13, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end39:                                     ; preds = %assert_end37\n  %91 = load i64, i64* %default_function.T_add.shape, align 8, !dbg !18, !tbaa !70\n  %92 = trunc i64 %91 to i32, !dbg !18\n  %93 = icmp eq i32 %92, 15, !dbg !18\n  br i1 %93, label %assert_end41, label %assert_fail40, !dbg !18, !prof !19\n\nassert_fail40:                                    ; preds = %assert_end39\n  %94 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %94(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.14, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end41:                                     ; preds = %assert_end39\n  %95 = getelementptr inbounds i64, i64* %default_function.T_add.shape, i64 1, !dbg !18\n  %96 = load i64, i64* %95, align 8, !dbg !18, !tbaa !80\n  %97 = trunc i64 %96 to i32, !dbg !18\n  %98 = icmp eq i32 %97, 19, !dbg !18\n  br i1 %98, label %assert_end43, label %assert_fail42, !dbg !18, !prof !19\n\nassert_fail42:                                    ; preds = %assert_end41\n  %99 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %99(i8* getelementptr inbounds ([196 x i8], [196 x i8]* @.str.15, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end43:                                     ; preds = %assert_end41\n  %100 = getelementptr inbounds i64, i64* %default_function.T_add.shape, i64 2, !dbg !18\n  %101 = load i64, i64* %100, align 8, !dbg !18, !tbaa !82\n  %102 = trunc i64 %101 to i32, !dbg !18\n  %103 = icmp eq i32 %102, 9, !dbg !18\n  br i1 %103, label %assert_end45, label %assert_fail44, !dbg !18, !prof !19\n\nassert_fail44:                                    ; preds = %assert_end43\n  %104 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %104(i8* getelementptr inbounds ([194 x i8], [194 x i8]* @.str.16, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end45:                                     ; preds = %assert_end43\n  %105 = icmp eq i64* %default_function.T_add.strides, null, !dbg !18\n  br i1 %105, label %if_end47, label %if_then46, !dbg !18, !prof !54\n\nif_then46:                                        ; preds = %assert_end45\n  %106 = load i64, i64* %default_function.T_add.strides, align 8, !dbg !18, !tbaa !85\n  %107 = trunc i64 %106 to i32, !dbg !18\n  %108 = icmp eq i32 %107, 171, !dbg !18\n  %109 = getelementptr inbounds i64, i64* %default_function.T_add.strides, i64 1, !dbg !18\n  %110 = load i64, i64* %109, align 8, !dbg !18, !tbaa !95\n  %111 = trunc i64 %110 to i32, !dbg !18\n  %112 = icmp eq i32 %111, 9, !dbg !18\n  %113 = getelementptr inbounds i64, i64* %default_function.T_add.strides, i64 2, !dbg !18\n  %114 = load i64, i64* %113, align 8, !dbg !18, !tbaa !97\n  %115 = trunc i64 %114 to i32, !dbg !18\n  %116 = icmp eq i32 %115, 1, !dbg !18\n  %117 = and i1 %112, %116, !dbg !18\n  %118 = and i1 %108, %117, !dbg !18\n  br i1 %118, label %if_end47, label %assert_fail48, !dbg !18, !prof !19\n\nif_end47:                                         ; preds = %assert_end45, %if_then46\n  %119 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 6, !dbg !18\n  %120 = load i64, i64* %119, align 8, !dbg !18\n  %121 = icmp eq i64 %120, 0, !dbg !18\n  br i1 %121, label %assert_end51, label %assert_fail50, !dbg !18, !prof !19\n\nassert_fail48:                                    ; preds = %if_then46\n  %122 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %122(i8* getelementptr inbounds ([253 x i8], [253 x i8]* @.str.17, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail50:                                    ; preds = %if_end47\n  %123 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %123(i8* getelementptr inbounds ([199 x i8], [199 x i8]* @.str.18, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end51:                                     ; preds = %if_end47\n  %124 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 1, i32 0, !dbg !18\n  %125 = load i32, i32* %124, align 4, !dbg !18\n  %126 = icmp eq i32 %125, 1, !dbg !18\n  br i1 %126, label %assert_end53, label %assert_fail52, !dbg !18, !prof !19\n\nassert_fail52:                                    ; preds = %assert_end51\n  %127 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %127(i8* getelementptr inbounds ([179 x i8], [179 x i8]* @.str.19, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end53:                                     ; preds = %assert_end51\n  %128 = getelementptr inbounds %1, %1* %T_add84, i64 0, i32 1, i32 1, !dbg !18\n  %129 = load i32, i32* %128, align 4, !dbg !18\n  %130 = icmp eq i32 %dev_id, %129, !dbg !18\n  br i1 %130, label %assert_end55, label %assert_fail54, !dbg !18, !prof !19\n\nassert_fail54:                                    ; preds = %assert_end53\n  %131 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %131(i8* getelementptr inbounds ([185 x i8], [185 x i8]* @.str.20, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end55:                                     ; preds = %assert_end53\n  %132 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 2, !dbg !18\n  %133 = load i32, i32* %132, align 4, !dbg !18\n  %134 = icmp eq i32 %133, 3, !dbg !18\n  br i1 %134, label %assert_end59, label %assert_fail56, !dbg !18, !prof !19\n\nassert_fail56:                                    ; preds = %assert_end55\n  %135 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %135(i8* getelementptr inbounds ([113 x i8], [113 x i8]* @.str.21, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end59:                                     ; preds = %assert_end55\n  %136 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 3, i32 2, !dbg !18\n  %137 = load i16, i16* %136, align 2, !dbg !18\n  %138 = icmp eq i16 %137, 1, !dbg !18\n  %139 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 3, i32 1, !dbg !18\n  %140 = load i8, i8* %139, align 1, !dbg !18\n  %141 = icmp eq i8 %140, 32, !dbg !18\n  %142 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 3, i32 0, !dbg !18\n  %143 = load i8, i8* %142, align 1, !dbg !18\n  %144 = icmp eq i8 %143, 2, !dbg !18\n  %145 = and i1 %141, %144, !dbg !18\n  %146 = and i1 %138, %145, !dbg !18\n  br i1 %146, label %assert_end61, label %assert_fail60, !dbg !18, !prof !19\n\nassert_fail60:                                    ; preds = %assert_end59\n  %147 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %147(i8* getelementptr inbounds ([247 x i8], [247 x i8]* @.str.22, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end61:                                     ; preds = %assert_end59\n  %148 = load i64, i64* %default_function.compute.shape, align 8, !dbg !18, !tbaa !100\n  %149 = trunc i64 %148 to i32, !dbg !18\n  %150 = icmp eq i32 %149, 15, !dbg !18\n  br i1 %150, label %assert_end63, label %assert_fail62, !dbg !18, !prof !19\n\nassert_fail62:                                    ; preds = %assert_end61\n  %151 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %151(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.23, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end63:                                     ; preds = %assert_end61\n  %152 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 1, !dbg !18\n  %153 = load i64, i64* %152, align 8, !dbg !18, !tbaa !110\n  %154 = trunc i64 %153 to i32, !dbg !18\n  %155 = icmp eq i32 %154, 19, !dbg !18\n  br i1 %155, label %assert_end65, label %assert_fail64, !dbg !18, !prof !19\n\nassert_fail64:                                    ; preds = %assert_end63\n  %156 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %156(i8* getelementptr inbounds ([202 x i8], [202 x i8]* @.str.24, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end65:                                     ; preds = %assert_end63\n  %157 = getelementptr inbounds i64, i64* %default_function.compute.shape, i64 2, !dbg !18\n  %158 = load i64, i64* %157, align 8, !dbg !18, !tbaa !112\n  %159 = trunc i64 %158 to i32, !dbg !18\n  %160 = icmp eq i32 %159, 9, !dbg !18\n  br i1 %160, label %assert_end67, label %assert_fail66, !dbg !18, !prof !19\n\nassert_fail66:                                    ; preds = %assert_end65\n  %161 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %161(i8* getelementptr inbounds ([200 x i8], [200 x i8]* @.str.25, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end67:                                     ; preds = %assert_end65\n  %162 = icmp eq i64* %default_function.compute.strides, null, !dbg !18\n  br i1 %162, label %if_end69, label %if_then68, !dbg !18, !prof !54\n\nif_then68:                                        ; preds = %assert_end67\n  %163 = load i64, i64* %default_function.compute.strides, align 8, !dbg !18, !tbaa !115\n  %164 = trunc i64 %163 to i32, !dbg !18\n  %165 = icmp eq i32 %164, 171, !dbg !18\n  %166 = getelementptr inbounds i64, i64* %default_function.compute.strides, i64 1, !dbg !18\n  %167 = load i64, i64* %166, align 8, !dbg !18, !tbaa !125\n  %168 = trunc i64 %167 to i32, !dbg !18\n  %169 = icmp eq i32 %168, 9, !dbg !18\n  %170 = getelementptr inbounds i64, i64* %default_function.compute.strides, i64 2, !dbg !18\n  %171 = load i64, i64* %170, align 8, !dbg !18, !tbaa !127\n  %172 = trunc i64 %171 to i32, !dbg !18\n  %173 = icmp eq i32 %172, 1, !dbg !18\n  %174 = and i1 %169, %173, !dbg !18\n  %175 = and i1 %165, %174, !dbg !18\n  br i1 %175, label %if_end69, label %assert_fail70, !dbg !18, !prof !19\n\nif_end69:                                         ; preds = %assert_end67, %if_then68\n  %176 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 6, !dbg !18\n  %177 = load i64, i64* %176, align 8, !dbg !18\n  %178 = icmp eq i64 %177, 0, !dbg !18\n  br i1 %178, label %assert_end73, label %assert_fail72, !dbg !18, !prof !19\n\nassert_fail70:                                    ; preds = %if_then68\n  %179 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %179(i8* getelementptr inbounds ([261 x i8], [261 x i8]* @.str.26, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_fail72:                                    ; preds = %if_end69\n  %180 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %180(i8* getelementptr inbounds ([205 x i8], [205 x i8]* @.str.27, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end73:                                     ; preds = %if_end69\n  %181 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 1, i32 0, !dbg !18\n  %182 = load i32, i32* %181, align 4, !dbg !18\n  %183 = icmp eq i32 %182, 1, !dbg !18\n  br i1 %183, label %assert_end75, label %assert_fail74, !dbg !18, !prof !19\n\nassert_fail74:                                    ; preds = %assert_end73\n  %184 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %184(i8* getelementptr inbounds ([185 x i8], [185 x i8]* @.str.28, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end75:                                     ; preds = %assert_end73\n  %185 = getelementptr inbounds %1, %1* %compute85, i64 0, i32 1, i32 1, !dbg !18\n  %186 = load i32, i32* %185, align 4, !dbg !18\n  %187 = icmp eq i32 %dev_id, %186, !dbg !18\n  br i1 %187, label %assert_end77, label %assert_fail76, !dbg !18, !prof !19\n\nassert_fail76:                                    ; preds = %assert_end75\n  %188 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !dbg !18, !tbaa !20\n  tail call void %188(i8* getelementptr inbounds ([191 x i8], [191 x i8]* @.str.29, i64 0, i64 0)), !dbg !18\n  ret i32 -1, !dbg !18\n\nassert_end77:                                     ; preds = %assert_end75\n  %189 = call fastcc i32 @default_function_compute_(i8* nonnull %stack_value, float* %T_add_void_ptr87, i32* nonnull %stack_tcode82.sub, float* %compute_void_ptr88, float* %ph_0_void_ptr86), !dbg !18\n  ret i32 %189, !dbg !18\n}\n\n; Function Attrs: nounwind willreturn\ndeclare void @llvm.assume(i1) #1\n\n; Function Attrs: noinline\ndefine internal fastcc i32 @default_function_compute_(i8* noalias %0, float* noalias align 64 %1, i32* noalias %2, float* noalias align 64 %3, float* noalias align 64 %4) unnamed_addr #2 {\nentry:\n  %5 = alloca i8*, align 8\n  %6 = bitcast i8* %0 to %0*\n  %7 = bitcast i8* %0 to float**\n  store float* %1, float** %7, align 8\n  store i32 3, i32* %2, align 4, !tbaa !130\n  %8 = getelementptr inbounds i8, i8* %0, i64 8\n  %9 = bitcast i8* %8 to float**\n  store float* %3, float** %9, align 8\n  %10 = getelementptr inbounds i32, i32* %2, i64 1\n  store i32 3, i32* %10, align 4, !tbaa !141\n  %11 = getelementptr inbounds i8, i8* %0, i64 16\n  %12 = bitcast i8* %11 to float**\n  store float* %4, float** %12, align 8\n  %13 = getelementptr inbounds i32, i32* %2, i64 2\n  store i32 3, i32* %13, align 4, !tbaa !143\n  %14 = getelementptr inbounds i8, i8* %0, i64 24\n  %15 = bitcast i8* %14 to %0*\n  %16 = getelementptr inbounds i32, i32* %2, i64 3\n  %17 = load i32 (i8*, %0*, i32*, i32, %0*, i32*)*, i32 (i8*, %0*, i32*, i32, %0*, i32*)** @__TVMFuncCall, align 8, !tbaa !20\n  %18 = load i8*, i8** @.tvm_func.default_function_kernel, align 8\n  %19 = icmp eq i8* %18, null\n  br i1 %19, label %handle_init, label %handle_init_end, !prof !54\n\nhandle_init:                                      ; preds = %entry\n  %20 = load i8*, i8** @__tvm_module_ctx, align 8, !tbaa !20\n  %21 = load i32 (i8*, i8*, i8**)*, i32 (i8*, i8*, i8**)** @__TVMBackendGetFuncFromEnv, align 8, !tbaa !20\n  %22 = call i32 %21(i8* %20, i8* getelementptr inbounds ([24 x i8], [24 x i8]* @.str.30, i64 0, i64 0), i8** nonnull %5)\n  %23 = icmp eq i32 %22, 0\n  br i1 %23, label %call_end, label %call_fail, !prof !19\n\nhandle_init_end:                                  ; preds = %entry, %call_end\n  %24 = phi i8* [ %18, %entry ], [ %27, %call_end ]\n  %25 = call i32 %17(i8* %24, %0* %6, i32* nonnull %2, i32 3, %0* nonnull %15, i32* nonnull %16)\n  %26 = icmp eq i32 %25, 0\n  br i1 %26, label %call_end2, label %call_fail, !prof !19\n\ncall_fail:                                        ; preds = %call_end2, %handle_init_end, %handle_init\n  %merge = phi i32 [ %22, %handle_init ], [ %25, %handle_init_end ], [ 0, %call_end2 ]\n  ret i32 %merge\n\ncall_end:                                         ; preds = %handle_init\n  %27 = load i8*, i8** %5, align 8\n  store i8* %27, i8** @.tvm_func.default_function_kernel, align 8\n  br label %handle_init_end\n\ncall_end2:                                        ; preds = %handle_init_end\n  %28 = bitcast i8* %14 to i64*\n  %29 = load i64, i64* %28, align 8\n  %kernel_error_code = trunc i64 %29 to i32\n  %30 = icmp eq i32 %kernel_error_code, 0\n  br i1 %30, label %call_fail, label %assert_fail, !prof !19\n\nassert_fail:                                      ; preds = %call_end2\n  %31 = load void (i8*)*, void (i8*)** @__TVMAPISetLastError, align 8, !tbaa !20\n  call void %31(i8* getelementptr inbounds ([68 x i8], [68 x i8]* @.str.31, i64 0, i64 0))\n  ret i32 -1\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local i16 @__truncsfhf2(float %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v0 = bitcast float %a0 to i32\n  %v1 = and i32 %v0, 2147483647\n  %v2 = add nsw i32 %v1, -947912704\n  %v3 = add nsw i32 %v1, -1199570944\n  %v4 = icmp ult i32 %v2, %v3\n  br i1 %v4, label %b1, label %b5\n\nb1:                                               ; preds = %b0\n  %v5 = lshr i32 %v0, 13\n  %v6 = and i32 %v5, 65535\n  %v7 = add nuw nsw i32 %v6, -114688\n  %v8 = and i32 %v0, 8191\n  %v9 = icmp ugt i32 %v8, 4096\n  br i1 %v9, label %b2, label %b3\n\nb2:                                               ; preds = %b1\n  %v10 = add nuw nsw i32 %v6, -114687\n  br label %b13\n\nb3:                                               ; preds = %b1\n  %v11 = icmp eq i32 %v8, 4096\n  br i1 %v11, label %b4, label %b13\n\nb4:                                               ; preds = %b3\n  %v12 = and i32 %v7, 65535\n  %v13 = and i32 %v5, 1\n  %v14 = add nuw nsw i32 %v12, %v13\n  br label %b13\n\nb5:                                               ; preds = %b0\n  %v15 = icmp ugt i32 %v1, 2139095040\n  br i1 %v15, label %b6, label %b7\n\nb6:                                               ; preds = %b5\n  %v16 = lshr i32 %v0, 13\n  %v17 = and i32 %v16, 511\n  %v18 = or i32 %v17, 32256\n  br label %b13\n\nb7:                                               ; preds = %b5\n  %v19 = icmp ugt i32 %v1, 1199570943\n  br i1 %v19, label %b13, label %b8\n\nb8:                                               ; preds = %b7\n  %v20 = icmp ult i32 %v1, 754974720\n  br i1 %v20, label %b13, label %b9\n\nb9:                                               ; preds = %b8\n  %v21 = lshr i32 %v1, 23\n  %v22 = sub nsw i32 113, %v21\n  %v23 = and i32 %v0, 8388607\n  %v24 = or i32 %v23, 8388608\n  %v25 = add nsw i32 %v21, -81\n  %v26 = shl i32 %v24, %v25\n  %v27 = icmp ne i32 %v26, 0\n  %v28 = lshr i32 %v24, %v22\n  %v29 = zext i1 %v27 to i32\n  %v30 = lshr i32 %v28, 13\n  %v31 = and i32 %v28, 8191\n  %v32 = or i32 %v31, %v29\n  %v33 = icmp ugt i32 %v32, 4096\n  br i1 %v33, label %b10, label %b11\n\nb10:                                              ; preds = %b9\n  %v34 = add nuw nsw i32 %v30, 1\n  br label %b13\n\nb11:                                              ; preds = %b9\n  %v35 = icmp eq i32 %v32, 4096\n  br i1 %v35, label %b12, label %b13\n\nb12:                                              ; preds = %b11\n  %v36 = and i32 %v30, 1\n  %v37 = add nuw nsw i32 %v36, %v30\n  br label %b13\n\nb13:                                              ; preds = %b12, %b11, %b10, %b8, %b7, %b6, %b4, %b3, %b2\n  %v38 = phi i32 [ %v18, %b6 ], [ %v10, %b2 ], [ %v14, %b4 ], [ %v7, %b3 ], [ 31744, %b7 ], [ 0, %b8 ], [ %v34, %b10 ], [ %v37, %b12 ], [ %v30, %b11 ]\n  %v39 = lshr i32 %v0, 16\n  %v40 = and i32 %v39, 32768\n  %v41 = or i32 %v38, %v40\n  %vlast = trunc i32 %v41 to i16\n  ret i16 %vlast\n}\n\n; Function Attrs: nounwind readnone\ndefine weak dso_local float @__extendhfsf2(i16 %a0) local_unnamed_addr #3 section \".text.tvm.fp16.conv\" {\nb0:\n  %v1 = and i16 %a0, 32767\n  %v2 = zext i16 %v1 to i32\n  %v3 = add nsw i16 %v1, -1024\n  %v4 = icmp ult i16 %v3, 30720\n  br i1 %v4, label %b1, label %b2\n\nb1:                                               ; preds = %b0\n  %v5 = shl nuw nsw i32 %v2, 13\n  %v6 = add nuw nsw i32 %v5, 939524096\n  br label %b6\n\nb2:                                               ; preds = %b0\n  %v7 = icmp ugt i16 %v1, 31743\n  br i1 %v7, label %b3, label %b4\n\nb3:                                               ; preds = %b2\n  %v8 = shl nuw nsw i32 %v2, 13\n  %v9 = or i32 %v8, 2139095040\n  br label %b6\n\nb4:                                               ; preds = %b2\n  %v10 = icmp eq i16 %v1, 0\n  br i1 %v10, label %b6, label %b5\n\nb5:                                               ; preds = %b4\n  %v11 = icmp ult i16 %v1, 256\n  %v12 = lshr i32 %v2, 8\n  %v13 = select i1 %v11, i32 %v2, i32 %v12\n  %v14 = select i1 %v11, i32 32, i32 24\n  %v15 = icmp ult i32 %v13, 16\n  %v16 = lshr i32 %v13, 4\n  %v17 = add nsw i32 %v14, -4\n  %v18 = select i1 %v15, i32 %v13, i32 %v16\n  %v19 = select i1 %v15, i32 %v14, i32 %v17\n  %v20 = icmp ult i32 %v18, 4\n  %v21 = lshr i32 %v18, 2\n  %v22 = add nsw i32 %v19, -2\n  %v23 = select i1 %v20, i32 %v18, i32 %v21\n  %v24 = select i1 %v20, i32 %v19, i32 %v22\n  %v25 = icmp ult i32 %v23, 2\n  %v26 = sub nsw i32 0, %v23\n  %v27 = select i1 %v25, i32 %v26, i32 -2\n  %v28 = add nsw i32 %v27, %v24\n  %v29 = add nsw i32 %v28, -8\n  %v30 = shl i32 %v2, %v29\n  %v31 = xor i32 %v30, 8388608\n  %v32 = shl i32 %v28, 23\n  %v33 = sub i32 1124073472, %v32\n  %v34 = or i32 %v31, %v33\n  br label %b6\n\nb6:                                               ; preds = %b5, %b4, %b3, %b1\n  %v35 = phi i32 [ %v6, %b1 ], [ %v9, %b3 ], [ %v34, %b5 ], [ 0, %b4 ]\n  %v36 = and i16 %a0, -32768\n  %v37 = zext i16 %v36 to i32\n  %v38 = shl nuw i32 %v37, 16\n  %v39 = or i32 %v35, %v38\n  %v40 = bitcast i32 %v39 to float\n  ret float %v40\n}\n\n; Function Attrs: nounwind readnone speculatable willreturn\ndeclare void @llvm.dbg.value(metadata, metadata, metadata) #4\n\nattributes #0 = { \"target-cpu\"=\"generic\" }\nattributes #1 = { nounwind willreturn }\nattributes #2 = { noinline \"target-cpu\"=\"generic\" }\nattributes #3 = { nounwind readnone \"target-cpu\"=\"generic\" \"target-features\" }\nattributes #4 = { nounwind readnone speculatable willreturn }\n\n!llvm.dbg.cu = !{!0}\n!llvm.module.flags = !{!3, !4}\n\n!0 = distinct !DICompileUnit(language: DW_LANG_C, file: !1, producer: \"TVM\", isOptimized: false, runtimeVersion: 0, emissionKind: FullDebug, enums: !2)\n!1 = !DIFile(filename: \"main.tir\", directory: \".\")\n!2 = !{}\n!3 = !{i32 2, !\"tvm_target\", !\"llvm -mtriple=x86_64-pc-linux-gnu\"}\n!4 = !{i32 4, !\"Debug Info Version\", i32 3}\n!5 = distinct !DISubprogram(name: \"main.tir\", scope: !1, file: !1, type: !6, spFlags: DISPFlagLocalToUnit | DISPFlagDefinition | DISPFlagOptimized, unit: !0, retainedNodes: !11)\n!6 = !DISubroutineType(types: !7)\n!7 = !{!8, !9, !10, !8, !9, !10, !9}\n!8 = !DIBasicType(name: \"int32\", size: 32, encoding: DW_ATE_signed)\n!9 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: null)\n!10 = !DIDerivedType(tag: DW_TAG_pointer_type, baseType: !8)\n!11 = !{!12, !13, !14, !15, !16, !17}\n!12 = !DILocalVariable(name: \"arg1\", arg: 1, scope: !5, file: !1, type: !9)\n!13 = !DILocalVariable(name: \"arg2\", arg: 2, scope: !5, file: !1, type: !10)\n!14 = !DILocalVariable(name: \"arg3\", arg: 3, scope: !5, file: !1, type: !8)\n!15 = !DILocalVariable(name: \"arg4\", arg: 4, scope: !5, file: !1, type: !9)\n!16 = !DILocalVariable(name: \"arg5\", arg: 5, scope: !5, file: !1, type: !10)\n!17 = !DILocalVariable(name: \"arg6\", arg: 6, scope: !5, file: !1, type: !9)\n!18 = !DILocation(line: 0, scope: !5)\n!19 = !{!\"branch_weights\", i32 1048576, i32 1}\n!20 = !{!21, !21, i64 0}\n!21 = !{!\"ctx_ptr\", !22, i64 0}\n!22 = !{!\"tvm-tbaa\"}\n!23 = !{!24, !24, i64 0}\n!24 = !{!\"0x2dc73d0.w4.b0\", !25, i64 0}\n!25 = !{!\"0x2dc73d0.w8.b0\", !26, i64 0}\n!26 = !{!\"0x2dc73d0.w16.b0\", !27, i64 0}\n!27 = !{!\"0x2dc73d0.w32.b0\", !28, i64 0}\n!28 = !{!\"0x2dc73d0.w64.b0\", !29, i64 0}\n!29 = !{!\"0x2dc73d0.w128.b0\", !30, i64 0}\n!30 = !{!\"0x2dc73d0.w256.b0\", !31, i64 0}\n!31 = !{!\"0x2dc73d0.w512.b0\", !32, i64 0}\n!32 = !{!\"0x2dc73d0.w1024.b0\", !33, i64 0}\n!33 = !{!\"0x2dc73d0\", !22, i64 0}\n!34 = !{!35, !35, i64 0}\n!35 = !{!\"0x2dc73d0.w4.b4\", !25, i64 0}\n!36 = !{!37, !37, i64 0}\n!37 = !{!\"0x2dc73d0.w4.b8\", !38, i64 0}\n!38 = !{!\"0x2dc73d0.w8.b8\", !26, i64 0}\n!39 = !{!40, !40, i64 0}\n!40 = !{!\"0x35abcf0.w8.b0\", !41, i64 0}\n!41 = !{!\"0x35abcf0.w16.b0\", !42, i64 0}\n!42 = !{!\"0x35abcf0.w32.b0\", !43, i64 0}\n!43 = !{!\"0x35abcf0.w64.b0\", !44, i64 0}\n!44 = !{!\"0x35abcf0.w128.b0\", !45, i64 0}\n!45 = !{!\"0x35abcf0.w256.b0\", !46, i64 0}\n!46 = !{!\"0x35abcf0.w512.b0\", !47, i64 0}\n!47 = !{!\"0x35abcf0.w1024.b0\", !48, i64 0}\n!48 = !{!\"0x35abcf0\", !22, i64 0}\n!49 = !{!50, !50, i64 0}\n!50 = !{!\"0x35abcf0.w8.b8\", !41, i64 0}\n!51 = !{!52, !52, i64 0}\n!52 = !{!\"0x35abcf0.w8.b16\", !53, i64 0}\n!53 = !{!\"0x35abcf0.w16.b16\", !42, i64 0}\n!54 = !{!\"branch_weights\", i32 1, i32 1048576}\n!55 = !{!56, !56, i64 0}\n!56 = !{!\"0x2dc5d30.w8.b0\", !57, i64 0}\n!57 = !{!\"0x2dc5d30.w16.b0\", !58, i64 0}\n!58 = !{!\"0x2dc5d30.w32.b0\", !59, i64 0}\n!59 = !{!\"0x2dc5d30.w64.b0\", !60, i64 0}\n!60 = !{!\"0x2dc5d30.w128.b0\", !61, i64 0}\n!61 = !{!\"0x2dc5d30.w256.b0\", !62, i64 0}\n!62 = !{!\"0x2dc5d30.w512.b0\", !63, i64 0}\n!63 = !{!\"0x2dc5d30.w1024.b0\", !64, i64 0}\n!64 = !{!\"0x2dc5d30\", !22, i64 0}\n!65 = !{!66, !66, i64 0}\n!66 = !{!\"0x2dc5d30.w8.b8\", !57, i64 0}\n!67 = !{!68, !68, i64 0}\n!68 = !{!\"0x2dc5d30.w8.b16\", !69, i64 0}\n!69 = !{!\"0x2dc5d30.w16.b16\", !58, i64 0}\n!70 = !{!71, !71, i64 0}\n!71 = !{!\"0x35980a0.w8.b0\", !72, i64 0}\n!72 = !{!\"0x35980a0.w16.b0\", !73, i64 0}\n!73 = !{!\"0x35980a0.w32.b0\", !74, i64 0}\n!74 = !{!\"0x35980a0.w64.b0\", !75, i64 0}\n!75 = !{!\"0x35980a0.w128.b0\", !76, i64 0}\n!76 = !{!\"0x35980a0.w256.b0\", !77, i64 0}\n!77 = !{!\"0x35980a0.w512.b0\", !78, i64 0}\n!78 = !{!\"0x35980a0.w1024.b0\", !79, i64 0}\n!79 = !{!\"0x35980a0\", !22, i64 0}\n!80 = !{!81, !81, i64 0}\n!81 = !{!\"0x35980a0.w8.b8\", !72, i64 0}\n!82 = !{!83, !83, i64 0}\n!83 = !{!\"0x35980a0.w8.b16\", !84, i64 0}\n!84 = !{!\"0x35980a0.w16.b16\", !73, i64 0}\n!85 = !{!86, !86, i64 0}\n!86 = !{!\"0x2dcf830.w8.b0\", !87, i64 0}\n!87 = !{!\"0x2dcf830.w16.b0\", !88, i64 0}\n!88 = !{!\"0x2dcf830.w32.b0\", !89, i64 0}\n!89 = !{!\"0x2dcf830.w64.b0\", !90, i64 0}\n!90 = !{!\"0x2dcf830.w128.b0\", !91, i64 0}\n!91 = !{!\"0x2dcf830.w256.b0\", !92, i64 0}\n!92 = !{!\"0x2dcf830.w512.b0\", !93, i64 0}\n!93 = !{!\"0x2dcf830.w1024.b0\", !94, i64 0}\n!94 = !{!\"0x2dcf830\", !22, i64 0}\n!95 = !{!96, !96, i64 0}\n!96 = !{!\"0x2dcf830.w8.b8\", !87, i64 0}\n!97 = !{!98, !98, i64 0}\n!98 = !{!\"0x2dcf830.w8.b16\", !99, i64 0}\n!99 = !{!\"0x2dcf830.w16.b16\", !88, i64 0}\n!100 = !{!101, !101, i64 0}\n!101 = !{!\"0x2dd10a0.w8.b0\", !102, i64 0}\n!102 = !{!\"0x2dd10a0.w16.b0\", !103, i64 0}\n!103 = !{!\"0x2dd10a0.w32.b0\", !104, i64 0}\n!104 = !{!\"0x2dd10a0.w64.b0\", !105, i64 0}\n!105 = !{!\"0x2dd10a0.w128.b0\", !106, i64 0}\n!106 = !{!\"0x2dd10a0.w256.b0\", !107, i64 0}\n!107 = !{!\"0x2dd10a0.w512.b0\", !108, i64 0}\n!108 = !{!\"0x2dd10a0.w1024.b0\", !109, i64 0}\n!109 = !{!\"0x2dd10a0\", !22, i64 0}\n!110 = !{!111, !111, i64 0}\n!111 = !{!\"0x2dd10a0.w8.b8\", !102, i64 0}\n!112 = !{!113, !113, i64 0}\n!113 = !{!\"0x2dd10a0.w8.b16\", !114, i64 0}\n!114 = !{!\"0x2dd10a0.w16.b16\", !103, i64 0}\n!115 = !{!116, !116, i64 0}\n!116 = !{!\"0x398aff0.w8.b0\", !117, i64 0}\n!117 = !{!\"0x398aff0.w16.b0\", !118, i64 0}\n!118 = !{!\"0x398aff0.w32.b0\", !119, i64 0}\n!119 = !{!\"0x398aff0.w64.b0\", !120, i64 0}\n!120 = !{!\"0x398aff0.w128.b0\", !121, i64 0}\n!121 = !{!\"0x398aff0.w256.b0\", !122, i64 0}\n!122 = !{!\"0x398aff0.w512.b0\", !123, i64 0}\n!123 = !{!\"0x398aff0.w1024.b0\", !124, i64 0}\n!124 = !{!\"0x398aff0\", !22, i64 0}\n!125 = !{!126, !126, i64 0}\n!126 = !{!\"0x398aff0.w8.b8\", !117, i64 0}\n!127 = !{!128, !128, i64 0}\n!128 = !{!\"0x398aff0.w8.b16\", !129, i64 0}\n!129 = !{!\"0x398aff0.w16.b16\", !118, i64 0}\n!130 = !{!131, !131, i64 0}\n!131 = !{!\"0x2dce420.w4.b0\", !132, i64 0}\n!132 = !{!\"0x2dce420.w8.b0\", !133, i64 0}\n!133 = !{!\"0x2dce420.w16.b0\", !134, i64 0}\n!134 = !{!\"0x2dce420.w32.b0\", !135, i64 0}\n!135 = !{!\"0x2dce420.w64.b0\", !136, i64 0}\n!136 = !{!\"0x2dce420.w128.b0\", !137, i64 0}\n!137 = !{!\"0x2dce420.w256.b0\", !138, i64 0}\n!138 = !{!\"0x2dce420.w512.b0\", !139, i64 0}\n!139 = !{!\"0x2dce420.w1024.b0\", !140, i64 0}\n!140 = !{!\"0x2dce420\", !22, i64 0}\n!141 = !{!142, !142, i64 0}\n!142 = !{!\"0x2dce420.w4.b4\", !132, i64 0}\n!143 = !{!144, !144, i64 0}\n!144 = !{!\"0x2dce420.w4.b8\", !145, i64 0}\n!145 = !{!\"0x2dce420.w8.b8\", !133, i64 0}\n", "cuda_code": "\n#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n     (__CUDACC_VER_MAJOR__ > 11))\n#define TVM_ENABLE_L2_PREFETCH 1\n#else\n#define TVM_ENABLE_L2_PREFETCH 0\n#endif\n\n#ifdef _WIN32\n  using uint = unsigned int;\n  using uchar = unsigned char;\n  using ushort = unsigned short;\n  using int64_t = long long;\n  using uint64_t = unsigned long long;\n#else\n  #define uint unsigned int\n  #define uchar unsigned char\n  #define ushort unsigned short\n  #define int64_t long long\n  #define uint64_t unsigned long long\n#endif\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0);\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 19, 9), \"float32\"), T_add: T.Buffer((15, 19, 9), \"float32\"), compute: T.Buffer((15, 19, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2565,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(285):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_add_1 = T.Buffer((2565,), data=T_add.data)\n                T_add_1[cse_var_1] = T.cos(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2565):\n            compute_1 = T.Buffer((2565,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": []}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused_k_fused_l_fused = 0; i_j_fused_k_fused_l_fused < 960; ++i_j_fused_k_fused_l_fused) {\n    new_buffer[i_j_fused_k_fused_l_fused] = data[i_j_fused_k_fused_l_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 5, 6), \"float32\"), buffer: T.Buffer((16, 2, 5, 6), \"float32\"), new_buffer: T.Buffer((16, 2, 5, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused_k_fused_l_fused in T.parallel(960):\n            new_buffer_1 = T.Buffer((960,), data=new_buffer.data)\n            data_1 = T.Buffer((960,), data=data.data)\n            new_buffer_1[i_j_fused_k_fused_l_fused] = data_1[i_j_fused_k_fused_l_fused]", "op_args": [16, 2, 5, 6]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1200; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = logf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 10, 10), \"float32\"), compute: T.Buffer((3, 4, 10, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1200):\n            compute_1 = T.Buffer((1200,), data=compute.data)\n            data_1 = T.Buffer((1200,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [3, 4, 10, 10]}{"op_name": "conv2d_winograd_weight_transform", "c_code": "void default_function_kernel(float* data, float* transform_weight) {\n  #pragma omp parallel for\n  for (int32_t eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused = 0; eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused < 272; ++eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused) {\n    for (int32_t nu_outer_inner_init = 0; nu_outer_inner_init < 7; ++nu_outer_inner_init) {\n      for (int32_t eps_inner_init = 0; eps_inner_init < 7; ++eps_inner_init) {\n        transform_weight[(((eps_inner_init * 1904) + (nu_outer_inner_init * 272)) + eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t r_kh_outer = 0; r_kh_outer < 2; ++r_kh_outer) {\n      for (int32_t r_kw_outer = 0; r_kw_outer < 6; ++r_kw_outer) {\n        for (int32_t nu_outer_inner = 0; nu_outer_inner < 7; ++nu_outer_inner) {\n          for (int32_t r_kh_inner = 0; r_kh_inner < 3; ++r_kh_inner) {\n            for (int32_t eps_inner = 0; eps_inner < 7; ++eps_inner) {\n              int cse_var_27 = (nu_outer_inner == 0);\n              int cse_var_26 = (nu_outer_inner == 1);\n              int cse_var_25 = (nu_outer_inner == 2);\n              int cse_var_24 = (nu_outer_inner == 3);\n              int cse_var_23 = (nu_outer_inner == 4);\n              int cse_var_22 = (nu_outer_inner == 5);\n              int cse_var_21 = (r_kw_outer == 0);\n              int cse_var_20 = (r_kw_outer == 1);\n              int cse_var_19 = (r_kw_outer == 2);\n              int cse_var_18 = (r_kw_outer == 3);\n              int cse_var_17 = (r_kw_outer == 4);\n              int cse_var_16 = (r_kw_outer == 5);\n              int cse_var_15 = (nu_outer_inner == 6);\n              int cse_var_14 = (eps_inner == 0);\n              int cse_var_13 = (eps_inner == 1);\n              int cse_var_12 = (eps_inner == 2);\n              int cse_var_11 = (eps_inner == 3);\n              int cse_var_10 = (eps_inner == 4);\n              int cse_var_9 = (eps_inner == 5);\n              int cse_var_8 = (eps_inner == 6);\n              int cse_var_7 = (((r_kh_outer * 3) + r_kh_inner) == 0);\n              int cse_var_6 = (((r_kh_outer * 3) + r_kh_inner) == 1);\n              int cse_var_5 = (((r_kh_outer * 3) + r_kh_inner) == 2);\n              int cse_var_4 = (((r_kh_outer * 3) + r_kh_inner) == 3);\n              int cse_var_3 = (((r_kh_outer * 3) + r_kh_inner) == 4);\n              int cse_var_2 = (((r_kh_outer * 3) + r_kh_inner) == 5);\n              transform_weight[(((eps_inner * 1904) + (nu_outer_inner * 272)) + eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused)] = (transform_weight[(((eps_inner * 1904) + (nu_outer_inner * 272)) + eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused)] + ((data[((((eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused * 36) + (r_kh_outer * 18)) + (r_kh_inner * 6)) + r_kw_outer)] * ((cse_var_8 && cse_var_2) ? 1.000000e+00f : ((cse_var_8 && cse_var_3) ? 0.000000e+00f : ((cse_var_8 && cse_var_4) ? 0.000000e+00f : ((cse_var_8 && cse_var_5) ? 0.000000e+00f : ((cse_var_8 && cse_var_6) ? 0.000000e+00f : ((cse_var_8 && cse_var_7) ? 0.000000e+00f : ((cse_var_9 && cse_var_2) ? 5.555556e-02f : ((cse_var_9 && cse_var_3) ? -1.111111e-01f : ((cse_var_9 && cse_var_4) ? 2.222222e-01f : ((cse_var_9 && cse_var_5) ? -4.444444e-01f : ((cse_var_9 && cse_var_6) ? 8.888889e-01f : ((cse_var_9 && cse_var_7) ? -1.777778e+00f : ((cse_var_10 && cse_var_2) ? 1.422222e+00f : ((cse_var_10 && cse_var_3) ? -7.111111e-01f : ((cse_var_10 && cse_var_4) ? 3.555556e-01f : ((cse_var_10 && cse_var_5) ? -1.777778e-01f : ((cse_var_10 && cse_var_6) ? 8.888889e-02f : ((cse_var_10 && cse_var_7) ? -4.444445e-02f : ((cse_var_11 && cse_var_2) ? -3.333334e-02f : ((cse_var_11 && cse_var_3) ? -6.666667e-02f : ((cse_var_11 && cse_var_4) ? -1.333333e-01f : ((cse_var_11 && cse_var_5) ? -2.666667e-01f : ((cse_var_11 && cse_var_6) ? -5.333334e-01f : ((cse_var_11 && cse_var_7) ? -1.066667e+00f : ((cse_var_12 && cse_var_2) ? 2.222222e-01f : ((cse_var_12 && cse_var_3) ? 2.222222e-01f : ((cse_var_12 && cse_var_4) ? 2.222222e-01f : ((cse_var_12 && cse_var_5) ? 2.222222e-01f : ((cse_var_12 && cse_var_6) ? 2.222222e-01f : ((cse_var_12 && cse_var_7) ? 2.222222e-01f : ((cse_var_13 && cse_var_2) ? -6.666667e-01f : ((cse_var_13 && cse_var_3) ? 6.666667e-01f : ((cse_var_13 && cse_var_4) ? -6.666667e-01f : ((cse_var_13 && cse_var_5) ? 6.666667e-01f : ((cse_var_13 && cse_var_6) ? -6.666667e-01f : ((cse_var_13 && cse_var_7) ? 6.666667e-01f : ((cse_var_14 && cse_var_2) ? 0.000000e+00f : ((cse_var_14 && cse_var_3) ? 0.000000e+00f : ((cse_var_14 && cse_var_4) ? 0.000000e+00f : ((cse_var_14 && cse_var_5) ? 0.000000e+00f : ((cse_var_14 && cse_var_6) ? 0.000000e+00f : ((cse_var_14 && cse_var_7) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))))))))))))))))))))))))) * ((cse_var_15 && cse_var_16) ? 1.000000e+00f : ((cse_var_15 && cse_var_17) ? 0.000000e+00f : ((cse_var_15 && cse_var_18) ? 0.000000e+00f : ((cse_var_15 && cse_var_19) ? 0.000000e+00f : ((cse_var_15 && cse_var_20) ? 0.000000e+00f : ((cse_var_15 && cse_var_21) ? 0.000000e+00f : ((cse_var_22 && cse_var_16) ? 5.555556e-02f : ((cse_var_22 && cse_var_17) ? -1.111111e-01f : ((cse_var_22 && cse_var_18) ? 2.222222e-01f : ((cse_var_22 && cse_var_19) ? -4.444444e-01f : ((cse_var_22 && cse_var_20) ? 8.888889e-01f : ((cse_var_22 && cse_var_21) ? -1.777778e+00f : ((cse_var_23 && cse_var_16) ? 1.422222e+00f : ((cse_var_23 && cse_var_17) ? -7.111111e-01f : ((cse_var_23 && cse_var_18) ? 3.555556e-01f : ((cse_var_23 && cse_var_19) ? -1.777778e-01f : ((cse_var_23 && cse_var_20) ? 8.888889e-02f : ((cse_var_23 && cse_var_21) ? -4.444445e-02f : ((cse_var_24 && cse_var_16) ? -3.333334e-02f : ((cse_var_24 && cse_var_17) ? -6.666667e-02f : ((cse_var_24 && cse_var_18) ? -1.333333e-01f : ((cse_var_24 && cse_var_19) ? -2.666667e-01f : ((cse_var_24 && cse_var_20) ? -5.333334e-01f : ((cse_var_24 && cse_var_21) ? -1.066667e+00f : ((cse_var_25 && cse_var_16) ? 2.222222e-01f : ((cse_var_25 && cse_var_17) ? 2.222222e-01f : ((cse_var_25 && cse_var_18) ? 2.222222e-01f : ((cse_var_25 && cse_var_19) ? 2.222222e-01f : ((cse_var_25 && cse_var_20) ? 2.222222e-01f : ((cse_var_25 && cse_var_21) ? 2.222222e-01f : ((cse_var_26 && cse_var_16) ? -6.666667e-01f : ((cse_var_26 && cse_var_17) ? 6.666667e-01f : ((cse_var_26 && cse_var_18) ? -6.666667e-01f : ((cse_var_26 && cse_var_19) ? 6.666667e-01f : ((cse_var_26 && cse_var_20) ? -6.666667e-01f : ((cse_var_26 && cse_var_21) ? 6.666667e-01f : ((cse_var_27 && cse_var_16) ? 0.000000e+00f : ((cse_var_27 && cse_var_17) ? 0.000000e+00f : ((cse_var_27 && cse_var_18) ? 0.000000e+00f : ((cse_var_27 && cse_var_19) ? 0.000000e+00f : ((cse_var_27 && cse_var_20) ? 0.000000e+00f : ((cse_var_27 && cse_var_21) ? 2.000000e+00f : 0.000000e+00f))))))))))))))))))))))))))))))))))))))))))));\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(238) default_function_kernel(float* __restrict__ data, float* __restrict__ transform_weight) {\n  float transform_weight_local[28];\n  __shared__ float data_shared[4896];\n  __shared__ float G_shared[42];\n  for (int eps_c_inner_init = 0; eps_c_inner_init < 7; ++eps_c_inner_init) {\n    for (int ci_c_inner_init = 0; ci_c_inner_init < 4; ++ci_c_inner_init) {\n      transform_weight_local[((eps_c_inner_init * 4) + ci_c_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer < 21; ++ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 7) + (((int)threadIdx.x) / 34)) < 144) {\n      data_shared[((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 238) + ((int)threadIdx.x))] = data[((((((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 119) + (((int)threadIdx.x) >> 1)) / 144) * 576) + (((int)blockIdx.x) * 288)) + (((ax0_ax1_fused_ax2_fused_ax3_fused_outer_outer * 238) + ((int)threadIdx.x)) % 288))];\n    }\n  }\n  if (((int)threadIdx.x) < 42) {\n    G_shared[((int)threadIdx.x)] = ((((int)threadIdx.x) == 41) ? 1.000000e+00f : ((((int)threadIdx.x) == 40) ? 0.000000e+00f : ((((int)threadIdx.x) == 39) ? 0.000000e+00f : ((((int)threadIdx.x) == 38) ? 0.000000e+00f : ((((int)threadIdx.x) == 37) ? 0.000000e+00f : ((((int)threadIdx.x) == 36) ? 0.000000e+00f : ((((int)threadIdx.x) == 35) ? 5.555556e-02f : ((((int)threadIdx.x) == 34) ? -1.111111e-01f : ((((int)threadIdx.x) == 33) ? 2.222222e-01f : ((((int)threadIdx.x) == 32) ? -4.444444e-01f : ((((int)threadIdx.x) == 31) ? 8.888889e-01f : ((((int)threadIdx.x) == 30) ? -1.777778e+00f : ((((int)threadIdx.x) == 29) ? 1.422222e+00f : ((((int)threadIdx.x) == 28) ? -7.111111e-01f : ((((int)threadIdx.x) == 27) ? 3.555556e-01f : ((((int)threadIdx.x) == 26) ? -1.777778e-01f : ((((int)threadIdx.x) == 25) ? 8.888889e-02f : ((((int)threadIdx.x) == 24) ? -4.444445e-02f : ((((int)threadIdx.x) == 23) ? -3.333334e-02f : ((((int)threadIdx.x) == 22) ? -6.666667e-02f : ((((int)threadIdx.x) == 21) ? -1.333333e-01f : ((((int)threadIdx.x) == 20) ? -2.666667e-01f : ((((int)threadIdx.x) == 19) ? -5.333334e-01f : ((((int)threadIdx.x) == 18) ? -1.066667e+00f : ((((int)threadIdx.x) == 17) ? 2.222222e-01f : ((((int)threadIdx.x) == 16) ? 2.222222e-01f : ((((int)threadIdx.x) == 15) ? 2.222222e-01f : ((((int)threadIdx.x) == 14) ? 2.222222e-01f : ((((int)threadIdx.x) == 13) ? 2.222222e-01f : ((((int)threadIdx.x) == 12) ? 2.222222e-01f : ((((int)threadIdx.x) == 11) ? -6.666667e-01f : ((((int)threadIdx.x) == 10) ? 6.666667e-01f : ((((int)threadIdx.x) == 9) ? -6.666667e-01f : ((((int)threadIdx.x) == 8) ? 6.666667e-01f : ((((int)threadIdx.x) == 7) ? -6.666667e-01f : ((((int)threadIdx.x) == 6) ? 6.666667e-01f : ((((int)threadIdx.x) == 5) ? 0.000000e+00f : ((((int)threadIdx.x) == 4) ? 0.000000e+00f : ((((int)threadIdx.x) == 3) ? 0.000000e+00f : ((((int)threadIdx.x) == 2) ? 0.000000e+00f : ((((int)threadIdx.x) == 1) ? 0.000000e+00f : 2.000000e+00f)))))))))))))))))))))))))))))))))))))))));\n  }\n  __syncthreads();\n  for (int r_kh_outer_inner = 0; r_kh_outer_inner < 2; ++r_kh_outer_inner) {\n    for (int r_kh_inner = 0; r_kh_inner < 3; ++r_kh_inner) {\n      for (int r_kw_inner = 0; r_kw_inner < 6; ++r_kw_inner) {\n        for (int eps_c_inner = 0; eps_c_inner < 7; ++eps_c_inner) {\n          for (int ci_c_inner = 0; ci_c_inner < 4; ++ci_c_inner) {\n            transform_weight_local[((eps_c_inner * 4) + ci_c_inner)] = (transform_weight_local[((eps_c_inner * 4) + ci_c_inner)] + ((data_shared[((((((((int)threadIdx.x) % 34) * 144) + (ci_c_inner * 36)) + (r_kh_outer_inner * 18)) + (r_kh_inner * 6)) + r_kw_inner)] * G_shared[(((eps_c_inner * 6) + (r_kh_outer_inner * 3)) + r_kh_inner)]) * G_shared[(((((int)threadIdx.x) / 34) * 6) + r_kw_inner)]));\n          }\n        }\n      }\n    }\n  }\n  for (int eps_inner = 0; eps_inner < 7; ++eps_inner) {\n    for (int ci_inner = 0; ci_inner < 4; ++ci_inner) {\n      transform_weight[(((((eps_inner * 1904) + ((((int)threadIdx.x) >> 1) * 16)) + (((int)blockIdx.x) * 8)) + ((((int)threadIdx.x) & 1) * 4)) + ci_inner)] = transform_weight_local[((eps_inner * 4) + ci_inner)];\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 16, 6, 6), \"float32\"), transform_weight: T.Buffer((7, 7, 17, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused in T.parallel(272):\n            transform_weight_1 = T.Buffer((13328,), data=transform_weight.data)\n            for nu_outer_inner_init, eps_inner_init in T.grid(7, 7):\n                transform_weight_1[eps_inner_init * 1904 + nu_outer_inner_init * 272 + eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused] = T.float32(0)\n            for r_kh_outer, r_kw_outer, nu_outer_inner, r_kh_inner, eps_inner in T.grid(2, 6, 7, 3, 7):\n                cse_var_28: T.int32 = r_kh_outer * 3 + r_kh_inner\n                cse_var_27: T.bool = nu_outer_inner == 0\n                cse_var_26: T.bool = nu_outer_inner == 1\n                cse_var_25: T.bool = nu_outer_inner == 2\n                cse_var_24: T.bool = nu_outer_inner == 3\n                cse_var_23: T.bool = nu_outer_inner == 4\n                cse_var_22: T.bool = nu_outer_inner == 5\n                cse_var_21: T.bool = r_kw_outer == 0\n                cse_var_20: T.bool = r_kw_outer == 1\n                cse_var_19: T.bool = r_kw_outer == 2\n                cse_var_18: T.bool = r_kw_outer == 3\n                cse_var_17: T.bool = r_kw_outer == 4\n                cse_var_16: T.bool = r_kw_outer == 5\n                cse_var_15: T.bool = nu_outer_inner == 6\n                cse_var_14: T.bool = eps_inner == 0\n                cse_var_13: T.bool = eps_inner == 1\n                cse_var_12: T.bool = eps_inner == 2\n                cse_var_11: T.bool = eps_inner == 3\n                cse_var_10: T.bool = eps_inner == 4\n                cse_var_9: T.bool = eps_inner == 5\n                cse_var_8: T.bool = eps_inner == 6\n                cse_var_7: T.bool = cse_var_28 == 0\n                cse_var_6: T.bool = cse_var_28 == 1\n                cse_var_5: T.bool = cse_var_28 == 2\n                cse_var_4: T.bool = cse_var_28 == 3\n                cse_var_3: T.bool = cse_var_28 == 4\n                cse_var_2: T.bool = cse_var_28 == 5\n                cse_var_1: T.int32 = eps_inner * 1904 + nu_outer_inner * 272 + eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused\n                data_1 = T.Buffer((9792,), data=data.data)\n                transform_weight_1[cse_var_1] = transform_weight_1[cse_var_1] + data_1[eps_outer_outer_outer_nu_outer_outer_outer_fused_co_outer_outer_outer_fused_ci_outer_outer_outer_fused_eps_outer_outer_inner_fused_nu_outer_outer_inner_fused_co_outer_outer_inner_fused_ci_outer_outer_inner_fused * 36 + r_kh_outer * 18 + r_kh_inner * 6 + r_kw_outer] * T.Select(cse_var_8 and cse_var_2, T.float32(1), T.Select(cse_var_8 and cse_var_3, T.float32(0), T.Select(cse_var_8 and cse_var_4, T.float32(0), T.Select(cse_var_8 and cse_var_5, T.float32(0), T.Select(cse_var_8 and cse_var_6, T.float32(0), T.Select(cse_var_8 and cse_var_7, T.float32(0), T.Select(cse_var_9 and cse_var_2, T.float32(0.0555555559694767), T.Select(cse_var_9 and cse_var_3, T.float32(-0.1111111119389534), T.Select(cse_var_9 and cse_var_4, T.float32(0.2222222238779068), T.Select(cse_var_9 and cse_var_5, T.float32(-0.4444444477558136), T.Select(cse_var_9 and cse_var_6, T.float32(0.8888888955116272), T.Select(cse_var_9 and cse_var_7, T.float32(-1.7777777910232544), T.Select(cse_var_10 and cse_var_2, T.float32(1.4222222566604614), T.Select(cse_var_10 and cse_var_3, T.float32(-0.71111112833023071), T.Select(cse_var_10 and cse_var_4, T.float32(0.35555556416511536), T.Select(cse_var_10 and cse_var_5, T.float32(-0.17777778208255768), T.Select(cse_var_10 and cse_var_6, T.float32(0.088888891041278839), T.Select(cse_var_10 and cse_var_7, T.float32(-0.04444444552063942), T.Select(cse_var_11 and cse_var_2, T.float32(-0.033333335071802139), T.Select(cse_var_11 and cse_var_3, T.float32(-0.066666670143604279), T.Select(cse_var_11 and cse_var_4, T.float32(-0.13333334028720856), T.Select(cse_var_11 and cse_var_5, T.float32(-0.26666668057441711), T.Select(cse_var_11 and cse_var_6, T.float32(-0.53333336114883423), T.Select(cse_var_11 and cse_var_7, T.float32(-1.0666667222976685), T.Select(cse_var_12 and cse_var_2, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_3, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_4, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_5, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_6, T.float32(0.2222222238779068), T.Select(cse_var_12 and cse_var_7, T.float32(0.2222222238779068), T.Select(cse_var_13 and cse_var_2, T.float32(-0.66666668653488159), T.Select(cse_var_13 and cse_var_3, T.float32(0.66666668653488159), T.Select(cse_var_13 and cse_var_4, T.float32(-0.66666668653488159), T.Select(cse_var_13 and cse_var_5, T.float32(0.66666668653488159), T.Select(cse_var_13 and cse_var_6, T.float32(-0.66666668653488159), T.Select(cse_var_13 and cse_var_7, T.float32(0.66666668653488159), T.Select(cse_var_14 and cse_var_2, T.float32(0), T.Select(cse_var_14 and cse_var_3, T.float32(0), T.Select(cse_var_14 and cse_var_4, T.float32(0), T.Select(cse_var_14 and cse_var_5, T.float32(0), T.Select(cse_var_14 and cse_var_6, T.float32(0), T.Select(cse_var_14 and cse_var_7, T.float32(2), T.float32(0))))))))))))))))))))))))))))))))))))))))))) * T.Select(cse_var_15 and cse_var_16, T.float32(1), T.Select(cse_var_15 and cse_var_17, T.float32(0), T.Select(cse_var_15 and cse_var_18, T.float32(0), T.Select(cse_var_15 and cse_var_19, T.float32(0), T.Select(cse_var_15 and cse_var_20, T.float32(0), T.Select(cse_var_15 and cse_var_21, T.float32(0), T.Select(cse_var_22 and cse_var_16, T.float32(0.0555555559694767), T.Select(cse_var_22 and cse_var_17, T.float32(-0.1111111119389534), T.Select(cse_var_22 and cse_var_18, T.float32(0.2222222238779068), T.Select(cse_var_22 and cse_var_19, T.float32(-0.4444444477558136), T.Select(cse_var_22 and cse_var_20, T.float32(0.8888888955116272), T.Select(cse_var_22 and cse_var_21, T.float32(-1.7777777910232544), T.Select(cse_var_23 and cse_var_16, T.float32(1.4222222566604614), T.Select(cse_var_23 and cse_var_17, T.float32(-0.71111112833023071), T.Select(cse_var_23 and cse_var_18, T.float32(0.35555556416511536), T.Select(cse_var_23 and cse_var_19, T.float32(-0.17777778208255768), T.Select(cse_var_23 and cse_var_20, T.float32(0.088888891041278839), T.Select(cse_var_23 and cse_var_21, T.float32(-0.04444444552063942), T.Select(cse_var_24 and cse_var_16, T.float32(-0.033333335071802139), T.Select(cse_var_24 and cse_var_17, T.float32(-0.066666670143604279), T.Select(cse_var_24 and cse_var_18, T.float32(-0.13333334028720856), T.Select(cse_var_24 and cse_var_19, T.float32(-0.26666668057441711), T.Select(cse_var_24 and cse_var_20, T.float32(-0.53333336114883423), T.Select(cse_var_24 and cse_var_21, T.float32(-1.0666667222976685), T.Select(cse_var_25 and cse_var_16, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_17, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_18, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_19, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_20, T.float32(0.2222222238779068), T.Select(cse_var_25 and cse_var_21, T.float32(0.2222222238779068), T.Select(cse_var_26 and cse_var_16, T.float32(-0.66666668653488159), T.Select(cse_var_26 and cse_var_17, T.float32(0.66666668653488159), T.Select(cse_var_26 and cse_var_18, T.float32(-0.66666668653488159), T.Select(cse_var_26 and cse_var_19, T.float32(0.66666668653488159), T.Select(cse_var_26 and cse_var_20, T.float32(-0.66666668653488159), T.Select(cse_var_26 and cse_var_21, T.float32(0.66666668653488159), T.Select(cse_var_27 and cse_var_16, T.float32(0), T.Select(cse_var_27 and cse_var_17, T.float32(0), T.Select(cse_var_27 and cse_var_18, T.float32(0), T.Select(cse_var_27 and cse_var_19, T.float32(0), T.Select(cse_var_27 and cse_var_20, T.float32(0), T.Select(cse_var_27 and cse_var_21, T.float32(2), T.float32(0)))))))))))))))))))))))))))))))))))))))))))", "op_args": [17, 16, 9, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_add[(((ax0 * 171) + (ax1 * 9)) + ax2)] = (cosf(ph_0[(((ax0 * 171) + (ax1 * 9)) + ax2)]) + ph_0[(((ax0 * 171) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 171) + (i1 * 9)) + i2)] = sinf(ph_0[(((i0 * 171) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 19, 9), \"float32\"), T_add: T.Buffer((15, 19, 9), \"float32\"), compute: T.Buffer((15, 19, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2565,), data=ph_0.data)\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(19, 9):\n                cse_var_1: T.int32 = ax0 * 171 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((2565,), data=T_add.data)\n                T_add_1[cse_var_1] = T.cos(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(19, 9):\n                cse_var_2: T.int32 = i0 * 171 + i1 * 9 + i2\n                compute_1 = T.Buffer((2565,), data=compute.data)\n                compute_1[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": []}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2016; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 20) + i3)] = log10f(data[((i0_i1_fused_i2_fused * 20) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 8, 20), \"float32\"), compute: T.Buffer((18, 14, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2016):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((40320,), data=compute.data)\n                data_1 = T.Buffer((40320,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [18, 14, 8, 20]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 114; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log2f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 1)) < 57) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 1, 1, 6), \"float32\"), compute: T.Buffer((19, 1, 1, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(114):\n            compute_1 = T.Buffer((114,), data=compute.data)\n            data_1 = T.Buffer((114,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log2(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [19, 1, 1, 6]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 104; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      depth_to_space[((i0_i1_fused_i2_fused * 10) + i3)] = data[((((((i0_i1_fused_i2_fused / 26) * 325) + (((i0_i1_fused_i2_fused % 26) % 2) * 130)) + ((i3 % 2) * 65)) + (((i0_i1_fused_i2_fused % 26) / 2) * 5)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) / 10) * 325) + ((((((((int)blockIdx.x) % 10) * 13) + (((int)threadIdx.x) >> 1)) / 5) % 2) * 130)) + (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 10) % 2) * 65)) + ((((((((int)blockIdx.x) % 10) * 13) + (((int)threadIdx.x) >> 1)) / 5) / 2) * 5)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 10) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 5, 13, 5), \"float32\"), depth_to_space: T.Buffer((4, 1, 26, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(104):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 26\n                depth_to_space_1 = T.Buffer((1040,), data=depth_to_space.data)\n                data_1 = T.Buffer((1300,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 10 + i3] = data_1[i0_i1_fused_i2_fused // 26 * 325 + T.truncmod(cse_var_1, 2) * 130 + T.truncmod(i3, 2) * 65 + T.Div(cse_var_1, 2) * 5 + T.Div(i3, 2)]", "op_args": [4, 5, 13, 5]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        compute[(((i0_i1_fused * 48) + (i2 * 4)) + i3)] = ((0.000000e+00f < data[(((i0_i1_fused * 48) + (i2 * 4)) + i3)]) ? data[(((i0_i1_fused * 48) + (i2 * 4)) + i3)] : (data[(((i0_i1_fused * 48) + (i2 * 4)) + i3)] * 5.000000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 3, 12, 4), \"float32\"), compute: T.Buffer((16, 3, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2, i3 in T.grid(12, 4):\n                cse_var_1: T.int32 = i0_i1_fused * 48 + i2 * 4 + i3\n                compute_1 = T.Buffer((2304,), data=compute.data)\n                data_1 = T.Buffer((2304,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [16, 3, 12, 4]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[8];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 8; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2178; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 8; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 8) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 8; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 11; ++k0) {\n    for (int k1 = 0; k1 < 16; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 9; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 1584) + (k1 * 99)) + (k2 * 9)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 16, 11, 9), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([8], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((8,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(8):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2178, 8):\n            data_1 = T.Buffer((17424,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 8 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(8):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [11, 16, 11, 9]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused < 4; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused) {\n    float T_softmax_maxelem[280];\n    float compute_1[280];\n    for (int32_t i0 = 0; i0 < 5; ++i0) {\n      for (int32_t i1 = 0; i1 < 8; ++i1) {\n        for (int32_t i2 = 0; i2 < 7; ++i2) {\n          T_softmax_maxelem[(((i0 * 56) + (i1 * 7)) + i2)] = -3.402823e+38f;\n          for (int32_t k = 0; k < 14; ++k) {\n            T_softmax_maxelem[(((i0 * 56) + (i1 * 7)) + i2)] = max(T_softmax_maxelem[(((i0 * 56) + (i1 * 7)) + i2)], data[((((((i0 * 3136) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 1) * 1568)) + (i1 * 196)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 1) * 98)) + (i2 * 14)) + k)]);\n          }\n        }\n      }\n    }\n    for (int32_t i0_1 = 0; i0_1 < 5; ++i0_1) {\n      for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n          compute_1[(((i0_1 * 56) + (i1_1 * 7)) + i2_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 14; ++k_1) {\n            compute_1[(((i0_1 * 56) + (i1_1 * 7)) + i2_1)] = (compute_1[(((i0_1 * 56) + (i1_1 * 7)) + i2_1)] + expf((data[((((((i0_1 * 3136) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 1) * 1568)) + (i1_1 * 196)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 1) * 98)) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[(((i0_1 * 56) + (i1_1 * 7)) + i2_1)])));\n          }\n        }\n      }\n    }\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 14; ++i3_outer_outer_inner) {\n      for (int32_t i1_outer_inner = 0; i1_outer_inner < 8; ++i1_outer_inner) {\n        for (int32_t i0_inner = 0; i0_inner < 5; ++i0_inner) {\n          for (int32_t i2_inner = 0; i2_inner < 7; ++i2_inner) {\n            compute[((((((i0_inner * 3136) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 1) * 1568)) + (i1_outer_inner * 196)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 1) * 98)) + (i2_inner * 14)) + i3_outer_outer_inner)] = ((data[((((((i0_inner * 3136) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused >> 1) * 1568)) + (i1_outer_inner * 196)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused & 1) * 98)) + (i2_inner * 14)) + i3_outer_outer_inner)] - T_softmax_maxelem[(((i0_inner * 56) + (i1_outer_inner * 7)) + i2_inner)]) - logf(compute_1[(((i0_inner * 56) + (i1_outer_inner * 7)) + i2_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 14; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 112) + (((int)threadIdx.x) * 14)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 14; ++k) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 448) + (((int)threadIdx.x) * 14)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 14))]) - __logf(compute_1[((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 14))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 14, 14), \"float32\"), compute: T.Buffer((5, 16, 14, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused in T.parallel(4):\n            T_softmax_maxelem = T.allocate([280], \"float32\", \"global\")\n            compute_1 = T.allocate([280], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((280,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((15680,), data=data.data)\n            for i0, i1, i2 in T.grid(5, 8, 7):\n                T_softmax_maxelem_1[i0 * 56 + i1 * 7 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(14):\n                    cse_var_1: T.int32 = i0 * 56 + i1 * 7 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 3136 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 2 * 1568 + i1 * 196 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 2 * 98 + i2 * 14 + k])\n            compute_2 = T.Buffer((280,), data=compute_1)\n            for i0, i1, i2 in T.grid(5, 8, 7):\n                compute_2[i0 * 56 + i1 * 7 + i2] = T.float32(0)\n                for k in range(14):\n                    cse_var_2: T.int32 = i0 * 56 + i1 * 7 + i2\n                    compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0 * 3136 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 2 * 1568 + i1 * 196 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 2 * 98 + i2 * 14 + k] - T_softmax_maxelem_1[cse_var_2])\n            for i3_outer_outer_inner, i1_outer_inner, i0_inner, i2_inner in T.grid(14, 8, 5, 7):\n                cse_var_4: T.int32 = i0_inner * 56 + i1_outer_inner * 7 + i2_inner\n                cse_var_3: T.int32 = i0_inner * 3136 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused // 2 * 1568 + i1_outer_inner * 196 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused % 2 * 98 + i2_inner * 14 + i3_outer_outer_inner\n                compute_3 = T.Buffer((15680,), data=compute.data)\n                compute_3[cse_var_3] = data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4] - T.log(compute_2[cse_var_4])", "op_args": [5, 16, 14, 14]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[56];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 50; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 56; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 56) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 88; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 175) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 14, 1, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([56], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((56,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(56):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(50, 56):\n            data_1 = T.Buffer((2800,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 56 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(56):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [20, 14, 1, 10]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    float tensor[38];\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          tensor[((ax2 * 2) + ax3)] = 0.000000e+00f;\n          tensor[((ax2 * 2) + ax3)] = (tensor[((ax2 * 2) + ax3)] + (data[((((ax0 * 608) + (ax1 * 38)) + (ax2 * 2)) + ax3)] * data[((((ax0 * 608) + (ax1 * 38)) + (ax2 * 2)) + ax3)]));\n        }\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 19; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 2; ++ax3_1) {\n          T_divide[((((ax0 * 608) + (ax1 * 38)) + (ax2_1 * 2)) + ax3_1)] = (data[((((ax0 * 608) + (ax1 * 38)) + (ax2_1 * 2)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax2_1 * 2) + ax3_1)])), 7.500000e-01f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 16, 19, 2), \"float32\"), T_divide: T.Buffer((18, 16, 19, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(18):\n            tensor = T.allocate([38], \"float32\", \"global\")\n            for ax1 in range(16):\n                tensor_1 = T.Buffer((38,), data=tensor)\n                data_1 = T.Buffer((10944,), data=data.data)\n                for ax2, ax3 in T.grid(19, 2):\n                    cse_var_3: T.int32 = ax2 * 2\n                    cse_var_2: T.int32 = cse_var_3 + ax3\n                    cse_var_1: T.int32 = ax0 * 608 + ax1 * 38 + cse_var_3 + ax3\n                    tensor_1[cse_var_2] = T.float32(0)\n                    tensor_1[cse_var_2] = tensor_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax2, ax3 in T.grid(19, 2):\n                    cse_var_5: T.int32 = ax2 * 2\n                    cse_var_4: T.int32 = ax0 * 608 + ax1 * 38 + cse_var_5 + ax3\n                    T_divide_1 = T.Buffer((10944,), data=T_divide.data)\n                    T_divide_1[cse_var_4] = data_1[cse_var_4] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[cse_var_5 + ax3], T.float32(0.75))", "op_args": [18, 16, 19, 2]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 56; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3)] = (data[((i0_i1_fused_i2_fused * 4) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 4, 4), \"float32\"), compute: T.Buffer((7, 2, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(56):\n            for i3 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3\n                compute_1 = T.Buffer((224,), data=compute.data)\n                data_1 = T.Buffer((224,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [7, 2, 4, 4]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((144 <= i0_i1_fused) ? (22 - (i0_i1_fused / 12)) : ((i0_i1_fused < 12) ? 0 : ((i0_i1_fused / 12) - 1))) * 9) + (((i0_i1_fused % 12) == 11) ? 8 : (((i0_i1_fused % 12) < 2) ? (1 - (i0_i1_fused % 12)) : ((i0_i1_fused % 12) - 2))))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = data[((((72 <= ((int)blockIdx.x)) ? (22 - (((int)blockIdx.x) / 6)) : ((((int)blockIdx.x) < 6) ? 0 : ((((int)blockIdx.x) / 6) - 1))) * 9) + (((((((int)blockIdx.x) % 6) * 2) + ((int)threadIdx.x)) == 11) ? ((19 - ((int)threadIdx.x)) - ((((int)blockIdx.x) % 6) * 2)) : (((((int)blockIdx.x) % 6) < 1) ? ((1 - ((int)threadIdx.x)) - ((((int)blockIdx.x) % 6) * 2)) : ((((((int)blockIdx.x) % 6) * 2) + ((int)threadIdx.x)) - 2))))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 9), \"float32\"), MirrorPadInput: T.Buffer((14, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(168):\n            cse_var_2: T.int32 = i0_i1_fused // 12\n            cse_var_1: T.int32 = i0_i1_fused % 12\n            MirrorPadInput_1 = T.Buffer((168,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((99,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(144 <= i0_i1_fused, 22 - cse_var_2, T.if_then_else(i0_i1_fused < 12, 0, cse_var_2 - 1)) * 9 + T.if_then_else(cse_var_1 == 11, 8, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]", "op_args": [9, 15, 11, 9]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[20];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 918; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 20; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 20) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 6; ++k0) {\n    for (int k1 = 0; k1 < 12; ++k1) {\n      for (int k2 = 0; k2 < 15; ++k2) {\n        for (int k3 = 0; k3 < 17; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 3060) + (k1 * 255)) + (k2 * 17)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 15, 17), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([20], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((20,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(20):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(918, 20):\n            data_1 = T.Buffer((18360,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 20 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(20):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [6, 12, 15, 17]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 192; ++i0_i1_fused) {\n    PadInput[i0_i1_fused] = (((((16 <= i0_i1_fused) && (i0_i1_fused < 160)) && (2 <= (i0_i1_fused & 15))) && ((i0_i1_fused & 15) < 15)) ? data[((((i0_i1_fused >> 4) * 13) + (i0_i1_fused & 15)) - 15)] : 0.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (((((4 <= ((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 2))) && (((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 2)) < 40)) && (1 <= (((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 1)) & 7))) && ((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) & 15) < 15)) ? data[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 2)) >> 2) * 13) + (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) & 15)) - 15)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 13), \"float32\"), PadInput: T.Buffer((12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(192):\n            cse_var_1: T.int32 = i0_i1_fused % 16\n            PadInput_1 = T.Buffer((192,), data=PadInput.data)\n            data_1 = T.Buffer((117,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(16 <= i0_i1_fused and i0_i1_fused < 160 and 2 <= cse_var_1 and cse_var_1 < 15, data_1[i0_i1_fused // 16 * 13 + cse_var_1 - 15], T.float32(0))", "op_args": [9, 10, 9, 13]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 45) + (i1 * 3)) + i2)] = roundf(data[(((i0 * 45) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 15, 3, 1), \"float32\"), compute: T.Buffer((9, 15, 3, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(15, 3):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 3 + i2\n                compute_1 = T.Buffer((405,), data=compute.data)\n                data_1 = T.Buffer((405,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [9, 15, 3, 1]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    float pad_temp[3];\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n        pad_temp[ax2_s] = ((1 <= ax2_s) ? data[((((ax0 * 34) + (ax1 * 2)) + ax2_s) - 1)] : -3.402823e+38f);\n      }\n      pool_max[((ax0 * 17) + ax1)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        pool_max[((ax0 * 17) + ax1)] = max(pool_max[((ax0 * 17) + ax1)], pad_temp[rv0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))], ((1 <= rv0) ? data[((((((int)blockIdx.x) * 34) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 17, 2), \"float32\"), pool_max: T.Buffer((16, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(16):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            for ax1 in range(17):\n                pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n                for ax2_s in range(3):\n                    data_1 = T.Buffer((544,), data=data.data)\n                    pad_temp_1[ax2_s] = T.if_then_else(1 <= ax2_s, data_1[ax0 * 34 + ax1 * 2 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((272,), data=pool_max.data)\n                pool_max_1[ax0 * 17 + ax1] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0 * 17 + ax1\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0])", "op_args": [16, 17, 16, 2]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1800; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3_s)] = (1.000000e+00f / sqrtf(data[((i0_i1_fused_i2_fused * 9) + i3_s)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 20, 6, 9), \"float32\"), compute: T.Buffer((15, 20, 6, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1800):\n            for i3_s in range(9):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 9 + i3_s\n                compute_1 = T.Buffer((16200,), data=compute.data)\n                data_1 = T.Buffer((16200,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [15, 20, 6, 9]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    float pad_temp[833];\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        for (int32_t ax3_s = 0; ax3_s < 49; ++ax3_s) {\n          pad_temp[((ax2 * 49) + ax3_s)] = ((((1 <= ax2) && (ax2 < 16)) && (1 <= ax3_s)) ? data[(((((ax0 * 6480) + (ax1 * 720)) + (ax2 * 48)) + ax3_s) - 49)] : -3.402823e+38f);\n        }\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 24; ++ax3) {\n          pool_max[((((ax0 * 1728) + (ax1 * 192)) + (ax2_1 * 24)) + ax3)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0 * 1728) + (ax1 * 192)) + (ax2_1 * 24)) + ax3)] = max(pool_max[((((ax0 * 1728) + (ax1 * 192)) + (ax2_1 * 24)) + ax3)], pad_temp[((((ax2_1 * 98) + (rv0 * 49)) + (ax3 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ((((1 <= ((((((int)blockIdx.x) & 3) * 4) + ((((int)threadIdx.x) / 24) * 2)) + rv0)) && (((((((int)threadIdx.x) / 24) + (rv0 >> 1)) >> 1) + (((int)blockIdx.x) & 3)) < 4)) && (1 <= (((((int)threadIdx.x) % 24) * 2) + rv1))) ? data[((((((((((int)blockIdx.x) >> 2) * 720) + ((((int)blockIdx.x) & 3) * 192)) + ((((int)threadIdx.x) / 24) * 96)) + (rv0 * 48)) + ((((int)threadIdx.x) % 24) * 2)) + rv1) - 49)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 9, 15, 48), \"float32\"), pool_max: T.Buffer((12, 9, 8, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(12):\n            pad_temp = T.allocate([833], \"float32\", \"global\")\n            for ax1 in range(9):\n                pad_temp_1 = T.Buffer((833,), data=pad_temp)\n                for ax2, ax3_s in T.grid(17, 49):\n                    data_1 = T.Buffer((77760,), data=data.data)\n                    pad_temp_1[ax2 * 49 + ax3_s] = T.if_then_else(1 <= ax2 and ax2 < 16 and 1 <= ax3_s, data_1[ax0 * 6480 + ax1 * 720 + ax2 * 48 + ax3_s - 49], T.float32(-3.4028234663852886e+38))\n                for ax2, ax3 in T.grid(8, 24):\n                    pool_max_1 = T.Buffer((20736,), data=pool_max.data)\n                    pool_max_1[ax0 * 1728 + ax1 * 192 + ax2 * 24 + ax3] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_1: T.int32 = ax0 * 1728 + ax1 * 192 + ax2 * 24 + ax3\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 98 + rv0 * 49 + ax3 * 2 + rv1])", "op_args": [12, 9, 15, 16]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 3 : ((ax0 == 2) ? 10 : ((ax0 == 1) ? 18 : 16)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 3 : ((((int)threadIdx.x) == 2) ? 10 : ((((int)threadIdx.x) == 1) ? 18 : 16)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 18, 10, 3), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 3, T.if_then_else(ax0 == 2, 10, T.if_then_else(ax0 == 1, 18, 16)))", "op_args": [16, 18, 10, 3]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 8721; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float pad_temp[27];\n    for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n          for (int32_t ax4_s = 0; ax4_s < 3; ++ax4_s) {\n            pad_temp[(((ax2 * 9) + (ax3 * 3)) + ax4_s)] = (((((1 <= ((((ax0_ax1_fused_ax2_fused_ax3_fused % 27) / 9) * 2) + ax2)) && ((((ax0_ax1_fused_ax2_fused_ax3_fused % 27) / 9) + (ax2 >> 1)) < 3)) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused % 9) * 2) + ax3))) && (1 <= ((ax4 * 2) + ax4_s))) ? data[(((((((((ax0_ax1_fused_ax2_fused_ax3_fused / 27) * 1440) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 27) / 9) * 576)) + (ax2 * 288)) + ((ax0_ax1_fused_ax2_fused_ax3_fused % 9) * 32)) + (ax3 * 16)) + (ax4 * 2)) + ax4_s) - 305)] : -3.402823e+38f);\n          }\n        }\n      }\n      pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n            pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4)] = max(pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4)], pad_temp[(((rv0 * 9) + (rv1 * 3)) + rv2)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))], (((((1 <= ((((((((int)blockIdx.x) * 19) + (((int)threadIdx.x) >> 1)) % 108) / 36) * 2) + rv0)) && ((((((((int)blockIdx.x) * 19) + (((int)threadIdx.x) >> 1)) % 108) / 36) + (rv0 >> 1)) < 3)) && (1 <= ((((((((int)blockIdx.x) * 19) + (((int)threadIdx.x) >> 1)) % 36) >> 2) * 2) + rv1))) && (1 <= (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[(((((((((((((int)blockIdx.x) * 19) + (((int)threadIdx.x) >> 1)) / 108) * 1440) + (((((((int)blockIdx.x) * 19) + (((int)threadIdx.x) >> 1)) % 108) / 36) * 576)) + (rv0 * 288)) + (((((((int)blockIdx.x) * 19) + (((int)threadIdx.x) >> 1)) % 36) >> 2) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 305)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 17, 5, 18, 16), \"float32\"), pool_max: T.Buffer((19, 17, 3, 9, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(8721):\n            pad_temp = T.allocate([27], \"float32\", \"global\")\n            for ax4 in range(8):\n                pad_temp_1 = T.Buffer((27,), data=pad_temp)\n                for ax2, ax3, ax4_s in T.grid(3, 3, 3):\n                    cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 9\n                    cse_var_2: T.int32 = ax4 * 2\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 27 // 9\n                    data_1 = T.Buffer((465120,), data=data.data)\n                    pad_temp_1[ax2 * 9 + ax3 * 3 + ax4_s] = T.if_then_else(1 <= cse_var_1 * 2 + ax2 and cse_var_1 + ax2 // 2 < 3 and 1 <= cse_var_3 * 2 + ax3 and 1 <= cse_var_2 + ax4_s, data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 27 * 1440 + cse_var_1 * 576 + ax2 * 288 + cse_var_3 * 32 + ax3 * 16 + cse_var_2 + ax4_s - 305], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((69768,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused * 8 + ax4\n                    pool_max_1[cse_var_4] = T.max(pool_max_1[cse_var_4], pad_temp_1[rv0 * 9 + rv1 * 3 + rv2])", "op_args": [19, 17, 5, 18]}{"op_name": "sigmoid", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 15; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i1 * 52) + (i2 * 13)) + i3)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[(((i1 * 52) + (i2 * 13)) + i3)]))));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 15, 4, 13), \"float32\"), compute: T.Buffer((1, 15, 4, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(15, 4, 13):\n            cse_var_1: T.int32 = i1 * 52 + i2 * 13 + i3\n            compute_1 = T.Buffer((780,), data=compute.data)\n            data_1 = T.Buffer((780,), data=data.data)\n            compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])", "op_args": [1, 15, 4, 13]}{"op_name": "relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 13; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 13) + i3)] = max(data[((i0_i1_fused_i2_fused * 13) + i3)], 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 1053) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 3, 12, 13), \"float32\"), compute: T.Buffer((18, 3, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            for i3 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 13 + i3\n                compute_1 = T.Buffer((8424,), data=compute.data)\n                data_1 = T.Buffer((8424,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))", "op_args": [18, 3, 12, 13]}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 3520; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_sign[ax0_ax1_fused_ax2_fused_ax3_fused] = ((0.000000e+00f < data[ax0_ax1_fused_ax2_fused_ax3_fused]) ? 1.000000e+00f : ((data[ax0_ax1_fused_ax2_fused_ax3_fused] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 4, 11, 16), \"float32\"), T_sign: T.Buffer((5, 4, 11, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(3520):\n            T_sign_1 = T.Buffer((3520,), data=T_sign.data)\n            data_1 = T.Buffer((3520,), data=data.data)\n            T_sign_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.if_then_else(T.float32(0) < data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(1), T.Select(data_1[ax0_ax1_fused_ax2_fused_ax3_fused] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [5, 4, 11, 16]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused < 990; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused) {\n    for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 3; ++i_inner) {\n        ScaleShift[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused / 45) * 270) + (c_inner * 135)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 5) * 27)) + (i_inner * 9)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 45) / 5))] = ((data[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused / 45) * 270) + (c_inner * 135)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 5) * 27)) + (i_inner * 9)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 45) / 5))] * Scale[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 90) / 45) * 2) + c_inner)]) + Shift[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 90) / 45) * 2) + c_inner)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 9) * 4) + (((int)threadIdx.x) / 15)) / 9)]) + Shift[((((((int)blockIdx.x) % 9) * 4) + (((int)threadIdx.x) / 15)) / 9)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 4, 15, 9), \"float32\"), Scale: T.Buffer((4,), \"float32\"), Shift: T.Buffer((4,), \"float32\"), ScaleShift: T.Buffer((11, 4, 15, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused in T.parallel(990):\n            for c_inner, i_inner in T.grid(2, 3):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 90 // 45 * 2 + c_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused // 45 * 270 + c_inner * 135 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 5 * 27 + i_inner * 9 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused % 45 // 5\n                ScaleShift_1 = T.Buffer((5940,), data=ScaleShift.data)\n                data_1 = T.Buffer((5940,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [11, 4, 15, 9]}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 33; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      for (int32_t i3 = 0; i3 < 16; ++i3) {\n        compute[(((i0_i1_fused * 288) + (i2 * 16)) + i3)] = sinhf(data[(((i0_i1_fused * 288) + (i2 * 16)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 3, 18, 16), \"float32\"), compute: T.Buffer((11, 3, 18, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(33):\n            for i2, i3 in T.grid(18, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 288 + i2 * 16 + i3\n                compute_1 = T.Buffer((9504,), data=compute.data)\n                data_1 = T.Buffer((9504,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])", "op_args": [11, 3, 18, 16]}{"op_name": "sqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 26752; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 16, 8, 19), \"float32\"), compute: T.Buffer((11, 16, 8, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(26752):\n            compute_1 = T.Buffer((26752,), data=compute.data)\n            data_1 = T.Buffer((26752,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [11, 16, 8, 19]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 1056) + (i1 * 176)) + (i2 * 11)) + i3)] = ((0.000000e+00f < data[((((i0 * 1056) + (i1 * 176)) + (i2 * 11)) + i3)]) ? data[((((i0 * 1056) + (i1 * 176)) + (i2 * 11)) + i3)] : (data[((((i0 * 1056) + (i1 * 176)) + (i2 * 11)) + i3)] * Scale[i3]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 11)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 6, 16, 11), \"float32\"), Scale: T.Buffer((11,), \"float32\"), compute: T.Buffer((9, 6, 16, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2, i3 in T.grid(6, 16, 11):\n                cse_var_1: T.int32 = i0 * 1056 + i1 * 176 + i2 * 11 + i3\n                compute_1 = T.Buffer((9504,), data=compute.data)\n                data_1 = T.Buffer((9504,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [9, 6, 16, 11]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused < 288; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused) {\n    for (int32_t cb_outer_inner = 0; cb_outer_inner < 16; ++cb_outer_inner) {\n      for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n        for (int32_t j_inner = 0; j_inner < 4; ++j_inner) {\n          ScaleShift[((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 72) * 9216) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 1) * 4608)) + (b_inner * 2304)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 3) >> 1) * 1152)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 72) >> 2) * 64)) + (j_inner * 16)) + cb_outer_inner)] = ((data[((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 72) * 9216) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 1) * 4608)) + (b_inner * 2304)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 3) >> 1) * 1152)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 72) >> 2) * 64)) + (j_inner * 16)) + cb_outer_inner)] * Scale[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 3) >> 1) * 16) + cb_outer_inner)]) + Shift[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 3) >> 1) * 16) + cb_outer_inner)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 48) / 24) * 16) + (((int)threadIdx.x) & 15))]) + Shift[((((((int)blockIdx.x) % 48) / 24) * 16) + (((int)threadIdx.x) & 15))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 6, 12, 16), \"float32\"), Scale: T.Buffer((2, 16), \"float32\"), Shift: T.Buffer((2, 16), \"float32\"), ScaleShift: T.Buffer((16, 2, 6, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused in T.parallel(288):\n            for cb_outer_inner, b_inner, j_inner in T.grid(16, 2, 4):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 4 // 2\n                cse_var_2: T.int32 = cse_var_3 * 16 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused // 72 * 9216 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 2 * 4608 + b_inner * 2304 + cse_var_3 * 1152 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 72 // 4 * 64 + j_inner * 16 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((36864,), data=ScaleShift.data)\n                data_1 = T.Buffer((36864,), data=data.data)\n                Scale_1 = T.Buffer((32,), data=Scale.data)\n                Shift_1 = T.Buffer((32,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [16, 16, 6, 12]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 9; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3)] = tanf(data[((i0_i1_fused_i2_fused * 9) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 15, 2, 9), \"float32\"), compute: T.Buffer((16, 15, 2, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 9 + i3\n                compute_1 = T.Buffer((4320,), data=compute.data)\n                data_1 = T.Buffer((4320,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [16, 15, 2, 9]}{"op_name": "tanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 14; ++i1) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i1 * 91) + (i2 * 13)) + i3)] = tanhf(data[(((i1 * 91) + (i2 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 14, 7, 13), \"float32\"), compute: T.Buffer((1, 14, 7, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(14, 7, 13):\n            cse_var_1: T.int32 = i1 * 91 + i2 * 13 + i3\n            compute_1 = T.Buffer((1274,), data=compute.data)\n            data_1 = T.Buffer((1274,), data=data.data)\n            compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])", "op_args": [1, 14, 7, 13]}{"op_name": "matmul", "c_code": "void default_function_kernel(float* T_matmul, float* left_matrix, float* right_matrix) {\n  for (int32_t ax1_outer_outer_outer = 0; ax1_outer_outer_outer < 5; ++ax1_outer_outer_outer) {\n    for (int32_t ax0_inner_init = 0; ax0_inner_init < 5; ++ax0_inner_init) {\n      T_matmul[((ax0_inner_init * 5) + ax1_outer_outer_outer)] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n        for (int32_t ax0_inner = 0; ax0_inner < 5; ++ax0_inner) {\n          T_matmul[((ax0_inner * 5) + ax1_outer_outer_outer)] = (T_matmul[((ax0_inner * 5) + ax1_outer_outer_outer)] + (left_matrix[(((ax0_inner * 4) + (k_outer * 2)) + k_inner)] * right_matrix[(((k_outer * 10) + (k_inner * 5)) + ax1_outer_outer_outer)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ T_matmul, float* __restrict__ left_matrix, float* __restrict__ right_matrix) {\n  float T_matmul_local[5];\n  __shared__ float left_matrix_shared[4];\n  __shared__ float right_matrix_shared[20];\n  for (int ax1_c_outer_inner_init = 0; ax1_c_outer_inner_init < 5; ++ax1_c_outer_inner_init) {\n    T_matmul_local[ax1_c_outer_inner_init] = 0.000000e+00f;\n  }\n  *(float4*)(left_matrix_shared + 0) = *(float4*)(left_matrix + (((int)blockIdx.x) * 4));\n  for (int ax0_ax1_fused_outer_outer = 0; ax0_ax1_fused_outer_outer < 20; ++ax0_ax1_fused_outer_outer) {\n    right_matrix_shared[ax0_ax1_fused_outer_outer] = right_matrix[ax0_ax1_fused_outer_outer];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n    for (int ax1_c_outer_inner = 0; ax1_c_outer_inner < 5; ++ax1_c_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_matmul_local[ax1_c_outer_inner] = (T_matmul_local[ax1_c_outer_inner] + (left_matrix_shared[((k_outer_inner * 2) + k_inner)] * right_matrix_shared[(((k_outer_inner * 10) + (k_inner * 5)) + ax1_c_outer_inner)]));\n      }\n    }\n  }\n  for (int ax1_inner = 0; ax1_inner < 5; ++ax1_inner) {\n    T_matmul[((((int)blockIdx.x) * 5) + ax1_inner)] = T_matmul_local[ax1_inner];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(left_matrix: T.Buffer((5, 4), \"float32\"), right_matrix: T.Buffer((4, 5), \"float32\"), T_matmul: T.Buffer((5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1_outer_outer_outer in range(5):\n            T_matmul_1 = T.Buffer((25,), data=T_matmul.data)\n            for ax0_inner_init in range(5):\n                T_matmul_1[ax0_inner_init * 5 + ax1_outer_outer_outer] = T.float32(0)\n            for k_outer, k_inner, ax0_inner in T.grid(2, 2, 5):\n                cse_var_1: T.int32 = ax0_inner * 5 + ax1_outer_outer_outer\n                left_matrix_1 = T.Buffer((20,), data=left_matrix.data)\n                right_matrix_1 = T.Buffer((20,), data=right_matrix.data)\n                T_matmul_1[cse_var_1] = T_matmul_1[cse_var_1] + left_matrix_1[ax0_inner * 4 + k_outer * 2 + k_inner] * right_matrix_1[k_outer * 10 + k_inner * 5 + ax1_outer_outer_outer]", "op_args": [3, 16, 5, 4]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 19; ++ax3) {\n          T_add[((((ax0 * 5415) + (ax1 * 285)) + (ax2 * 19)) + ax3)] = (sqrtf(data[((((ax0 * 5415) + (ax1 * 285)) + (ax2 * 19)) + ax3)]) + cosf(data_1[((((ax0 * 5415) + (ax1 * 285)) + (ax2 * 19)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 19, 15, 19), \"float32\"), data_1: T.Buffer((18, 19, 15, 19), \"float32\"), T_add: T.Buffer((18, 19, 15, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(18):\n            for ax1, ax2, ax3 in T.grid(19, 15, 19):\n                cse_var_1: T.int32 = ax0 * 5415 + ax1 * 285 + ax2 * 19 + ax3\n                T_add_1 = T.Buffer((97470,), data=T_add.data)\n                data_2 = T.Buffer((97470,), data=data.data)\n                data_3 = T.Buffer((97470,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [18, 19, 15, 19]}{"op_name": "multi_out_op", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 80; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf((data[i0_i1_fused_i2_fused_i3_fused] + data_1[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = cosf((data[((i0_i1_fused * 5) + i2)] + data_1[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 5, 1), \"float32\"), data_1: T.Buffer((4, 4, 5, 1), \"float32\"), compute: T.Buffer((4, 4, 5, 1), \"float32\"), compute_1: T.Buffer((4, 4, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((80,), data=data.data)\n        data_3 = T.Buffer((80,), data=data_1.data)\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(80):\n            compute_2 = T.Buffer((80,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_2[i0_i1_fused_i2_fused_i3_fused] + data_3[i0_i1_fused_i2_fused_i3_fused])\n        for i0_i1_fused in T.parallel(16):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((80,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(data_2[cse_var_1] + data_3[cse_var_1])", "op_args": [4, 4, 5, 1]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          T_softmax_maxelem[0] = -3.402823e+38f;\n          for (int32_t k = 0; k < 10; ++k) {\n            T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k)]);\n          }\n          T_softmax_expsum[0] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 10; ++k_1) {\n              int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + k_1)] - T_softmax_maxelem[0])));\n          }\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 650) + (i1 * 50)) + (i2 * 10)) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 10; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 350) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 455) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 10; ++k) {\n    if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 455) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 10)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 2275) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 13, 5, 10), \"float32\"), T_softmax_norm: T.Buffer((7, 13, 5, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1, i2, i3 in T.grid(13, 5, 10):\n                cse_var_1: T.int32 = i0 * 650 + i1 * 50 + i2 * 10 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((4550,), data=data.data)\n                for k in range(10):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0 * 650 + i1 * 50 + i2 * 10 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(10):\n                    cse_var_2: T.int32 = i0 * 650 + i1 * 50 + i2 * 10 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((4550,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [7, 13, 5, 10]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3520; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 220) * 220) + ((((i0_i1_fused_i2_fused_i3_fused % 220) / 11) % 5) * 44)) + ((((i0_i1_fused_i2_fused_i3_fused % 220) / 11) / 10) * 22)) + ((i0_i1_fused_i2_fused_i3_fused % 11) * 2)) + ((((i0_i1_fused_i2_fused_i3_fused % 220) / 11) % 10) / 5))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 55) * 220) + ((((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 220) / 11) % 5) * 44)) + ((((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 220) / 11) / 10) * 22)) + ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 11) * 2)) + ((((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 220) / 11) % 10) / 5))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 5, 2, 22), \"float32\"), space_to_depth: T.Buffer((16, 20, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3520):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 220 // 11\n            space_to_depth_1 = T.Buffer((3520,), data=space_to_depth.data)\n            data_1 = T.Buffer((3520,), data=data.data)\n            space_to_depth_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 220 * 220 + T.truncmod(cse_var_1, 5) * 44 + T.Div(cse_var_1, 10) * 22 + i0_i1_fused_i2_fused_i3_fused % 11 * 2 + T.Div(T.truncmod(cse_var_1, 10), 5)]", "op_args": [16, 5, 1, 11]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 72; ++ax0_ax1_fused_ax2_fused) {\n    T_strided_slice[ax0_ax1_fused_ax2_fused] = a[(((((ax0_ax1_fused_ax2_fused / 24) * 54) + (((ax0_ax1_fused_ax2_fused % 24) / 6) * 9)) + (ax0_ax1_fused_ax2_fused % 6)) + 75)];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = a[((((((int)blockIdx.x) * 54) + ((((int)threadIdx.x) / 6) * 9)) + (((int)threadIdx.x) % 6)) + 75)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((19, 6, 9), \"float32\"), T_strided_slice: T.Buffer((3, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(72):\n            T_strided_slice_1 = T.Buffer((72,), data=T_strided_slice.data)\n            a_1 = T.Buffer((1026,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused_ax2_fused] = a_1[ax0_ax1_fused_ax2_fused // 24 * 54 + ax0_ax1_fused_ax2_fused % 24 // 6 * 9 + ax0_ax1_fused_ax2_fused % 6 + 75]", "op_args": [13, 19, 6, 9]}{"op_name": "unpack_NCHWc_to_nchw", "c_code": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 440; ++n_c_fused) {\n    for (int32_t h = 0; h < 2; ++h) {\n      for (int32_t w = 0; w < 16; ++w) {\n        output_unpack[(((n_c_fused * 32) + (h * 16)) + w)] = packed_out[(((((n_c_fused >> 1) * 64) + (h * 32)) + (w * 2)) + (n_c_fused & 1))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(55) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] = packed_out[(((((((((int)blockIdx.x) * 55) + ((int)threadIdx.x)) >> 6) * 64) + (((((((int)blockIdx.x) * 23) + ((int)threadIdx.x)) & 31) >> 4) * 32)) + ((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) & 15) * 2)) + ((((((int)blockIdx.x) * 55) + ((int)threadIdx.x)) & 63) >> 5))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((20, 11, 2, 16, 2), \"float32\"), output_unpack: T.Buffer((20, 22, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused in T.parallel(440):\n            for h, w in T.grid(2, 16):\n                output_unpack_1 = T.Buffer((14080,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((14080,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 32 + h * 16 + w] = packed_out_1[n_c_fused // 2 * 64 + h * 32 + w * 2 + n_c_fused % 2]", "op_args": [20, 11, 2, 16]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 60; ++i3) {\n      resize[((i0_i1_fused_i2_fused * 60) + i3)] = data[((((i0_i1_fused_i2_fused / 24) * 360) + (((i0_i1_fused_i2_fused % 24) / 2) * 30)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(55) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 11) + (((int)threadIdx.x) / 5)) / 288) * 360) + ((((((((int)blockIdx.x) * 11) + (((int)threadIdx.x) / 5)) % 288) / 12) / 2) * 30)) + ((((((int)blockIdx.x) * 55) + ((int)threadIdx.x)) % 60) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 11, 12, 30), \"float32\"), resize: T.Buffer((10, 11, 24, 60), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            for i3 in range(60):\n                resize_1 = T.Buffer((158400,), data=resize.data)\n                data_1 = T.Buffer((39600,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 60 + i3] = data_1[i0_i1_fused_i2_fused // 24 * 360 + T.Div(i0_i1_fused_i2_fused % 24, 2) * 30 + T.Div(i3, 2)]", "op_args": [10, 11, 6, 15]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    float T_multiply_red[323];\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_multiply_red[((ax1 * 19) + ax2)] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 4; ++k1) {\n          T_multiply_red[((ax1 * 19) + ax2)] = (T_multiply_red[((ax1 * 19) + ax2)] + (data[((((ax0 * 1292) + (k1 * 323)) + (ax1 * 19)) + ax2)] * data[((((ax0 * 1292) + (k1 * 323)) + (ax1 * 19)) + ax2)]));\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 4; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 17; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 19; ++ax3) {\n          T_cast[((((ax0 * 1292) + (ax1_1 * 323)) + (ax2_1 * 19)) + ax3)] = ((data[((((ax0 * 1292) + (ax1_1 * 323)) + (ax2_1 * 19)) + ax3)] * weight[ax1_1]) * (1.000000e+00f / sqrtf(((T_multiply_red[((ax2_1 * 19) + ax3)] * 2.500000e-01f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 323) {\n    T_cast[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 1292) / 323)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 323) * 323) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 323))] * 2.500000e-01f) + 1.000000e-05f))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 4; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) / 17) * 1292) + (k1 * 323)) + ((((int)blockIdx.x) % 17) * 19)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) / 17) * 1292) + (k1 * 323)) + ((((int)blockIdx.x) % 17) * 19)) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 17, 19), \"float32\"), weight: T.Buffer((19,), \"float32\"), T_cast: T.Buffer((2, 4, 17, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            T_multiply_red = T.allocate([323], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((323,), data=T_multiply_red)\n            data_1 = T.Buffer((2584,), data=data.data)\n            for ax1, ax2 in T.grid(17, 19):\n                T_multiply_red_1[ax1 * 19 + ax2] = T.float32(0)\n                for k1 in range(4):\n                    cse_var_3: T.int32 = ax1 * 19\n                    cse_var_2: T.int32 = cse_var_3 + ax2\n                    cse_var_1: T.int32 = ax0 * 1292 + k1 * 323 + cse_var_3 + ax2\n                    T_multiply_red_1[cse_var_2] = T_multiply_red_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax1, ax2, ax3 in T.grid(4, 17, 19):\n                cse_var_5: T.int32 = ax2 * 19\n                cse_var_4: T.int32 = ax0 * 1292 + ax1 * 323 + cse_var_5 + ax3\n                T_cast_1 = T.Buffer((2584,), data=T_cast.data)\n                T_cast_1[cse_var_4] = data_1[cse_var_4] * weight[ax1] * T.rsqrt(T_multiply_red_1[cse_var_5 + ax3] * T.float32(0.25) + T.float32(1.0000000000000001e-05))", "op_args": [2, 4, 17, 19]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    float T_reshape[12];\n    float T_reshape_1[12];\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      T_reshape[ax1] = moving_mean[ax1];\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      T_reshape_1[ax1_1] = moving_var[ax1_1];\n    }\n    for (int32_t ax1_2 = 0; ax1_2 < 12; ++ax1_2) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_divide[((((ax0 * 3240) + (ax1_2 * 270)) + (ax2 * 18)) + ax3)] = ((data[((((ax0 * 3240) + (ax1_2 * 270)) + (ax2 * 18)) + ax3)] - T_reshape[ax1_2]) / sqrtf((T_reshape_1[ax1_2] + 1.000000e-05f)));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 54) * 2) + (((int)threadIdx.x) / 30)) / 9)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 54) * 2) + (((int)threadIdx.x) / 30)) / 9)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 12, 15, 18), \"float32\"), gamma: T.Buffer((12,), \"float32\"), beta: T.Buffer((12,), \"float32\"), moving_mean: T.Buffer((12,), \"float32\"), moving_var: T.Buffer((12,), \"float32\"), T_divide: T.Buffer((14, 12, 15, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(14):\n            T_reshape = T.allocate([12], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([12], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((12,), data=T_reshape, align=32)\n            for ax1 in range(12):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            T_reshape_3 = T.Buffer((12,), data=T_reshape_1, align=32)\n            for ax1 in range(12):\n                T_reshape_3[ax1] = moving_var[ax1]\n            for ax1, ax2, ax3 in T.grid(12, 15, 18):\n                cse_var_1: T.int32 = ax0 * 3240 + ax1 * 270 + ax2 * 18 + ax3\n                T_divide_1 = T.Buffer((45360,), data=T_divide.data)\n                data_1 = T.Buffer((45360,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[ax1]) / T.sqrt(T_reshape_3[ax1] + T.float32(1.0000000000000001e-05))", "op_args": [14, 12, 15, 18]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 462; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 2; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 2) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 2) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 231) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 14, 3, 2), \"float32\"), compute: T.Buffer((11, 14, 3, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            for i3 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 2 + i3\n                compute_1 = T.Buffer((924,), data=compute.data)\n                data_1 = T.Buffer((924,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [11, 14, 3, 2]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 3; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 3) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 3) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 4, 3), \"float32\"), compute: T.Buffer((10, 16, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            for i3 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 3 + i3\n                compute_1 = T.Buffer((1920,), data=compute.data)\n                data_1 = T.Buffer((1920,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [10, 16, 4, 3]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 300; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 19) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 1, 20, 19), \"float32\"), compute: T.Buffer((15, 1, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((5700,), data=compute.data)\n                data_1 = T.Buffer((5700,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [15, 1, 20, 19]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 9792; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < ((((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 5) + 5) % 8) == 0) ? (((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 5) + 5) >> 3) : ((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 5) + 5) >> 3) + 1)) - ((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 5) >> 3)); ++rv0) {\n      for (int32_t rv1 = 0; rv1 < (((((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 5) + 5) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 5) + 5) >> 3) : (((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 5) + 5) >> 3) + 1)) - (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 5) >> 3)); ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[((((((ax0_ax1_fused_ax2_fused_ax3_fused >> 6) * 25) + (((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 5) >> 3) * 5)) + (rv0 * 5)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 5) >> 3)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((int)blockIdx.x) & 1) * 20) + ((((int)threadIdx.x) >> 3) * 5)) + 5) % 8) == 0) ? ((((((int)blockIdx.x) & 1) * 5) + ((((((int)threadIdx.x) >> 3) * 5) + 5) >> 2)) >> 1) : (((((((int)blockIdx.x) & 1) * 5) + ((((((int)threadIdx.x) >> 3) * 5) + 5) >> 2)) >> 1) + 1)) - ((((((int)blockIdx.x) & 1) * 5) + (((int)threadIdx.x) >> 3)) >> 1)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 5) + 5) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 5) + 5) >> 3) : (((((((int)threadIdx.x) & 7) * 5) + 5) >> 3) + 1)) - (((((int)threadIdx.x) & 7) * 5) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[((((((((int)blockIdx.x) >> 1) * 25) + (((((((int)blockIdx.x) & 1) * 5) + (((int)threadIdx.x) >> 3)) >> 1) * 5)) + (rv0 * 5)) + (((((int)threadIdx.x) & 7) * 5) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 9, 5, 5), \"float32\"), adaptive_pool_max: T.Buffer((17, 9, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(9792):\n            adaptive_pool_max_1 = T.Buffer((9792,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(T.Let(T.Let(T.Let(T.Select(cse_var_3 % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 8, where={cse_var_1: cse_var_3 // 8}), where={cse_var_3: cse_var_2 + 5}), where={cse_var_2: ax0_ax1_fused_ax2_fused_ax3_fused % 64 // 8 * 5}), T.Let(T.Let(T.Let(T.Select(cse_var_6 % 8 == 0, cse_var_4, cse_var_4 + 1) - cse_var_5 // 8, where={cse_var_4: cse_var_6 // 8}), where={cse_var_6: cse_var_5 + 5}), where={cse_var_5: ax0_ax1_fused_ax2_fused_ax3_fused % 8 * 5})):\n                cse_var_2 = T.int32()\n                cse_var_3 = T.int32()\n                cse_var_1 = T.int32()\n                cse_var_5 = T.int32()\n                cse_var_6 = T.int32()\n                cse_var_4 = T.int32()\n                data_1 = T.Buffer((3825,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 64 * 25 + ax0_ax1_fused_ax2_fused_ax3_fused % 64 // 8 * 5 // 8 * 5 + rv0 * 5 + ax0_ax1_fused_ax2_fused_ax3_fused % 8 * 5 // 8 + rv1])", "op_args": [17, 9, 5, 5]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 18; ++i3) {\n          compute[((((i0 * 324) + (i1 * 108)) + (i2 * 18)) + i3)] = sinf(data[((((i0 * 324) + (i1 * 108)) + (i2 * 18)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 3, 6, 18), \"float32\"), compute: T.Buffer((16, 3, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            for i1, i2, i3 in T.grid(3, 6, 18):\n                cse_var_1: T.int32 = i0 * 324 + i1 * 108 + i2 * 18 + i3\n                compute_1 = T.Buffer((5184,), data=compute.data)\n                data_1 = T.Buffer((5184,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [16, 3, 6, 18]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 208; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 12; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 12) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 12) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 13, 2, 12), \"float32\"), compute: T.Buffer((8, 13, 2, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(208):\n            for i3 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3\n                compute_1 = T.Buffer((2496,), data=compute.data)\n                data_1 = T.Buffer((2496,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [8, 13, 2, 12]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 19; ++i3) {\n          compute[((((i0 * 3420) + (i1 * 171)) + (i2 * 19)) + i3)] = sinf(data[((((i0 * 3420) + (i1 * 171)) + (i2 * 19)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 2565) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 20, 9, 19), \"float32\"), compute: T.Buffer((3, 20, 9, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(20, 9, 19):\n                cse_var_1: T.int32 = i0 * 3420 + i1 * 171 + i2 * 19 + i3\n                compute_1 = T.Buffer((10260,), data=compute.data)\n                data_1 = T.Buffer((10260,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [3, 20, 9, 19]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2268; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 17) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 9, 17), \"float32\"), compute: T.Buffer((18, 14, 9, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2268):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 17 + i3\n                compute_1 = T.Buffer((38556,), data=compute.data)\n                data_1 = T.Buffer((38556,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [18, 14, 9, 17]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 96; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 2) + 2) % 8) == 0) ? (((ax2 * 5) + 5) >> 2) : ((((ax2 * 5) + 5) >> 2) + 1)) - ((ax2 * 10) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 5) + 5) % 8) == 0) ? (((ax3 * 5) + 5) >> 3) : ((((ax3 * 5) + 5) >> 3) + 1)) - ((ax3 * 5) >> 3)); ++rv1) {\n            adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = max(adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)], data[(((((ax0_ax1_fused * 50) + (((ax2 * 10) >> 3) * 5)) + (rv0 * 5)) + ((ax3 * 5) >> 3)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 2) + 2) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 5) + 5) >> 2) : (((((((int)threadIdx.x) >> 3) * 5) + 5) >> 2) + 1)) - (((((int)threadIdx.x) >> 3) * 10) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 5) + 5) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 5) + 5) >> 3) : (((((((int)threadIdx.x) & 7) * 5) + 5) >> 3) + 1)) - (((((int)threadIdx.x) & 7) * 5) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 50) + ((((((int)threadIdx.x) >> 3) * 10) >> 3) * 5)) + (rv0 * 5)) + (((((int)threadIdx.x) & 7) * 5) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 6, 10, 5), \"float32\"), adaptive_pool_max: T.Buffer((16, 6, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(96):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((6144,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 2 + 2) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 10 // 8, where={cse_var_1: (ax2 * 5 + 5) // 4}), T.Let(T.Let(T.Let(T.Select(cse_var_4 % 8 == 0, cse_var_2, cse_var_2 + 1) - cse_var_3 // 8, where={cse_var_2: cse_var_4 // 8}), where={cse_var_4: cse_var_3 + 5}), where={cse_var_3: ax3 * 5})):\n                    cse_var_1 = T.int32()\n                    cse_var_3 = T.int32()\n                    cse_var_4 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_5: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((4800,), data=data.data)\n                    adaptive_pool_max_1[cse_var_5] = T.max(adaptive_pool_max_1[cse_var_5], data_1[ax0_ax1_fused * 50 + ax2 * 10 // 8 * 5 + rv0 * 5 + ax3 * 5 // 8 + rv1])", "op_args": [16, 6, 10, 5]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2025; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) / 3)) < 675) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 9, 5, 3), \"float32\"), compute: T.Buffer((15, 9, 5, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2025):\n            compute_1 = T.Buffer((2025,), data=compute.data)\n            data_1 = T.Buffer((2025,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [15, 9, 5, 3]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2600; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 4, 13, 10), \"float32\"), compute: T.Buffer((5, 4, 13, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2600):\n            compute_1 = T.Buffer((2600,), data=compute.data)\n            data_1 = T.Buffer((2600,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [5, 4, 13, 10]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1280; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < ((((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 2) + 2) % 8) == 0) ? (((ax0_ax1_fused_ax2_fused_ax3_fused & 63) + 8) >> 5) : ((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) + 8) >> 5) + 1)) - ((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 5)); ++rv0) {\n      adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[((((ax0_ax1_fused_ax2_fused_ax3_fused >> 5) * 8) + (rv0 * 8)) + (ax0_ax1_fused_ax2_fused_ax3_fused & 7))]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((int)blockIdx.x) & 3) * 4) + ((((int)threadIdx.x) >> 3) * 2)) + 2) % 8) == 0) ? ((((((int)threadIdx.x) + 8) >> 4) + (((int)blockIdx.x) & 3)) >> 1) : (((((((int)threadIdx.x) + 8) >> 4) + (((int)blockIdx.x) & 3)) >> 1) + 1)) - ((((int)blockIdx.x) & 3) >> 1)); ++rv0) {\n    adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) >> 1) * 8) + (rv0 * 8)) + (((int)threadIdx.x) & 7))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 1, 2, 8), \"float32\"), adaptive_pool_max: T.Buffer((20, 1, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1280):\n            adaptive_pool_max_1 = T.Buffer((1280,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(T.Let(T.Let(T.Select((cse_var_2 // 8 * 2 + 2) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 32, where={cse_var_1: (cse_var_2 + 8) // 32}), where={cse_var_2: ax0_ax1_fused_ax2_fused_ax3_fused % 64})):\n                cse_var_2 = T.int32()\n                cse_var_1 = T.int32()\n                data_1 = T.Buffer((320,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 32 * 8 + rv0 * 8 + ax0_ax1_fused_ax2_fused_ax3_fused % 8])", "op_args": [20, 1, 2, 8]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 76000; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 20, 19), \"float32\"), compute: T.Buffer((10, 20, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(76000):\n            compute_1 = T.Buffer((76000,), data=compute.data)\n            data_1 = T.Buffer((76000,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [10, 20, 20, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_5) {\n  float auto_scheduler_layout_transform[90];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 585; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n    for (int32_t ax4 = 0; ax4 < 3; ++ax4) {\n      for (int32_t ax8 = 0; ax8 < 15; ++ax8) {\n        auto_scheduler_layout_transform[(((ax3 * 45) + (ax4 * 15)) + ax8)] = ph_5[(((ax8 * 6) + (ax4 * 2)) + ax3)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_inner_init = 0; b_inner_init < 15; ++b_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 13; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 26) + (i_inner_init * 2)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 3; ++k_outer) {\n      for (int32_t b_inner = 0; b_inner < 15; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 13; ++i_inner) {\n          T_batch_matmul_NN[(((b_inner * 26) + (i_inner * 2)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[(((b_inner * 26) + (i_inner * 2)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (ph_0[(((b_inner * 39) + (i_inner * 3)) + k_outer)] * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 45) + (k_outer * 15)) + b_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 585; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 585; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  float T_batch_matmul_NN_local[9];\n  __shared__ float ph_5_shared[1];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 3; ++i_c_outer_inner_init) {\n    T_batch_matmul_NN_local[i_c_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(i_c_outer_inner_init + 3)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(i_c_outer_inner_init + 6)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 8; ++k_outer_outer) {\n    __syncthreads();\n    ph_5_shared[0] = ph_5[((k_outer_outer * 2) + ((int)blockIdx.x))];\n    __syncthreads();\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 3; ++i_c_outer_inner) {\n      T_batch_matmul_NN_local[i_c_outer_inner] = (T_batch_matmul_NN_local[i_c_outer_inner] + (ph_0[((i_c_outer_inner * 8) + k_outer_outer)] * ph_5_shared[0]));\n      T_batch_matmul_NN_local[(i_c_outer_inner + 3)] = (T_batch_matmul_NN_local[(i_c_outer_inner + 3)] + (ph_0[(((i_c_outer_inner * 8) + k_outer_outer) + 24)] * ph_5_shared[0]));\n      T_batch_matmul_NN_local[(i_c_outer_inner + 6)] = (T_batch_matmul_NN_local[(i_c_outer_inner + 6)] + (ph_0[(((i_c_outer_inner * 8) + k_outer_outer) + 48)] * ph_5_shared[0]));\n    }\n  }\n  for (int i_inner = 0; i_inner < 3; ++i_inner) {\n    T_batch_matmul_NN[((i_inner * 2) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];\n    T_batch_matmul_NN[(((i_inner * 2) + ((int)blockIdx.x)) + 6)] = T_batch_matmul_NN_local[(i_inner + 3)];\n    T_batch_matmul_NN[(((i_inner * 2) + ((int)blockIdx.x)) + 12)] = T_batch_matmul_NN_local[(i_inner + 6)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 13, 3), \"float32\"), ph_3: T.Buffer((15, 13, 3), \"float32\"), ph_5: T.Buffer((15, 3, 2), \"float32\"), T_add: T.Buffer((15, 13, 3), \"float32\"), T_batch_matmul_NN: T.Buffer((15, 13, 2), \"float32\"), compute: T.Buffer((15, 13, 3), \"float32\"), compute_1: T.Buffer((15, 13, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([90], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((585,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(585):\n            T_add_1 = T.Buffer((585,), data=T_add.data)\n            ph_3_1 = T.Buffer((585,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((90,), data=auto_scheduler_layout_transform)\n        for ax3, ax4, ax8 in T.grid(2, 3, 15):\n            ph_5_1 = T.Buffer((90,), data=ph_5.data)\n            auto_scheduler_layout_transform_1[ax3 * 45 + ax4 * 15 + ax8] = ph_5_1[ax8 * 6 + ax4 * 2 + ax3]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((390,), data=T_batch_matmul_NN.data)\n            for b_inner_init, i_inner_init in T.grid(15, 13):\n                T_batch_matmul_NN_1[b_inner_init * 26 + i_inner_init * 2 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for k_outer, b_inner, i_inner in T.grid(3, 15, 13):\n                cse_var_1: T.int32 = b_inner * 26 + i_inner * 2 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 39 + i_inner * 3 + k_outer] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 45 + k_outer * 15 + b_inner]\n        for i0_i1_fused_i2_fused in T.parallel(585):\n            compute_2 = T.Buffer((585,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(585):\n            compute_2 = T.Buffer((585,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["add", "batch_matmul", "cos", "cos"]]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 108; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 6, 1, 1), \"float32\"), compute: T.Buffer((18, 6, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(108):\n            compute_1 = T.Buffer((108,), data=compute.data)\n            data_1 = T.Buffer((108,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [18, 6, 1, 1]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 19; ++i3) {\n          compute[((((i0 * 3420) + (i1 * 228)) + (i2 * 19)) + i3)] = sinf(data[((((i0 * 3420) + (i1 * 228)) + (i2 * 19)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 5985) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 15, 12, 19), \"float32\"), compute: T.Buffer((14, 15, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(14):\n            for i1, i2, i3 in T.grid(15, 12, 19):\n                cse_var_1: T.int32 = i0 * 3420 + i1 * 228 + i2 * 19 + i3\n                compute_1 = T.Buffer((47880,), data=compute.data)\n                data_1 = T.Buffer((47880,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [14, 15, 12, 19]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n          adaptive_pool_max[((((ax0 * 1280) + (ax1 * 64)) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < ((((((ax2 * 6) + 6) % 8) == 0) ? (((ax2 * 7) + 7) >> 2) : ((((ax2 * 7) + 7) >> 2) + 1)) - ((ax2 * 14) >> 3)); ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 2; ++rv1) {\n              adaptive_pool_max[((((ax0 * 1280) + (ax1 * 64)) + (ax2 * 8)) + ax3)] = max(adaptive_pool_max[((((ax0 * 1280) + (ax1 * 64)) + (ax2 * 8)) + ax3)], data[((((((ax0 * 4480) + (ax1 * 224)) + (((ax2 * 14) >> 3) * 16)) + (rv0 * 16)) + (ax3 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) & 7) * 6) + 6) % 8) == 0) ? ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) & 7) * 7) + 7) >> 2) : (((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) & 7) * 7) + 7) >> 2) + 1)) - (((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) & 7) * 14) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < 2; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], data[((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) >> 3) * 224) + ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) & 7) * 14) >> 3) * 16)) + (rv0 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 14, 16), \"float32\"), adaptive_pool_max: T.Buffer((4, 20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            for ax1, ax2, ax3 in T.grid(20, 8, 8):\n                adaptive_pool_max_1 = T.Buffer((5120,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0 * 1280 + ax1 * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 6 + 6) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 14 // 8, where={cse_var_1: (ax2 * 7 + 7) // 4}), 2):\n                    cse_var_1 = T.int32()\n                    cse_var_2: T.int32 = ax0 * 1280 + ax1 * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((17920,), data=data.data)\n                    adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[ax0 * 4480 + ax1 * 224 + ax2 * 14 // 8 * 16 + rv0 * 16 + ax3 * 2 + rv1])", "op_args": [4, 20, 14, 16]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 49725; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(62) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 62) + ((int)threadIdx.x)) < 49725) {\n    compute[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 62) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 15, 15, 17), \"float32\"), compute: T.Buffer((13, 15, 15, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(49725):\n            compute_1 = T.Buffer((49725,), data=compute.data)\n            data_1 = T.Buffer((49725,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sin(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [13, 15, 15, 17]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 6) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 6) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 10, 6, 6), \"float32\"), compute: T.Buffer((12, 10, 6, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 6 + i3\n                compute_1 = T.Buffer((4320,), data=compute.data)\n                data_1 = T.Buffer((4320,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [12, 10, 6, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 56; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_add[((ax0_ax1_fused * 19) + ax2)] = (ph_0[((ax0_ax1_fused * 19) + ax2)] + (ph_0[((ax0_ax1_fused * 19) + ax2)] + ph_3[((ax0_ax1_fused * 19) + ax2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1064; ++ax0_ax1_fused_ax2_fused) {\n    T_add_1[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = sinf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1064; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 14, 19), \"float32\"), ph_3: T.Buffer((4, 14, 19), \"float32\"), T_add: T.Buffer((4, 14, 19), \"float32\"), T_add_1: T.Buffer((4, 14, 19), \"float32\"), compute: T.Buffer((4, 14, 19), \"float32\"), compute_1: T.Buffer((4, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1064,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1064,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(56):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_add_2 = T.Buffer((1064,), data=T_add.data)\n                T_add_2[cse_var_1] = ph_0_1[cse_var_1] + (ph_0_1[cse_var_1] + ph_3_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1064):\n            T_add_2 = T.Buffer((1064,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(56):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((1064,), data=compute.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1064):\n            compute_2 = T.Buffer((1064,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["add", "add", "add", "sin", "cos"]]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 286; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = sinf(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 11, 2, 16), \"float32\"), compute: T.Buffer((13, 11, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(286):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((4576,), data=compute.data)\n                data_1 = T.Buffer((4576,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [13, 11, 2, 16]}{"op_name": "sin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        compute[(((i0_i1_fused * 160) + (i2 * 8)) + i3)] = sinf(data[(((i0_i1_fused * 160) + (i2 * 8)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 4, 20, 8), \"float32\"), compute: T.Buffer((4, 4, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(16):\n            for i2, i3 in T.grid(20, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 160 + i2 * 8 + i3\n                compute_1 = T.Buffer((2560,), data=compute.data)\n                data_1 = T.Buffer((2560,), data=data.data)\n                compute_1[cse_var_1] = T.sin(data_1[cse_var_1])", "op_args": [4, 4, 20, 8]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 32; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv1 = 0; rv1 < (((((ax3 + 1) % 8) == 0) ? (((ax3 * 9) + 9) >> 3) : ((((ax3 * 9) + 9) >> 3) + 1)) - ax3); ++rv1) {\n        adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = max(adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)], data[(((ax0_ax1_fused_ax2_fused * 9) + ax3) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv1 = 0; rv1 < ((((((((int)threadIdx.x) & 7) + 1) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 3) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 3) + 1)) - (((int)threadIdx.x) & 7)); ++rv1) {\n    adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 18) + ((((int)threadIdx.x) >> 3) * 9)) + rv1) + (((int)threadIdx.x) & 7))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 2, 8, 9), \"float32\"), adaptive_pool_max: T.Buffer((2, 2, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(32):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((256,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv1 in range(T.Let(T.Select((ax3 + 1) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax3, where={cse_var_1: (ax3 * 9 + 9) // 8})):\n                    cse_var_1 = T.int32()\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((288,), data=data.data)\n                    adaptive_pool_max_1[cse_var_2] = T.max(adaptive_pool_max_1[cse_var_2], data_1[ax0_ax1_fused_ax2_fused * 9 + ax3 + rv1])", "op_args": [2, 2, 8, 9]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 360; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add_1[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 8, 15), \"float32\"), ph_3: T.Buffer((3, 8, 15), \"float32\"), T_add: T.Buffer((3, 8, 15), \"float32\"), T_add_1: T.Buffer((3, 8, 15), \"float32\"), compute: T.Buffer((3, 8, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_2 = T.Buffer((360,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_2 = T.Buffer((360,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_1 = T.Buffer((360,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["add", "add", "add", "cos"]]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          compute[((((i0 * 54) + (i1 * 27)) + (i2 * 3)) + i3)] = fabsf(data[((((i0 * 54) + (i1 * 27)) + (i2 * 3)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 2, 9, 3), \"float32\"), compute: T.Buffer((10, 2, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(2, 9, 3):\n                cse_var_1: T.int32 = i0 * 54 + i1 * 27 + i2 * 3 + i3\n                compute_1 = T.Buffer((540,), data=compute.data)\n                data_1 = T.Buffer((540,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [10, 2, 9, 3]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 663; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 11; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3)] = fabsf(data[((i0_i1_fused_i2_fused * 11) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 17, 3, 11), \"float32\"), compute: T.Buffer((13, 17, 3, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(663):\n            for i3 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 11 + i3\n                compute_1 = T.Buffer((7293,), data=compute.data)\n                data_1 = T.Buffer((7293,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [13, 17, 3, 11]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 24; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = fabsf(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 3, 16), \"float32\"), compute: T.Buffer((2, 4, 3, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(24):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((384,), data=compute.data)\n                data_1 = T.Buffer((384,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [2, 4, 3, 16]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 208; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i0_i1_fused * 24) + (i2 * 6)) + i3)] = fabsf(data[(((i0_i1_fused * 24) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 16, 4, 6), \"float32\"), compute: T.Buffer((13, 16, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(208):\n            for i2, i3 in T.grid(4, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 24 + i2 * 6 + i3\n                compute_1 = T.Buffer((4992,), data=compute.data)\n                data_1 = T.Buffer((4992,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [13, 16, 4, 6]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((ax2 + 1) % 8) == 0) ? ((ax2 + 1) >> 3) : (((ax2 + 1) >> 3) + 1)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < ((((((ax3 * 4) + 4) % 8) == 0) ? (((ax3 * 5) + 5) >> 1) : ((((ax3 * 5) + 5) >> 1) + 1)) - ((ax3 * 20) >> 3)); ++rv1) {\n            adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = max(adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)], data[((((ax0_ax1_fused * 20) + (rv0 * 20)) + ((ax3 * 20) >> 3)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) >> 3)) + 1) % 8) == 0) ? ((((((int)threadIdx.x) + 8) >> 5) + (((int)blockIdx.x) & 1)) >> 1) : (((((((int)threadIdx.x) + 8) >> 5) + (((int)blockIdx.x) & 1)) >> 1) + 1)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 4) + 4) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 5) + 5) >> 1) : (((((((int)threadIdx.x) & 7) * 5) + 5) >> 1) + 1)) - (((((int)threadIdx.x) & 7) * 20) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) >> 1) * 20) + (rv0 * 20)) + (((((int)threadIdx.x) & 7) * 20) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 2, 1, 20), \"float32\"), adaptive_pool_max: T.Buffer((3, 2, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((384,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 + 1) % 8 == 0, cse_var_1, cse_var_1 + 1), where={cse_var_1: (ax2 + 1) // 8}), T.Let(T.Select((ax3 * 4 + 4) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 20 // 8, where={cse_var_2: (ax3 * 5 + 5) // 2})):\n                    cse_var_1 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((120,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0_ax1_fused * 20 + rv0 * 20 + ax3 * 20 // 8 + rv1])", "op_args": [3, 2, 1, 20]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 9; ++i3) {\n        compute[(((i0_i1_fused * 36) + (i2 * 9)) + i3)] = fabsf(data[(((i0_i1_fused * 36) + (i2 * 9)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 4, 9), \"float32\"), compute: T.Buffer((10, 20, 4, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(200):\n            for i2, i3 in T.grid(4, 9):\n                cse_var_1: T.int32 = i0_i1_fused * 36 + i2 * 9 + i3\n                compute_1 = T.Buffer((7200,), data=compute.data)\n                data_1 = T.Buffer((7200,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [10, 20, 4, 9]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2080; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = fabsf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 13, 2, 10), \"float32\"), compute: T.Buffer((8, 13, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2080):\n            compute_1 = T.Buffer((2080,), data=compute.data)\n            data_1 = T.Buffer((2080,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.fabs(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [8, 13, 2, 10]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3564; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3)] = fabsf(data[((i0_i1_fused_i2_fused * 17) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 18, 18, 17), \"float32\"), compute: T.Buffer((11, 18, 18, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3564):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 17 + i3\n                compute_1 = T.Buffer((60588,), data=compute.data)\n                data_1 = T.Buffer((60588,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [11, 18, 18, 17]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 187; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused * 17) + i3)] = fabsf(data[((i0_i1_fused * 17) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) < 3179) {\n    compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 11, 1, 17), \"float32\"), compute: T.Buffer((17, 11, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(187):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i3\n                compute_1 = T.Buffer((3179,), data=compute.data)\n                data_1 = T.Buffer((3179,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [17, 11, 1, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 48; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_add[((ax0_ax1_fused * 3) + ax2)] = ((ph_0[((ax0_ax1_fused * 3) + ax2)] + sinf(ph_0[((ax0_ax1_fused * 3) + ax2)])) + ph_0[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 3), \"float32\"), T_add: T.Buffer((8, 6, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(48):\n            for ax2 in range(3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_add_1 = T.Buffer((144,), data=T_add.data)\n                ph_0_1 = T.Buffer((144,), data=ph_0.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.sin(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]", "op_args": [["sin", "add", "add"]]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1496; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n      adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < ((((((ax0_ax1_fused_ax2_fused & 7) + 1) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused & 7) * 9) + 9) >> 3) : (((((ax0_ax1_fused_ax2_fused & 7) * 9) + 9) >> 3) + 1)) - (ax0_ax1_fused_ax2_fused & 7)); ++rv0) {\n        for (int32_t rv1 = 0; rv1 < ((((((ax3 * 2) + 2) % 8) == 0) ? (((ax3 * 9) + 9) >> 2) : ((((ax3 * 9) + 9) >> 2) + 1)) - ((ax3 * 18) >> 3)); ++rv1) {\n          adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)] = max(adaptive_pool_max[((ax0_ax1_fused_ax2_fused * 8) + ax3)], data[((((((ax0_ax1_fused_ax2_fused >> 3) * 162) + (rv0 * 18)) + ((ax0_ax1_fused_ax2_fused & 7) * 18)) + ((ax3 * 18) >> 3)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) >> 3)) + 1) % 8) == 0) ? ((((((int)blockIdx.x) & 1) * 9) + ((((((int)threadIdx.x) >> 3) * 9) + 9) >> 2)) >> 1) : (((((((int)blockIdx.x) & 1) * 9) + ((((((int)threadIdx.x) >> 3) * 9) + 9) >> 2)) >> 1) + 1)) - (((int)threadIdx.x) >> 3)) - ((((int)blockIdx.x) & 1) * 4)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 2) + 2) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 9) + 9) >> 2) : (((((((int)threadIdx.x) & 7) * 9) + 9) >> 2) + 1)) - (((((int)threadIdx.x) & 7) * 18) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((((((int)blockIdx.x) >> 1) * 162) + ((((int)blockIdx.x) & 1) * 72)) + ((((int)threadIdx.x) >> 3) * 18)) + (rv0 * 18)) + (((((int)threadIdx.x) & 7) * 18) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 11, 9, 18), \"float32\"), adaptive_pool_max: T.Buffer((17, 11, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1496):\n            for ax3 in range(8):\n                adaptive_pool_max_1 = T.Buffer((11968,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Let(T.Select((cse_var_2 + 1) % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2, where={cse_var_1: (cse_var_2 * 9 + 9) // 8}), where={cse_var_2: ax0_ax1_fused_ax2_fused % 8}), T.Let(T.Select((ax3 * 2 + 2) % 8 == 0, cse_var_3, cse_var_3 + 1) - ax3 * 18 // 8, where={cse_var_3: (ax3 * 9 + 9) // 4})):\n                    cse_var_2 = T.int32()\n                    cse_var_1 = T.int32()\n                    cse_var_3 = T.int32()\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 8 + ax3\n                    data_1 = T.Buffer((30294,), data=data.data)\n                    adaptive_pool_max_1[cse_var_4] = T.max(adaptive_pool_max_1[cse_var_4], data_1[ax0_ax1_fused_ax2_fused // 8 * 162 + rv0 * 18 + ax0_ax1_fused_ax2_fused % 8 * 18 + ax3 * 18 // 8 + rv1])", "op_args": [17, 11, 9, 18]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i3 = 0; i3 < 14; ++i3) {\n        compute[(((i0 * 98) + (i1 * 14)) + i3)] = fabsf(data[(((i0 * 98) + (i1 * 14)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 7, 1, 14), \"float32\"), compute: T.Buffer((15, 7, 1, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(15):\n            for i1, i3 in T.grid(7, 14):\n                cse_var_1: T.int32 = i0 * 98 + i1 * 14 + i3\n                compute_1 = T.Buffer((1470,), data=compute.data)\n                data_1 = T.Buffer((1470,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [15, 7, 1, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 77) + (i1 * 7)) + i2)] = cosf(ph_0[(((i0 * 77) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 77) + (i1_1 * 7)) + i2_1)] = cosf(sinf(ph_0[(((i0_1 * 77) + (i1_1 * 7)) + i2_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 11, 7), \"float32\"), compute: T.Buffer((4, 11, 7), \"float32\"), compute_1: T.Buffer((4, 11, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((308,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(11, 7):\n                cse_var_1: T.int32 = i0 * 77 + i1 * 7 + i2\n                compute_2 = T.Buffer((308,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(11, 7):\n                cse_var_2: T.int32 = i0 * 77 + i1 * 7 + i2\n                compute_2 = T.Buffer((308,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(T.sin(ph_0_1[cse_var_2]))", "op_args": [["cos", "sin", "cos"]]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 448; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 14; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3)] = fabsf(data[((i0_i1_fused_i2_fused * 14) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 4, 16, 14), \"float32\"), compute: T.Buffer((7, 4, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            for i3 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3\n                compute_1 = T.Buffer((6272,), data=compute.data)\n                data_1 = T.Buffer((6272,), data=data.data)\n                compute_1[cse_var_1] = T.fabs(data_1[cse_var_1])", "op_args": [7, 4, 16, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_8) {\n  float auto_scheduler_layout_transform[126];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1386; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1386; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + cosf(sinf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1386; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add_1[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n    for (int32_t ax7 = 0; ax7 < 9; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 7; ++ax8) {\n        auto_scheduler_layout_transform[(((ax5 * 63) + (ax7 * 7)) + ax8)] = ph_8[(((ax5 * 63) + (ax8 * 9)) + ax7)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 11; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 7; ++b_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 77) + (b_inner_init * 11)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 9; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 7; ++b_inner) {\n          T_batch_matmul_NN[(((b_outer_inner * 77) + (b_inner * 11)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[(((b_outer_inner * 77) + (b_inner * 11)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (cosf(ph_0[((((b_outer_inner * 693) + (b_inner * 99)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 9)) + k_inner)]) * auto_scheduler_layout_transform[(((b_outer_inner * 63) + (k_inner * 7)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __cosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[1];\n  __shared__ float ph_8_shared[40];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  if (((int)threadIdx.x) < 40) {\n    ph_8_shared[((int)threadIdx.x)] = ph_8[((int)threadIdx.x)];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 5; ++k_inner) {\n    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (__cosf(ph_0[((((int)threadIdx.x) * 5) + k_inner)]) * ph_8_shared[(((((int)threadIdx.x) >> 3) * 5) + k_inner)]));\n  }\n  T_batch_matmul_NN[((int)threadIdx.x)] = T_batch_matmul_NN_local[0];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), ph_8: T.Buffer((14, 9, 1), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_add: T.Buffer((14, 11, 9), \"float32\"), T_add_1: T.Buffer((14, 11, 9), \"float32\"), T_batch_matmul_NN: T.Buffer((14, 11, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([126], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1386):\n            compute_1 = T.Buffer((1386,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1386):\n            T_add_2 = T.Buffer((1386,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.cos(T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1386):\n            T_add_2 = T.Buffer((1386,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((126,), data=auto_scheduler_layout_transform)\n        for ax5, ax7, ax8 in T.grid(2, 9, 7):\n            cse_var_1: T.int32 = ax5 * 63\n            ph_8_1 = T.Buffer((126,), data=ph_8.data)\n            auto_scheduler_layout_transform_1[cse_var_1 + ax7 * 7 + ax8] = ph_8_1[cse_var_1 + ax8 * 9 + ax7]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(11):\n            T_batch_matmul_NN_1 = T.Buffer((154,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, b_inner_init in T.grid(2, 7):\n                T_batch_matmul_NN_1[b_outer_inner_init * 77 + b_inner_init * 11 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for b_outer_inner, k_inner, b_inner in T.grid(2, 9, 7):\n                cse_var_2: T.int32 = b_outer_inner * 77 + b_inner * 11 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.cos(ph_0_1[b_outer_inner * 693 + b_inner * 99 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 9 + k_inner]) * auto_scheduler_layout_transform_1[b_outer_inner * 63 + k_inner * 7 + b_inner]", "op_args": [["cos", "sin", "cos", "add", "cos", "add", "batch_matmul"]]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1920; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < ((((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 7) + 7) % 8) == 0) ? (((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 7) + 7) >> 3) : ((((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 7) + 7) >> 3) + 1)) - ((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 7) >> 3)); ++rv0) {\n      for (int32_t rv1 = 0; rv1 < (((((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 3) + 3) % 8) == 0) ? ((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 11) + 11) >> 3) : (((((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 11) + 11) >> 3) + 1)) - (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 11) >> 3)); ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[((((((ax0_ax1_fused_ax2_fused_ax3_fused >> 6) * 77) + (((((ax0_ax1_fused_ax2_fused_ax3_fused & 63) >> 3) * 7) >> 3) * 11)) + (rv0 * 11)) + (((ax0_ax1_fused_ax2_fused_ax3_fused & 7) * 11) >> 3)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 7) + 7) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 7) + 7) >> 3) : (((((((int)threadIdx.x) >> 3) * 7) + 7) >> 3) + 1)) - (((((int)threadIdx.x) >> 3) * 7) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((int)threadIdx.x) & 7) * 3) + 3) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 11) + 11) >> 3) : (((((((int)threadIdx.x) & 7) * 11) + 11) >> 3) + 1)) - (((((int)threadIdx.x) & 7) * 11) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 77) + ((((((int)threadIdx.x) >> 3) * 7) >> 3) * 11)) + (rv0 * 11)) + (((((int)threadIdx.x) & 7) * 11) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 2, 7, 11), \"float32\"), adaptive_pool_max: T.Buffer((15, 2, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1920):\n            adaptive_pool_max_1 = T.Buffer((1920,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(T.Let(T.Let(T.Let(T.Select(cse_var_3 % 8 == 0, cse_var_1, cse_var_1 + 1) - cse_var_2 // 8, where={cse_var_1: cse_var_3 // 8}), where={cse_var_3: cse_var_2 + 7}), where={cse_var_2: ax0_ax1_fused_ax2_fused_ax3_fused % 64 // 8 * 7}), T.Let(T.Let(T.Let(T.Select((cse_var_5 * 3 + 3) % 8 == 0, cse_var_4, cse_var_4 + 1) - cse_var_6 // 8, where={cse_var_4: (cse_var_6 + 11) // 8}), where={cse_var_6: cse_var_5 * 11}), where={cse_var_5: ax0_ax1_fused_ax2_fused_ax3_fused % 8})):\n                cse_var_2 = T.int32()\n                cse_var_3 = T.int32()\n                cse_var_1 = T.int32()\n                cse_var_5 = T.int32()\n                cse_var_6 = T.int32()\n                cse_var_4 = T.int32()\n                data_1 = T.Buffer((2310,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 64 * 77 + ax0_ax1_fused_ax2_fused_ax3_fused % 64 // 8 * 7 // 8 * 11 + rv0 * 11 + ax0_ax1_fused_ax2_fused_ax3_fused % 8 * 11 // 8 + rv1])", "op_args": [15, 2, 7, 11]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4860; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = fabsf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) < 405) {\n    compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 9, 3, 15), \"float32\"), compute: T.Buffer((12, 9, 3, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4860):\n            compute_1 = T.Buffer((4860,), data=compute.data)\n            data_1 = T.Buffer((4860,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.fabs(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [12, 9, 3, 15]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 78; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 78; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] + (ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1])) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      T_subtract[((ax0 * 6) + ax1)] = (ph_0[((ax0 * 6) + ax1)] - ph_3[((ax0 * 6) + ax1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 6, 1), \"float32\"), ph_3: T.Buffer((13, 6, 1), \"float32\"), T_divide: T.Buffer((13, 6, 1), \"float32\"), T_multiply: T.Buffer((13, 6, 1), \"float32\"), T_subtract: T.Buffer((13, 6, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((78,), data=ph_0.data)\n        ph_3_1 = T.Buffer((78,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(78):\n            T_divide_1 = T.Buffer((78,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(78):\n            T_multiply_1 = T.Buffer((78,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + (ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(13):\n            for ax1 in range(6):\n                cse_var_1: T.int32 = ax0 * 6 + ax1\n                T_subtract_1 = T.Buffer((78,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]", "op_args": [["divide", "subtract", "add", "multiply", "subtract"]]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 11115; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = fabsf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 13, 3, 15), \"float32\"), compute: T.Buffer((19, 13, 3, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(11115):\n            compute_1 = T.Buffer((11115,), data=compute.data)\n            data_1 = T.Buffer((11115,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.fabs(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [19, 13, 3, 15]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 323; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = fabsf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n        compute_1[(((i0 * 285) + (i1 * 15)) + i2_1)] = fabsf(asinhf(ph_0[(((i0 * 285) + (i1 * 15)) + i2_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 19, 15), \"float32\"), compute: T.Buffer((17, 19, 15), \"float32\"), compute_1: T.Buffer((17, 19, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4845,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(323):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((4845,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(19, 15):\n                cse_var_2: T.int32 = i0 * 285 + i1 * 15 + i2\n                compute_2 = T.Buffer((4845,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(T.asinh(ph_0_1[cse_var_2]))", "op_args": [["abs", "asinh", "abs"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_multiply[(((ax0 * 99) + (ax1 * 9)) + ax2)] = ((ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)] + (ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)] - ph_3[(((ax0 * 99) + (ax1 * 9)) + ax2)])) * ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1386; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1386; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1386; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 14; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 11; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 9; ++ax2_1) {\n        T_add[(((ax0_1 * 99) + (ax1_1 * 9)) + ax2_1)] = (((ph_0[(((ax0_1 * 99) + (ax1_1 * 9)) + ax2_1)] / ph_3[(((ax0_1 * 99) + (ax1_1 * 9)) + ax2_1)]) * ph_0[(((ax0_1 * 99) + (ax1_1 * 9)) + ax2_1)]) + (ph_0[(((ax0_1 * 99) + (ax1_1 * 9)) + ax2_1)] / ph_3[(((ax0_1 * 99) + (ax1_1 * 9)) + ax2_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_4(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) + (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), ph_3: T.Buffer((14, 11, 9), \"float32\"), T_multiply: T.Buffer((14, 11, 9), \"float32\"), T_subtract: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), compute_1: T.Buffer((14, 11, 9), \"float32\"), T_add: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1386,), data=ph_3.data)\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_1: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_multiply_1 = T.Buffer((1386,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = (ph_0_1[cse_var_1] + (ph_0_1[cse_var_1] - ph_3_1[cse_var_1])) * ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1386):\n            T_subtract_1 = T.Buffer((1386,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1386):\n            compute_2 = T.Buffer((1386,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1386):\n            compute_2 = T.Buffer((1386,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_2: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((1386,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] / ph_3_1[cse_var_2] * ph_0_1[cse_var_2] + ph_0_1[cse_var_2] / ph_3_1[cse_var_2]", "op_args": [["divide", "subtract", "add", "multiply", "subtract", "cos", "acosh", "multiply", "add"]]}{"op_name": "abs", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 16320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = fabsf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fabsf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 12, 4, 20), \"float32\"), compute: T.Buffer((17, 12, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(16320):\n            compute_1 = T.Buffer((16320,), data=compute.data)\n            data_1 = T.Buffer((16320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.fabs(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [17, 12, 4, 20]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = fabsf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 924; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 924; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 924; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 14, 11), \"float32\"), compute: T.Buffer((6, 14, 11), \"float32\"), compute_1: T.Buffer((6, 14, 11), \"float32\"), compute_2: T.Buffer((6, 14, 11), \"float32\"), compute_3: T.Buffer((6, 14, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((924,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_4 = T.Buffer((924,), data=compute.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(924):\n            compute_4 = T.Buffer((924,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(924):\n            compute_4 = T.Buffer((924,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(924):\n            compute_4 = T.Buffer((924,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["abs", "asinh", "abs", "atanh", "acos"]]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4320; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = cosf(data[((i0_i1_fused_i2_fused * 19) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 20, 12, 19), \"float32\"), compute: T.Buffer((18, 20, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4320):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((82080,), data=compute.data)\n                data_1 = T.Buffer((82080,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [18, 20, 12, 19]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n          adaptive_pool_max[((((ax0 * 1152) + (ax1 * 64)) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < ((((((ax2 * 5) + 5) % 8) == 0) ? (((ax2 * 13) + 13) >> 3) : ((((ax2 * 13) + 13) >> 3) + 1)) - ((ax2 * 13) >> 3)); ++rv0) {\n            for (int32_t rv1 = 0; rv1 < ((((((ax3 * 5) + 5) % 8) == 0) ? (((ax3 * 13) + 13) >> 3) : ((((ax3 * 13) + 13) >> 3) + 1)) - ((ax3 * 13) >> 3)); ++rv1) {\n              adaptive_pool_max[((((ax0 * 1152) + (ax1 * 64)) + (ax2 * 8)) + ax3)] = max(adaptive_pool_max[((((ax0 * 1152) + (ax1 * 64)) + (ax2 * 8)) + ax3)], data[((((((ax0 * 3042) + (ax1 * 169)) + (((ax2 * 13) >> 3) * 13)) + (rv0 * 13)) + ((ax3 * 13) >> 3)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < ((((((((((((int)blockIdx.x) * 27) + ((int)threadIdx.x)) & 63) >> 3) * 5) + 5) % 8) == 0) ? (((((((((int)blockIdx.x) * 27) + ((int)threadIdx.x)) & 63) >> 3) * 13) + 13) >> 3) : ((((((((((int)blockIdx.x) * 27) + ((int)threadIdx.x)) & 63) >> 3) * 13) + 13) >> 3) + 1)) - ((((((((int)blockIdx.x) * 27) + ((int)threadIdx.x)) & 63) >> 3) * 13) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < (((((((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 5) + 5) % 8) == 0) ? ((((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 13) + 13) >> 3) : (((((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 13) + 13) >> 3) + 1)) - (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 13) >> 3)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))], data[((((((((((int)blockIdx.x) * 27) + ((int)threadIdx.x)) >> 6) * 169) + (((((((((int)blockIdx.x) * 27) + ((int)threadIdx.x)) & 63) >> 3) * 13) >> 3) * 13)) + (rv0 * 13)) + (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 13) >> 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 18, 13, 13), \"float32\"), adaptive_pool_max: T.Buffer((9, 18, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(9):\n            for ax1, ax2, ax3 in T.grid(18, 8, 8):\n                adaptive_pool_max_1 = T.Buffer((10368,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0 * 1152 + ax1 * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 5 + 5) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 13 // 8, where={cse_var_1: (ax2 * 13 + 13) // 8}), T.Let(T.Select((ax3 * 5 + 5) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 13 // 8, where={cse_var_2: (ax3 * 13 + 13) // 8})):\n                    cse_var_1 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_3: T.int32 = ax0 * 1152 + ax1 * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((27378,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0 * 3042 + ax1 * 169 + ax2 * 13 // 8 * 13 + rv0 * 13 + ax3 * 13 // 8 + rv1])", "op_args": [9, 18, 13, 13]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 144) + (i1 * 12)) + i2)] = atanhf(ph_0[(((i0 * 144) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 576; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 576; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      compute_2[((i0_i1_fused * 12) + i2_1)] = asinhf(fabsf(ph_0[((i0_i1_fused * 12) + i2_1)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((int)blockIdx.x)] = (acosf(ph_0[((int)blockIdx.x)]) + ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 12, 12), \"float32\"), compute: T.Buffer((4, 12, 12), \"float32\"), compute_1: T.Buffer((4, 12, 12), \"float32\"), T_subtract: T.Buffer((4, 12, 12), \"float32\"), T_add: T.Buffer((4, 12, 12), \"float32\"), compute_2: T.Buffer((4, 12, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((576,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_3 = T.Buffer((576,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(12, 12):\n                cse_var_1: T.int32 = i0 * 144 + i1 * 12 + i2\n                compute_3 = T.Buffer((576,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(576):\n            T_subtract_1 = T.Buffer((576,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(576):\n            T_add_1 = T.Buffer((576,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((576,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(T.fabs(ph_0_1[cse_var_2]))", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_subtract[((ax0 * 4) + ax2)] = (ph_0[((ax0 * 4) + ax2)] - sinf((ph_0[((ax0 * 4) + ax2)] + acosf(ph_0[((ax0 * 4) + ax2)]))));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = cosf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - __sinf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 1, 4), \"float32\"), T_subtract: T.Buffer((8, 1, 4), \"float32\"), compute: T.Buffer((8, 1, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        for ax0 in T.parallel(8):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0 * 4 + ax2\n                T_subtract_1 = T.Buffer((32,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.sin(ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(8):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_1 = T.Buffer((32,), data=compute.data)\n                compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2])", "op_args": [["acos", "add", "sin", "subtract", "cos"]]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 2; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 2) + i3)] = cosf(data[((i0_i1_fused_i2_fused * 2) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 7, 2), \"float32\"), compute: T.Buffer((2, 20, 7, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            for i3 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 2 + i3\n                compute_1 = T.Buffer((560,), data=compute.data)\n                data_1 = T.Buffer((560,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [2, 20, 7, 2]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = cosf(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 2, 16), \"float32\"), compute: T.Buffer((7, 20, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((4480,), data=compute.data)\n                data_1 = T.Buffer((4480,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [7, 20, 2, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 45) + (i1 * 5)) + i2)] = acosf(cosf(ph_0[(((i0 * 45) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 495; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - sinf((ph_0[ax0_ax1_fused_ax2_fused] + acosf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 495; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 495; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(acosf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 5), \"float32\"), T_subtract: T.Buffer((11, 9, 5), \"float32\"), compute: T.Buffer((11, 9, 5), \"float32\"), compute_1: T.Buffer((11, 9, 5), \"float32\"), T_mod: T.Buffer((11, 9, 5), \"float32\"), compute_2: T.Buffer((11, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((495,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_3 = T.Buffer((495,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(9, 5):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 5 + i2\n                compute_3 = T.Buffer((495,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acos(T.cos(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(495):\n            T_subtract_1 = T.Buffer((495,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(495):\n            T_mod_1 = T.Buffer((495,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_3 = T.Buffer((495,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.acos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["acos", "add", "sin", "subtract", "cos", "atanh", "acos", "mod", "atan"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 52) + (i1 * 13)) + i2)] = fabsf(asinhf(ph_0[(((i0 * 52) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 8; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 4; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n        compute_1[(((i0_1 * 52) + (i1_1 * 13)) + i2_1)] = atanhf(ph_0[(((i0_1 * 52) + (i1_1 * 13)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 416; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 416; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 416; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 416; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 416; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add_1[ax0_ax1_fused_ax2_fused_2] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_2])) + fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 4, 13), \"float32\"), compute: T.Buffer((8, 4, 13), \"float32\"), compute_1: T.Buffer((8, 4, 13), \"float32\"), T_subtract: T.Buffer((8, 4, 13), \"float32\"), T_add: T.Buffer((8, 4, 13), \"float32\"), compute_2: T.Buffer((8, 4, 13), \"float32\"), compute_3: T.Buffer((8, 4, 13), \"float32\"), T_add_1: T.Buffer((8, 4, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((416,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(4, 13):\n                cse_var_1: T.int32 = i0 * 52 + i1 * 13 + i2\n                compute_4 = T.Buffer((416,), data=compute.data)\n                compute_4[cse_var_1] = T.fabs(T.asinh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(4, 13):\n                cse_var_2: T.int32 = i0 * 52 + i1 * 13 + i2\n                compute_4 = T.Buffer((416,), data=compute_1.data)\n                compute_4[cse_var_2] = T.atanh(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(416):\n            T_subtract_1 = T.Buffer((416,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(416):\n            T_add_2 = T.Buffer((416,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(416):\n            compute_4 = T.Buffer((416,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(416):\n            compute_4 = T.Buffer((416,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(416):\n            T_add_2 = T.Buffer((416,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add"]]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4914; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = cosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 3, 13, 14), \"float32\"), compute: T.Buffer((9, 3, 13, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4914):\n            compute_1 = T.Buffer((4914,), data=compute.data)\n            data_1 = T.Buffer((4914,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [9, 3, 13, 14]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 72; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 6) + i3)] = cosf(data[((i0_i1_fused_i2_fused * 6) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 3, 3, 6), \"float32\"), compute: T.Buffer((8, 3, 3, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 6 + i3\n                compute_1 = T.Buffer((432,), data=compute.data)\n                data_1 = T.Buffer((432,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [8, 3, 3, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_mod, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 48; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - sinf((ph_0[ax0_ax1_fused_ax2_fused] + acosf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 48; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_2[((i0_i1_fused * 8) + i2)] = atanf(acosf(ph_0[((i0_i1_fused * 8) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 48; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add[ax0_ax1_fused_ax2_fused_2] = (asinhf(acosf(ph_0[ax0_ax1_fused_ax2_fused_2])) + acosf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 48; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = acoshf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_2])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 48; ++i0_i1_fused_i2_fused_3) {\n    compute_4[i0_i1_fused_i2_fused_3] = asinf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 48; ++ax0_ax1_fused_ax2_fused_3) {\n    T_divide[ax0_ax1_fused_ax2_fused_3] = (asinhf(acosf(ph_0[ax0_ax1_fused_ax2_fused_3])) / acosf(ph_0[ax0_ax1_fused_ax2_fused_3]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_8(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - __sinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), T_subtract: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_mod: T.Buffer((3, 2, 8), \"float32\"), compute_2: T.Buffer((3, 2, 8), \"float32\"), T_add: T.Buffer((3, 2, 8), \"float32\"), compute_3: T.Buffer((3, 2, 8), \"float32\"), compute_4: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_5 = T.Buffer((48,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_5 = T.Buffer((48,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_subtract_1 = T.Buffer((48,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_mod_1 = T.Buffer((48,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_5 = T.Buffer((48,), data=compute_2.data)\n                compute_5[cse_var_1] = T.atan(T.acos(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_add_1 = T.Buffer((48,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.asinh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_5 = T.Buffer((48,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acosh(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_5 = T.Buffer((48,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asin(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asinh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) / T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["acos", "add", "sin", "subtract", "cos", "atanh", "acos", "mod", "atan", "asinh", "add", "acosh", "asin", "divide"]]}{"op_name": "adaptive_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 247; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < ((((((ax2 * 2) + 2) % 8) == 0) ? (((ax2 * 9) + 9) >> 2) : ((((ax2 * 9) + 9) >> 2) + 1)) - ((ax2 * 18) >> 3)); ++rv0) {\n          for (int32_t rv1 = 0; rv1 < (((((ax3 + 1) % 8) == 0) ? (((ax3 * 17) + 17) >> 3) : ((((ax3 * 17) + 17) >> 3) + 1)) - (ax3 * 2)); ++rv1) {\n            adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)] = max(adaptive_pool_max[(((ax0_ax1_fused * 64) + (ax2 * 8)) + ax3)], data[(((((ax0_ax1_fused * 306) + (((ax2 * 18) >> 3) * 17)) + (rv0 * 17)) + (ax3 * 2)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < (((((((((int)threadIdx.x) >> 3) * 2) + 2) % 8) == 0) ? ((((((int)threadIdx.x) >> 3) * 9) + 9) >> 2) : (((((((int)threadIdx.x) >> 3) * 9) + 9) >> 2) + 1)) - (((((int)threadIdx.x) >> 3) * 18) >> 3)); ++rv0) {\n    for (int rv1 = 0; rv1 < ((((((((int)threadIdx.x) & 7) + 1) % 8) == 0) ? ((((((int)threadIdx.x) & 7) * 17) + 17) >> 3) : (((((((int)threadIdx.x) & 7) * 17) + 17) >> 3) + 1)) - ((((int)threadIdx.x) & 7) * 2)); ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((((int)blockIdx.x) * 306) + ((((((int)threadIdx.x) >> 3) * 18) >> 3) * 17)) + (rv0 * 17)) + ((((int)threadIdx.x) & 7) * 2)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 19, 18, 17), \"float32\"), adaptive_pool_max: T.Buffer((13, 19, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(247):\n            for ax2, ax3 in T.grid(8, 8):\n                adaptive_pool_max_1 = T.Buffer((15808,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0_ax1_fused * 64 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(T.Let(T.Select((ax2 * 2 + 2) % 8 == 0, cse_var_1, cse_var_1 + 1) - ax2 * 18 // 8, where={cse_var_1: (ax2 * 9 + 9) // 4}), T.Let(T.Select((ax3 + 1) % 8 == 0, cse_var_2, cse_var_2 + 1) - ax3 * 2, where={cse_var_2: (ax3 * 17 + 17) // 8})):\n                    cse_var_1 = T.int32()\n                    cse_var_2 = T.int32()\n                    cse_var_3: T.int32 = ax0_ax1_fused * 64 + ax2 * 8 + ax3\n                    data_1 = T.Buffer((75582,), data=data.data)\n                    adaptive_pool_max_1[cse_var_3] = T.max(adaptive_pool_max_1[cse_var_3], data_1[ax0_ax1_fused * 306 + ax2 * 18 // 8 * 17 + rv0 * 17 + ax3 * 2 + rv1])", "op_args": [13, 19, 18, 17]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 13; ++i1) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 10; ++i3) {\n        compute[(((i1 * 60) + (i2 * 10)) + i3)] = cosf(data[(((i1 * 60) + (i2 * 10)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 13, 6, 10), \"float32\"), compute: T.Buffer((1, 13, 6, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(13, 6, 10):\n            cse_var_1: T.int32 = i1 * 60 + i2 * 10 + i3\n            compute_1 = T.Buffer((780,), data=compute.data)\n            data_1 = T.Buffer((780,), data=data.data)\n            compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [1, 13, 6, 10]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 168; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n        T_add[(((ax0_ax1_fused * 12) + (ax2 * 3)) + ax3)] = (data[(((ax0_ax1_fused * 12) + (ax2 * 3)) + ax3)] + data_1[(((ax0_ax1_fused * 12) + (ax2 * 3)) + ax3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 14, 4, 3), \"float32\"), data_1: T.Buffer((12, 14, 4, 3), \"float32\"), T_add: T.Buffer((12, 14, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(168):\n            for ax2, ax3 in T.grid(4, 3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2 * 3 + ax3\n                T_add_1 = T.Buffer((2016,), data=T_add.data)\n                data_2 = T.Buffer((2016,), data=data.data)\n                data_3 = T.Buffer((2016,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [12, 14, 4, 3]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i2 = 0; i2 < 17; ++i2) {\n    for (int32_t i3 = 0; i3 < 7; ++i3) {\n      compute[((i2 * 7) + i3)] = cosf(data[((i2 * 7) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 1, 17, 7), \"float32\"), compute: T.Buffer((1, 1, 17, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i2, i3 in T.grid(17, 7):\n            cse_var_1: T.int32 = i2 * 7 + i3\n            compute_1 = T.Buffer((119,), data=compute.data)\n            data_1 = T.Buffer((119,), data=data.data)\n            compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [1, 1, 17, 7]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = fabsf(asinhf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 176; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 176; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 176; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 176; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_add_1[(((ax0 * 16) + (ax1 * 8)) + ax2)] = (acoshf(fabsf(ph_0[(((ax0 * 16) + (ax1 * 8)) + ax2)])) + fabsf(ph_0[(((ax0 * 16) + (ax1 * 8)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 176; ++i0_i1_fused_i2_fused_3) {\n    compute_4[i0_i1_fused_i2_fused_3] = asinhf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 176; ++i0_i1_fused_i2_fused_4) {\n    compute_5[i0_i1_fused_i2_fused_4] = acosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 176; ++i0_i1_fused_i2_fused_5) {\n    compute_6[i0_i1_fused_i2_fused_5] = cosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 8), \"float32\"), compute: T.Buffer((11, 2, 8), \"float32\"), compute_1: T.Buffer((11, 2, 8), \"float32\"), T_subtract: T.Buffer((11, 2, 8), \"float32\"), T_add: T.Buffer((11, 2, 8), \"float32\"), compute_2: T.Buffer((11, 2, 8), \"float32\"), compute_3: T.Buffer((11, 2, 8), \"float32\"), T_add_1: T.Buffer((11, 2, 8), \"float32\"), compute_4: T.Buffer((11, 2, 8), \"float32\"), compute_5: T.Buffer((11, 2, 8), \"float32\"), compute_6: T.Buffer((11, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((176,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_7 = T.Buffer((176,), data=compute.data)\n                compute_7[cse_var_1] = T.fabs(T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_7 = T.Buffer((176,), data=compute_1.data)\n            compute_7[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(176):\n            T_subtract_1 = T.Buffer((176,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(176):\n            T_add_2 = T.Buffer((176,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_7 = T.Buffer((176,), data=compute_2.data)\n            compute_7[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_7 = T.Buffer((176,), data=compute_3.data)\n            compute_7[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(2, 8):\n                cse_var_2: T.int32 = ax0 * 16 + ax1 * 8 + ax2\n                T_add_2 = T.Buffer((176,), data=T_add_1.data)\n                T_add_2[cse_var_2] = T.acosh(T.fabs(ph_0_1[cse_var_2])) + T.fabs(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_7 = T.Buffer((176,), data=compute_4.data)\n            compute_7[i0_i1_fused_i2_fused] = T.asinh(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_7 = T.Buffer((176,), data=compute_5.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_7 = T.Buffer((176,), data=compute_6.data)\n            compute_7[i0_i1_fused_i2_fused] = T.cos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add", "asinh", "acos", "cos"]]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 7; ++ax3) {\n          T_add[((((ax0 * 147) + (ax1 * 49)) + (ax2 * 7)) + ax3)] = (data[((((ax0 * 147) + (ax1 * 49)) + (ax2 * 7)) + ax3)] + data_1[((((ax0 * 147) + (ax1 * 49)) + (ax2 * 7)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 7)) < 105) {\n    T_add[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 3, 7, 7), \"float32\"), data_1: T.Buffer((5, 3, 7, 7), \"float32\"), T_add: T.Buffer((5, 3, 7, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            for ax1, ax2, ax3 in T.grid(3, 7, 7):\n                cse_var_1: T.int32 = ax0 * 147 + ax1 * 49 + ax2 * 7 + ax3\n                T_add_1 = T.Buffer((735,), data=T_add.data)\n                data_2 = T.Buffer((735,), data=data.data)\n                data_3 = T.Buffer((735,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [5, 3, 7, 7]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 3; ++i1) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i1 * 260) + (i2 * 13)) + i3)] = cosf(data[(((i1 * 260) + (i2 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 12)) < 65) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 3, 20, 13), \"float32\"), compute: T.Buffer((1, 3, 20, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(3, 20, 13):\n            cse_var_1: T.int32 = i1 * 260 + i2 * 13 + i3\n            compute_1 = T.Buffer((780,), data=compute.data)\n            data_1 = T.Buffer((780,), data=data.data)\n            compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [1, 3, 20, 13]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        T_add[(((ax0_ax1_fused * 40) + (ax2 * 20)) + ax3)] = (data[(((ax0_ax1_fused * 40) + (ax2 * 20)) + ax3)] + data_1[(((ax0_ax1_fused * 40) + (ax2 * 20)) + ax3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 3, 2, 20), \"float32\"), data_1: T.Buffer((2, 3, 2, 20), \"float32\"), T_add: T.Buffer((2, 3, 2, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2, ax3 in T.grid(2, 20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 40 + ax2 * 20 + ax3\n                T_add_1 = T.Buffer((240,), data=T_add.data)\n                data_2 = T.Buffer((240,), data=data.data)\n                data_3 = T.Buffer((240,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [2, 3, 2, 20]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1792; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = cosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 2, 8), \"float32\"), compute: T.Buffer((7, 16, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1792):\n            compute_1 = T.Buffer((1792,), data=compute.data)\n            data_1 = T.Buffer((1792,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [7, 16, 2, 8]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 57; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n        T_add[(((ax0_ax1_fused * 176) + (ax2 * 16)) + ax3)] = (data[(((ax0_ax1_fused * 176) + (ax2 * 16)) + ax3)] + data_1[(((ax0_ax1_fused * 176) + (ax2 * 16)) + ax3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 3, 11, 16), \"float32\"), data_1: T.Buffer((19, 3, 11, 16), \"float32\"), T_add: T.Buffer((19, 3, 11, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(57):\n            for ax2, ax3 in T.grid(11, 16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 176 + ax2 * 16 + ax3\n                T_add_1 = T.Buffer((10032,), data=T_add.data)\n                data_2 = T.Buffer((10032,), data=data.data)\n                data_3 = T.Buffer((10032,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [19, 3, 11, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_divide, float* T_divide_1, float* T_mod, float* T_mod_1, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 864; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 864; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 864; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanf(acosf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 864; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (asinhf(acosf(ph_0[ax0_ax1_fused_ax2_fused_1])) + acosf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 864; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = acoshf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 864; ++i0_i1_fused_i2_fused_4) {\n    compute_4[i0_i1_fused_i2_fused_4] = asinf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 864; ++ax0_ax1_fused_ax2_fused_2) {\n    T_divide[ax0_ax1_fused_ax2_fused_2] = (asinhf(acosf(ph_0[ax0_ax1_fused_ax2_fused_2])) / acosf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 864; ++i0_i1_fused_i2_fused_5) {\n    compute_5[i0_i1_fused_i2_fused_5] = asinf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 864; ++ax0_ax1_fused_ax2_fused_3) {\n    T_subtract[ax0_ax1_fused_ax2_fused_3] = (ph_0[ax0_ax1_fused_ax2_fused_3] - sinf((ph_0[ax0_ax1_fused_ax2_fused_3] + acosf(ph_0[ax0_ax1_fused_ax2_fused_3]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_divide_1[(((ax0 * 48) + (ax1 * 6)) + ax2)] = (sinf((ph_0[(((ax0 * 48) + (ax1 * 6)) + ax2)] + acosf(ph_0[(((ax0 * 48) + (ax1 * 6)) + ax2)]))) / ph_0[(((ax0 * 48) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 864; ++i0_i1_fused_i2_fused_6) {\n    compute_6[i0_i1_fused_i2_fused_6] = acosf(sinf((ph_0[i0_i1_fused_i2_fused_6] + acosf(ph_0[i0_i1_fused_i2_fused_6]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_4 = 0; ax0_ax1_fused_ax2_fused_4 < 864; ++ax0_ax1_fused_ax2_fused_4) {\n    T_add_1[ax0_ax1_fused_ax2_fused_4] = (sinf((ph_0[ax0_ax1_fused_ax2_fused_4] + acosf(ph_0[ax0_ax1_fused_ax2_fused_4]))) + ph_0[ax0_ax1_fused_ax2_fused_4]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_5 = 0; ax0_ax1_fused_ax2_fused_5 < 864; ++ax0_ax1_fused_ax2_fused_5) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_5] = fmodf(sinf((ph_0[ax0_ax1_fused_ax2_fused_5] + acosf(ph_0[ax0_ax1_fused_ax2_fused_5]))), ph_0[ax0_ax1_fused_ax2_fused_5]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_7(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_13(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_12(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 8, 6), \"float32\"), T_subtract: T.Buffer((18, 8, 6), \"float32\"), compute: T.Buffer((18, 8, 6), \"float32\"), compute_1: T.Buffer((18, 8, 6), \"float32\"), T_mod: T.Buffer((18, 8, 6), \"float32\"), compute_2: T.Buffer((18, 8, 6), \"float32\"), T_add: T.Buffer((18, 8, 6), \"float32\"), compute_3: T.Buffer((18, 8, 6), \"float32\"), compute_4: T.Buffer((18, 8, 6), \"float32\"), T_divide: T.Buffer((18, 8, 6), \"float32\"), compute_5: T.Buffer((18, 8, 6), \"float32\"), T_divide_1: T.Buffer((18, 8, 6), \"float32\"), compute_6: T.Buffer((18, 8, 6), \"float32\"), T_add_1: T.Buffer((18, 8, 6), \"float32\"), T_mod_1: T.Buffer((18, 8, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((864,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute.data)\n            compute_7[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute_1.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_mod_2 = T.Buffer((864,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute_2.data)\n            compute_7[i0_i1_fused_i2_fused] = T.atan(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_add_2 = T.Buffer((864,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.asinh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute_3.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acosh(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute_4.data)\n            compute_7[i0_i1_fused_i2_fused] = T.asin(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_divide_2 = T.Buffer((864,), data=T_divide.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.asinh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) / T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute_5.data)\n            compute_7[i0_i1_fused_i2_fused] = T.asin(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_subtract_1 = T.Buffer((864,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(8, 6):\n                cse_var_1: T.int32 = ax0 * 48 + ax1 * 6 + ax2\n                T_divide_2 = T.Buffer((864,), data=T_divide_1.data)\n                T_divide_2[cse_var_1] = T.sin(ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1])) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_7 = T.Buffer((864,), data=compute_6.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused] + T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_add_2 = T.Buffer((864,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_mod_2 = T.Buffer((864,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])), ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["acos", "add", "sin", "subtract", "cos", "atanh", "acos", "mod", "atan", "asinh", "add", "acosh", "asin", "divide", "asin", "divide", "acos", "add", "mod"]]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 100) + (i1 * 20)) + (i2 * 10)) + i3)] = cosf(data[((((i0 * 100) + (i1 * 20)) + (i2 * 10)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 5, 2, 10), \"float32\"), compute: T.Buffer((11, 5, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            for i1, i2, i3 in T.grid(5, 2, 10):\n                cse_var_1: T.int32 = i0 * 100 + i1 * 20 + i2 * 10 + i3\n                compute_1 = T.Buffer((1100,), data=compute.data)\n                data_1 = T.Buffer((1100,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [11, 5, 2, 10]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 969; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_add[ax0_ax1_fused_ax2_fused_ax3_fused] = (data[ax0_ax1_fused_ax2_fused_ax3_fused] + data_1[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) / 3)) < 323) {\n    T_add[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 1, 17, 3), \"float32\"), data_1: T.Buffer((19, 1, 17, 3), \"float32\"), T_add: T.Buffer((19, 1, 17, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(969):\n            T_add_1 = T.Buffer((969,), data=T_add.data)\n            data_2 = T.Buffer((969,), data=data.data)\n            data_3 = T.Buffer((969,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_2[ax0_ax1_fused_ax2_fused_ax3_fused] + data_3[ax0_ax1_fused_ax2_fused_ax3_fused]", "op_args": [19, 1, 17, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 836; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 76) + (i1 * 19)) + i2)] = fabsf(ph_0[(((i0 * 76) + (i1 * 19)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 44; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_multiply_1[((ax0_ax1_fused * 19) + ax2)] = (cosf(ph_0[((ax0_ax1_fused * 19) + ax2)]) * ph_0[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 4, 19), \"float32\"), ph_3: T.Buffer((11, 4, 19), \"float32\"), T_multiply: T.Buffer((11, 4, 19), \"float32\"), compute: T.Buffer((11, 4, 19), \"float32\"), T_multiply_1: T.Buffer((11, 4, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((836,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(836):\n            T_multiply_2 = T.Buffer((836,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((836,), data=ph_3.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(4, 19):\n                cse_var_1: T.int32 = i0 * 76 + i1 * 19 + i2\n                compute_1 = T.Buffer((836,), data=compute.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(44):\n            for ax2 in range(19):\n                cse_var_2: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_multiply_2 = T.Buffer((836,), data=T_multiply_1.data)\n                T_multiply_2[cse_var_2] = T.cos(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]", "op_args": [["multiply", "abs", "cos", "multiply"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* compute_7, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5054; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5054; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 5054; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5054; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 5054; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 5054; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add_1[ax0_ax1_fused_ax2_fused_2] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_2])) + fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 5054; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = asinhf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 5054; ++i0_i1_fused_i2_fused_4) {\n    compute_4[i0_i1_fused_i2_fused_4] = acosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 5054; ++i0_i1_fused_i2_fused_5) {\n    compute_5[i0_i1_fused_i2_fused_5] = cosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_multiply[(((ax0 * 266) + (ax1 * 19)) + ax2)] = (acoshf(fabsf(ph_0[(((ax0 * 266) + (ax1 * 19)) + ax2)])) * fabsf(ph_0[(((ax0 * 266) + (ax1 * 19)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 5054; ++i0_i1_fused_i2_fused_6) {\n    compute_6[i0_i1_fused_i2_fused_6] = cosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_6])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_7 = 0; i0_i1_fused_i2_fused_7 < 5054; ++i0_i1_fused_i2_fused_7) {\n    compute_7[i0_i1_fused_i2_fused_7] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_7])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_5(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])) + fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 14, 19), \"float32\"), compute: T.Buffer((19, 14, 19), \"float32\"), T_subtract: T.Buffer((19, 14, 19), \"float32\"), T_add: T.Buffer((19, 14, 19), \"float32\"), compute_1: T.Buffer((19, 14, 19), \"float32\"), compute_2: T.Buffer((19, 14, 19), \"float32\"), T_add_1: T.Buffer((19, 14, 19), \"float32\"), compute_3: T.Buffer((19, 14, 19), \"float32\"), compute_4: T.Buffer((19, 14, 19), \"float32\"), compute_5: T.Buffer((19, 14, 19), \"float32\"), T_multiply: T.Buffer((19, 14, 19), \"float32\"), compute_6: T.Buffer((19, 14, 19), \"float32\"), compute_7: T.Buffer((19, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5054,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute.data)\n            compute_8[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(5054):\n            T_subtract_1 = T.Buffer((5054,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(5054):\n            T_add_2 = T.Buffer((5054,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_1.data)\n            compute_8[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_2.data)\n            compute_8[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(5054):\n            T_add_2 = T.Buffer((5054,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_3.data)\n            compute_8[i0_i1_fused_i2_fused] = T.asinh(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_4.data)\n            compute_8[i0_i1_fused_i2_fused] = T.acos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_5.data)\n            compute_8[i0_i1_fused_i2_fused] = T.cos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(14, 19):\n                cse_var_1: T.int32 = ax0 * 266 + ax1 * 19 + ax2\n                T_multiply_1 = T.Buffer((5054,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.acosh(T.fabs(ph_0_1[cse_var_1])) * T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_6.data)\n            compute_8[i0_i1_fused_i2_fused] = T.cos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_8 = T.Buffer((5054,), data=compute_7.data)\n            compute_8[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add", "asinh", "acos", "cos", "multiply", "cos", "acos"]]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 408; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 11; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3)] = cosf(data[((i0_i1_fused_i2_fused * 11) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 6, 4, 11), \"float32\"), compute: T.Buffer((17, 6, 4, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(408):\n            for i3 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 11 + i3\n                compute_1 = T.Buffer((4488,), data=compute.data)\n                data_1 = T.Buffer((4488,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [17, 6, 4, 11]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 833; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 9) + ax3)] = (data[((ax0_ax1_fused_ax2_fused * 9) + ax3)] + data_1[((ax0_ax1_fused_ax2_fused * 9) + ax3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 7497) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 7, 7, 9), \"float32\"), data_1: T.Buffer((17, 7, 7, 9), \"float32\"), T_add: T.Buffer((17, 7, 7, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(833):\n            for ax3 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 9 + ax3\n                T_add_1 = T.Buffer((7497,), data=T_add.data)\n                data_2 = T.Buffer((7497,), data=data.data)\n                data_3 = T.Buffer((7497,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [17, 7, 7, 9]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 560; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 5; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 5) + i3)] = cosf(data[((i0_i1_fused_i2_fused * 5) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 4, 7, 5), \"float32\"), compute: T.Buffer((20, 4, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(560):\n            for i3 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 5 + i3\n                compute_1 = T.Buffer((2800,), data=compute.data)\n                data_1 = T.Buffer((2800,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [20, 4, 7, 5]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_add[((((ax0 * 5832) + (ax1 * 324)) + (ax2 * 18)) + ax3)] = (data[((((ax0 * 5832) + (ax1 * 324)) + (ax2 * 18)) + ax3)] + data_1[((((ax0 * 5832) + (ax1 * 324)) + (ax2 * 18)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 18, 18, 18), \"float32\"), data_1: T.Buffer((11, 18, 18, 18), \"float32\"), T_add: T.Buffer((11, 18, 18, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(11):\n            for ax1, ax2, ax3 in T.grid(18, 18, 18):\n                cse_var_1: T.int32 = ax0 * 5832 + ax1 * 324 + ax2 * 18 + ax3\n                T_add_1 = T.Buffer((64152,), data=T_add.data)\n                data_2 = T.Buffer((64152,), data=data.data)\n                data_3 = T.Buffer((64152,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [11, 18, 18, 18]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_add[((((ax0 * 810) + (ax1 * 270)) + (ax2 * 18)) + ax3)] = (data[((((ax0 * 810) + (ax1 * 270)) + (ax2 * 18)) + ax3)] + data_1[((((ax0 * 810) + (ax1 * 270)) + (ax2 * 18)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 3, 15, 18), \"float32\"), data_1: T.Buffer((2, 3, 15, 18), \"float32\"), T_add: T.Buffer((2, 3, 15, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            for ax1, ax2, ax3 in T.grid(3, 15, 18):\n                cse_var_1: T.int32 = ax0 * 810 + ax1 * 270 + ax2 * 18 + ax3\n                T_add_1 = T.Buffer((1620,), data=T_add.data)\n                data_2 = T.Buffer((1620,), data=data.data)\n                data_3 = T.Buffer((1620,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [2, 3, 15, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 300; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * atanf((ph_0[ax0_ax1_fused_ax2_fused] * atanhf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * atanf((ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * atanhf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 5, 4), \"float32\"), T_multiply: T.Buffer((15, 5, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(300):\n            T_multiply_1 = T.Buffer((300,), data=T_multiply.data)\n            ph_0_1 = T.Buffer((300,), data=ph_0.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.atan(ph_0_1[ax0_ax1_fused_ax2_fused] * T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]))", "op_args": [["atanh", "multiply", "atan", "multiply"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* compute_7, float* compute_8, float* compute_9, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 648; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 648; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 648; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 648; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 648; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add_1[ax0_ax1_fused_ax2_fused_2] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_2])) + fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute_3[(((i0 * 162) + (i1 * 9)) + i2)] = asinhf(acoshf(fabsf(ph_0[(((i0 * 162) + (i1 * 9)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 648; ++i0_i1_fused_i2_fused_3) {\n    compute_4[i0_i1_fused_i2_fused_3] = acosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 648; ++i0_i1_fused_i2_fused_4) {\n    compute_5[i0_i1_fused_i2_fused_4] = cosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 648; ++ax0_ax1_fused_ax2_fused_3) {\n    T_multiply[ax0_ax1_fused_ax2_fused_3] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_3])) * fabsf(ph_0[ax0_ax1_fused_ax2_fused_3]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 648; ++i0_i1_fused_i2_fused_5) {\n    compute_6[i0_i1_fused_i2_fused_5] = cosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 648; ++i0_i1_fused_i2_fused_6) {\n    compute_7[i0_i1_fused_i2_fused_6] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_6])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_7 = 0; i0_i1_fused_i2_fused_7 < 648; ++i0_i1_fused_i2_fused_7) {\n    compute_8[i0_i1_fused_i2_fused_7] = atanhf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_7])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_8 = 0; i0_i1_fused_i2_fused_8 < 648; ++i0_i1_fused_i2_fused_8) {\n    compute_9[i0_i1_fused_i2_fused_8] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_8])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_5(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((int)blockIdx.x)] = (acoshf(fabsf(ph_0[((int)blockIdx.x)])) + fabsf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_13(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_12(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 18, 9), \"float32\"), compute: T.Buffer((4, 18, 9), \"float32\"), T_subtract: T.Buffer((4, 18, 9), \"float32\"), T_add: T.Buffer((4, 18, 9), \"float32\"), compute_1: T.Buffer((4, 18, 9), \"float32\"), compute_2: T.Buffer((4, 18, 9), \"float32\"), T_add_1: T.Buffer((4, 18, 9), \"float32\"), compute_3: T.Buffer((4, 18, 9), \"float32\"), compute_4: T.Buffer((4, 18, 9), \"float32\"), compute_5: T.Buffer((4, 18, 9), \"float32\"), T_multiply: T.Buffer((4, 18, 9), \"float32\"), compute_6: T.Buffer((4, 18, 9), \"float32\"), compute_7: T.Buffer((4, 18, 9), \"float32\"), compute_8: T.Buffer((4, 18, 9), \"float32\"), compute_9: T.Buffer((4, 18, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((648,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute.data)\n            compute_10[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_subtract_1 = T.Buffer((648,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_add_2 = T.Buffer((648,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_1.data)\n            compute_10[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_2.data)\n            compute_10[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_add_2 = T.Buffer((648,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(18, 9):\n                cse_var_1: T.int32 = i0 * 162 + i1 * 9 + i2\n                compute_10 = T.Buffer((648,), data=compute_3.data)\n                compute_10[cse_var_1] = T.asinh(T.acosh(T.fabs(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_4.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_5.data)\n            compute_10[i0_i1_fused_i2_fused] = T.cos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_multiply_1 = T.Buffer((648,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) * T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_6.data)\n            compute_10[i0_i1_fused_i2_fused] = T.cos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_7.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_8.data)\n            compute_10[i0_i1_fused_i2_fused] = T.atanh(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_10 = T.Buffer((648,), data=compute_9.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add", "asinh", "acos", "cos", "multiply", "cos", "acos", "atanh", "acos"]]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 720; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_add[ax0_ax1_fused_ax2_fused_ax3_fused] = (data[ax0_ax1_fused_ax2_fused_ax3_fused] + data_1[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 6, 12, 5), \"float32\"), data_1: T.Buffer((2, 6, 12, 5), \"float32\"), T_add: T.Buffer((2, 6, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(720):\n            T_add_1 = T.Buffer((720,), data=T_add.data)\n            data_2 = T.Buffer((720,), data=data.data)\n            data_3 = T.Buffer((720,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_2[ax0_ax1_fused_ax2_fused_ax3_fused] + data_3[ax0_ax1_fused_ax2_fused_ax3_fused]", "op_args": [2, 6, 12, 5]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 16; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_mod[((ax0_ax1_fused * 12) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 12) + ax2)], ph_3[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 16; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 12; ++ax2_1) {\n      T_add[((ax0_ax1_fused_1 * 12) + ax2_1)] = (ph_0[((ax0_ax1_fused_1 * 12) + ax2_1)] + ph_3[((ax0_ax1_fused_1 * 12) + ax2_1)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 16; ++i1) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i1 * 12) + i2)] = fabsf(ph_0[((i1 * 12) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((int)blockIdx.x)] = fmodf(ph_0[((int)blockIdx.x)], ph_3[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 16, 12), \"float32\"), ph_3: T.Buffer((1, 16, 12), \"float32\"), T_mod: T.Buffer((1, 16, 12), \"float32\"), T_add: T.Buffer((1, 16, 12), \"float32\"), compute: T.Buffer((1, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((192,), data=ph_0.data)\n        ph_3_1 = T.Buffer((192,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(16):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_mod_1 = T.Buffer((192,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(16):\n            for ax2 in range(12):\n                cse_var_2: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_add_1 = T.Buffer((192,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] + ph_3_1[cse_var_2]\n        for i1, i2 in T.grid(16, 12):\n            cse_var_3: T.int32 = i1 * 12 + i2\n            compute_1 = T.Buffer((192,), data=compute.data)\n            compute_1[cse_var_3] = T.fabs(ph_0_1[cse_var_3])", "op_args": [["mod", "add", "abs"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 35) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)] = sinf(ph_0[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            ph_3_1 = T.Buffer((315,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "cos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = cosf(data[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 17, 1), \"float32\"), compute: T.Buffer((8, 16, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(128):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_1 = T.Buffer((2176,), data=compute.data)\n                data_1 = T.Buffer((2176,), data=data.data)\n                compute_1[cse_var_1] = T.cos(data_1[cse_var_1])", "op_args": [8, 16, 17, 1]}{"op_name": "add", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n          T_add[((((ax0 * 336) + (ax1 * 21)) + (ax2 * 3)) + ax3)] = (data[((((ax0 * 336) + (ax1 * 21)) + (ax2 * 3)) + ax3)] + data_1[((((ax0 * 336) + (ax1 * 21)) + (ax2 * 3)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 7, 3), \"float32\"), data_1: T.Buffer((2, 16, 7, 3), \"float32\"), T_add: T.Buffer((2, 16, 7, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            for ax1, ax2, ax3 in T.grid(16, 7, 3):\n                cse_var_1: T.int32 = ax0 * 336 + ax1 * 21 + ax2 * 3 + ax3\n                T_add_1 = T.Buffer((672,), data=T_add.data)\n                data_2 = T.Buffer((672,), data=data.data)\n                data_3 = T.Buffer((672,), data=data_1.data)\n                T_add_1[cse_var_1] = data_2[cse_var_1] + data_3[cse_var_1]", "op_args": [2, 16, 7, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 99) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 99) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_mod[(((ax0 * 99) + (ax1 * 9)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]), ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(acosf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(11, 9):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_2: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = asinf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_1[(((i0 * 24) + (i1 * 4)) + i2_1)] = asinhf(atanhf(ph_0[(((i0 * 24) + (i1 * 4)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 6, 4), \"float32\"), compute: T.Buffer((10, 6, 4), \"float32\"), compute_1: T.Buffer((10, 6, 4), \"float32\"), compute_2: T.Buffer((10, 6, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_3 = T.Buffer((240,), data=compute.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(6, 4):\n                cse_var_2: T.int32 = i0 * 24 + i1 * 4 + i2\n                compute_3 = T.Buffer((240,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asinh(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["asin", "atanh", "asinh", "cos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3200; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3200; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 160) + (i1 * 16)) + i2)] = sinf(asinhf(ph_0[(((i0 * 160) + (i1 * 16)) + i2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 10, 16), \"float32\"), ph_3: T.Buffer((20, 10, 16), \"float32\"), T_mod: T.Buffer((20, 10, 16), \"float32\"), compute: T.Buffer((20, 10, 16), \"float32\"), compute_1: T.Buffer((20, 10, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3200,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3200):\n            T_mod_1 = T.Buffer((3200,), data=T_mod.data)\n            ph_3_1 = T.Buffer((3200,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3200):\n            compute_2 = T.Buffer((3200,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(10, 16):\n                cse_var_1: T.int32 = i0 * 160 + i1 * 16 + i2\n                compute_2 = T.Buffer((3200,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(T.asinh(ph_0_1[cse_var_1]))", "op_args": [["mod", "abs", "asinh", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_divide, float* T_divide_1, float* T_mod, float* T_mod_1, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* compute_7, float* compute_8, float* compute_9, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute_5[i0_i1_fused_i2_fused] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 385; ++i0_i1_fused_i2_fused_1) {\n    compute_9[i0_i1_fused_i2_fused_1] = acosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 385; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 385; ++i0_i1_fused_i2_fused_2) {\n    compute_8[i0_i1_fused_i2_fused_2] = atanf(acosf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 385; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (asinhf(acosf(ph_0[ax0_ax1_fused_ax2_fused_1])) + acosf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 385; ++i0_i1_fused_i2_fused_3) {\n    compute_7[i0_i1_fused_i2_fused_3] = acoshf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 385; ++i0_i1_fused_i2_fused_4) {\n    compute_6[i0_i1_fused_i2_fused_4] = asinf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 385; ++ax0_ax1_fused_ax2_fused_2) {\n    T_divide[ax0_ax1_fused_ax2_fused_2] = (asinhf(acosf(ph_0[ax0_ax1_fused_ax2_fused_2])) / acosf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 385; ++i0_i1_fused_i2_fused_5) {\n    compute[i0_i1_fused_i2_fused_5] = asinf(asinhf(acosf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 385; ++ax0_ax1_fused_ax2_fused_3) {\n    T_subtract[ax0_ax1_fused_ax2_fused_3] = (ph_0[ax0_ax1_fused_ax2_fused_3] - sinf((ph_0[ax0_ax1_fused_ax2_fused_3] + acosf(ph_0[ax0_ax1_fused_ax2_fused_3]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 35; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_divide_1[((ax0_ax1_fused * 11) + ax2)] = (sinf((ph_0[((ax0_ax1_fused * 11) + ax2)] + acosf(ph_0[((ax0_ax1_fused * 11) + ax2)]))) / ph_0[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 385; ++i0_i1_fused_i2_fused_6) {\n    compute_4[i0_i1_fused_i2_fused_6] = acosf(sinf((ph_0[i0_i1_fused_i2_fused_6] + acosf(ph_0[i0_i1_fused_i2_fused_6]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_4 = 0; ax0_ax1_fused_ax2_fused_4 < 385; ++ax0_ax1_fused_ax2_fused_4) {\n    T_add_1[ax0_ax1_fused_ax2_fused_4] = (sinf((ph_0[ax0_ax1_fused_ax2_fused_4] + acosf(ph_0[ax0_ax1_fused_ax2_fused_4]))) + ph_0[ax0_ax1_fused_ax2_fused_4]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_5 = 0; ax0_ax1_fused_ax2_fused_5 < 385; ++ax0_ax1_fused_ax2_fused_5) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_5] = fmodf(sinf((ph_0[ax0_ax1_fused_ax2_fused_5] + acosf(ph_0[ax0_ax1_fused_ax2_fused_5]))), ph_0[ax0_ax1_fused_ax2_fused_5]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_7 = 0; i0_i1_fused_i2_fused_7 < 385; ++i0_i1_fused_i2_fused_7) {\n    compute_3[i0_i1_fused_i2_fused_7] = atanf(fmodf(sinf((ph_0[i0_i1_fused_i2_fused_7] + acosf(ph_0[i0_i1_fused_i2_fused_7]))), ph_0[i0_i1_fused_i2_fused_7]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_8 = 0; i0_i1_fused_i2_fused_8 < 385; ++i0_i1_fused_i2_fused_8) {\n    compute_2[i0_i1_fused_i2_fused_8] = sinf(fmodf(sinf((ph_0[i0_i1_fused_i2_fused_8] + acosf(ph_0[i0_i1_fused_i2_fused_8]))), ph_0[i0_i1_fused_i2_fused_8]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_9 = 0; i0_i1_fused_i2_fused_9 < 385; ++i0_i1_fused_i2_fused_9) {\n    compute_1[i0_i1_fused_i2_fused_9] = acosf(fmodf(sinf((ph_0[i0_i1_fused_i2_fused_9] + acosf(ph_0[i0_i1_fused_i2_fused_9]))), ph_0[i0_i1_fused_i2_fused_9]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_12(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_16(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fmodf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_7(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_15(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(fmodf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_13(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_5(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinhf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_14(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(fmodf(__sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), T_subtract: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute_2: T.Buffer((5, 7, 11), \"float32\"), T_add: T.Buffer((5, 7, 11), \"float32\"), compute_3: T.Buffer((5, 7, 11), \"float32\"), compute_4: T.Buffer((5, 7, 11), \"float32\"), T_divide: T.Buffer((5, 7, 11), \"float32\"), compute_5: T.Buffer((5, 7, 11), \"float32\"), T_divide_1: T.Buffer((5, 7, 11), \"float32\"), compute_6: T.Buffer((5, 7, 11), \"float32\"), T_add_1: T.Buffer((5, 7, 11), \"float32\"), T_mod_1: T.Buffer((5, 7, 11), \"float32\"), compute_7: T.Buffer((5, 7, 11), \"float32\"), compute_8: T.Buffer((5, 7, 11), \"float32\"), compute_9: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute.data)\n            compute_10[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_1.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_mod_2 = T.Buffer((385,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_2.data)\n            compute_10[i0_i1_fused_i2_fused] = T.atan(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_add_2 = T.Buffer((385,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.asinh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_3.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acosh(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_4.data)\n            compute_10[i0_i1_fused_i2_fused] = T.asin(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_divide_2 = T.Buffer((385,), data=T_divide.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.asinh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) / T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_5.data)\n            compute_10[i0_i1_fused_i2_fused] = T.asin(T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_subtract_1 = T.Buffer((385,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused in T.parallel(35):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_divide_2 = T.Buffer((385,), data=T_divide_1.data)\n                T_divide_2[cse_var_1] = T.sin(ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1])) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_6.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused] + T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_add_2 = T.Buffer((385,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_mod_2 = T.Buffer((385,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_7.data)\n            compute_10[i0_i1_fused_i2_fused] = T.atan(T.truncmod(T.sin(ph_0_1[i0_i1_fused_i2_fused] + T.acos(ph_0_1[i0_i1_fused_i2_fused])), ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_8.data)\n            compute_10[i0_i1_fused_i2_fused] = T.sin(T.truncmod(T.sin(ph_0_1[i0_i1_fused_i2_fused] + T.acos(ph_0_1[i0_i1_fused_i2_fused])), ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_10 = T.Buffer((385,), data=compute_9.data)\n            compute_10[i0_i1_fused_i2_fused] = T.acos(T.truncmod(T.sin(ph_0_1[i0_i1_fused_i2_fused] + T.acos(ph_0_1[i0_i1_fused_i2_fused])), ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["acos", "add", "sin", "subtract", "cos", "atanh", "acos", "mod", "atan", "asinh", "add", "acosh", "asin", "divide", "asin", "divide", "acos", "add", "mod", "mod", "atan", "sin", "acos"]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7560; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = atanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 6, 9), \"float32\"), compute: T.Buffer((7, 20, 6, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(7560):\n            compute_1 = T.Buffer((7560,), data=compute.data)\n            data_1 = T.Buffer((7560,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atan(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [7, 20, 6, 9]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n        compute[(((i0_i1_fused * 121) + (i2 * 11)) + i3_s)] = atanf(data[(((i0_i1_fused * 121) + (i2 * 11)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 5445) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 6, 11, 11), \"float32\"), compute: T.Buffer((15, 6, 11, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(90):\n            for i2, i3_s in T.grid(11, 11):\n                cse_var_1: T.int32 = i0_i1_fused * 121 + i2 * 11 + i3_s\n                compute_1 = T.Buffer((10890,), data=compute.data)\n                data_1 = T.Buffer((10890,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [15, 6, 11, 11]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1824; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1824; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1824; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 19, 16), \"float32\"), compute: T.Buffer((6, 19, 16), \"float32\"), T_multiply: T.Buffer((6, 19, 16), \"float32\"), compute_1: T.Buffer((6, 19, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1824,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1824):\n            compute_2 = T.Buffer((1824,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1824):\n            T_multiply_1 = T.Buffer((1824,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1824):\n            compute_2 = T.Buffer((1824,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["acosh", "abs", "multiply", "abs"]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 19; ++i3) {\n        compute[(((i0_i1_fused * 380) + (i2 * 19)) + i3)] = atanf(data[(((i0_i1_fused * 380) + (i2 * 19)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 9, 20, 19), \"float32\"), compute: T.Buffer((8, 9, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(72):\n            for i2, i3 in T.grid(20, 19):\n                cse_var_1: T.int32 = i0_i1_fused * 380 + i2 * 19 + i3\n                compute_1 = T.Buffer((27360,), data=compute.data)\n                data_1 = T.Buffer((27360,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [8, 9, 20, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 684; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 684; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_multiply[(((ax0 * 228) + (ax1 * 12)) + ax2)] = (sinf(ph_0[(((ax0 * 228) + (ax1 * 12)) + ax2)]) * ph_0[(((ax0 * 228) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 684; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 19, 12), \"float32\"), ph_3: T.Buffer((3, 19, 12), \"float32\"), T_mod: T.Buffer((3, 19, 12), \"float32\"), compute: T.Buffer((3, 19, 12), \"float32\"), T_multiply: T.Buffer((3, 19, 12), \"float32\"), compute_1: T.Buffer((3, 19, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((684,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(684):\n            T_mod_1 = T.Buffer((684,), data=T_mod.data)\n            ph_3_1 = T.Buffer((684,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(684):\n            compute_2 = T.Buffer((684,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(19, 12):\n                cse_var_1: T.int32 = ax0 * 228 + ax1 * 12 + ax2\n                T_multiply_1 = T.Buffer((684,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.sin(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(684):\n            compute_2 = T.Buffer((684,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "atan", "sin", "multiply", "abs"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_mod, float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* compute_7, float* compute_8, float* compute_9, float* compute_10, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 306; ++i0_i1_fused_i2_fused) {\n    compute_7[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 306; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 306; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 306; ++i0_i1_fused_i2_fused_1) {\n    compute_10[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 306; ++i0_i1_fused_i2_fused_2) {\n    compute_9[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 306; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add_1[ax0_ax1_fused_ax2_fused_2] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_2])) + fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 306; ++i0_i1_fused_i2_fused_3) {\n    compute_8[i0_i1_fused_i2_fused_3] = asinhf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 306; ++i0_i1_fused_i2_fused_4) {\n    compute[i0_i1_fused_i2_fused_4] = acosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 306; ++i0_i1_fused_i2_fused_5) {\n    compute_6[i0_i1_fused_i2_fused_5] = cosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 306; ++ax0_ax1_fused_ax2_fused_3) {\n    T_multiply[ax0_ax1_fused_ax2_fused_3] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_3])) * fabsf(ph_0[ax0_ax1_fused_ax2_fused_3]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 306; ++i0_i1_fused) {\n    compute_5[i0_i1_fused] = cosf(fabsf(asinhf(ph_0[i0_i1_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 306; ++i0_i1_fused_i2_fused_6) {\n    compute_4[i0_i1_fused_i2_fused_6] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_6])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_7 = 0; i0_i1_fused_i2_fused_7 < 306; ++i0_i1_fused_i2_fused_7) {\n    compute_3[i0_i1_fused_i2_fused_7] = atanhf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_7])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_8 = 0; i0_i1_fused_i2_fused_8 < 306; ++i0_i1_fused_i2_fused_8) {\n    compute_2[i0_i1_fused_i2_fused_8] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_8])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_9 = 0; i0_i1_fused_i2_fused_9 < 306; ++i0_i1_fused_i2_fused_9) {\n    compute_1[i0_i1_fused_i2_fused_9] = asinf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_9])) * ph_0[i0_i1_fused_i2_fused_9]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_4 = 0; ax0_ax1_fused_ax2_fused_4 < 306; ++ax0_ax1_fused_ax2_fused_4) {\n    T_mod[ax0_ax1_fused_ax2_fused_4] = fmodf((fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_4])) * ph_0[ax0_ax1_fused_ax2_fused_4]), fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_4])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_13(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_14(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_15(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_12(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 17, 1), \"float32\"), compute: T.Buffer((18, 17, 1), \"float32\"), T_subtract: T.Buffer((18, 17, 1), \"float32\"), T_add: T.Buffer((18, 17, 1), \"float32\"), compute_1: T.Buffer((18, 17, 1), \"float32\"), compute_2: T.Buffer((18, 17, 1), \"float32\"), T_add_1: T.Buffer((18, 17, 1), \"float32\"), compute_3: T.Buffer((18, 17, 1), \"float32\"), compute_4: T.Buffer((18, 17, 1), \"float32\"), compute_5: T.Buffer((18, 17, 1), \"float32\"), T_multiply: T.Buffer((18, 17, 1), \"float32\"), compute_6: T.Buffer((18, 17, 1), \"float32\"), compute_7: T.Buffer((18, 17, 1), \"float32\"), compute_8: T.Buffer((18, 17, 1), \"float32\"), compute_9: T.Buffer((18, 17, 1), \"float32\"), compute_10: T.Buffer((18, 17, 1), \"float32\"), T_mod: T.Buffer((18, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((306,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute.data)\n            compute_11[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            T_subtract_1 = T.Buffer((306,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            T_add_2 = T.Buffer((306,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_1.data)\n            compute_11[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_2.data)\n            compute_11[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            T_add_2 = T.Buffer((306,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_3.data)\n            compute_11[i0_i1_fused_i2_fused] = T.asinh(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_4.data)\n            compute_11[i0_i1_fused_i2_fused] = T.acos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_5.data)\n            compute_11[i0_i1_fused_i2_fused] = T.cos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            T_multiply_1 = T.Buffer((306,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) * T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_6.data)\n            compute_11[i0_i1_fused] = T.cos(T.fabs(T.asinh(ph_0_1[i0_i1_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_7.data)\n            compute_11[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_8.data)\n            compute_11[i0_i1_fused_i2_fused] = T.atanh(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_9.data)\n            compute_11[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_11 = T.Buffer((306,), data=compute_10.data)\n            compute_11[i0_i1_fused_i2_fused] = T.asin(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            T_mod_1 = T.Buffer((306,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])))", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add", "asinh", "acos", "cos", "multiply", "cos", "acos", "atanh", "acos", "multiply", "asin", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_mod[(((ax0 * 45) + (ax1 * 5)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 45) + (ax1 * 5)) + ax2)]), ph_0[(((ax0 * 45) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 495; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 5), \"float32\"), compute: T.Buffer((11, 9, 5), \"float32\"), T_mod: T.Buffer((11, 9, 5), \"float32\"), compute_1: T.Buffer((11, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((495,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(9, 5):\n                cse_var_1: T.int32 = ax0 * 45 + ax1 * 5 + ax2\n                T_mod_1 = T.Buffer((495,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan"]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n        compute[(((i0_i1_fused * 99) + (i2 * 11)) + i3_s)] = atanf(data[(((i0_i1_fused * 99) + (i2 * 11)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 20, 9, 11), \"float32\"), compute: T.Buffer((6, 20, 9, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(120):\n            for i2, i3_s in T.grid(9, 11):\n                cse_var_1: T.int32 = i0_i1_fused * 99 + i2 * 11 + i3_s\n                compute_1 = T.Buffer((11880,), data=compute.data)\n                data_1 = T.Buffer((11880,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [6, 20, 9, 11]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 13; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 3; ++i3) {\n        compute[(((i1 * 9) + (i2 * 3)) + i3)] = atanf(data[(((i1 * 9) + (i2 * 3)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 117) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 13, 3, 3), \"float32\"), compute: T.Buffer((1, 13, 3, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(13, 3, 3):\n            cse_var_1: T.int32 = i1 * 9 + i2 * 3 + i3\n            compute_1 = T.Buffer((117,), data=compute.data)\n            data_1 = T.Buffer((117,), data=data.data)\n            compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [1, 13, 3, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_add[(((ax0 * 102) + (ax1 * 17)) + ax2)] = (ph_0[(((ax0 * 102) + (ax1 * 17)) + ax2)] + ph_3[(((ax0 * 102) + (ax1 * 17)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1836; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 102) + (i1 * 17)) + i2)] = asinf(cosf(ph_0[(((i0 * 102) + (i1 * 17)) + i2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 6, 17), \"float32\"), ph_3: T.Buffer((18, 6, 17), \"float32\"), T_add: T.Buffer((18, 6, 17), \"float32\"), T_divide: T.Buffer((18, 6, 17), \"float32\"), compute: T.Buffer((18, 6, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1836,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1836,), data=ph_3.data)\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(6, 17):\n                cse_var_1: T.int32 = ax0 * 102 + ax1 * 17 + ax2\n                T_add_1 = T.Buffer((1836,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1836):\n            T_divide_1 = T.Buffer((1836,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(6, 17):\n                cse_var_2: T.int32 = i0 * 102 + i1 * 17 + i2\n                compute_1 = T.Buffer((1836,), data=compute.data)\n                compute_1[cse_var_2] = T.asin(T.cos(ph_0_1[cse_var_2]))", "op_args": [["add", "divide", "cos", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2160; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_subtract_1[(((ax0 * 180) + (ax1 * 9)) + ax2)] = (asinhf(ph_0[(((ax0 * 180) + (ax1 * 9)) + ax2)]) - ph_0[(((ax0 * 180) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 9), \"float32\"), ph_3: T.Buffer((12, 20, 9), \"float32\"), T_subtract: T.Buffer((12, 20, 9), \"float32\"), T_add: T.Buffer((12, 20, 9), \"float32\"), T_subtract_1: T.Buffer((12, 20, 9), \"float32\"), compute: T.Buffer((12, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2160,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_add_1 = T.Buffer((2160,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(20, 9):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 9 + ax2\n                T_subtract_2 = T.Buffer((2160,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_1 = T.Buffer((2160,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["subtract", "add", "asinh", "subtract", "asin"]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n          compute[((((i0 * 192) + (i1 * 12)) + (i2 * 3)) + i3_s)] = atanf(data[((((i0 * 192) + (i1 * 12)) + (i2 * 3)) + i3_s)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 16, 4, 3), \"float32\"), compute: T.Buffer((20, 16, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1, i2, i3_s in T.grid(16, 4, 3):\n                cse_var_1: T.int32 = i0 * 192 + i1 * 12 + i2 * 3 + i3_s\n                compute_1 = T.Buffer((3840,), data=compute.data)\n                data_1 = T.Buffer((3840,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [20, 16, 4, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 135) + (i1 * 15)) + i2)] = cosf(ph_0[(((i0 * 135) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1755; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_subtract[(((ax0 * 135) + (ax1 * 15)) + ax2)] = (ph_0[(((ax0 * 135) + (ax1 * 15)) + ax2)] - cosf(ph_0[(((ax0 * 135) + (ax1 * 15)) + ax2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 15), \"float32\"), compute: T.Buffer((13, 9, 15), \"float32\"), T_add: T.Buffer((13, 9, 15), \"float32\"), T_subtract: T.Buffer((13, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1755,), data=ph_0.data)\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(9, 15):\n                cse_var_1: T.int32 = i0 * 135 + i1 * 15 + i2\n                compute_1 = T.Buffer((1755,), data=compute.data)\n                compute_1[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_add_1 = T.Buffer((1755,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(9, 15):\n                cse_var_2: T.int32 = ax0 * 135 + ax1 * 15 + ax2\n                T_subtract_1 = T.Buffer((1755,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] - T.cos(ph_0_1[cse_var_2])", "op_args": [["cos", "acosh", "add", "cos", "subtract"]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 3) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 3) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 10, 20, 3), \"float32\"), compute: T.Buffer((13, 10, 20, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2600):\n            for i3_s in range(3):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 3 + i3_s\n                compute_1 = T.Buffer((7800,), data=compute.data)\n                data_1 = T.Buffer((7800,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [13, 10, 20, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_mod[(((ax0 * 77) + (ax1 * 11)) + ax2)] = fmodf(ph_0[(((ax0 * 77) + (ax1 * 11)) + ax2)], ph_3[(((ax0 * 77) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 385; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), ph_3: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(7, 11):\n                cse_var_1: T.int32 = ax0 * 77 + ax1 * 11 + ax2\n                T_mod_1 = T.Buffer((385,), data=T_mod.data)\n                ph_3_1 = T.Buffer((385,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "atan", "sin", "acos"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  float T_softmax_maxelem[399];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 399; ++i0_i1_fused_i2_fused) {\n    T_softmax_maxelem[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 2; ++k) {\n      T_softmax_maxelem[i0_i1_fused_i2_fused] = max(T_softmax_maxelem[i0_i1_fused_i2_fused], data[((i0_i1_fused_i2_fused * 2) + k)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 2; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)])));\n        }\n        for (int32_t i3_s = 0; i3_s < 2; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 266) + (i1 * 14)) + (i2 * 2)) + i3_s)] - T_softmax_maxelem[(((i0 * 133) + (i1 * 7)) + i2)])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 2; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 38) + (((int)threadIdx.x) * 2)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 399) {\n    T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 2; ++k) {\n    if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 399) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 399) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))])) / T_softmax_expsum[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 19, 7, 2), \"float32\"), T_softmax_norm: T.Buffer((3, 19, 7, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([399], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((399,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((798,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(399):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(2):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 2 + k])\n        for i0 in T.parallel(3):\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1, i2 in T.grid(19, 7):\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(2):\n                    cse_var_2: T.int32 = i0 * 133 + i1 * 7 + i2\n                    cse_var_1: T.int32 = i0 * 266 + i1 * 14 + i2 * 2 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[cse_var_2])\n                for i3_s in range(2):\n                    cse_var_4: T.int32 = i0 * 133 + i1 * 7 + i2\n                    cse_var_3: T.int32 = i0 * 266 + i1 * 14 + i2 * 2 + i3_s\n                    T_softmax_norm_1 = T.Buffer((798,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_3] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4]) / T_softmax_expsum_1[0]", "op_args": [3, 19, 7, 2]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2100; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 14; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3_s)] = atanf(data[((i0_i1_fused_i2_fused * 14) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 3675) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 14, 15, 14), \"float32\"), compute: T.Buffer((10, 14, 15, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2100):\n            for i3_s in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3_s\n                compute_1 = T.Buffer((29400,), data=compute.data)\n                data_1 = T.Buffer((29400,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [10, 14, 15, 14]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1836; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = atanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 459) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 17, 1, 12), \"float32\"), compute: T.Buffer((9, 17, 1, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1836):\n            compute_1 = T.Buffer((1836,), data=compute.data)\n            data_1 = T.Buffer((1836,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atan(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [9, 17, 1, 12]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 42) + (i1 * 6)) + i2)] = cosf(ph_0[(((i0 * 42) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 12; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n        compute_2[(((i0_1 * 42) + (i1_1 * 6)) + i2_1)] = acosf(atanf(ph_0[(((i0_1 * 42) + (i1_1 * 6)) + i2_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(atanf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 7, 6), \"float32\"), ph_3: T.Buffer((12, 7, 6), \"float32\"), T_add: T.Buffer((12, 7, 6), \"float32\"), compute: T.Buffer((12, 7, 6), \"float32\"), compute_1: T.Buffer((12, 7, 6), \"float32\"), compute_2: T.Buffer((12, 7, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((504,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(504):\n            T_add_1 = T.Buffer((504,), data=T_add.data)\n            ph_3_1 = T.Buffer((504,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(7, 6):\n                cse_var_1: T.int32 = i0 * 42 + i1 * 6 + i2\n                compute_3 = T.Buffer((504,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_3 = T.Buffer((504,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(7, 6):\n                cse_var_2: T.int32 = i0 * 42 + i1 * 6 + i2\n                compute_3 = T.Buffer((504,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acos(T.atan(ph_0_1[cse_var_2]))", "op_args": [["add", "cos", "atan", "atan", "acos"]]}{"op_name": "atan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 132; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      for (int32_t i3 = 0; i3 < 17; ++i3) {\n        compute[(((i0_i1_fused * 34) + (i2 * 17)) + i3)] = atanf(data[(((i0_i1_fused * 34) + (i2 * 17)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = atanf(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 11, 2, 17), \"float32\"), compute: T.Buffer((12, 11, 2, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(132):\n            for i2, i3 in T.grid(2, 17):\n                cse_var_1: T.int32 = i0_i1_fused * 34 + i2 * 17 + i3\n                compute_1 = T.Buffer((4488,), data=compute.data)\n                data_1 = T.Buffer((4488,), data=data.data)\n                compute_1[cse_var_1] = T.atan(data_1[cse_var_1])", "op_args": [12, 11, 2, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 99; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_mod[((ax0_ax1_fused * 5) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 5) + ax2)]), ph_0[((ax0_ax1_fused * 5) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 495; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 5), \"float32\"), compute: T.Buffer((11, 9, 5), \"float32\"), T_mod: T.Buffer((11, 9, 5), \"float32\"), compute_1: T.Buffer((11, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((495,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(99):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_mod_1 = T.Buffer((495,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        T_softmax_maxelem[0] = -3.402823e+38f;\n        for (int32_t k = 0; k < 9; ++k) {\n          T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k)]);\n        }\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 9; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + k_1)] - T_softmax_maxelem[0])));\n        }\n        for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 54) + (i1 * 27)) + (i2 * 9)) + i3_s)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])) / T_softmax_expsum[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((int)blockIdx.x)] = -3.402823e+38f;\n  for (int k = 0; k < 9; ++k) {\n    T_softmax_maxelem[((int)blockIdx.x)] = max(T_softmax_maxelem[((int)blockIdx.x)], data[((((int)blockIdx.x) * 9) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 45) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 9; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 45) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 9)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 2, 3, 9), \"float32\"), T_softmax_norm: T.Buffer((15, 2, 3, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(15):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1, i2 in T.grid(2, 3):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((810,), data=data.data)\n                for k in range(9):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0 * 54 + i1 * 27 + i2 * 9 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(9):\n                    cse_var_1: T.int32 = i0 * 54 + i1 * 27 + i2 * 9 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                for i3_s in range(9):\n                    cse_var_2: T.int32 = i0 * 54 + i1 * 27 + i2 * 9 + i3_s\n                    T_softmax_norm_1 = T.Buffer((810,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [15, 2, 3, 9]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_mod, float* T_mod_1, float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* compute_7, float* compute_8, float* compute_9, float* compute_10, float* compute_11, float* compute_12, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4760; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4760; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 4760; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4760; ++i0_i1_fused_i2_fused_1) {\n    compute_12[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4760; ++i0_i1_fused_i2_fused_2) {\n    compute_11[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 4760; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add_1[ax0_ax1_fused_ax2_fused_2] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_2])) + fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 4760; ++i0_i1_fused_i2_fused_3) {\n    compute_10[i0_i1_fused_i2_fused_3] = asinhf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 4760; ++i0_i1_fused_i2_fused_4) {\n    compute_9[i0_i1_fused_i2_fused_4] = acosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 4760; ++i0_i1_fused_i2_fused_5) {\n    compute_8[i0_i1_fused_i2_fused_5] = cosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 4760; ++ax0_ax1_fused_ax2_fused_3) {\n    T_multiply[ax0_ax1_fused_ax2_fused_3] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_3])) * fabsf(ph_0[ax0_ax1_fused_ax2_fused_3]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 4760; ++i0_i1_fused_i2_fused_6) {\n    compute_7[i0_i1_fused_i2_fused_6] = cosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_6])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_7 = 0; i0_i1_fused_i2_fused_7 < 4760; ++i0_i1_fused_i2_fused_7) {\n    compute_6[i0_i1_fused_i2_fused_7] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_7])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_8 = 0; i0_i1_fused_i2_fused_8 < 4760; ++i0_i1_fused_i2_fused_8) {\n    compute_5[i0_i1_fused_i2_fused_8] = atanhf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_8])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_9 = 0; i0_i1_fused_i2_fused_9 < 4760; ++i0_i1_fused_i2_fused_9) {\n    compute_4[i0_i1_fused_i2_fused_9] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_9])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_3[(((i0 * 280) + (i1 * 14)) + i2)] = asinf((fabsf(asinhf(ph_0[(((i0 * 280) + (i1 * 14)) + i2)])) * ph_0[(((i0 * 280) + (i1 * 14)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_4 = 0; ax0_ax1_fused_ax2_fused_4 < 4760; ++ax0_ax1_fused_ax2_fused_4) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_4] = fmodf((fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_4])) * ph_0[ax0_ax1_fused_ax2_fused_4]), fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_10 = 0; i0_i1_fused_i2_fused_10 < 4760; ++i0_i1_fused_i2_fused_10) {\n    compute_2[i0_i1_fused_i2_fused_10] = fabsf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_10])) * ph_0[i0_i1_fused_i2_fused_10]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_5 = 0; ax0_ax1_fused_ax2_fused_5 < 4760; ++ax0_ax1_fused_ax2_fused_5) {\n    T_mod[ax0_ax1_fused_ax2_fused_5] = fmodf((fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_5])) * ph_0[ax0_ax1_fused_ax2_fused_5]), fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_11 = 0; i0_i1_fused_i2_fused_11 < 4760; ++i0_i1_fused_i2_fused_11) {\n    compute[i0_i1_fused_i2_fused_11] = acosf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_11])) * ph_0[i0_i1_fused_i2_fused_11]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_14(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_15(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), fabsf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_16(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_9(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) * fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_12(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_13(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_18(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_17(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 14), \"float32\"), compute: T.Buffer((17, 20, 14), \"float32\"), T_subtract: T.Buffer((17, 20, 14), \"float32\"), T_add: T.Buffer((17, 20, 14), \"float32\"), compute_1: T.Buffer((17, 20, 14), \"float32\"), compute_2: T.Buffer((17, 20, 14), \"float32\"), T_add_1: T.Buffer((17, 20, 14), \"float32\"), compute_3: T.Buffer((17, 20, 14), \"float32\"), compute_4: T.Buffer((17, 20, 14), \"float32\"), compute_5: T.Buffer((17, 20, 14), \"float32\"), T_multiply: T.Buffer((17, 20, 14), \"float32\"), compute_6: T.Buffer((17, 20, 14), \"float32\"), compute_7: T.Buffer((17, 20, 14), \"float32\"), compute_8: T.Buffer((17, 20, 14), \"float32\"), compute_9: T.Buffer((17, 20, 14), \"float32\"), compute_10: T.Buffer((17, 20, 14), \"float32\"), T_mod: T.Buffer((17, 20, 14), \"float32\"), compute_11: T.Buffer((17, 20, 14), \"float32\"), T_mod_1: T.Buffer((17, 20, 14), \"float32\"), compute_12: T.Buffer((17, 20, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4760,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute.data)\n            compute_13[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_subtract_1 = T.Buffer((4760,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_add_2 = T.Buffer((4760,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_1.data)\n            compute_13[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_2.data)\n            compute_13[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_add_2 = T.Buffer((4760,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_3.data)\n            compute_13[i0_i1_fused_i2_fused] = T.asinh(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_4.data)\n            compute_13[i0_i1_fused_i2_fused] = T.acos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_5.data)\n            compute_13[i0_i1_fused_i2_fused] = T.cos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_multiply_1 = T.Buffer((4760,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) * T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_6.data)\n            compute_13[i0_i1_fused_i2_fused] = T.cos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_7.data)\n            compute_13[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_8.data)\n            compute_13[i0_i1_fused_i2_fused] = T.atanh(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_9.data)\n            compute_13[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(20, 14):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 14 + i2\n                compute_13 = T.Buffer((4760,), data=compute_10.data)\n                compute_13[cse_var_1] = T.asin(T.fabs(T.asinh(ph_0_1[cse_var_1])) * ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_mod_2 = T.Buffer((4760,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_11.data)\n            compute_13[i0_i1_fused_i2_fused] = T.fabs(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_mod_2 = T.Buffer((4760,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_13 = T.Buffer((4760,), data=compute_12.data)\n            compute_13[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add", "asinh", "acos", "cos", "multiply", "cos", "acos", "atanh", "acos", "multiply", "asin", "mod", "abs", "mod", "acos"]]}{"op_name": "sum", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[8];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 8; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2565; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 8; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 8) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 8; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 642; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 2565) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 9, 12, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([8], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((8,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(8):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2565, 8):\n            data_1 = T.Buffer((20520,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 8 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(8):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [19, 9, 12, 10]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 121; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = atanhf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_multiply[(((ax0 * 121) + (ax1 * 11)) + ax2)] = (acoshf(ph_0[(((ax0 * 121) + (ax1 * 11)) + ax2)]) * ph_0[(((ax0 * 121) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1331; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 11, 11), \"float32\"), compute: T.Buffer((11, 11, 11), \"float32\"), T_multiply: T.Buffer((11, 11, 11), \"float32\"), compute_1: T.Buffer((11, 11, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1331,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(121):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_2 = T.Buffer((1331,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(11, 11):\n                cse_var_2: T.int32 = ax0 * 121 + ax1 * 11 + ax2\n                T_multiply_1 = T.Buffer((1331,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(1331):\n            compute_2 = T.Buffer((1331,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acosh", "multiply", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_subtract[(((ax0 * 180) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 180) + (ax1 * 9)) + ax2)] - ph_3[(((ax0 * 180) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2160; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused_1] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 9), \"float32\"), ph_3: T.Buffer((12, 20, 9), \"float32\"), T_subtract: T.Buffer((12, 20, 9), \"float32\"), T_add: T.Buffer((12, 20, 9), \"float32\"), T_subtract_1: T.Buffer((12, 20, 9), \"float32\"), compute: T.Buffer((12, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2160,), data=ph_3.data)\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(20, 9):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 9 + ax2\n                T_subtract_2 = T.Buffer((2160,), data=T_subtract.data)\n                T_subtract_2[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_add_1 = T.Buffer((2160,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_1 = T.Buffer((2160,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["subtract", "add", "asinh", "subtract", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 135) + (i1 * 15)) + i2)] = cosf(ph_0[(((i0 * 135) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1755; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1755; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - cosf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 15), \"float32\"), compute: T.Buffer((13, 9, 15), \"float32\"), T_add: T.Buffer((13, 9, 15), \"float32\"), T_subtract: T.Buffer((13, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1755,), data=ph_0.data)\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(9, 15):\n                cse_var_1: T.int32 = i0 * 135 + i1 * 15 + i2\n                compute_1 = T.Buffer((1755,), data=compute.data)\n                compute_1[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_add_1 = T.Buffer((1755,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_subtract_1 = T.Buffer((1755,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.cos(ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["cos", "acosh", "add", "cos", "subtract"]]}{"op_name": "sum", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[51];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 51; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 308; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 51; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 51) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 51; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 0.000000e+00f;\n  for (int k0 = 0; k0 < 17; ++k0) {\n    for (int k1 = 0; k1 < 14; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 6; ++k3) {\n          data_red[0] = (data_red[0] + data[((((k0 * 924) + (k1 * 66)) + (k2 * 6)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 14, 11, 6), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([51], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((51,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(51):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(308, 51):\n            data_1 = T.Buffer((15708,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 51 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(51):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [17, 14, 11, 6]}{"op_name": "sum", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[60];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 48; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 60; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 60) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 90; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 2, 10, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([60], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((60,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(60):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(48, 60):\n            data_1 = T.Buffer((2880,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 60 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(60):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [12, 2, 10, 12]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      for (int32_t i3 = 0; i3 < 5; ++i3) {\n        T_softmax_maxelem[0] = -3.402823e+38f;\n        for (int32_t k = 0; k < 5; ++k) {\n          T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused * 25) + (i2 * 5)) + k)]);\n        }\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 5; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 25) + (i2 * 5)) + k_1)] - T_softmax_maxelem[0])));\n        }\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 25) + (i2 * 5)) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) < 225) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 5; ++k) {\n    if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) < 225) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 320) + (((int)threadIdx.x) * 5)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) < 1125) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(28) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 14) + (((int)threadIdx.x) >> 1)) < 225) {\n    T_softmax_expsum[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 5; ++k) {\n    if (((((int)blockIdx.x) * 14) + (((int)threadIdx.x) >> 1)) < 225) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 140) + (((int)threadIdx.x) * 5)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 5, 5, 5), \"float32\"), T_softmax_norm: T.Buffer((18, 5, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(90):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2, i3 in T.grid(5, 5):\n                cse_var_1: T.int32 = i0_i1_fused * 25 + i2 * 5 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((2250,), data=data.data)\n                for k in range(5):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 25 + i2 * 5 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(5):\n                    cse_var_2: T.int32 = i0_i1_fused * 25 + i2 * 5 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((2250,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [18, 5, 5, 5]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 154; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = atanhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 154; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_mod[((ax0_ax1_fused * 9) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 9) + ax2)]), ph_0[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(154):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(154):\n            for ax2 in range(9):\n                cse_var_2: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_mod[(((ax0 * 77) + (ax1 * 11)) + ax2)] = fmodf(ph_0[(((ax0 * 77) + (ax1 * 11)) + ax2)], ph_3[(((ax0 * 77) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = atanf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), ph_3: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(7, 11):\n                cse_var_1: T.int32 = ax0 * 77 + ax1 * 11 + ax2\n                T_mod_1 = T.Buffer((385,), data=T_mod.data)\n                ph_3_1 = T.Buffer((385,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(35):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_2 = T.Buffer((385,), data=compute.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "atan", "sin", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_6) {\n  float auto_scheduler_layout_transform[440];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 176; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 176; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n    for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n      for (int32_t ax7 = 0; ax7 < 11; ++ax7) {\n        for (int32_t ax8 = 0; ax8 < 4; ++ax8) {\n          auto_scheduler_layout_transform[((((ax5 * 220) + (ax6 * 44)) + (ax7 * 4)) + ax8)] = ph_6[((((ax5 * 220) + (ax8 * 55)) + (ax7 * 5)) + ax6)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        for (int32_t b_inner_init = 0; b_inner_init < 4; ++b_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 40) + (b_inner_init * 10)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 11; ++k_inner) {\n          for (int32_t b_inner = 0; b_inner < 4; ++b_inner) {\n            T_batch_matmul_NN[((((b_outer_inner * 40) + (b_inner * 10)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner)] = (T_batch_matmul_NN[((((b_outer_inner * 40) + (b_inner * 10)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner)] + (ph_0[((((b_outer_inner * 88) + (b_inner * 22)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 11)) + k_inner)] * auto_scheduler_layout_transform[((((b_outer_inner * 220) + (j_outer_inner * 44)) + (k_inner * 4)) + b_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf(acosf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN_local[20];\n  __shared__ float ph_6_shared[160];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 2)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 4)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 6)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 8)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 10)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 12)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 14)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 16)] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 18)] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 16; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_6_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x))] = ph_6[((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n    for (int k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n        T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (ph_0[(((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner)] * ph_6_shared[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN_local[(b_c_inner + 2)] = (T_batch_matmul_NN_local[(b_c_inner + 2)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 4)] * ph_6_shared[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN_local[(b_c_inner + 4)] = (T_batch_matmul_NN_local[(b_c_inner + 4)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 8)] * ph_6_shared[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN_local[(b_c_inner + 6)] = (T_batch_matmul_NN_local[(b_c_inner + 6)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 12)] * ph_6_shared[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN_local[(b_c_inner + 8)] = (T_batch_matmul_NN_local[(b_c_inner + 8)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 16)] * ph_6_shared[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN_local[(b_c_inner + 10)] = (T_batch_matmul_NN_local[(b_c_inner + 10)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 80)] * ph_6_shared[(((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5)) + 80)]));\n        T_batch_matmul_NN_local[(b_c_inner + 12)] = (T_batch_matmul_NN_local[(b_c_inner + 12)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 84)] * ph_6_shared[(((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5)) + 80)]));\n        T_batch_matmul_NN_local[(b_c_inner + 14)] = (T_batch_matmul_NN_local[(b_c_inner + 14)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 88)] * ph_6_shared[(((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5)) + 80)]));\n        T_batch_matmul_NN_local[(b_c_inner + 16)] = (T_batch_matmul_NN_local[(b_c_inner + 16)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 92)] * ph_6_shared[(((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5)) + 80)]));\n        T_batch_matmul_NN_local[(b_c_inner + 18)] = (T_batch_matmul_NN_local[(b_c_inner + 18)] + (ph_0[((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 2)) + k_inner) + 96)] * ph_6_shared[(((((((((int)threadIdx.x) / 5) * 40) + (b_c_inner * 20)) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5)) + 80)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 5)] = T_batch_matmul_NN_local[(b_inner + 2)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 10)] = T_batch_matmul_NN_local[(b_inner + 4)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 15)] = T_batch_matmul_NN_local[(b_inner + 6)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 20)] = T_batch_matmul_NN_local[(b_inner + 8)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 100)] = T_batch_matmul_NN_local[(b_inner + 10)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 105)] = T_batch_matmul_NN_local[(b_inner + 12)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 110)] = T_batch_matmul_NN_local[(b_inner + 14)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 115)] = T_batch_matmul_NN_local[(b_inner + 16)];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 5) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 5)) + 120)] = T_batch_matmul_NN_local[(b_inner + 18)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 2, 11), \"float32\"), ph_6: T.Buffer((8, 11, 5), \"float32\"), compute: T.Buffer((8, 2, 11), \"float32\"), compute_1: T.Buffer((8, 2, 11), \"float32\"), compute_2: T.Buffer((8, 2, 11), \"float32\"), T_batch_matmul_NN: T.Buffer((8, 2, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([440], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((176,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((440,), data=auto_scheduler_layout_transform)\n        for ax5, ax6, ax7, ax8 in T.grid(2, 5, 11, 4):\n            cse_var_1: T.int32 = ax5 * 220\n            ph_6_1 = T.Buffer((440,), data=ph_6.data)\n            auto_scheduler_layout_transform_1[cse_var_1 + ax6 * 44 + ax7 * 4 + ax8] = ph_6_1[cse_var_1 + ax8 * 55 + ax7 * 5 + ax6]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((80,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(2, 5, 4):\n                T_batch_matmul_NN_1[b_outer_inner_init * 40 + b_inner_init * 10 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner_init] = T.float32(0)\n            for b_outer_inner, j_outer_inner, k_inner, b_inner in T.grid(2, 5, 11, 4):\n                cse_var_2: T.int32 = b_outer_inner * 40 + b_inner * 10 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[b_outer_inner * 88 + b_inner * 22 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 11 + k_inner] * auto_scheduler_layout_transform_1[b_outer_inner * 220 + j_outer_inner * 44 + k_inner * 4 + b_inner]", "op_args": [["asinh", "acos", "cos", "sin", "batch_matmul"]]}{"op_name": "sum", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[28];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 28; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 252; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 28; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 28) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 28; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 221; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 441) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 14, 14), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([28], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((28,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(28):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(252, 28):\n            data_1 = T.Buffer((7056,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 28 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(28):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [3, 12, 14, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 308; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 308; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 308; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 308; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (sinf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 4), \"float32\"), ph_3: T.Buffer((7, 11, 4), \"float32\"), T_mod: T.Buffer((7, 11, 4), \"float32\"), compute: T.Buffer((7, 11, 4), \"float32\"), compute_1: T.Buffer((7, 11, 4), \"float32\"), T_multiply: T.Buffer((7, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((308,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_mod_1 = T.Buffer((308,), data=T_mod.data)\n            ph_3_1 = T.Buffer((308,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_multiply_1 = T.Buffer((308,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["mod", "atan", "sin", "acos", "multiply"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 228) + (i1 * 19)) + i2)] = expf((ph_0[(((i0 * 228) + (i1 * 19)) + i2)] + asinhf(ph_0[(((i0 * 228) + (i1 * 19)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 20; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_1[(((i0_1 * 228) + (i1_1 * 19)) + i2_1)] = acoshf(ph_0[(((i0_1 * 228) + (i1_1 * 19)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 12, 19), \"float32\"), compute: T.Buffer((20, 12, 19), \"float32\"), compute_1: T.Buffer((20, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4560,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(12, 19):\n                cse_var_1: T.int32 = i0 * 228 + i1 * 19 + i2\n                compute_2 = T.Buffer((4560,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(12, 19):\n                cse_var_2: T.int32 = i0 * 228 + i1 * 19 + i2\n                compute_2 = T.Buffer((4560,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])", "op_args": [["asinh", "add", "exp", "acosh"]]}{"op_name": "sum", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 0.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 330; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 0.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] + data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 0.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 413; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 825) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] + data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] + t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 11, 4, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(330, 40):\n            data_1 = T.Buffer((13200,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] + data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(0)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(40):\n            data_red_1[0] = data_red_1[0] + data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [15, 11, 4, 20]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + acosf((ph_0[ax0_ax1_fused_ax2_fused] / asinf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + acosf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 6, 18), \"float32\"), T_add: T.Buffer((20, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_add_1 = T.Buffer((2160,), data=T_add.data)\n            ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused] / T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]))", "op_args": [["asin", "divide", "acos", "add"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 18; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i0_i1_fused * 126) + (i2 * 18)) + k)]);\n      }\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 18; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 126) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0])));\n        }\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 126) + (i2 * 18)) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 49) {\n    T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 49) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 49) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 49) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 18)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)])) / T_softmax_expsum[(((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) / 9)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 14, 7, 18), \"float32\"), T_softmax_norm: T.Buffer((2, 14, 7, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(28):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(7):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((3528,), data=data.data)\n                for k in range(18):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused * 126 + i2 * 18 + k])\n                for i3 in range(18):\n                    cse_var_1: T.int32 = i0_i1_fused * 126 + i2 * 18 + i3\n                    T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                    T_softmax_expsum_1[0] = T.float32(0)\n                    for k in range(18):\n                        cse_var_2: T.int32 = i0_i1_fused * 126 + i2 * 18 + k\n                        T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                    T_softmax_norm_1 = T.Buffer((3528,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [2, 14, 7, 18]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 15, 2, 8), \"float32\"), compute: T.Buffer((18, 15, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4320):\n            compute_1 = T.Buffer((4320,), data=compute.data)\n            data_1 = T.Buffer((4320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cosh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [18, 15, 2, 8]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 40) + (i1 * 5)) + i2)] = cosf(ph_0[(((i0 * 40) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_1[((i0_i1_fused * 5) + i2_1)] = atanf(acosf(ph_0[((i0_i1_fused * 5) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 8, 5), \"float32\"), ph_3: T.Buffer((12, 8, 5), \"float32\"), T_multiply: T.Buffer((12, 8, 5), \"float32\"), compute: T.Buffer((12, 8, 5), \"float32\"), compute_1: T.Buffer((12, 8, 5), \"float32\"), compute_2: T.Buffer((12, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_multiply_1 = T.Buffer((480,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((480,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(8, 5):\n                cse_var_1: T.int32 = i0 * 40 + i1 * 5 + i2\n                compute_3 = T.Buffer((480,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((480,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(T.acos(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_3 = T.Buffer((480,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["multiply", "cos", "acos", "atan", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = atanf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 45; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 7) + i2_1)] = sinf(ph_0[((i0_i1_fused_1 * 7) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            ph_3_1 = T.Buffer((315,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i1 * 17) + i2)] = coshf(data[((i1 * 17) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 51) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 17, 1), \"float32\"), compute: T.Buffer((1, 12, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2 in T.grid(12, 17):\n            cse_var_1: T.int32 = i1 * 17 + i2\n            compute_1 = T.Buffer((204,), data=compute.data)\n            data_1 = T.Buffer((204,), data=data.data)\n            compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [1, 12, 17, 1]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 44; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = atanhf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 440; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n        compute_1[(((i0 * 40) + (i1 * 10)) + i2_1)] = acosf(ph_0[(((i0 * 40) + (i1 * 10)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 4, 10), \"float32\"), compute: T.Buffer((11, 4, 10), \"float32\"), T_subtract: T.Buffer((11, 4, 10), \"float32\"), compute_1: T.Buffer((11, 4, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((440,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(44):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_2 = T.Buffer((440,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(440):\n            T_subtract_1 = T.Buffer((440,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(4, 10):\n                cse_var_2: T.int32 = i0 * 40 + i1 * 10 + i2\n                compute_2 = T.Buffer((440,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "subtract", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_add_2, float* T_mod, float* T_mod_1, float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* compute_5, float* compute_6, float* compute_7, float* compute_8, float* compute_9, float* compute_10, float* compute_11, float* compute_12, float* compute_13, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute_8[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 48; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add_1[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 48; ++i0_i1_fused_i2_fused_1) {\n    compute_13[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 48; ++i0_i1_fused_i2_fused_2) {\n    compute_12[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 48; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add_2[ax0_ax1_fused_ax2_fused_2] = (acoshf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_2])) + fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 48; ++i0_i1_fused_i2_fused_3) {\n    compute_11[i0_i1_fused_i2_fused_3] = asinhf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 48; ++i0_i1_fused_i2_fused_4) {\n    compute_10[i0_i1_fused_i2_fused_4] = acosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_5 = 0; i0_i1_fused_i2_fused_5 < 48; ++i0_i1_fused_i2_fused_5) {\n    compute_9[i0_i1_fused_i2_fused_5] = cosf(acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_5])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_multiply[(((ax0 * 24) + (ax1 * 3)) + ax2)] = (acoshf(fabsf(ph_0[(((ax0 * 24) + (ax1 * 3)) + ax2)])) * fabsf(ph_0[(((ax0 * 24) + (ax1 * 3)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_6 = 0; i0_i1_fused_i2_fused_6 < 48; ++i0_i1_fused_i2_fused_6) {\n    compute[i0_i1_fused_i2_fused_6] = cosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_6])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_7 = 0; i0_i1_fused_i2_fused_7 < 48; ++i0_i1_fused_i2_fused_7) {\n    compute_7[i0_i1_fused_i2_fused_7] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_7])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_8 = 0; i0_i1_fused_i2_fused_8 < 48; ++i0_i1_fused_i2_fused_8) {\n    compute_6[i0_i1_fused_i2_fused_8] = atanhf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_8])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_9 = 0; i0_i1_fused_i2_fused_9 < 48; ++i0_i1_fused_i2_fused_9) {\n    compute_5[i0_i1_fused_i2_fused_9] = acosf(fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_9])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_10 = 0; i0_i1_fused_i2_fused_10 < 48; ++i0_i1_fused_i2_fused_10) {\n    compute_4[i0_i1_fused_i2_fused_10] = asinf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_10])) * ph_0[i0_i1_fused_i2_fused_10]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_3 = 0; ax0_ax1_fused_ax2_fused_3 < 48; ++ax0_ax1_fused_ax2_fused_3) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_3] = fmodf((fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_3])) * ph_0[ax0_ax1_fused_ax2_fused_3]), fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_3])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_11 = 0; i0_i1_fused_i2_fused_11 < 48; ++i0_i1_fused_i2_fused_11) {\n    compute_3[i0_i1_fused_i2_fused_11] = fabsf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_11])) * ph_0[i0_i1_fused_i2_fused_11]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_4 = 0; ax0_ax1_fused_ax2_fused_4 < 48; ++ax0_ax1_fused_ax2_fused_4) {\n    T_mod[ax0_ax1_fused_ax2_fused_4] = fmodf((fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_4])) * ph_0[ax0_ax1_fused_ax2_fused_4]), fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_4])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_12 = 0; i0_i1_fused_i2_fused_12 < 48; ++i0_i1_fused_i2_fused_12) {\n    compute_2[i0_i1_fused_i2_fused_12] = acosf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_12])) * ph_0[i0_i1_fused_i2_fused_12]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_13 = 0; i0_i1_fused_i2_fused_13 < 48; ++i0_i1_fused_i2_fused_13) {\n    compute_1[i0_i1_fused_i2_fused_13] = cosf((fabsf(asinhf(ph_0[i0_i1_fused_i2_fused_13])) * ph_0[i0_i1_fused_i2_fused_13]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_5 = 0; ax0_ax1_fused_ax2_fused_5 < 48; ++ax0_ax1_fused_ax2_fused_5) {\n    T_add[ax0_ax1_fused_ax2_fused_5] = ((fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_5])) * ph_0[ax0_ax1_fused_ax2_fused_5]) + fabsf(asinhf(ph_0[ax0_ax1_fused_ax2_fused_5])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_8(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_20(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_13(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_11(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_7(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_6(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_10(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_16(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_14(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_18(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_17(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_12(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_19(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_9(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_15(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), fabsf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 8, 3), \"float32\"), compute: T.Buffer((2, 8, 3), \"float32\"), T_subtract: T.Buffer((2, 8, 3), \"float32\"), T_add: T.Buffer((2, 8, 3), \"float32\"), compute_1: T.Buffer((2, 8, 3), \"float32\"), compute_2: T.Buffer((2, 8, 3), \"float32\"), T_add_1: T.Buffer((2, 8, 3), \"float32\"), compute_3: T.Buffer((2, 8, 3), \"float32\"), compute_4: T.Buffer((2, 8, 3), \"float32\"), compute_5: T.Buffer((2, 8, 3), \"float32\"), T_multiply: T.Buffer((2, 8, 3), \"float32\"), compute_6: T.Buffer((2, 8, 3), \"float32\"), compute_7: T.Buffer((2, 8, 3), \"float32\"), compute_8: T.Buffer((2, 8, 3), \"float32\"), compute_9: T.Buffer((2, 8, 3), \"float32\"), compute_10: T.Buffer((2, 8, 3), \"float32\"), T_mod: T.Buffer((2, 8, 3), \"float32\"), compute_11: T.Buffer((2, 8, 3), \"float32\"), T_mod_1: T.Buffer((2, 8, 3), \"float32\"), compute_12: T.Buffer((2, 8, 3), \"float32\"), compute_13: T.Buffer((2, 8, 3), \"float32\"), T_add_2: T.Buffer((2, 8, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute.data)\n            compute_14[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_subtract_1 = T.Buffer((48,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_add_3 = T.Buffer((48,), data=T_add.data)\n            T_add_3[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_1.data)\n            compute_14[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_2.data)\n            compute_14[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_add_3 = T.Buffer((48,), data=T_add_1.data)\n            T_add_3[ax0_ax1_fused_ax2_fused] = T.acosh(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) + T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_3.data)\n            compute_14[i0_i1_fused_i2_fused] = T.asinh(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_4.data)\n            compute_14[i0_i1_fused_i2_fused] = T.acos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_5.data)\n            compute_14[i0_i1_fused_i2_fused] = T.cos(T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(8, 3):\n                cse_var_1: T.int32 = ax0 * 24 + ax1 * 3 + ax2\n                T_multiply_1 = T.Buffer((48,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.acosh(T.fabs(ph_0_1[cse_var_1])) * T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_6.data)\n            compute_14[i0_i1_fused_i2_fused] = T.cos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_7.data)\n            compute_14[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_8.data)\n            compute_14[i0_i1_fused_i2_fused] = T.atanh(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_9.data)\n            compute_14[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_10.data)\n            compute_14[i0_i1_fused_i2_fused] = T.asin(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_mod_2 = T.Buffer((48,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_11.data)\n            compute_14[i0_i1_fused_i2_fused] = T.fabs(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_mod_2 = T.Buffer((48,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_12.data)\n            compute_14[i0_i1_fused_i2_fused] = T.acos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_14 = T.Buffer((48,), data=compute_13.data)\n            compute_14[i0_i1_fused_i2_fused] = T.cos(T.fabs(T.asinh(ph_0_1[i0_i1_fused_i2_fused])) * ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_add_3 = T.Buffer((48,), data=T_add_2.data)\n            T_add_3[ax0_ax1_fused_ax2_fused] = T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused] + T.fabs(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]))", "op_args": [["abs", "asinh", "abs", "atanh", "acos", "subtract", "add", "asinh", "cos", "acosh", "add", "asinh", "acos", "cos", "multiply", "cos", "acos", "atanh", "acos", "multiply", "asin", "mod", "abs", "mod", "acos", "cos", "add"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_divide_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_add[(((ax0 * 195) + (ax1 * 15)) + ax2)] = (ph_0[(((ax0 * 195) + (ax1 * 15)) + ax2)] + ph_3[(((ax0 * 195) + (ax1 * 15)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 16; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 13; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 15; ++ax2_1) {\n        T_divide[(((ax0_1 * 195) + (ax1_1 * 15)) + ax2_1)] = (ph_0[(((ax0_1 * 195) + (ax1_1 * 15)) + ax2_1)] / ph_3[(((ax0_1 * 195) + (ax1_1 * 15)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3120; ++ax0_ax1_fused_ax2_fused) {\n    T_divide_1[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 13, 15), \"float32\"), ph_3: T.Buffer((16, 13, 15), \"float32\"), T_add: T.Buffer((16, 13, 15), \"float32\"), T_divide: T.Buffer((16, 13, 15), \"float32\"), T_divide_1: T.Buffer((16, 13, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3120,), data=ph_0.data)\n        ph_3_1 = T.Buffer((3120,), data=ph_3.data)\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(13, 15):\n                cse_var_1: T.int32 = ax0 * 195 + ax1 * 15 + ax2\n                T_add_1 = T.Buffer((3120,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(13, 15):\n                cse_var_2: T.int32 = ax0 * 195 + ax1 * 15 + ax2\n                T_divide_2 = T.Buffer((3120,), data=T_divide.data)\n                T_divide_2[cse_var_2] = ph_0_1[cse_var_2] / ph_3_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(3120):\n            T_divide_2 = T.Buffer((3120,), data=T_divide_1.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["add", "divide", "sin", "divide"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 5320) + (i1 * 280)) + (i2 * 20)) + i3)] = coshf(data[((((i0 * 5320) + (i1 * 280)) + (i2 * 20)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 19, 14, 20), \"float32\"), compute: T.Buffer((16, 19, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            for i1, i2, i3 in T.grid(19, 14, 20):\n                cse_var_1: T.int32 = i0 * 5320 + i1 * 280 + i2 * 20 + i3\n                compute_1 = T.Buffer((85120,), data=compute.data)\n                data_1 = T.Buffer((85120,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [16, 19, 14, 20]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* ph_0, float* ph_4) {\n  float T_batch_matmul_NN_rf[76];\n  #pragma omp parallel for\n  for (int32_t b_i_fused_j_fused = 0; b_i_fused_j_fused < 4; ++b_i_fused_j_fused) {\n    for (int32_t k_outer = 0; k_outer < 19; ++k_outer) {\n      T_batch_matmul_NN_rf[((b_i_fused_j_fused * 19) + k_outer)] = 0.000000e+00f;\n      T_batch_matmul_NN_rf[((b_i_fused_j_fused * 19) + k_outer)] = (T_batch_matmul_NN_rf[((b_i_fused_j_fused * 19) + k_outer)] + ((ph_0[((b_i_fused_j_fused * 19) + k_outer)] / cosf(ph_0[((b_i_fused_j_fused * 19) + k_outer)])) * ph_4[k_outer]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4; ++ax0_ax1_fused_ax2_fused) {\n    T_batch_matmul_NN[ax0_ax1_fused_ax2_fused] = 0.000000e+00f;\n    for (int32_t k_outer_v = 0; k_outer_v < 19; ++k_outer_v) {\n      T_batch_matmul_NN[ax0_ax1_fused_ax2_fused] = (T_batch_matmul_NN[ax0_ax1_fused_ax2_fused] + T_batch_matmul_NN_rf[((ax0_ax1_fused_ax2_fused * 19) + k_outer_v)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float T_divide_shared[320];\n  __shared__ float ph_4_shared[40];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n    T_batch_matmul_NN_local[i_c_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(i_c_outer_inner_init + 2)] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 20; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    T_divide_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 16) + ((int)threadIdx.x))] = (ph_0[((ax0_ax1_fused_ax2_fused_outer_outer * 16) + ((int)threadIdx.x))] / __cosf(ph_0[((ax0_ax1_fused_ax2_fused_outer_outer * 16) + ((int)threadIdx.x))]));\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 2; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 4) + (((int)threadIdx.x) >> 2)) < 5) {\n      *(float2*)(ph_4_shared + ((ax0_ax1_fused_ax2_fused_outer_outer_1 * 32) + (((int)threadIdx.x) * 2))) = *(float2*)(ph_4 + ((ax0_ax1_fused_ax2_fused_outer_outer_1 * 32) + (((int)threadIdx.x) * 2)));\n    }\n  }\n  __syncthreads();\n  for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n    for (int k_inner = 0; k_inner < 5; ++k_inner) {\n      T_batch_matmul_NN_local[i_c_outer_inner] = (T_batch_matmul_NN_local[i_c_outer_inner] + (T_divide_shared[(((((((int)threadIdx.x) >> 1) * 40) + ((((int)threadIdx.x) & 1) * 10)) + (i_c_outer_inner * 5)) + k_inner)] * ph_4_shared[(((((int)threadIdx.x) >> 1) * 5) + k_inner)]));\n      T_batch_matmul_NN_local[(i_c_outer_inner + 2)] = (T_batch_matmul_NN_local[(i_c_outer_inner + 2)] + (T_divide_shared[((((((((int)threadIdx.x) >> 1) * 40) + ((((int)threadIdx.x) & 1) * 10)) + (i_c_outer_inner * 5)) + k_inner) + 20)] * ph_4_shared[(((((int)threadIdx.x) >> 1) * 5) + k_inner)]));\n    }\n  }\n  for (int i_inner = 0; i_inner < 2; ++i_inner) {\n    T_batch_matmul_NN[((((((int)threadIdx.x) >> 1) * 8) + ((((int)threadIdx.x) & 1) * 2)) + i_inner)] = T_batch_matmul_NN_local[i_inner];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 1) * 8) + ((((int)threadIdx.x) & 1) * 2)) + i_inner) + 4)] = T_batch_matmul_NN_local[(i_inner + 2)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 19), \"float32\"), ph_4: T.Buffer((1, 19, 1), \"float32\"), T_batch_matmul_NN: T.Buffer((1, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_batch_matmul_NN_rf = T.allocate([76], \"float32\", \"global\")\n        T_batch_matmul_NN_rf_1 = T.Buffer((76,), data=T_batch_matmul_NN_rf)\n        for b_i_fused_j_fused in T.parallel(4):\n            for k_outer in range(19):\n                cse_var_1: T.int32 = b_i_fused_j_fused * 19 + k_outer\n                T_batch_matmul_NN_rf_1[cse_var_1] = T.float32(0)\n                ph_0_1 = T.Buffer((76,), data=ph_0.data)\n                ph_4_1 = T.Buffer((19,), data=ph_4.data)\n                T_batch_matmul_NN_rf_1[cse_var_1] = T_batch_matmul_NN_rf_1[cse_var_1] + ph_0_1[cse_var_1] / T.cos(ph_0_1[cse_var_1]) * ph_4_1[k_outer]\n        for ax0_ax1_fused_ax2_fused in T.parallel(4):\n            T_batch_matmul_NN_1 = T.Buffer((4,), data=T_batch_matmul_NN.data)\n            T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused] = T.float32(0)\n            for k_outer_v in range(19):\n                T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused] = T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused] + T_batch_matmul_NN_rf_1[ax0_ax1_fused_ax2_fused * 19 + k_outer_v]", "op_args": [["cos", "divide", "batch_matmul"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    float T_softmax_maxelem[162];\n    float T_softmax_expsum[162];\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        T_softmax_maxelem[((i1 * 18) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 12; ++k) {\n          T_softmax_maxelem[((i1 * 18) + i2)] = max(T_softmax_maxelem[((i1 * 18) + i2)], data[((((i0 * 1944) + (i1 * 216)) + (i2 * 12)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 9; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        T_softmax_expsum[((i1_1 * 18) + i2_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 12; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[((i1_1 * 18) + i2_1)] = (T_softmax_expsum[((i1_1 * 18) + i2_1)] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1944) + (i1_1 * 216)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)])));\n        }\n      }\n    }\n    for (int32_t i1_2 = 0; i1_2 < 9; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 18; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1944) + (i1_2 * 216)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[((i1_2 * 18) + i2_2)])) / T_softmax_expsum[((i1_2 * 18) + i2_2)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 12; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 891) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 12; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 891) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 384) + (((int)threadIdx.x) * 12)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 2673) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 3)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 9, 18, 12), \"float32\"), T_softmax_norm: T.Buffer((11, 9, 18, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(11):\n            T_softmax_maxelem = T.allocate([162], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([162], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((162,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((21384,), data=data.data)\n            for i1, i2 in T.grid(9, 18):\n                T_softmax_maxelem_1[i1 * 18 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(12):\n                    cse_var_1: T.int32 = i1 * 18 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 1944 + i1 * 216 + i2 * 12 + k])\n            T_softmax_expsum_1 = T.Buffer((162,), data=T_softmax_expsum)\n            for i1, i2 in T.grid(9, 18):\n                T_softmax_expsum_1[i1 * 18 + i2] = T.float32(0)\n                for k in range(12):\n                    cse_var_3: T.int32 = i1 * 18 + i2\n                    cse_var_2: T.int32 = i0 * 1944 + i1 * 216 + i2 * 12 + k\n                    T_softmax_expsum_1[cse_var_3] = T_softmax_expsum_1[cse_var_3] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n            for i1, i2, i3_s in T.grid(9, 18, 12):\n                cse_var_5: T.int32 = i1 * 18 + i2\n                cse_var_4: T.int32 = i0 * 1944 + i1 * 216 + i2 * 12 + i3_s\n                T_softmax_norm_1 = T.Buffer((21384,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[cse_var_5]", "op_args": [11, 9, 18, 12]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      compute_1[((i0 * 3) + i1)] = atanhf(acosf(ph_0[((i0 * 3) + i1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      T_add[((ax0 * 3) + ax1)] = (ph_0[((ax0 * 3) + ax1)] + asinf(ph_0[((ax0 * 3) + ax1)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = fabsf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 3, 1), \"float32\"), compute: T.Buffer((2, 3, 1), \"float32\"), compute_1: T.Buffer((2, 3, 1), \"float32\"), T_add: T.Buffer((2, 3, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((6,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(6):\n            compute_2 = T.Buffer((6,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(2):\n            for i1 in range(3):\n                cse_var_1: T.int32 = i0 * 3 + i1\n                compute_2 = T.Buffer((6,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(T.acos(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(2):\n            for ax1 in range(3):\n                cse_var_2: T.int32 = ax0 * 3 + ax1\n                T_add_1 = T.Buffer((6,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] + T.asin(ph_0_1[cse_var_2])", "op_args": [["abs", "acos", "atanh", "asin", "add"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_mod[(((ax0 * 35) + (ax1 * 7)) + ax2)] = fmodf(ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)], ph_3[(((ax0 * 35) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_1[(((i0 * 35) + (i1 * 7)) + i2)] = sinf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(5, 7):\n                cse_var_1: T.int32 = ax0 * 35 + ax1 * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                ph_3_1 = T.Buffer((315,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 140) + (i1 * 7)) + i2)] = asinhf(ph_0[(((i0 * 140) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 560; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf((ph_0[i0_i1_fused_i2_fused] + ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 20, 7), \"float32\"), compute: T.Buffer((4, 20, 7), \"float32\"), T_add: T.Buffer((4, 20, 7), \"float32\"), compute_1: T.Buffer((4, 20, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((560,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(20, 7):\n                cse_var_1: T.int32 = i0 * 140 + i1 * 7 + i2\n                compute_2 = T.Buffer((560,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(560):\n            T_add_1 = T.Buffer((560,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(560):\n            compute_2 = T.Buffer((560,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] + T.ceil(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["asinh", "abs", "add", "ceil", "add", "acos"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 448; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = coshf(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 4, 8, 16), \"float32\"), compute: T.Buffer((14, 4, 8, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((7168,), data=compute.data)\n                data_1 = T.Buffer((7168,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [14, 4, 8, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_add[(((ax0 * 105) + (ax1 * 7)) + ax2)] = (ph_0[(((ax0 * 105) + (ax1 * 7)) + ax2)] + ph_3[(((ax0 * 105) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 7; ++ax2_1) {\n      T_add_1[((ax0_ax1_fused * 7) + ax2_1)] = (fabsf(ph_0[((ax0_ax1_fused * 7) + ax2_1)]) + ph_0[((ax0_ax1_fused * 7) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 15, 7), \"float32\"), ph_3: T.Buffer((3, 15, 7), \"float32\"), T_add: T.Buffer((3, 15, 7), \"float32\"), compute: T.Buffer((3, 15, 7), \"float32\"), T_add_1: T.Buffer((3, 15, 7), \"float32\"), compute_1: T.Buffer((3, 15, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(15, 7):\n                cse_var_1: T.int32 = ax0 * 105 + ax1 * 7 + ax2\n                T_add_2 = T.Buffer((315,), data=T_add.data)\n                ph_3_1 = T.Buffer((315,), data=ph_3.data)\n                T_add_2[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(7):\n                cse_var_2: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_add_2 = T.Buffer((315,), data=T_add_1.data)\n                T_add_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["add", "acos", "abs", "add", "atan"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 35; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_mod[((ax0_ax1_fused * 11) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 11) + ax2)], ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 77) + (i1 * 11)) + i2)] = atanf(ph_0[(((i0 * 77) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), ph_3: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(35):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_mod_1 = T.Buffer((385,), data=T_mod.data)\n                ph_3_1 = T.Buffer((385,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(7, 11):\n                cse_var_2: T.int32 = i0 * 77 + i1 * 11 + i2\n                compute_2 = T.Buffer((385,), data=compute.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "atan", "sin", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_mod[(((ax0 * 35) + (ax1 * 7)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)]), ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = atanf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(acosf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(5, 7):\n                cse_var_1: T.int32 = ax0 * 35 + ax1 * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    float T_softmax_maxelem[14];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 11; ++k) {\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((i0 * 154) + (i2 * 11)) + k)]);\n      }\n    }\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 11; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0 * 154) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1])));\n      }\n      for (int32_t i3_s = 0; i3_s < 11; ++i3_s) {\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0 * 154) + (i2_1 * 11)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0 * 154) + (i2_1 * 11)) + i3_s)] - T_softmax_maxelem[i2_1])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 11; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 22) + (((int)threadIdx.x) * 11)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 11; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 11)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 1, 14, 11), \"float32\"), T_softmax_norm: T.Buffer((16, 1, 14, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            T_softmax_maxelem = T.allocate([14], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((14,), data=T_softmax_maxelem, align=32)\n            data_1 = T.Buffer((2464,), data=data.data)\n            for i2 in range(14):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(11):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0 * 154 + i2 * 11 + k])\n            for i2 in range(14):\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(11):\n                    cse_var_1: T.int32 = i0 * 154 + i2 * 11 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n                for i3_s in range(11):\n                    cse_var_2: T.int32 = i0 * 154 + i2 * 11 + i3_s\n                    T_softmax_norm_1 = T.Buffer((2464,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[0]", "op_args": [16, 1, 14, 11]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 495; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 45) + (i1 * 5)) + i2)] = atanf(ph_0[(((i0 * 45) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 5), \"float32\"), compute: T.Buffer((11, 9, 5), \"float32\"), T_mod: T.Buffer((11, 9, 5), \"float32\"), compute_1: T.Buffer((11, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((495,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(495):\n            T_mod_1 = T.Buffer((495,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(9, 5):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 5 + i2\n                compute_2 = T.Buffer((495,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])", "op_args": [["atanh", "acos", "mod", "atan"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 12; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = coshf(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 1, 6, 16), \"float32\"), compute: T.Buffer((2, 1, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(12):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((192,), data=compute.data)\n                data_1 = T.Buffer((192,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [2, 1, 6, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 45) + (i1 * 5)) + i2)] = atanhf(ph_0[(((i0 * 45) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_mod[(((ax0 * 45) + (ax1 * 5)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 45) + (ax1 * 5)) + ax2)]), ph_0[(((ax0 * 45) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 5), \"float32\"), compute: T.Buffer((11, 9, 5), \"float32\"), T_mod: T.Buffer((11, 9, 5), \"float32\"), compute_1: T.Buffer((11, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((495,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(9, 5):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 5 + i2\n                compute_2 = T.Buffer((495,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(9, 5):\n                cse_var_2: T.int32 = ax0 * 45 + ax1 * 5 + ax2\n                T_mod_1 = T.Buffer((495,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_subtract[(((ax0 * 18) + (ax1 * 2)) + ax2)] = (asinhf(ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)]) - ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 18) + (i1 * 2)) + i2)] = asinf(asinhf(ph_0[(((i0 * 18) + (i1 * 2)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf((ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(9, 2):\n                cse_var_1: T.int32 = ax0 * 18 + ax1 * 2 + ax2\n                T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(9, 2):\n                cse_var_2: T.int32 = i0 * 18 + i1 * 2 + i2\n                compute_2 = T.Buffer((360,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(T.asinh(ph_0_1[cse_var_2]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_1 = T.Buffer((360,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2160; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 2160; ++ax0_ax1_fused_ax2_fused_2) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused_2] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused_2]) - ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = asinf(asinhf(ph_0[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 9), \"float32\"), ph_3: T.Buffer((12, 20, 9), \"float32\"), T_subtract: T.Buffer((12, 20, 9), \"float32\"), T_add: T.Buffer((12, 20, 9), \"float32\"), T_subtract_1: T.Buffer((12, 20, 9), \"float32\"), compute: T.Buffer((12, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2160,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_add_1 = T.Buffer((2160,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((2160,), data=compute.data)\n                compute_1[cse_var_1] = T.asin(T.asinh(ph_0_1[cse_var_1]))", "op_args": [["subtract", "add", "asinh", "subtract", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2160; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 2160; ++ax0_ax1_fused_ax2_fused_2) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused_2] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused_2]) - ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = asinf(asinhf(ph_0[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 9), \"float32\"), ph_3: T.Buffer((12, 20, 9), \"float32\"), T_subtract: T.Buffer((12, 20, 9), \"float32\"), T_add: T.Buffer((12, 20, 9), \"float32\"), T_subtract_1: T.Buffer((12, 20, 9), \"float32\"), compute: T.Buffer((12, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2160,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_add_1 = T.Buffer((2160,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((2160,), data=compute.data)\n                compute_1[cse_var_1] = T.asin(T.asinh(ph_0_1[cse_var_1]))", "op_args": [["subtract", "add", "asinh", "subtract", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 221; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 18) + ax2)] = (ph_0[((ax0_ax1_fused * 18) + ax2)] - ph_3[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3978; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 221; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 18; ++ax2_1) {\n      T_multiply[((ax0_ax1_fused_1 * 18) + ax2_1)] = (asinhf(ph_0[((ax0_ax1_fused_1 * 18) + ax2_1)]) * ph_0[((ax0_ax1_fused_1 * 18) + ax2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 13, 18), \"float32\"), ph_3: T.Buffer((17, 13, 18), \"float32\"), T_subtract: T.Buffer((17, 13, 18), \"float32\"), compute: T.Buffer((17, 13, 18), \"float32\"), T_multiply: T.Buffer((17, 13, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3978,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(221):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_subtract_1 = T.Buffer((3978,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((3978,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(3978):\n            compute_1 = T.Buffer((3978,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(221):\n            for ax2 in range(18):\n                cse_var_2: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_multiply_1 = T.Buffer((3978,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]", "op_args": [["subtract", "acosh", "asinh", "multiply"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = cosf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_add[(((ax0 * 180) + (ax1 * 12)) + ax2)] = (acoshf(ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]) + ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1620; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf((ph_0[i0_i1_fused_i2_fused] - cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((1620,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(15, 12):\n                cse_var_2: T.int32 = ax0 * 180 + ax1 * 12 + ax2\n                T_add_1 = T.Buffer((1620,), data=T_add.data)\n                T_add_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] - T.cos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1755; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1755; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 117; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 15) + ax2)] = (ph_0[((ax0_ax1_fused * 15) + ax2)] - cosf(ph_0[((ax0_ax1_fused * 15) + ax2)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 15), \"float32\"), compute: T.Buffer((13, 9, 15), \"float32\"), T_add: T.Buffer((13, 9, 15), \"float32\"), T_subtract: T.Buffer((13, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1755,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1755):\n            compute_1 = T.Buffer((1755,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_add_1 = T.Buffer((1755,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(117):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_subtract_1 = T.Buffer((1755,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.cos(ph_0_1[cse_var_1])", "op_args": [["cos", "acosh", "add", "cos", "subtract"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1755; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1755; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 117; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 15) + ax2)] = (ph_0[((ax0_ax1_fused * 15) + ax2)] - cosf(ph_0[((ax0_ax1_fused * 15) + ax2)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 15), \"float32\"), compute: T.Buffer((13, 9, 15), \"float32\"), T_add: T.Buffer((13, 9, 15), \"float32\"), T_subtract: T.Buffer((13, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1755,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1755):\n            compute_1 = T.Buffer((1755,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_add_1 = T.Buffer((1755,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(117):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_subtract_1 = T.Buffer((1755,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.cos(ph_0_1[cse_var_1])", "op_args": [["cos", "acosh", "add", "cos", "subtract"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_7) {\n  float auto_scheduler_layout_transform[88];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 176; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 176; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n      for (int32_t ax7 = 0; ax7 < 11; ++ax7) {\n        for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n          auto_scheduler_layout_transform[((((ax0_ax1_fused_ax2_fused * 44) + (ax5 * 22)) + (ax7 * 2)) + ax8)] = ph_7[((((ax0_ax1_fused_ax2_fused * 44) + (ax5 * 22)) + (ax8 * 11)) + ax7)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 4; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n        T_batch_matmul_NN[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 8) + (b_outer_inner_init * 4)) + (b_inner_init * 2)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1))] = 0.000000e+00f;\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 11; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n          T_batch_matmul_NN[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 8) + (b_outer_inner * 4)) + (b_inner * 2)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1))] = (T_batch_matmul_NN[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 8) + (b_outer_inner * 4)) + (b_inner * 2)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1))] + (sinf(ph_0[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 88) + (b_outer_inner * 44)) + (b_inner * 22)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1) * 11)) + k_inner)]) * auto_scheduler_layout_transform[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 44) + (b_outer_inner * 22)) + (k_inner * 2)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_7) {\n  float T_batch_matmul_NN_local[16];\n  __shared__ float compute_shared[64];\n  __shared__ float ph_7_shared[8];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n      for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n        T_batch_matmul_NN_local[(((b_c_inner_init * 4) + (i_c_outer_inner_init * 2)) + i_c_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 16; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))] = __sinf(ph_0[(((ax0_ax1_fused_ax2_fused_outer_outer * 20) + (((int)threadIdx.x) * 5)) + k_outer_outer)]);\n    }\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 2; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      ph_7_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 4) + ((int)threadIdx.x))] = ph_7[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 20) + (((int)threadIdx.x) * 5)) + k_outer_outer)];\n    }\n    __syncthreads();\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n      for (int b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n        for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n          T_batch_matmul_NN_local[(((b_c_inner * 4) + (i_c_outer_inner * 2)) + i_c_inner)] = (T_batch_matmul_NN_local[(((b_c_inner * 4) + (i_c_outer_inner * 2)) + i_c_inner)] + (compute_shared[((((((((int)threadIdx.x) >> 1) * 32) + (b_c_inner * 8)) + ((((int)threadIdx.x) & 1) * 4)) + (i_c_outer_inner * 2)) + i_c_inner)] * ph_7_shared[(((((int)threadIdx.x) >> 1) * 4) + b_c_inner)]));\n        }\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    for (int i_inner = 0; i_inner < 4; ++i_inner) {\n      T_batch_matmul_NN[(((((((int)threadIdx.x) >> 1) * 32) + (b_inner * 8)) + ((((int)threadIdx.x) & 1) * 4)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 4) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 2, 11), \"float32\"), ph_7: T.Buffer((8, 11, 1), \"float32\"), compute: T.Buffer((8, 2, 11), \"float32\"), compute_1: T.Buffer((8, 2, 11), \"float32\"), compute_2: T.Buffer((8, 2, 11), \"float32\"), T_batch_matmul_NN: T.Buffer((8, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([88], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((176,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((88,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2):\n            for ax5, ax7, ax8 in T.grid(2, 11, 2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 44 + ax5 * 22\n                ph_7_1 = T.Buffer((88,), data=ph_7.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax7 * 2 + ax8] = ph_7_1[cse_var_1 + ax8 * 11 + ax7]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(4):\n            T_batch_matmul_NN_1 = T.Buffer((16,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, b_inner_init in T.grid(2, 2):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 2 * 8 + b_outer_inner_init * 4 + b_inner_init * 2 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2] = T.float32(0)\n            for b_outer_inner, k_inner, b_inner in T.grid(2, 11, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 8 + b_outer_inner * 4 + b_inner * 2 + cse_var_4\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.sin(ph_0_1[cse_var_3 * 88 + b_outer_inner * 44 + b_inner * 22 + cse_var_4 * 11 + k_inner]) * auto_scheduler_layout_transform_1[cse_var_3 * 44 + b_outer_inner * 22 + k_inner * 2 + b_inner]", "op_args": [["asinh", "acos", "exp", "cos", "sin", "batch_matmul"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 110; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = coshf(data[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 715) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 11, 13, 1), \"float32\"), compute: T.Buffer((10, 11, 13, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(110):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_1 = T.Buffer((1430,), data=compute.data)\n                data_1 = T.Buffer((1430,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [10, 11, 13, 1]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    float T_softmax_maxelem[10];\n    float T_softmax_expsum[10];\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        T_softmax_maxelem[i2] = -3.402823e+38f;\n        for (int32_t k = 0; k < 11; ++k) {\n          T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[((((i0 * 1210) + (i1 * 110)) + (i2 * 11)) + k)]);\n        }\n      }\n      for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 11; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1210) + (i1 * 110)) + (i2_1 * 11)) + k_1)] - T_softmax_maxelem[i2_1])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 10; ++i2_2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1210) + (i1 * 110)) + (i2_2 * 11)) + i3)] - T_softmax_maxelem[i2_2])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 11; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 88) + (((int)threadIdx.x) * 11)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 605) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)])) / T_softmax_expsum[(((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 11)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 11; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 22) + (((int)threadIdx.x) * 11)) + k)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 11, 10, 11), \"float32\"), T_softmax_norm: T.Buffer((8, 11, 10, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            T_softmax_maxelem = T.allocate([10], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([10], \"float32\", \"global\")\n            for i1 in range(11):\n                T_softmax_maxelem_1 = T.Buffer((10,), data=T_softmax_maxelem, align=32)\n                data_1 = T.Buffer((9680,), data=data.data)\n                for i2 in range(10):\n                    T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                    for k in range(11):\n                        T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0 * 1210 + i1 * 110 + i2 * 11 + k])\n                T_softmax_expsum_1 = T.Buffer((10,), data=T_softmax_expsum, align=32)\n                for i2 in range(10):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    for k in range(11):\n                        cse_var_1: T.int32 = i0 * 1210 + i1 * 110 + i2 * 11 + k\n                        T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n                for i2, i3 in T.grid(10, 11):\n                    cse_var_2: T.int32 = i0 * 1210 + i1 * 110 + i2 * 11 + i3\n                    T_softmax_norm_1 = T.Buffer((9680,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[i2]", "op_args": [8, 11, 10, 11]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 35; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_mod[((ax0_ax1_fused * 11) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 11) + ax2)], ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 385; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), ph_3: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(35):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_mod_1 = T.Buffer((385,), data=T_mod.data)\n                ph_3_1 = T.Buffer((385,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "atan", "sin", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 308; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 308; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 44) + (i1 * 4)) + i2)] = acosf(sinf(ph_0[(((i0 * 44) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 308; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (sinf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 4), \"float32\"), ph_3: T.Buffer((7, 11, 4), \"float32\"), T_mod: T.Buffer((7, 11, 4), \"float32\"), compute: T.Buffer((7, 11, 4), \"float32\"), compute_1: T.Buffer((7, 11, 4), \"float32\"), T_multiply: T.Buffer((7, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((308,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_mod_1 = T.Buffer((308,), data=T_mod.data)\n            ph_3_1 = T.Buffer((308,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(11, 4):\n                cse_var_1: T.int32 = i0 * 44 + i1 * 4 + i2\n                compute_2 = T.Buffer((308,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(T.sin(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_multiply_1 = T.Buffer((308,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["mod", "atan", "sin", "acos", "multiply"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2312; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 289) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 17, 1, 8), \"float32\"), compute: T.Buffer((17, 17, 1, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2312):\n            compute_1 = T.Buffer((2312,), data=compute.data)\n            data_1 = T.Buffer((2312,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cosh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [17, 17, 1, 8]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3800; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3800; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3800; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 10, 20), \"float32\"), ph_3: T.Buffer((19, 10, 20), \"float32\"), T_add: T.Buffer((19, 10, 20), \"float32\"), compute: T.Buffer((19, 10, 20), \"float32\"), compute_1: T.Buffer((19, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3800,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3800):\n            T_add_1 = T.Buffer((3800,), data=T_add.data)\n            ph_3_1 = T.Buffer((3800,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3800):\n            compute_2 = T.Buffer((3800,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3800):\n            compute_2 = T.Buffer((3800,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["add", "exp", "acosh", "acosh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 32; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 32; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_2[(((i0 * 16) + (i1 * 2)) + i2)] = cosf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_3[(((i0_1 * 16) + (i1_1 * 2)) + i2_1)] = sinf(ph_0[(((i0_1 * 16) + (i1_1 * 2)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 8, 2), \"float32\"), compute: T.Buffer((2, 8, 2), \"float32\"), compute_1: T.Buffer((2, 8, 2), \"float32\"), compute_2: T.Buffer((2, 8, 2), \"float32\"), compute_3: T.Buffer((2, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_4 = T.Buffer((32,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_4 = T.Buffer((32,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_4 = T.Buffer((32,), data=compute_2.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_2: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_4 = T.Buffer((32,), data=compute_3.data)\n                compute_4[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["asinh", "acos", "exp", "cos", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  float compute_4[2964];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2964; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2964; ++i0_i1_fused_i2_fused_1) {\n    compute_4[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2964; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = expf(compute_4[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_2[(((i0 * 156) + (i1 * 12)) + i2)] = acosf(compute_4[(((i0 * 156) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 247; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      compute_3[((i0_i1_fused * 12) + i2_1)] = atanf((ph_0[((i0_i1_fused * 12) + i2_1)] * ph_3[((i0_i1_fused * 12) + i2_1)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 13, 12), \"float32\"), ph_3: T.Buffer((19, 13, 12), \"float32\"), compute: T.Buffer((19, 13, 12), \"float32\"), compute_1: T.Buffer((19, 13, 12), \"float32\"), compute_2: T.Buffer((19, 13, 12), \"float32\"), compute_3: T.Buffer((19, 13, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_4 = T.allocate([2964], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((2964,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2964):\n            compute_5 = T.Buffer((2964,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        compute_5 = T.Buffer((2964,), data=compute_4)\n        for i0_i1_fused_i2_fused in T.parallel(2964):\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2964):\n            compute_6 = T.Buffer((2964,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.exp(compute_5[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(13, 12):\n                cse_var_1: T.int32 = i0 * 156 + i1 * 12 + i2\n                compute_6 = T.Buffer((2964,), data=compute_2.data)\n                compute_6[cse_var_1] = T.acos(compute_5[cse_var_1])\n        for i0_i1_fused in T.parallel(247):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_6 = T.Buffer((2964,), data=compute_3.data)\n                ph_3_1 = T.Buffer((2964,), data=ph_3.data)\n                compute_6[cse_var_2] = T.atan(ph_0_1[cse_var_2] * ph_3_1[cse_var_2])", "op_args": [["multiply", "cos", "exp", "exp", "acos", "atan"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  float T_softmax_maxelem[13];\n  float T_softmax_expsum[13];\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 14; ++k) {\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((i1 * 182) + (i2 * 14)) + k)]);\n      }\n    }\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      T_softmax_expsum[i2_1] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 14; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i1 * 182) + (i2_1 * 14)) + k_1)] - T_softmax_maxelem[i2_1])));\n      }\n    }\n    for (int32_t i2_2 = 0; i2_2 < 13; ++i2_2) {\n      for (int32_t i3_s = 0; i3_s < 14; ++i3_s) {\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i1 * 182) + (i2_2 * 14)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i1 * 182) + (i2_2 * 14)) + i3_s)] - T_softmax_maxelem[i2_2])) / T_softmax_expsum[i2_2]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int k = 0; k < 14; ++k) {\n    T_softmax_maxelem[((int)threadIdx.x)] = max(T_softmax_maxelem[((int)threadIdx.x)], data[((((int)threadIdx.x) * 14) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(26) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int k = 0; k < 14; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((int)threadIdx.x)] = (T_softmax_expsum[((int)threadIdx.x)] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)threadIdx.x) * 14) + k)] - T_softmax_maxelem[((int)threadIdx.x)])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(14) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] - T_softmax_maxelem[((int)blockIdx.x)])) / T_softmax_expsum[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 2, 13, 14), \"float32\"), T_softmax_norm: T.Buffer((1, 2, 13, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([13], \"float32\", \"global\")\n        T_softmax_expsum = T.allocate([13], \"float32\", \"global\")\n        for i1 in range(2):\n            T_softmax_maxelem_1 = T.Buffer((13,), data=T_softmax_maxelem, align=32)\n            data_1 = T.Buffer((364,), data=data.data)\n            for i2 in range(13):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(14):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i1 * 182 + i2 * 14 + k])\n            T_softmax_expsum_1 = T.Buffer((13,), data=T_softmax_expsum, align=32)\n            for i2 in range(13):\n                T_softmax_expsum_1[i2] = T.float32(0)\n                for k in range(14):\n                    cse_var_1: T.int32 = i1 * 182 + i2 * 14 + k\n                    T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n            for i2, i3_s in T.grid(13, 14):\n                cse_var_2: T.int32 = i1 * 182 + i2 * 14 + i3_s\n                T_softmax_norm_1 = T.Buffer((364,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[i2]", "op_args": [1, 2, 13, 14]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 170; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 14; ++i3) {\n        compute[(((i0_i1_fused * 56) + (i2 * 14)) + i3)] = coshf(data[(((i0_i1_fused * 56) + (i2 * 14)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 17, 4, 14), \"float32\"), compute: T.Buffer((10, 17, 4, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(170):\n            for i2, i3 in T.grid(4, 14):\n                cse_var_1: T.int32 = i0_i1_fused * 56 + i2 * 14 + i3\n                compute_1 = T.Buffer((9520,), data=compute.data)\n                data_1 = T.Buffer((9520,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [10, 17, 4, 14]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        compute[(((i0_i1_fused * 360) + (i2 * 18)) + i3)] = coshf(data[(((i0_i1_fused * 360) + (i2 * 18)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 1, 20, 18), \"float32\"), compute: T.Buffer((11, 1, 20, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(11):\n            for i2, i3 in T.grid(20, 18):\n                cse_var_1: T.int32 = i0_i1_fused * 360 + i2 * 18 + i3\n                compute_1 = T.Buffer((3960,), data=compute.data)\n                data_1 = T.Buffer((3960,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [11, 1, 20, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[5054];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5054; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 266) + (i1 * 19)) + i2)] = cosf(ph_0[(((i0 * 266) + (i1 * 19)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5054; ++i0_i1_fused_i2_fused) {\n    compute_3[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5054; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 266; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_2[((i0_i1_fused * 19) + i2_1)] = acosf(compute_3[((i0_i1_fused * 19) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 14, 19), \"float32\"), ph_3: T.Buffer((19, 14, 19), \"float32\"), T_multiply: T.Buffer((19, 14, 19), \"float32\"), compute: T.Buffer((19, 14, 19), \"float32\"), compute_1: T.Buffer((19, 14, 19), \"float32\"), compute_2: T.Buffer((19, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([5054], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((5054,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5054):\n            T_multiply_1 = T.Buffer((5054,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((5054,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(14, 19):\n                cse_var_1: T.int32 = i0 * 266 + i1 * 19 + i2\n                compute_4 = T.Buffer((5054,), data=compute.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        compute_4 = T.Buffer((5054,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_5 = T.Buffer((5054,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(266):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_5 = T.Buffer((5054,), data=compute_2.data)\n                compute_5[cse_var_2] = T.acos(compute_4[cse_var_2])", "op_args": [["multiply", "cos", "exp", "exp", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 140; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_divide[((ax0_ax1_fused * 5) + ax2)] = (ph_0[((ax0_ax1_fused * 5) + ax2)] / ph_3[((ax0_ax1_fused * 5) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 700; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = atanhf(ceilf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 700; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanhf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 10, 5), \"float32\"), ph_3: T.Buffer((14, 10, 5), \"float32\"), T_divide: T.Buffer((14, 10, 5), \"float32\"), compute: T.Buffer((14, 10, 5), \"float32\"), compute_1: T.Buffer((14, 10, 5), \"float32\"), compute_2: T.Buffer((14, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((700,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(140):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_divide_1 = T.Buffer((700,), data=T_divide.data)\n                ph_3_1 = T.Buffer((700,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(700):\n            compute_3 = T.Buffer((700,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(140):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((700,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atanh(T.ceil(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(700):\n            compute_3 = T.Buffer((700,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["divide", "ceil", "ceil", "atanh", "atanh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 100; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 100; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - acoshf(acosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 100; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - acoshf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 10, 10), \"float32\"), compute: T.Buffer((1, 10, 10), \"float32\"), T_subtract: T.Buffer((1, 10, 10), \"float32\"), T_add: T.Buffer((1, 10, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((100,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(100):\n            compute_1 = T.Buffer((100,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(100):\n            T_subtract_1 = T.Buffer((100,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.acosh(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(100):\n            T_add_1 = T.Buffer((100,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["exp", "acos", "acosh", "subtract", "cos", "add"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 112; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] - ph_3[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 448; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 448; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanf(ceilf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 8, 4), \"float32\"), ph_3: T.Buffer((14, 8, 4), \"float32\"), T_subtract: T.Buffer((14, 8, 4), \"float32\"), compute: T.Buffer((14, 8, 4), \"float32\"), compute_1: T.Buffer((14, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((448,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(112):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_subtract_1 = T.Buffer((448,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((448,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            compute_2 = T.Buffer((448,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            compute_2 = T.Buffer((448,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["subtract", "acosh", "ceil", "atan"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      for (int32_t i3 = 0; i3 < 15; ++i3) {\n        compute[(((i0_i1_fused * 135) + (i2 * 15)) + i3)] = coshf(data[(((i0_i1_fused * 135) + (i2 * 15)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 9, 9, 15), \"float32\"), compute: T.Buffer((5, 9, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(45):\n            for i2, i3 in T.grid(9, 15):\n                cse_var_1: T.int32 = i0_i1_fused * 135 + i2 * 15 + i3\n                compute_1 = T.Buffer((6075,), data=compute.data)\n                data_1 = T.Buffer((6075,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [5, 9, 9, 15]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 162) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 162) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 648; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(ceilf(ph_0[((int)blockIdx.x)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 18, 9), \"float32\"), compute: T.Buffer((4, 18, 9), \"float32\"), compute_1: T.Buffer((4, 18, 9), \"float32\"), compute_2: T.Buffer((4, 18, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((648,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(18, 9):\n                cse_var_1: T.int32 = i0 * 162 + i1 * 9 + i2\n                compute_3 = T.Buffer((648,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_3 = T.Buffer((648,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_3 = T.Buffer((648,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "ceil", "acos", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 45) + (i1 * 5)) + i2)] = atanhf(ph_0[(((i0 * 45) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 495; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 495; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 5), \"float32\"), compute: T.Buffer((11, 9, 5), \"float32\"), T_mod: T.Buffer((11, 9, 5), \"float32\"), compute_1: T.Buffer((11, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((495,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(9, 5):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 5 + i2\n                compute_2 = T.Buffer((495,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(495):\n            T_mod_1 = T.Buffer((495,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(495):\n            compute_2 = T.Buffer((495,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 13; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = ceilf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0 * 8) + i2_1)] = fabsf(ph_0[((i0 * 8) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 8), \"float32\"), compute: T.Buffer((13, 1, 8), \"float32\"), compute_1: T.Buffer((13, 1, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((104,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(13):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((104,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0 in T.parallel(13):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0 * 8 + i2\n                compute_2 = T.Buffer((104,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2])", "op_args": [["ceil", "abs"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 2; ++ax0_ax1_fused) {\n    T_subtract[ax0_ax1_fused] = (ph_0[ax0_ax1_fused] - ph_3[ax0_ax1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    T_divide[ax0] = (ph_0[ax0] / ph_3[ax0]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 1, 1), \"float32\"), ph_3: T.Buffer((2, 1, 1), \"float32\"), T_subtract: T.Buffer((2, 1, 1), \"float32\"), T_divide: T.Buffer((2, 1, 1), \"float32\"), compute: T.Buffer((2, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(2):\n            T_subtract_1 = T.Buffer((2,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused] = ph_0_1[ax0_ax1_fused] - ph_3_1[ax0_ax1_fused]\n        for ax0 in T.parallel(2):\n            T_divide_1 = T.Buffer((2,), data=T_divide.data)\n            T_divide_1[ax0] = ph_0_1[ax0] / ph_3_1[ax0]\n        for i0_i1_fused_i2_fused in T.parallel(2):\n            compute_1 = T.Buffer((2,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["subtract", "divide", "acosh", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 35) + (i1 * 7)) + i2)] = atanhf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_2[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)] = sinf(ph_0[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2040; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    T_softmax_maxelem[0] = -3.402823e+38f;\n    for (int32_t k = 0; k < 18; ++k) {\n      T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((i0_i1_fused_i2_fused * 18) + k)]);\n    }\n    T_softmax_expsum[0] = 0.000000e+00f;\n    for (int32_t k_1 = 0; k_1 < 18; ++k_1) {\n        int32_t v_ = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0])));\n    }\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n        int32_t v__1 = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[((i0_i1_fused_i2_fused * 18) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 18; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 216) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 255) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 255) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 2295) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 12, 17, 18), \"float32\"), T_softmax_norm: T.Buffer((10, 12, 17, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2040):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n            T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n            data_1 = T.Buffer((36720,), data=data.data)\n            for k in range(18):\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 18 + k])\n            T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n            T_softmax_expsum_1[0] = T.float32(0)\n            for k in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + k\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n            for i3 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                T_softmax_norm_1 = T.Buffer((36720,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [10, 12, 17, 18]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i0_i1_fused * 60) + (i2 * 6)) + i3)] = coshf(data[(((i0_i1_fused * 60) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 4, 10, 6), \"float32\"), compute: T.Buffer((14, 4, 10, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(56):\n            for i2, i3 in T.grid(10, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 60 + i2 * 6 + i3\n                compute_1 = T.Buffer((3360,), data=compute.data)\n                data_1 = T.Buffer((3360,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [14, 4, 10, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_multiply[((ax1 * 11) + ax2)] = (ph_0[((ax1 * 11) + ax2)] * ph_3[((ax1 * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 220; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(fmodf(ph_0[i0_i1_fused_i2_fused], asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_1[((i1 * 11) + i2)] = ceilf(fmodf(ph_0[((i1 * 11) + i2)], asinf(ph_0[((i1 * 11) + i2)])));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(fmodf(ph_0[((int)blockIdx.x)], asinf(ph_0[((int)blockIdx.x)])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 20, 11), \"float32\"), ph_3: T.Buffer((1, 20, 11), \"float32\"), T_multiply: T.Buffer((1, 20, 11), \"float32\"), compute: T.Buffer((1, 20, 11), \"float32\"), compute_1: T.Buffer((1, 20, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((220,), data=ph_0.data)\n        for ax1, ax2 in T.grid(20, 11):\n            cse_var_1: T.int32 = ax1 * 11 + ax2\n            T_multiply_1 = T.Buffer((220,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((220,), data=ph_3.data)\n            T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(220):\n            compute_2 = T.Buffer((220,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.asin(ph_0_1[i0_i1_fused_i2_fused])))\n        for i1, i2 in T.grid(20, 11):\n            cse_var_2: T.int32 = i1 * 11 + i2\n            compute_2 = T.Buffer((220,), data=compute_1.data)\n            compute_2[cse_var_2] = T.ceil(T.truncmod(ph_0_1[cse_var_2], T.asin(ph_0_1[cse_var_2])))", "op_args": [["multiply", "asin", "mod", "exp", "ceil"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] + asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 16) + (i1 * 8)) + i2)] = acoshf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_divide[(((ax0 * 16) + (ax1 * 8)) + ax2)] = (asinf(ph_0[(((ax0 * 16) + (ax1 * 8)) + ax2)]) / ph_0[(((ax0 * 16) + (ax1 * 8)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf((ph_0[((int)blockIdx.x)] + asinhf(ph_0[((int)blockIdx.x)])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] + T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(2, 8):\n                cse_var_2: T.int32 = ax0 * 16 + ax1 * 8 + ax2\n                T_divide_1 = T.Buffer((48,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.asin(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1386; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_mod[(((ax0 * 99) + (ax1 * 9)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]), ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1386):\n            compute_1 = T.Buffer((1386,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_1: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = atanhf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 6; ++i1) {\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      compute_1[((i1 * 14) + i2_1)] = acosf(ph_0[((i1 * 14) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 6, 14), \"float32\"), compute: T.Buffer((1, 6, 14), \"float32\"), compute_1: T.Buffer((1, 6, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((84,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_2 = T.Buffer((84,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i1, i2 in T.grid(6, 14):\n            cse_var_2: T.int32 = i1 * 14 + i2\n            compute_2 = T.Buffer((84,), data=compute_1.data)\n            compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])", "op_args": [["atanh", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_add[(((ax0 * 180) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 180) + (ax1 * 9)) + ax2)] + ph_3[(((ax0 * 180) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2160; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused_1] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 9), \"float32\"), ph_3: T.Buffer((12, 20, 9), \"float32\"), T_subtract: T.Buffer((12, 20, 9), \"float32\"), T_add: T.Buffer((12, 20, 9), \"float32\"), T_subtract_1: T.Buffer((12, 20, 9), \"float32\"), compute: T.Buffer((12, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2160,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(20, 9):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((2160,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_subtract_2 = T.Buffer((2160,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_1 = T.Buffer((2160,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["subtract", "add", "asinh", "subtract", "asin"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 110; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 18) + i3)] = coshf(data[((i0_i1_fused_i2_fused * 18) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 10, 1, 18), \"float32\"), compute: T.Buffer((11, 10, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(110):\n            for i3 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                compute_1 = T.Buffer((1980,), data=compute.data)\n                data_1 = T.Buffer((1980,), data=data.data)\n                compute_1[cse_var_1] = T.cosh(data_1[cse_var_1])", "op_args": [11, 10, 1, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_subtract[(((ax0 * 240) + (ax1 * 20)) + ax2)] = (ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)] - ph_3[(((ax0 * 240) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2880; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 12; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n        T_subtract_1[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)] = (asinhf(ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)]) - ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 12, 20), \"float32\"), ph_3: T.Buffer((12, 12, 20), \"float32\"), T_subtract: T.Buffer((12, 12, 20), \"float32\"), T_add: T.Buffer((12, 12, 20), \"float32\"), T_subtract_1: T.Buffer((12, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2880,), data=ph_3.data)\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_subtract_2 = T.Buffer((2880,), data=T_subtract.data)\n                T_subtract_2[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_add_1 = T.Buffer((2880,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_2: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_subtract_2 = T.Buffer((2880,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]", "op_args": [["subtract", "add", "asinh", "subtract"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 135) + (i1 * 15)) + i2)] = cosf(ph_0[(((i0 * 135) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1755; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1755; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - cosf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 15), \"float32\"), compute: T.Buffer((13, 9, 15), \"float32\"), T_add: T.Buffer((13, 9, 15), \"float32\"), T_subtract: T.Buffer((13, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1755,), data=ph_0.data)\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(9, 15):\n                cse_var_1: T.int32 = i0 * 135 + i1 * 15 + i2\n                compute_1 = T.Buffer((1755,), data=compute.data)\n                compute_1[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_add_1 = T.Buffer((1755,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1755):\n            T_subtract_1 = T.Buffer((1755,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.cos(ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["cos", "acosh", "add", "cos", "subtract"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3240; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 2, 6, 18), \"float32\"), compute: T.Buffer((15, 2, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3240):\n            compute_1 = T.Buffer((3240,), data=compute.data)\n            data_1 = T.Buffer((3240,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cosh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [15, 2, 6, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 468; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 468; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 52; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0_i1_fused * 9) + i2)] = cosf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 13, 9), \"float32\"), compute: T.Buffer((4, 13, 9), \"float32\"), T_add: T.Buffer((4, 13, 9), \"float32\"), compute_1: T.Buffer((4, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_2 = T.Buffer((468,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(468):\n            T_add_1 = T.Buffer((468,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(52):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((468,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])", "op_args": [["cos", "acosh", "add", "cos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 80) + (i1 * 10)) + i2)] = fabsf(ph_0[(((i0 * 80) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 12; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n        compute_1[(((i0_1 * 80) + (i1_1 * 10)) + i2_1)] = atanf(ph_0[(((i0_1 * 80) + (i1_1 * 10)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = fabsf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 8, 10), \"float32\"), compute: T.Buffer((12, 8, 10), \"float32\"), T_multiply: T.Buffer((12, 8, 10), \"float32\"), compute_1: T.Buffer((12, 8, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(8, 10):\n                cse_var_1: T.int32 = i0 * 80 + i1 * 10 + i2\n                compute_2 = T.Buffer((960,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            T_multiply_1 = T.Buffer((960,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(8, 10):\n                cse_var_2: T.int32 = i0 * 80 + i1 * 10 + i2\n                compute_2 = T.Buffer((960,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])", "op_args": [["abs", "sin", "multiply", "atan"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 792; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 72) + (i1 * 9)) + i2)] = asinf(ph_0[(((i0 * 72) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 11; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_1[(((i0_1 * 72) + (i1_1 * 9)) + i2_1)] = fabsf(atanf(ph_0[(((i0_1 * 72) + (i1_1 * 9)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 792; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_1] = (atanf(ph_0[ax0_ax1_fused_ax2_fused_1]) / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 8, 9), \"float32\"), ph_3: T.Buffer((11, 8, 9), \"float32\"), T_mod: T.Buffer((11, 8, 9), \"float32\"), compute: T.Buffer((11, 8, 9), \"float32\"), compute_1: T.Buffer((11, 8, 9), \"float32\"), T_divide: T.Buffer((11, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((792,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(792):\n            T_mod_1 = T.Buffer((792,), data=T_mod.data)\n            ph_3_1 = T.Buffer((792,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(8, 9):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 9 + i2\n                compute_2 = T.Buffer((792,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(8, 9):\n                cse_var_2: T.int32 = i0 * 72 + i1 * 9 + i2\n                compute_2 = T.Buffer((792,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(T.atan(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(792):\n            T_divide_1 = T.Buffer((792,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["mod", "asin", "atan", "abs", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 385; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 77) + (i1 * 11)) + i2)] = atanf(ph_0[(((i0 * 77) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), ph_3: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_mod_1 = T.Buffer((385,), data=T_mod.data)\n            ph_3_1 = T.Buffer((385,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(7, 11):\n                cse_var_1: T.int32 = i0 * 77 + i1 * 11 + i2\n                compute_2 = T.Buffer((385,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_2 = T.Buffer((385,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "atan", "sin", "acos"]]}{"op_name": "fast_softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    float T_softmax_maxelem[182];\n    float T_softmax_expsum[14];\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        T_softmax_maxelem[((i1 * 14) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 17; ++k) {\n          T_softmax_maxelem[((i1 * 14) + i2)] = max(T_softmax_maxelem[((i1 * 14) + i2)], data[((((i0 * 3094) + (i1 * 238)) + (i2 * 17)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 13; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 17; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 3094) + (i1_1 * 238)) + (i2_1 * 17)) + k_1)] - T_softmax_maxelem[((i1_1 * 14) + i2_1)])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 14; ++i2_2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 3094) + (i1_1 * 238)) + (i2_2 * 17)) + i3)] - T_softmax_maxelem[((i1_1 * 14) + i2_2)])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 17; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 442) + (((int)threadIdx.x) * 17)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 4641) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)])) / T_softmax_expsum[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 17)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 273) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 17; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 273) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 544) + (((int)threadIdx.x) * 17)) + k)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 13, 14, 17), \"float32\"), T_softmax_norm: T.Buffer((12, 13, 14, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(12):\n            T_softmax_maxelem = T.allocate([182], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([14], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((182,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((37128,), data=data.data)\n            for i1, i2 in T.grid(13, 14):\n                T_softmax_maxelem_1[i1 * 14 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(17):\n                    cse_var_1: T.int32 = i1 * 14 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 3094 + i1 * 238 + i2 * 17 + k])\n            for i1 in range(13):\n                T_softmax_expsum_1 = T.Buffer((14,), data=T_softmax_expsum, align=32)\n                for i2 in range(14):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    for k in range(17):\n                        cse_var_3: T.int32 = i1 * 14 + i2\n                        cse_var_2: T.int32 = i0 * 3094 + i1 * 238 + i2 * 17 + k\n                        T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n                for i2, i3 in T.grid(14, 17):\n                    cse_var_5: T.int32 = i1 * 14 + i2\n                    cse_var_4: T.int32 = i0 * 3094 + i1 * 238 + i2 * 17 + i3\n                    T_softmax_norm_1 = T.Buffer((37128,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[i2]", "op_args": [12, 13, 14, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 16) + (i1 * 8)) + i2)] = expf(acosf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 22; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_2[((i0_i1_fused * 8) + i2_1)] = cosf(ph_0[((i0_i1_fused * 8) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 8), \"float32\"), compute: T.Buffer((11, 2, 8), \"float32\"), compute_1: T.Buffer((11, 2, 8), \"float32\"), compute_2: T.Buffer((11, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((176,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_3 = T.Buffer((176,), data=compute_1.data)\n                compute_3[cse_var_1] = T.exp(T.acos(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(22):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((176,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2])", "op_args": [["asinh", "acos", "exp", "cos"]]}{"op_name": "cosh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 78336; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = coshf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = coshf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 18, 16, 17), \"float32\"), compute: T.Buffer((16, 18, 16, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(78336):\n            compute_1 = T.Buffer((78336,), data=compute.data)\n            data_1 = T.Buffer((78336,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.cosh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [16, 18, 16, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_mod[((ax0_ax1_fused * 7) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 7) + ax2)]), ph_0[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 315; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 32; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 32; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_2[(((i0 * 16) + (i1 * 2)) + i2)] = cosf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_3[(((i0_1 * 16) + (i1_1 * 2)) + i2_1)] = sinf(ph_0[(((i0_1 * 16) + (i1_1 * 2)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 8, 2), \"float32\"), compute: T.Buffer((2, 8, 2), \"float32\"), compute_1: T.Buffer((2, 8, 2), \"float32\"), compute_2: T.Buffer((2, 8, 2), \"float32\"), compute_3: T.Buffer((2, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_4 = T.Buffer((32,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_4 = T.Buffer((32,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_4 = T.Buffer((32,), data=compute_2.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_2: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_4 = T.Buffer((32,), data=compute_3.data)\n                compute_4[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["asinh", "acos", "exp", "cos", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_multiply[(((ax0 * 266) + (ax1 * 14)) + ax2)] = (ph_0[(((ax0 * 266) + (ax1 * 14)) + ax2)] * ph_3[(((ax0 * 266) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2394; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2394; ++i0_i1_fused_i2_fused_1) {\n    float compute_2[1];\n    compute_2[0] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n    compute_1[i0_i1_fused_i2_fused_1] = expf(compute_2[0]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 19, 14), \"float32\"), ph_3: T.Buffer((9, 19, 14), \"float32\"), T_multiply: T.Buffer((9, 19, 14), \"float32\"), compute: T.Buffer((9, 19, 14), \"float32\"), compute_1: T.Buffer((9, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2394,), data=ph_0.data)\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(19, 14):\n                cse_var_1: T.int32 = ax0 * 266 + ax1 * 14 + ax2\n                T_multiply_1 = T.Buffer((2394,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((2394,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2394):\n            compute_2 = T.Buffer((2394,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2394):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            compute_3 = T.Buffer((1,), data=compute_2, align=4)\n            compute_3[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_4 = T.Buffer((2394,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(compute_3[0])", "op_args": [["multiply", "cos", "exp", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] + asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 16) + (i1 * 8)) + i2)] = acoshf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 8) + ax2)]) / ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] + T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((48,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.asin(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(data[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((int)blockIdx.x)] = acosf(data[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 5, 8, 1), \"float32\"), compute: T.Buffer((7, 5, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_1 = T.Buffer((280,), data=compute.data)\n            data_1 = T.Buffer((280,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acos(data_1[i0_i1_fused_i2_fused])", "op_args": [7, 5, 8, 1]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 72; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = acosf(ceilf(ph_0[((i0_i1_fused * 18) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 72; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 18), \"float32\"), compute: T.Buffer((1, 4, 18), \"float32\"), compute_1: T.Buffer((1, 4, 18), \"float32\"), compute_2: T.Buffer((1, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((72,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((72,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acos(T.ceil(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "ceil", "acos", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[5054];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5054; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5054; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 266; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_3[((i0_i1_fused * 19) + i2)] = expf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5054; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 266; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 19) + i2_1)] = acosf(compute_3[((i0_i1_fused_1 * 19) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acosf(__expf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 14, 19), \"float32\"), ph_3: T.Buffer((19, 14, 19), \"float32\"), T_multiply: T.Buffer((19, 14, 19), \"float32\"), compute: T.Buffer((19, 14, 19), \"float32\"), compute_1: T.Buffer((19, 14, 19), \"float32\"), compute_2: T.Buffer((19, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([5054], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((5054,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5054):\n            T_multiply_1 = T.Buffer((5054,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((5054,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_4 = T.Buffer((5054,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((5054,), data=compute_3)\n        for i0_i1_fused in T.parallel(266):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(5054):\n            compute_5 = T.Buffer((5054,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(266):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_5 = T.Buffer((5054,), data=compute_2.data)\n                compute_5[cse_var_2] = T.acos(compute_4[cse_var_2])", "op_args": [["multiply", "cos", "exp", "exp", "acos"]]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3696; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 7, 11, 6), \"float32\"), compute: T.Buffer((8, 7, 11, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3696):\n            compute_1 = T.Buffer((3696,), data=compute.data)\n            data_1 = T.Buffer((3696,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [8, 7, 11, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 162) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 162) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_2[((i0_i1_fused * 9) + i2_1)] = expf(ph_0[((i0_i1_fused * 9) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 18, 9), \"float32\"), compute: T.Buffer((4, 18, 9), \"float32\"), compute_1: T.Buffer((4, 18, 9), \"float32\"), compute_2: T.Buffer((4, 18, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((648,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(18, 9):\n                cse_var_1: T.int32 = i0 * 162 + i1 * 9 + i2\n                compute_3 = T.Buffer((648,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_3 = T.Buffer((648,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(72):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((648,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])", "op_args": [["atanh", "ceil", "acos", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 507; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 39; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 13) + ax2)] = (ph_0[((ax0_ax1_fused * 13) + ax2)] - asinhf(acosf(ph_0[((ax0_ax1_fused * 13) + ax2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 39; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute_1[((i0_i1_fused * 13) + i2)] = acoshf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 3, 13), \"float32\"), compute: T.Buffer((13, 3, 13), \"float32\"), T_subtract: T.Buffer((13, 3, 13), \"float32\"), compute_1: T.Buffer((13, 3, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((507,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(507):\n            compute_2 = T.Buffer((507,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(39):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_subtract_1 = T.Buffer((507,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.asinh(T.acos(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(39):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((507,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])", "op_args": [["exp", "acos", "asinh", "subtract", "acosh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 210; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 70; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = asinf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 70; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_multiply_1[((ax0_ax1_fused * 3) + ax2)] = (cosf(ph_0[((ax0_ax1_fused * 3) + ax2)]) * ph_0[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 7, 3), \"float32\"), ph_3: T.Buffer((10, 7, 3), \"float32\"), T_multiply: T.Buffer((10, 7, 3), \"float32\"), compute: T.Buffer((10, 7, 3), \"float32\"), T_multiply_1: T.Buffer((10, 7, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((210,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(210):\n            T_multiply_2 = T.Buffer((210,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((210,), data=ph_3.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(70):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_1 = T.Buffer((210,), data=compute.data)\n                compute_1[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(70):\n            for ax2 in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_multiply_2 = T.Buffer((210,), data=T_multiply_1.data)\n                T_multiply_2[cse_var_2] = T.cos(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]", "op_args": [["multiply", "asin", "cos", "multiply"]]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 96; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 17; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 18; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[(((ax0_ax1_fused * 306) + (rv0 * 18)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 17; ++rv0) {\n    for (int rv1 = 0; rv1 < 18; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 612) + (((int)threadIdx.x) * 306)) + (rv0 * 18)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 8, 17, 18), \"float32\"), adaptive_pool_max: T.Buffer((12, 8, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(96):\n            adaptive_pool_max_1 = T.Buffer((96,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(17, 18):\n                data_1 = T.Buffer((29376,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused], data_1[ax0_ax1_fused * 306 + rv0 * 18 + rv1])", "op_args": [12, 8, 17, 18]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 13; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 13) + i3_s)] = acosf(data[((i0_i1_fused_i2_fused * 13) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 16, 10, 13), \"float32\"), compute: T.Buffer((12, 16, 10, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            for i3_s in range(13):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 13 + i3_s\n                compute_1 = T.Buffer((24960,), data=compute.data)\n                data_1 = T.Buffer((24960,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [12, 16, 10, 13]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 20; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 11) + ax2)] = (ph_0[((ax0_ax1_fused * 11) + ax2)] * ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 220; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(fmodf(ph_0[i0_i1_fused_i2_fused], asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_1[((i1 * 11) + i2)] = ceilf(fmodf(ph_0[((i1 * 11) + i2)], asinf(ph_0[((i1 * 11) + i2)])));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 20, 11), \"float32\"), ph_3: T.Buffer((1, 20, 11), \"float32\"), T_multiply: T.Buffer((1, 20, 11), \"float32\"), compute: T.Buffer((1, 20, 11), \"float32\"), compute_1: T.Buffer((1, 20, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((220,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(20):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_multiply_1 = T.Buffer((220,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((220,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(220):\n            compute_2 = T.Buffer((220,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.asin(ph_0_1[i0_i1_fused_i2_fused])))\n        for i1, i2 in T.grid(20, 11):\n            cse_var_2: T.int32 = i1 * 11 + i2\n            compute_2 = T.Buffer((220,), data=compute_1.data)\n            compute_2[cse_var_2] = T.ceil(T.truncmod(ph_0_1[cse_var_2], T.asin(ph_0_1[cse_var_2])))", "op_args": [["multiply", "asin", "mod", "exp", "ceil"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 35) + (i1 * 7)) + i2)] = atanhf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 99) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 99) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_mod[(((ax0 * 99) + (ax1 * 9)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]), ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(acosf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(11, 9):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_2: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 42; ++ax0_ax1_fused_ax2_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 12; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused], data[(((ax0_ax1_fused_ax2_fused * 36) + (rv0 * 3)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 21) {\n    adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 12; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 21) {\n        adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 1152) + (((int)threadIdx.x) * 36)) + (rv0 * 3)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 7, 12, 3), \"float32\"), adaptive_pool_max: T.Buffer((6, 7, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(42):\n            adaptive_pool_max_1 = T.Buffer((42,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(12, 3):\n                data_1 = T.Buffer((1512,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused], data_1[ax0_ax1_fused_ax2_fused * 36 + rv0 * 3 + rv1])", "op_args": [6, 7, 12, 3]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3)] = acosf(data[((i0_i1_fused_i2_fused * 10) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 16, 10), \"float32\"), compute: T.Buffer((6, 4, 16, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3\n                compute_1 = T.Buffer((3840,), data=compute.data)\n                data_1 = T.Buffer((3840,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [6, 4, 16, 10]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = atanf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 13; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        compute_1[(((i0_1 * 16) + (i1_1 * 8)) + i2_1)] = sinf(atanhf(ph_0[(((i0_1 * 16) + (i1_1 * 8)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 26; ++i0_i1_fused) {\n    for (int32_t i2_2 = 0; i2_2 < 8; ++i2_2) {\n      compute_2[((i0_i1_fused * 8) + i2_2)] = expf(ph_0[((i0_i1_fused * 8) + i2_2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 2, 8), \"float32\"), compute: T.Buffer((13, 2, 8), \"float32\"), compute_1: T.Buffer((13, 2, 8), \"float32\"), compute_2: T.Buffer((13, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((208,), data=ph_0.data)\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_3 = T.Buffer((208,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_2: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_3 = T.Buffer((208,), data=compute_1.data)\n                compute_3[cse_var_2] = T.sin(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused in T.parallel(26):\n            for i2 in range(8):\n                cse_var_3: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((208,), data=compute_2.data)\n                compute_3[cse_var_3] = T.exp(ph_0_1[cse_var_3])", "op_args": [["atan", "atanh", "sin", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = expf((ph_0[(((i0 * 16) + (i1 * 8)) + i2)] + asinhf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    float compute_3[1];\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        compute_3[0] = expf(ph_0[(((ax0 * 98) + (ax1 * 7)) + ax2)]);\n        T_mod[(((ax0 * 98) + (ax1 * 7)) + ax2)] = fmodf(compute_3[0], ph_0[(((ax0 * 98) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1960; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1960; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(__expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 14, 7), \"float32\"), compute: T.Buffer((20, 14, 7), \"float32\"), T_mod: T.Buffer((20, 14, 7), \"float32\"), compute_1: T.Buffer((20, 14, 7), \"float32\"), compute_2: T.Buffer((20, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1960,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1960):\n            compute_3 = T.Buffer((1960,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(20):\n            compute_3 = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(14, 7):\n                cse_var_1: T.int32 = ax0 * 98 + ax1 * 7 + ax2\n                compute_4 = T.Buffer((1,), data=compute_3, align=4)\n                compute_4[0] = T.exp(ph_0_1[cse_var_1])\n                T_mod_1 = T.Buffer((1960,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(compute_4[0], ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1960):\n            compute_3 = T.Buffer((1960,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1960):\n            compute_3 = T.Buffer((1960,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["abs", "exp", "mod", "acos", "ceil"]]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 3; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 3) + i3)] = acosf(data[((i0_i1_fused_i2_fused * 3) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 10, 3), \"float32\"), compute: T.Buffer((4, 16, 10, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            for i3 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 3 + i3\n                compute_1 = T.Buffer((1920,), data=compute.data)\n                data_1 = T.Buffer((1920,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [4, 16, 10, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = expf((ph_0[((i0_i1_fused * 19) + i2)] + asinhf(ph_0[((i0_i1_fused * 19) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 240; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 19) + i2_1)] = acoshf(ph_0[((i0_i1_fused_1 * 19) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 12, 19), \"float32\"), compute: T.Buffer((20, 12, 19), \"float32\"), compute_1: T.Buffer((20, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4560,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((4560,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((4560,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])", "op_args": [["asinh", "add", "exp", "acosh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = expf(fmodf(ph_0[((i0_i1_fused * 15) + i2)], (ph_0[((i0_i1_fused * 15) + i2)] + acosf((ph_0[((i0_i1_fused * 15) + i2)] / asinf(ph_0[((i0_i1_fused * 15) + i2)]))))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + acosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))))));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 15), \"float32\"), compute: T.Buffer((8, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_1 = T.Buffer((720,), data=compute.data)\n                ph_0_1 = T.Buffer((720,), data=ph_0.data)\n                compute_1[cse_var_1] = T.exp(T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1] / T.asin(ph_0_1[cse_var_1]))))", "op_args": [["asin", "divide", "acos", "add", "mod", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 5; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = ceilf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 5; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 13) + i2_1)] = acosf(ph_0[((i0_i1_fused_1 * 13) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 1, 13), \"float32\"), compute: T.Buffer((5, 1, 13), \"float32\"), compute_1: T.Buffer((5, 1, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((65,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(5):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((65,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(5):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((65,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])", "op_args": [["ceil", "acos"]]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 9; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[(((ax0_ax1_fused * 27) + (rv0 * 9)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 9; ++rv1) {\n      adaptive_pool_max[((int)threadIdx.x)] = max(adaptive_pool_max[((int)threadIdx.x)], data[(((((int)threadIdx.x) * 27) + (rv0 * 9)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 8, 3, 9), \"float32\"), adaptive_pool_max: T.Buffer((4, 8, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(32):\n            adaptive_pool_max_1 = T.Buffer((32,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(3, 9):\n                data_1 = T.Buffer((864,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused], data_1[ax0_ax1_fused * 27 + rv0 * 9 + rv1])", "op_args": [4, 8, 3, 9]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2860; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 11, 2, 10), \"float32\"), compute: T.Buffer((13, 11, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2860):\n            compute_1 = T.Buffer((2860,), data=compute.data)\n            data_1 = T.Buffer((2860,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [13, 11, 2, 10]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 76; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 13; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 9; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[(((ax0_ax1_fused_ax2_fused_ax3_fused * 117) + (rv0 * 9)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 13; ++rv0) {\n    for (int rv1 = 0; rv1 < 9; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 2223) + (((int)threadIdx.x) * 117)) + (rv0 * 9)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 19, 13, 9), \"float32\"), adaptive_pool_max: T.Buffer((4, 19, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(76):\n            adaptive_pool_max_1 = T.Buffer((76,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(13, 9):\n                data_1 = T.Buffer((8892,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 117 + rv0 * 9 + rv1])", "op_args": [4, 19, 13, 9]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7938; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 7, 9, 14), \"float32\"), compute: T.Buffer((9, 7, 9, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(7938):\n            compute_1 = T.Buffer((7938,), data=compute.data)\n            data_1 = T.Buffer((7938,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [9, 7, 9, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 42) + (i1 * 3)) + i2)] = asinhf(ph_0[(((i0 * 42) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 14; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n        compute_1[(((i0_1 * 42) + (i1_1 * 3)) + i2_1)] = asinf(fabsf(ph_0[(((i0_1 * 42) + (i1_1 * 3)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 378; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 14, 3), \"float32\"), compute: T.Buffer((9, 14, 3), \"float32\"), compute_1: T.Buffer((9, 14, 3), \"float32\"), compute_2: T.Buffer((9, 14, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((378,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(14, 3):\n                cse_var_1: T.int32 = i0 * 42 + i1 * 3 + i2\n                compute_3 = T.Buffer((378,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(14, 3):\n                cse_var_2: T.int32 = i0 * 42 + i1 * 3 + i2\n                compute_3 = T.Buffer((378,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asin(T.fabs(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(378):\n            compute_3 = T.Buffer((378,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["asinh", "abs", "asin", "atanh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = sinf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1560; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 10, 12), \"float32\"), compute: T.Buffer((13, 10, 12), \"float32\"), compute_1: T.Buffer((13, 10, 12), \"float32\"), compute_2: T.Buffer((13, 10, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1560,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((1560,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_3 = T.Buffer((1560,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_3 = T.Buffer((1560,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["sin", "asin", "asin", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = asinhf(fmodf(ph_0[((i0_i1_fused * 6) + i2)], fabsf(ph_0[((i0_i1_fused * 6) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1200; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1200; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(asinhf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = fmodf(asinhf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 20, 6), \"float32\"), compute: T.Buffer((10, 20, 6), \"float32\"), compute_1: T.Buffer((10, 20, 6), \"float32\"), T_mod: T.Buffer((10, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1200,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(200):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((1200,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(T.truncmod(ph_0_1[cse_var_1], T.fabs(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(1200):\n            compute_2 = T.Buffer((1200,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1200):\n            T_mod_1 = T.Buffer((1200,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["abs", "mod", "asinh", "asinh", "asinh", "mod"]]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 36; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 5; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 19; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[(((ax0_ax1_fused_ax2_fused_ax3_fused * 95) + (rv0 * 19)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 5; ++rv0) {\n    for (int rv1 = 0; rv1 < 19; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 855) + (((int)threadIdx.x) * 95)) + (rv0 * 19)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 9, 5, 19), \"float32\"), adaptive_pool_max: T.Buffer((4, 9, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(36):\n            adaptive_pool_max_1 = T.Buffer((36,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(5, 19):\n                data_1 = T.Buffer((3420,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 95 + rv0 * 19 + rv1])", "op_args": [4, 9, 5, 19]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 9600; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = acosf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 3, 10, 20), \"float32\"), compute: T.Buffer((16, 3, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(9600):\n            compute_1 = T.Buffer((9600,), data=compute.data)\n            data_1 = T.Buffer((9600,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.acos(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [16, 3, 10, 20]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_2[((i0_i1_fused * 7) + i2)] = sinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 247; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 9; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 7; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[(((ax0_ax1_fused_ax2_fused_ax3_fused * 63) + (rv0 * 7)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 9; ++rv0) {\n    for (int rv1 = 0; rv1 < 7; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 819) + (((int)threadIdx.x) * 63)) + (rv0 * 7)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 19, 9, 7), \"float32\"), adaptive_pool_max: T.Buffer((13, 19, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(247):\n            adaptive_pool_max_1 = T.Buffer((247,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(9, 7):\n                data_1 = T.Buffer((15561,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 63 + rv0 * 7 + rv1])", "op_args": [13, 19, 9, 7]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2295; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 13; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 13) + i3_s)] = acosf(data[((i0_i1_fused_i2_fused * 13) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 17, 15, 13), \"float32\"), compute: T.Buffer((9, 17, 15, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2295):\n            for i3_s in range(13):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 13 + i3_s\n                compute_1 = T.Buffer((29835,), data=compute.data)\n                data_1 = T.Buffer((29835,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [9, 17, 15, 13]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = expf((ph_0[(((i0 * 16) + (i1 * 8)) + i2)] + asinhf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused * 8) + i2_1)] = acoshf(ph_0[((i0_i1_fused * 8) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "acos", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 14; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3)] = acosf(data[((i0_i1_fused_i2_fused * 14) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = acosf(data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 6, 16, 14), \"float32\"), compute: T.Buffer((4, 6, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            for i3 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3\n                compute_1 = T.Buffer((5376,), data=compute.data)\n                data_1 = T.Buffer((5376,), data=data.data)\n                compute_1[cse_var_1] = T.acos(data_1[cse_var_1])", "op_args": [4, 6, 16, 14]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 42; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 14; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 17; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused] = max(adaptive_pool_max[ax0_ax1_fused_ax2_fused_ax3_fused], data[(((ax0_ax1_fused_ax2_fused_ax3_fused * 238) + (rv0 * 17)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) < 42) {\n    adaptive_pool_max[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 14; ++rv0) {\n    for (int rv1 = 0; rv1 < 17; ++rv1) {\n      if (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) < 42) {\n        adaptive_pool_max[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 1190) + (((int)threadIdx.x) * 238)) + (rv0 * 17)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 7, 14, 17), \"float32\"), adaptive_pool_max: T.Buffer((6, 7, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(42):\n            adaptive_pool_max_1 = T.Buffer((42,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(14, 17):\n                data_1 = T.Buffer((9996,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused], data_1[ax0_ax1_fused_ax2_fused_ax3_fused * 238 + rv0 * 17 + rv1])", "op_args": [6, 7, 14, 17]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3825; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 3825) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 1, 15), \"float32\"), compute: T.Buffer((15, 17, 1, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3825):\n            compute_1 = T.Buffer((3825,), data=compute.data)\n            data_1 = T.Buffer((3825,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asin(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [15, 17, 1, 15]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 360; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 2; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 16; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[(((ax0_ax1_fused * 32) + (rv0 * 16)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 2; ++rv0) {\n    for (int rv1 = 0; rv1 < 16; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 288) + (((int)threadIdx.x) * 32)) + (rv0 * 16)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 18, 2, 16), \"float32\"), adaptive_pool_max: T.Buffer((20, 18, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(360):\n            adaptive_pool_max_1 = T.Buffer((360,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(2, 16):\n                data_1 = T.Buffer((11520,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused], data_1[ax0_ax1_fused * 32 + rv0 * 16 + rv1])", "op_args": [20, 18, 2, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 140; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 70; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 2) + ax2)] = (sinf(ph_0[((ax0_ax1_fused * 2) + ax2)]) - ph_0[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 140; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 10, 2), \"float32\"), compute: T.Buffer((7, 10, 2), \"float32\"), T_subtract: T.Buffer((7, 10, 2), \"float32\"), compute_1: T.Buffer((7, 10, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((140,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(140):\n            compute_2 = T.Buffer((140,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(70):\n            for ax2 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_subtract_1 = T.Buffer((140,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.sin(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(140):\n            compute_2 = T.Buffer((140,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["sin", "sin", "subtract", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = acosf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4320; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4320; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4320; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = cosf(ceilf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 18), \"float32\"), compute: T.Buffer((16, 15, 18), \"float32\"), compute_1: T.Buffer((16, 15, 18), \"float32\"), compute_2: T.Buffer((16, 15, 18), \"float32\"), compute_3: T.Buffer((16, 15, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4320,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_4 = T.Buffer((4320,), data=compute.data)\n                compute_4[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(4320):\n            compute_4 = T.Buffer((4320,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4320):\n            compute_4 = T.Buffer((4320,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4320):\n            compute_4 = T.Buffer((4320,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["acos", "cos", "atan", "cos", "ceil", "cos"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        compute[(((i0_i1_fused * 209) + (i2 * 11)) + i3)] = asinf(data[(((i0_i1_fused * 209) + (i2 * 11)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 1463) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 2, 19, 11), \"float32\"), compute: T.Buffer((14, 2, 19, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(28):\n            for i2, i3 in T.grid(19, 11):\n                cse_var_1: T.int32 = i0_i1_fused * 209 + i2 * 11 + i3\n                compute_1 = T.Buffer((5852,), data=compute.data)\n                data_1 = T.Buffer((5852,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [14, 2, 19, 11]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_mod[(((ax0 * 35) + (ax1 * 7)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)]), ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 315; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(5, 7):\n                cse_var_1: T.int32 = ax0 * 35 + ax1 * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2304; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 2; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 2) + i3_s)] = asinf(data[((i0_i1_fused_i2_fused * 2) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 12, 16, 2), \"float32\"), compute: T.Buffer((12, 12, 16, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2304):\n            for i3_s in range(2):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 2 + i3_s\n                compute_1 = T.Buffer((4608,), data=compute.data)\n                data_1 = T.Buffer((4608,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [12, 12, 16, 2]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 64; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_multiply[(((ax0 * 32) + (ax1 * 16)) + ax2)] = (ph_0[(((ax0 * 32) + (ax1 * 16)) + ax2)] * ph_3[(((ax0 * 32) + (ax1 * 16)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 64; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 2, 16), \"float32\"), ph_3: T.Buffer((2, 2, 16), \"float32\"), T_mod: T.Buffer((2, 2, 16), \"float32\"), T_multiply: T.Buffer((2, 2, 16), \"float32\"), compute: T.Buffer((2, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((64,), data=ph_0.data)\n        ph_3_1 = T.Buffer((64,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(64):\n            T_mod_1 = T.Buffer((64,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(2, 16):\n                cse_var_1: T.int32 = ax0 * 32 + ax1 * 16 + ax2\n                T_multiply_1 = T.Buffer((64,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(64):\n            compute_1 = T.Buffer((64,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["mod", "multiply", "ceil"]]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 26; ++ax0_ax1_fused) {\n    adaptive_pool_max[ax0_ax1_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 13; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 16; ++rv1) {\n        adaptive_pool_max[ax0_ax1_fused] = max(adaptive_pool_max[ax0_ax1_fused], data[(((ax0_ax1_fused * 208) + (rv0 * 16)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 13; ++rv0) {\n    for (int rv1 = 0; rv1 < 16; ++rv1) {\n      adaptive_pool_max[((int)threadIdx.x)] = max(adaptive_pool_max[((int)threadIdx.x)], data[(((((int)threadIdx.x) * 208) + (rv0 * 16)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 13, 13, 16), \"float32\"), adaptive_pool_max: T.Buffer((2, 13, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(26):\n            adaptive_pool_max_1 = T.Buffer((26,), data=adaptive_pool_max.data)\n            adaptive_pool_max_1[ax0_ax1_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0, rv1 in T.grid(13, 16):\n                data_1 = T.Buffer((5408,), data=data.data)\n                adaptive_pool_max_1[ax0_ax1_fused] = T.max(adaptive_pool_max_1[ax0_ax1_fused], data_1[ax0_ax1_fused * 208 + rv0 * 16 + rv1])", "op_args": [2, 13, 13, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_subtract[(((ax0 * 18) + (ax1 * 2)) + ax2)] = (asinhf(ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)]) - ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 360; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf((ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(9, 2):\n                cse_var_1: T.int32 = ax0 * 18 + ax1 * 2 + ax2\n                T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_1 = T.Buffer((360,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    float compute[1];\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        compute[0] = expf(ph_0[(((ax0 * 153) + (ax1 * 9)) + ax2)]);\n        T_mod[(((ax0 * 153) + (ax1 * 9)) + ax2)] = fmodf(ph_0[(((ax0 * 153) + (ax1 * 9)) + ax2)], atanhf((ph_0[(((ax0 * 153) + (ax1 * 9)) + ax2)] + compute[0])));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], atanhf((ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] + __expf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 17, 9), \"float32\"), T_mod: T.Buffer((2, 17, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(2):\n            compute = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(17, 9):\n                cse_var_1: T.int32 = ax0 * 153 + ax1 * 9 + ax2\n                compute_1 = T.Buffer((1,), data=compute, align=4)\n                ph_0_1 = T.Buffer((306,), data=ph_0.data)\n                compute_1[0] = T.exp(ph_0_1[cse_var_1])\n                T_mod_1 = T.Buffer((306,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1] + compute_1[0]))", "op_args": [["exp", "add", "atanh", "mod"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1600; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 10; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3_s)] = asinf(data[((i0_i1_fused_i2_fused * 10) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 20, 4, 10), \"float32\"), compute: T.Buffer((20, 20, 4, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1600):\n            for i3_s in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3_s\n                compute_1 = T.Buffer((16000,), data=compute.data)\n                data_1 = T.Buffer((16000,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [20, 20, 4, 10]}{"op_name": "global_pool_max", "c_code": "void default_function_kernel(float* adaptive_pool_max, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      adaptive_pool_max[((ax0 * 12) + ax1)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 2; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 16; ++rv1) {\n          adaptive_pool_max[((ax0 * 12) + ax1)] = max(adaptive_pool_max[((ax0 * 12) + ax1)], data[((((ax0 * 384) + (ax1 * 32)) + (rv0 * 16)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ adaptive_pool_max, float* __restrict__ data) {\n  adaptive_pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 2; ++rv0) {\n    for (int rv1 = 0; rv1 < 16; ++rv1) {\n      adaptive_pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(adaptive_pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], data[((((((int)blockIdx.x) * 256) + (((int)threadIdx.x) * 32)) + (rv0 * 16)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 12, 2, 16), \"float32\"), adaptive_pool_max: T.Buffer((8, 12, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(8):\n            for ax1 in range(12):\n                adaptive_pool_max_1 = T.Buffer((96,), data=adaptive_pool_max.data)\n                adaptive_pool_max_1[ax0 * 12 + ax1] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(2, 16):\n                    cse_var_1: T.int32 = ax0 * 12 + ax1\n                    data_1 = T.Buffer((3072,), data=data.data)\n                    adaptive_pool_max_1[cse_var_1] = T.max(adaptive_pool_max_1[cse_var_1], data_1[ax0 * 384 + ax1 * 32 + rv0 * 16 + rv1])", "op_args": [8, 12, 2, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_mod[((ax0_ax1_fused * 7) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 7) + ax2)], ph_3[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = atanf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                ph_3_1 = T.Buffer((315,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1620; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_add[(((ax0 * 180) + (ax1 * 12)) + ax2)] = (acoshf(ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]) + ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1620; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] - cosf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(15, 12):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 12 + ax2\n                T_add_1 = T.Buffer((1620,), data=T_add.data)\n                T_add_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] - T.cos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 5; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 5) + i3)] = asinf(data[((i0_i1_fused_i2_fused * 5) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 6, 10, 5), \"float32\"), compute: T.Buffer((3, 6, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            for i3 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 5 + i3\n                compute_1 = T.Buffer((900,), data=compute.data)\n                data_1 = T.Buffer((900,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [3, 6, 10, 5]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float adaptive_pool_sum[8];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      adaptive_pool_sum[ax1] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 7; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 17; ++rv1) {\n          adaptive_pool_sum[ax1] = (adaptive_pool_sum[ax1] + data[((((ax0 * 952) + (ax1 * 119)) + (rv0 * 17)) + rv1)]);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n      adaptive_pool_avg[((ax0 * 8) + ax1_1)] = (adaptive_pool_sum[ax1_1] * 8.403361e-03f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 8.403361e-03f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 7; ++rv0) {\n    for (int rv1 = 0; rv1 < 17; ++rv1) {\n      adaptive_pool_sum[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] + data[(((((int)threadIdx.x) * 119) + (rv0 * 17)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 8, 7, 17), \"float32\"), adaptive_pool_avg: T.Buffer((4, 8, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            adaptive_pool_sum = T.allocate([8], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((8,), data=adaptive_pool_sum, align=32)\n            for ax1 in range(8):\n                adaptive_pool_sum_1[ax1] = T.float32(0)\n                for rv0, rv1 in T.grid(7, 17):\n                    data_1 = T.Buffer((3808,), data=data.data)\n                    adaptive_pool_sum_1[ax1] = adaptive_pool_sum_1[ax1] + data_1[ax0 * 952 + ax1 * 119 + rv0 * 17 + rv1]\n            for ax1 in range(8):\n                adaptive_pool_avg_1 = T.Buffer((32,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 8 + ax1] = adaptive_pool_sum_1[ax1] * T.float32(0.0084033613445378148)", "op_args": [4, 8, 7, 17]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 16; ++i3) {\n        compute[(((i0_i1_fused * 96) + (i2 * 16)) + i3)] = asinf(data[(((i0_i1_fused * 96) + (i2 * 16)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 1, 6, 16), \"float32\"), compute: T.Buffer((10, 1, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(10):\n            for i2, i3 in T.grid(6, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 96 + i2 * 16 + i3\n                compute_1 = T.Buffer((960,), data=compute.data)\n                data_1 = T.Buffer((960,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [10, 1, 6, 16]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    float adaptive_pool_sum[14];\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      adaptive_pool_sum[ax1] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 14; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          adaptive_pool_sum[ax1] = (adaptive_pool_sum[ax1] + data[((((ax0 * 588) + (ax1 * 42)) + (rv0 * 3)) + rv1)]);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 14; ++ax1_1) {\n      adaptive_pool_avg[((ax0 * 14) + ax1_1)] = (adaptive_pool_sum[ax1_1] * 2.380952e-02f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * 2.380952e-02f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 63) {\n    adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int rv0 = 0; rv0 < 14; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 63) {\n        adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 672) + (((int)threadIdx.x) * 42)) + (rv0 * 3)) + rv1)]);\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 14, 3), \"float32\"), adaptive_pool_avg: T.Buffer((18, 14, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(18):\n            adaptive_pool_sum = T.allocate([14], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((14,), data=adaptive_pool_sum, align=32)\n            for ax1 in range(14):\n                adaptive_pool_sum_1[ax1] = T.float32(0)\n                for rv0, rv1 in T.grid(14, 3):\n                    data_1 = T.Buffer((10584,), data=data.data)\n                    adaptive_pool_sum_1[ax1] = adaptive_pool_sum_1[ax1] + data_1[ax0 * 588 + ax1 * 42 + rv0 * 3 + rv1]\n            for ax1 in range(14):\n                adaptive_pool_avg_1 = T.Buffer((252,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 14 + ax1] = adaptive_pool_sum_1[ax1] * T.float32(0.023809523809523808)", "op_args": [18, 14, 14, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  float compute_1[5320];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5320; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5320; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / compute_1[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5320; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = atanf((ph_0[i0_i1_fused_i2_fused_1] / compute_1[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 280; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_subtract_1[((ax0_ax1_fused * 19) + ax2)] = (fmodf(ph_0[((ax0_ax1_fused * 19) + ax2)], ph_3[((ax0_ax1_fused * 19) + ax2)]) - ph_0[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 14, 19), \"float32\"), ph_3: T.Buffer((20, 14, 19), \"float32\"), T_subtract: T.Buffer((20, 14, 19), \"float32\"), compute: T.Buffer((20, 14, 19), \"float32\"), T_subtract_1: T.Buffer((20, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([5320], \"float32\", \"global\")\n        compute_2 = T.Buffer((5320,), data=compute_1)\n        ph_0_1 = T.Buffer((5320,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(5320):\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(5320):\n            T_subtract_2 = T.Buffer((5320,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / compute_2[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(5320):\n            compute_3 = T.Buffer((5320,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] / compute_2[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(280):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_subtract_2 = T.Buffer((5320,), data=T_subtract_1.data)\n                ph_3_1 = T.Buffer((5320,), data=ph_3.data)\n                T_subtract_2[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]) - ph_0_1[cse_var_1]", "op_args": [["mod", "exp", "divide", "subtract", "atan", "subtract"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 98) + (i1 * 7)) + i2)] = acoshf(ph_0[(((i0 * 98) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1176; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_2[((i0_i1_fused * 7) + i2_1)] = atanf(ph_0[((i0_i1_fused * 7) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 7), \"float32\"), compute: T.Buffer((12, 14, 7), \"float32\"), compute_1: T.Buffer((12, 14, 7), \"float32\"), compute_2: T.Buffer((12, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1176,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(14, 7):\n                cse_var_1: T.int32 = i0 * 98 + i1 * 7 + i2\n                compute_3 = T.Buffer((1176,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1176):\n            compute_3 = T.Buffer((1176,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(168):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((1176,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])", "op_args": [["acosh", "atanh", "sin", "atan"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 768; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 18) + i3)] = asinf(data[((i0_i1_fused_i2_fused * 18) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 16, 4, 18), \"float32\"), compute: T.Buffer((12, 16, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(768):\n            for i3 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                compute_1 = T.Buffer((13824,), data=compute.data)\n                data_1 = T.Buffer((13824,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [12, 16, 4, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_subtract[(((ax0 * 216) + (ax1 * 18)) + ax2)] = (ph_0[(((ax0 * 216) + (ax1 * 18)) + ax2)] - ph_3[(((ax0 * 216) + (ax1 * 18)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3456; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3456; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 12, 18), \"float32\"), ph_3: T.Buffer((16, 12, 18), \"float32\"), T_subtract: T.Buffer((16, 12, 18), \"float32\"), compute: T.Buffer((16, 12, 18), \"float32\"), compute_1: T.Buffer((16, 12, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3456,), data=ph_0.data)\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(12, 18):\n                cse_var_1: T.int32 = ax0 * 216 + ax1 * 18 + ax2\n                T_subtract_1 = T.Buffer((3456,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((3456,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(3456):\n            compute_2 = T.Buffer((3456,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3456):\n            compute_2 = T.Buffer((3456,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["subtract", "abs", "cos"]]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  float adaptive_pool_sum[1];\n  for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 13; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 7; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((ax1 * 91) + (rv0 * 7)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax1] = (adaptive_pool_sum[0] * 1.098901e-02f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 1.098901e-02f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 13; ++rv0) {\n    for (int rv1 = 0; rv1 < 7; ++rv1) {\n      adaptive_pool_sum[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] + data[(((((int)threadIdx.x) * 91) + (rv0 * 7)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 15, 13, 7), \"float32\"), adaptive_pool_avg: T.Buffer((1, 15, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n        for ax1 in range(15):\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(13, 7):\n                data_1 = T.Buffer((1365,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax1 * 91 + rv0 * 7 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((15,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax1] = adaptive_pool_sum_1[0] * T.float32(0.01098901098901099)", "op_args": [1, 15, 13, 7]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 308; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 308; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 308; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 77; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 4) + ax2)] = (sinf(ph_0[((ax0_ax1_fused * 4) + ax2)]) * ph_0[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 4), \"float32\"), ph_3: T.Buffer((7, 11, 4), \"float32\"), T_mod: T.Buffer((7, 11, 4), \"float32\"), compute: T.Buffer((7, 11, 4), \"float32\"), compute_1: T.Buffer((7, 11, 4), \"float32\"), T_multiply: T.Buffer((7, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((308,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_mod_1 = T.Buffer((308,), data=T_mod.data)\n            ph_3_1 = T.Buffer((308,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(77):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_multiply_1 = T.Buffer((308,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.sin(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]", "op_args": [["mod", "atan", "sin", "acos", "multiply"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2688; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3_s)] = asinf(data[((i0_i1_fused_i2_fused * 4) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 16, 14, 4), \"float32\"), compute: T.Buffer((12, 16, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2688):\n            for i3_s in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3_s\n                compute_1 = T.Buffer((10752,), data=compute.data)\n                data_1 = T.Buffer((10752,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [12, 16, 14, 4]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  float adaptive_pool_sum[1];\n  for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((ax1 * 9) + (rv0 * 3)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax1] = (adaptive_pool_sum[0] * 1.111111e-01f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 1.111111e-01f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      adaptive_pool_sum[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] + data[(((((int)threadIdx.x) * 9) + (rv0 * 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 15, 3, 3), \"float32\"), adaptive_pool_avg: T.Buffer((1, 15, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n        for ax1 in range(15):\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(3, 3):\n                data_1 = T.Buffer((135,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax1 * 9 + rv0 * 3 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((15,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax1] = adaptive_pool_sum_1[0] * T.float32(0.1111111111111111)", "op_args": [1, 15, 3, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 154; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = atanhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 154; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_mod[((ax0_ax1_fused * 9) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 9) + ax2)]), ph_0[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(154):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(154):\n            for ax2 in range(9):\n                cse_var_2: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 72) + (i1 * 6)) + i2)] = acosf(ph_0[(((i0 * 72) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 192; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_1[((i0_i1_fused * 6) + i2_1)] = asinhf(ph_0[((i0_i1_fused * 6) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 12, 6), \"float32\"), compute: T.Buffer((16, 12, 6), \"float32\"), compute_1: T.Buffer((16, 12, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1152,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(12, 6):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 6 + i2\n                compute_2 = T.Buffer((1152,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(192):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((1152,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2])", "op_args": [["acos", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 35) + (i1 * 7)) + i2)] = atanhf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_2[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)] = sinf(ph_0[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 50; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      for (int32_t i3 = 0; i3 < 5; ++i3) {\n        compute[(((i0_i1_fused * 85) + (i2 * 5)) + i3)] = asinf(data[(((i0_i1_fused * 85) + (i2 * 5)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 5, 17, 5), \"float32\"), compute: T.Buffer((10, 5, 17, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(50):\n            for i2, i3 in T.grid(17, 5):\n                cse_var_1: T.int32 = i0_i1_fused * 85 + i2 * 5 + i3\n                compute_1 = T.Buffer((4250,), data=compute.data)\n                data_1 = T.Buffer((4250,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [10, 5, 17, 5]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  float adaptive_pool_sum[1];\n  for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 13; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 9; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((ax1 * 117) + (rv0 * 9)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax1] = (adaptive_pool_sum[0] * 8.547009e-03f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 8.547009e-03f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 13; ++rv0) {\n    for (int rv1 = 0; rv1 < 9; ++rv1) {\n      adaptive_pool_sum[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] + data[(((((int)threadIdx.x) * 117) + (rv0 * 9)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 17, 13, 9), \"float32\"), adaptive_pool_avg: T.Buffer((1, 17, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n        for ax1 in range(17):\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(13, 9):\n                data_1 = T.Buffer((1989,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax1 * 117 + rv0 * 9 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((17,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax1] = adaptive_pool_sum_1[0] * T.float32(0.0085470085470085479)", "op_args": [1, 17, 13, 9]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 66; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = acosf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 66; ++ax0_ax1_fused) {\n    float compute_1[18];\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[i2_1] = expf(ph_0[((ax0_ax1_fused * 18) + i2_1)]);\n    }\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 18) + ax2)] = (0.000000e+00f - compute_1[ax2]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (0.000000e+00f - __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 6, 18), \"float32\"), compute: T.Buffer((11, 6, 18), \"float32\"), T_subtract: T.Buffer((11, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1188,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(66):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_1 = T.Buffer((1188,), data=compute.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(66):\n            compute_1 = T.allocate([18], \"float32\", \"global\")\n            compute_2 = T.Buffer((18,), data=compute_1)\n            for i2 in range(18):\n                compute_2[i2] = T.exp(ph_0_1[ax0_ax1_fused * 18 + i2])\n            for ax2 in range(18):\n                T_subtract_1 = T.Buffer((1188,), data=T_subtract.data)\n                T_subtract_1[ax0_ax1_fused * 18 + ax2] = T.float32(0) - compute_2[ax2]", "op_args": [["acos", "exp", "add", "subtract"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] + asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_1[((i0_i1_fused * 8) + i2)] = acoshf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 8) + ax2)]) / ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] + T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((48,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.asin(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 84; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_1[((i0_i1_fused * 14) + i2)] = acosf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 6, 14), \"float32\"), compute: T.Buffer((1, 6, 14), \"float32\"), compute_1: T.Buffer((1, 6, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((84,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(84):\n            compute_2 = T.Buffer((84,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_2 = T.Buffer((84,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])", "op_args": [["atanh", "acos"]]}{"op_name": "asin", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 128) + (i1 * 8)) + (i2 * 4)) + i3)] = asinf(data[((((i0 * 128) + (i1 * 8)) + (i2 * 4)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = asinf(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 16, 2, 4), \"float32\"), compute: T.Buffer((10, 16, 2, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(16, 2, 4):\n                cse_var_1: T.int32 = i0 * 128 + i1 * 8 + i2 * 4 + i3\n                compute_1 = T.Buffer((1280,), data=compute.data)\n                data_1 = T.Buffer((1280,), data=data.data)\n                compute_1[cse_var_1] = T.asin(data_1[cse_var_1])", "op_args": [10, 16, 2, 4]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_mod[((ax0_ax1_fused * 7) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 7) + ax2)], ph_3[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = sinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(ph_0[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                ph_3_1 = T.Buffer((315,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float adaptive_pool_sum[1];\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      adaptive_pool_sum[0] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 14; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((((ax0 * 126) + (ax1 * 42)) + (rv0 * 3)) + rv1)]);\n        }\n      }\n      adaptive_pool_avg[((ax0 * 3) + ax1)] = (adaptive_pool_sum[0] * 2.380952e-02f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] * 2.380952e-02f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)threadIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 14; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      adaptive_pool_sum[((int)threadIdx.x)] = (adaptive_pool_sum[((int)threadIdx.x)] + data[(((((int)threadIdx.x) * 42) + (rv0 * 3)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 3, 14, 3), \"float32\"), adaptive_pool_avg: T.Buffer((4, 3, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            for ax1 in range(3):\n                adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n                adaptive_pool_sum_1[0] = T.float32(0)\n                for rv0, rv1 in T.grid(14, 3):\n                    data_1 = T.Buffer((504,), data=data.data)\n                    adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0 * 126 + ax1 * 42 + rv0 * 3 + rv1]\n                adaptive_pool_avg_1 = T.Buffer((12,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 3 + ax1] = adaptive_pool_sum_1[0] * T.float32(0.023809523809523808)", "op_args": [4, 3, 14, 3]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3276; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 18, 2, 7), \"float32\"), compute: T.Buffer((13, 18, 2, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3276):\n            compute_1 = T.Buffer((3276,), data=compute.data)\n            data_1 = T.Buffer((3276,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asinh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [13, 18, 2, 7]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 256; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 7; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 18; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((ax0_ax1_fused * 126) + (rv0 * 18)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax0_ax1_fused] = (adaptive_pool_sum[0] * 7.936508e-03f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * 7.936508e-03f);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 7; ++rv0) {\n    for (int rv1 = 0; rv1 < 18; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 2016) + (((int)threadIdx.x) * 126)) + (rv0 * 18)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 16, 7, 18), \"float32\"), adaptive_pool_avg: T.Buffer((16, 16, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(256):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(7, 18):\n                data_1 = T.Buffer((32256,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused * 126 + rv0 * 18 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((256,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused] = adaptive_pool_sum_1[0] * T.float32(0.0079365079365079361)", "op_args": [16, 16, 7, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[96];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n    for (int32_t ax7 = 0; ax7 < 8; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 6; ++ax8) {\n        auto_scheduler_layout_transform[(((ax4 * 48) + (ax7 * 6)) + ax8)] = ph_4[(((ax8 * 16) + (ax4 * 8)) + ax7)];\n      }\n    }\n  }\n  for (int32_t b_inner_init = 0; b_inner_init < 6; ++b_inner_init) {\n    for (int32_t i_inner_init = 0; i_inner_init < 10; ++i_inner_init) {\n      T_batch_matmul_NN[((b_inner_init * 10) + i_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n    for (int32_t k_inner = 0; k_inner < 8; ++k_inner) {\n      for (int32_t b_inner = 0; b_inner < 6; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 10; ++i_inner) {\n          T_batch_matmul_NN[((b_inner * 10) + i_inner)] = (T_batch_matmul_NN[((b_inner * 10) + i_inner)] + (acosf(ph_0[((((b_inner * 160) + (i_inner * 16)) + (k_outer * 8)) + k_inner)]) * auto_scheduler_layout_transform[(((k_outer * 48) + (k_inner * 6)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float compute_shared[32];\n  __shared__ float ph_4_shared[8];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    T_batch_matmul_NN_local[b_c_outer_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_inner_s < 10; ++ax0_ax1_fused_ax2_fused_inner_s) {\n      if (((((int)threadIdx.x) * 5) + (ax0_ax1_fused_ax2_fused_inner_s >> 1)) < 16) {\n        if (((int)threadIdx.x) < 4) {\n          compute_shared[((((int)threadIdx.x) * 10) + ax0_ax1_fused_ax2_fused_inner_s)] = acosf(ph_0[(((((((((int)threadIdx.x) * 5) + (ax0_ax1_fused_ax2_fused_inner_s >> 1)) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((((int)threadIdx.x) * 2) + ax0_ax1_fused_ax2_fused_inner_s) & 3) * 5)) + k_outer_outer)]);\n        }\n      }\n    }\n    if (((int)threadIdx.x) < 8) {\n      ph_4_shared[((int)threadIdx.x)] = ph_4[((((int)threadIdx.x) * 5) + k_outer_outer)];\n    }\n    __syncthreads();\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      T_batch_matmul_NN_local[b_c_outer_inner] = (T_batch_matmul_NN_local[b_c_outer_inner] + (compute_shared[((((((int)threadIdx.x) >> 2) * 8) + (b_c_outer_inner * 4)) + (((int)threadIdx.x) & 3))] * ph_4_shared[(((((int)threadIdx.x) >> 2) * 2) + b_c_outer_inner)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 2) * 16) + (b_inner * 8)) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) & 3))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 10, 16), \"float32\"), ph_4: T.Buffer((6, 16, 1), \"float32\"), compute: T.Buffer((6, 10, 16), \"float32\"), T_batch_matmul_NN: T.Buffer((6, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([96], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_1 = T.Buffer((960,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((96,), data=auto_scheduler_layout_transform)\n        for ax4, ax7, ax8 in T.grid(2, 8, 6):\n            ph_4_1 = T.Buffer((96,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[ax4 * 48 + ax7 * 6 + ax8] = ph_4_1[ax8 * 16 + ax4 * 8 + ax7]\n        T_batch_matmul_NN_1 = T.Buffer((60,), data=T_batch_matmul_NN.data)\n        for b_inner_init, i_inner_init in T.grid(6, 10):\n            T_batch_matmul_NN_1[b_inner_init * 10 + i_inner_init] = T.float32(0)\n        for k_outer, k_inner, b_inner, i_inner in T.grid(2, 8, 6, 10):\n            cse_var_1: T.int32 = b_inner * 10 + i_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + T.acos(ph_0_1[b_inner * 160 + i_inner * 16 + k_outer * 8 + k_inner]) * auto_scheduler_layout_transform_1[k_outer * 48 + k_inner * 6 + b_inner]", "op_args": [["cos", "acos", "batch_matmul"]]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 323; ++ax0_ax1_fused_ax2_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv0 = 0; rv0 < 8; ++rv0) {\n      for (int32_t rv1 = 0; rv1 < 20; ++rv1) {\n        adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[(((ax0_ax1_fused_ax2_fused * 160) + (rv0 * 20)) + rv1)]);\n      }\n    }\n    adaptive_pool_avg[ax0_ax1_fused_ax2_fused] = (adaptive_pool_sum[0] * 6.250000e-03f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(19) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  adaptive_pool_avg[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] * 6.250000e-03f);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((int)blockIdx.x)] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 8; ++rv0) {\n    for (int rv1 = 0; rv1 < 20; ++rv1) {\n      adaptive_pool_sum[((int)blockIdx.x)] = (adaptive_pool_sum[((int)blockIdx.x)] + data[(((((int)blockIdx.x) * 160) + (rv0 * 20)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 19, 8, 20), \"float32\"), adaptive_pool_avg: T.Buffer((17, 19, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(323):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv0, rv1 in T.grid(8, 20):\n                data_1 = T.Buffer((51680,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused_ax2_fused * 160 + rv0 * 20 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((323,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused_ax2_fused] = adaptive_pool_sum_1[0] * T.float32(0.0062500000000000003)", "op_args": [17, 19, 8, 20]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1100; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 14; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3_s)] = asinhf(data[((i0_i1_fused_i2_fused * 14) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 5, 11, 14), \"float32\"), compute: T.Buffer((20, 5, 11, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1100):\n            for i3_s in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3_s\n                compute_1 = T.Buffer((15400,), data=compute.data)\n                data_1 = T.Buffer((15400,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [20, 5, 11, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 35) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)] = sinf(ph_0[(((i0_1 * 35) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            ph_3_1 = T.Buffer((315,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 540; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_subtract[(((ax0 * 54) + (ax1 * 18)) + ax2)] = (0.000000e+00f - acoshf(ph_0[(((ax0 * 54) + (ax1 * 18)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_1[(((i0 * 54) + (i1 * 18)) + i2)] = asinhf(ceilf(ph_0[(((i0 * 54) + (i1 * 18)) + i2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (0.000000e+00f - acoshf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 3, 18), \"float32\"), compute: T.Buffer((10, 3, 18), \"float32\"), T_subtract: T.Buffer((10, 3, 18), \"float32\"), compute_1: T.Buffer((10, 3, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_2 = T.Buffer((540,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(3, 18):\n                cse_var_1: T.int32 = ax0 * 54 + ax1 * 18 + ax2\n                T_subtract_1 = T.Buffer((540,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.float32(0) - T.acosh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(3, 18):\n                cse_var_2: T.int32 = i0 * 54 + i1 * 18 + i2\n                compute_2 = T.Buffer((540,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(T.ceil(ph_0_1[cse_var_2]))", "op_args": [["exp", "acosh", "add", "subtract", "ceil", "asinh"]]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 144; ++ax0_ax1_fused) {\n    float adaptive_pool_sum[1];\n    adaptive_pool_sum[0] = 0.000000e+00f;\n    for (int32_t rv1 = 0; rv1 < 12; ++rv1) {\n      adaptive_pool_sum[0] = (adaptive_pool_sum[0] + data[((ax0_ax1_fused * 12) + rv1)]);\n    }\n    adaptive_pool_avg[ax0_ax1_fused] = (adaptive_pool_sum[0] * 8.333333e-02f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 9) {\n    adaptive_pool_avg[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * 8.333333e-02f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv1 = 0; rv1 < 12; ++rv1) {\n    adaptive_pool_sum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] + data[(((((int)blockIdx.x) * 144) + (((int)threadIdx.x) * 12)) + rv1)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 12, 1, 12), \"float32\"), adaptive_pool_avg: T.Buffer((12, 12, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(144):\n            adaptive_pool_sum = T.allocate([1], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((1,), data=adaptive_pool_sum, align=4)\n            adaptive_pool_sum_1[0] = T.float32(0)\n            for rv1 in range(12):\n                data_1 = T.Buffer((1728,), data=data.data)\n                adaptive_pool_sum_1[0] = adaptive_pool_sum_1[0] + data_1[ax0_ax1_fused * 12 + rv1]\n            adaptive_pool_avg_1 = T.Buffer((144,), data=adaptive_pool_avg.data)\n            adaptive_pool_avg_1[ax0_ax1_fused] = adaptive_pool_sum_1[0] * T.float32(0.083333333333333329)", "op_args": [12, 12, 1, 12]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1008; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 72) + (i1 * 4)) + i2)] = acosf(ph_0[(((i0 * 72) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1008; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 18, 4), \"float32\"), ph_3: T.Buffer((14, 18, 4), \"float32\"), T_divide: T.Buffer((14, 18, 4), \"float32\"), compute: T.Buffer((14, 18, 4), \"float32\"), compute_1: T.Buffer((14, 18, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1008,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1008):\n            T_divide_1 = T.Buffer((1008,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1008,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(18, 4):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 4 + i2\n                compute_2 = T.Buffer((1008,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_2 = T.Buffer((1008,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["divide", "acos", "exp"]]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 616; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3)] = asinhf(data[((i0_i1_fused_i2_fused * 17) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 4, 11, 17), \"float32\"), compute: T.Buffer((14, 4, 11, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(616):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 17 + i3\n                compute_1 = T.Buffer((10472,), data=compute.data)\n                data_1 = T.Buffer((10472,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [14, 4, 11, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_mod[((ax0_ax1_fused * 7) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 7) + ax2)]), ph_0[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_1[(((i0 * 35) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "global_pool_avg", "c_code": "void default_function_kernel(float* adaptive_pool_avg, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    float adaptive_pool_sum[9];\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      adaptive_pool_sum[ax1] = 0.000000e+00f;\n      for (int32_t rv0 = 0; rv0 < 11; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 12; ++rv1) {\n          adaptive_pool_sum[ax1] = (adaptive_pool_sum[ax1] + data[((((ax0 * 1188) + (ax1 * 132)) + (rv0 * 12)) + rv1)]);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 9; ++ax1_1) {\n      adaptive_pool_avg[((ax0 * 9) + ax1_1)] = (adaptive_pool_sum[ax1_1] * 7.575758e-03f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ adaptive_pool_avg, float* __restrict__ adaptive_pool_sum) {\n  if (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) < 99) {\n    adaptive_pool_avg[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * 7.575758e-03f);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ adaptive_pool_sum, float* __restrict__ data) {\n  adaptive_pool_sum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int rv0 = 0; rv0 < 11; ++rv0) {\n    for (int rv1 = 0; rv1 < 12; ++rv1) {\n      adaptive_pool_sum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (adaptive_pool_sum[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] + data[((((((int)blockIdx.x) * 1188) + (((int)threadIdx.x) * 132)) + (rv0 * 12)) + rv1)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 9, 11, 12), \"float32\"), adaptive_pool_avg: T.Buffer((11, 9, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(11):\n            adaptive_pool_sum = T.allocate([9], \"float32\", \"global\")\n            adaptive_pool_sum_1 = T.Buffer((9,), data=adaptive_pool_sum, align=32)\n            for ax1 in range(9):\n                adaptive_pool_sum_1[ax1] = T.float32(0)\n                for rv0, rv1 in T.grid(11, 12):\n                    data_1 = T.Buffer((13068,), data=data.data)\n                    adaptive_pool_sum_1[ax1] = adaptive_pool_sum_1[ax1] + data_1[ax0 * 1188 + ax1 * 132 + rv0 * 12 + rv1]\n            for ax1 in range(9):\n                adaptive_pool_avg_1 = T.Buffer((99,), data=adaptive_pool_avg.data)\n                adaptive_pool_avg_1[ax0 * 9 + ax1] = adaptive_pool_sum_1[ax1] * T.float32(0.007575757575757576)", "op_args": [11, 9, 11, 12]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1386; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1386; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1386):\n            compute_1 = T.Buffer((1386,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1386):\n            T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 24) + (i1 * 8)) + i2)] = cosf(ph_0[(((i0 * 24) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 21; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused * 8) + i2_1)] = atanf(ph_0[((i0_i1_fused * 8) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 3, 8), \"float32\"), compute: T.Buffer((7, 3, 8), \"float32\"), compute_1: T.Buffer((7, 3, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((168,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(3, 8):\n                cse_var_1: T.int32 = i0 * 24 + i1 * 8 + i2\n                compute_2 = T.Buffer((168,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(21):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((168,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])", "op_args": [["cos", "atan"]]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1836; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 18) + i3)] = asinhf(data[((i0_i1_fused_i2_fused * 18) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 17, 6, 18), \"float32\"), compute: T.Buffer((18, 17, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1836):\n            for i3 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                compute_1 = T.Buffer((33048,), data=compute.data)\n                data_1 = T.Buffer((33048,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [18, 17, 6, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_subtract[(((ax0 * 18) + (ax1 * 2)) + ax2)] = (asinhf(ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)]) - ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i0_i1_fused * 2) + i2)] = asinf((ph_0[((i0_i1_fused * 2) + i2)] - ph_3[((i0_i1_fused * 2) + i2)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(9, 2):\n                cse_var_1: T.int32 = ax0 * 18 + ax1 * 2 + ax2\n                T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_1 = T.Buffer((360,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((360,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 323; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 2; ++i3_s) {\n        compute[(((i0_i1_fused * 30) + (i2 * 2)) + i3_s)] = asinhf(data[(((i0_i1_fused * 30) + (i2 * 2)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 28) + (((int)threadIdx.x) >> 1)) < 4845) {\n    compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 17, 15, 2), \"float32\"), compute: T.Buffer((19, 17, 15, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(323):\n            for i2, i3_s in T.grid(15, 2):\n                cse_var_1: T.int32 = i0_i1_fused * 30 + i2 * 2 + i3_s\n                compute_1 = T.Buffer((9690,), data=compute.data)\n                data_1 = T.Buffer((9690,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [19, 17, 15, 2]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_mod[(((ax0 * 65) + (ax1 * 5)) + ax2)] = fmodf(ph_0[(((ax0 * 65) + (ax1 * 5)) + ax2)], ph_3[(((ax0 * 65) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 195; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 3; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 13; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 5; ++ax2_1) {\n        T_add[(((ax0_1 * 65) + (ax1_1 * 5)) + ax2_1)] = (fabsf(ph_0[(((ax0_1 * 65) + (ax1_1 * 5)) + ax2_1)]) + ph_0[(((ax0_1 * 65) + (ax1_1 * 5)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 13, 5), \"float32\"), ph_3: T.Buffer((3, 13, 5), \"float32\"), T_mod: T.Buffer((3, 13, 5), \"float32\"), compute: T.Buffer((3, 13, 5), \"float32\"), T_add: T.Buffer((3, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((195,), data=ph_0.data)\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(13, 5):\n                cse_var_1: T.int32 = ax0 * 65 + ax1 * 5 + ax2\n                T_mod_1 = T.Buffer((195,), data=T_mod.data)\n                ph_3_1 = T.Buffer((195,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(195):\n            compute_1 = T.Buffer((195,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(13, 5):\n                cse_var_2: T.int32 = ax0 * 65 + ax1 * 5 + ax2\n                T_add_1 = T.Buffer((195,), data=T_add.data)\n                T_add_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]", "op_args": [["mod", "sin", "abs", "add"]]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 198; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 9; ++i3) {\n      DilatedInput[((i0_i1_fused_i2_fused * 9) + i3)] = data[((i0_i1_fused_i2_fused * 9) + i3)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 2, 11, 9), \"float32\"), DilatedInput: T.Buffer((9, 2, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(198):\n            for i3 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 9 + i3\n                DilatedInput_1 = T.Buffer((1782,), data=DilatedInput.data)\n                data_1 = T.Buffer((1782,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [9, 2, 11, 9]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 105; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 105; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_2[((i0 * 15) + i2)] = asinf(ph_0[((i0 * 15) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 1, 15), \"float32\"), compute: T.Buffer((7, 1, 15), \"float32\"), compute_1: T.Buffer((7, 1, 15), \"float32\"), compute_2: T.Buffer((7, 1, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((105,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(105):\n            compute_3 = T.Buffer((105,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(105):\n            compute_3 = T.Buffer((105,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(7):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0 * 15 + i2\n                compute_3 = T.Buffer((105,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])", "op_args": [["cos", "abs", "abs", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = cosf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1620; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 135; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 12) + i2_1)] = asinhf((ph_0[((i0_i1_fused_1 * 12) + i2_1)] - cosf(ph_0[((i0_i1_fused_1 * 12) + i2_1)])));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((1620,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1620):\n            T_add_1 = T.Buffer((1620,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((1620,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2] - T.cos(ph_0_1[cse_var_2]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n          compute[((((i0 * 3510) + (i1 * 270)) + (i2 * 15)) + i3_s)] = asinhf(data[((((i0 * 3510) + (i1 * 270)) + (i2 * 15)) + i3_s)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(39) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 39) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 39) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 13, 18, 15), \"float32\"), compute: T.Buffer((7, 13, 18, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3_s in T.grid(13, 18, 15):\n                cse_var_1: T.int32 = i0 * 3510 + i1 * 270 + i2 * 15 + i3_s\n                compute_1 = T.Buffer((24570,), data=compute.data)\n                data_1 = T.Buffer((24570,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [7, 13, 18, 15]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3570; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 17, 2, 15), \"float32\"), DilatedInput: T.Buffer((7, 17, 2, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3570):\n            DilatedInput_1 = T.Buffer((3570,), data=DilatedInput.data)\n            data_1 = T.Buffer((3570,), data=data.data)\n            DilatedInput_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused]", "op_args": [7, 17, 2, 15]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1197; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 14; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3_s)] = asinhf(data[((i0_i1_fused_i2_fused * 14) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 8379) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 9, 19, 14), \"float32\"), compute: T.Buffer((7, 9, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1197):\n            for i3_s in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3_s\n                compute_1 = T.Buffer((16758,), data=compute.data)\n                data_1 = T.Buffer((16758,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [7, 9, 19, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 27) + (i1 * 9)) + i2)] = expf(ph_0[(((i0 * 27) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 486; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 486; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf((ph_0[i0_i1_fused_i2_fused] - atanhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 3, 9), \"float32\"), compute: T.Buffer((18, 3, 9), \"float32\"), T_subtract: T.Buffer((18, 3, 9), \"float32\"), compute_1: T.Buffer((18, 3, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((486,), data=ph_0.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(3, 9):\n                cse_var_1: T.int32 = i0 * 27 + i1 * 9 + i2\n                compute_2 = T.Buffer((486,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(486):\n            T_subtract_1 = T.Buffer((486,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(486):\n            compute_2 = T.Buffer((486,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] - T.atanh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["exp", "acos", "subtract", "atanh", "subtract", "atan"]]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 20; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        DilatedInput[(((i0_i1_fused * 96) + (i2 * 6)) + i3)] = data[(((i0_i1_fused * 96) + (i2 * 6)) + i3)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 1, 16, 6), \"float32\"), DilatedInput: T.Buffer((20, 1, 16, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(20):\n            for i2, i3 in T.grid(16, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 96 + i2 * 6 + i3\n                DilatedInput_1 = T.Buffer((1920,), data=DilatedInput.data)\n                data_1 = T.Buffer((1920,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [20, 1, 16, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t i2 = 0; i2 < 3; ++i2) {\n    compute_1[i2] = atanf(ph_0[i2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 1, 3), \"float32\"), compute: T.Buffer((1, 1, 3), \"float32\"), compute_1: T.Buffer((1, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3):\n            compute_2 = T.Buffer((3,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i2 in range(3):\n            compute_2 = T.Buffer((3,), data=compute_1.data)\n            compute_2[i2] = T.atan(ph_0_1[i2])", "op_args": [["acos", "atan"]]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 540; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = asinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 9, 10, 2), \"float32\"), compute: T.Buffer((3, 9, 10, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(540):\n            compute_1 = T.Buffer((540,), data=compute.data)\n            data_1 = T.Buffer((540,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.asinh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [3, 9, 10, 2]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = atanf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_2[(((i0 * 35) + (i1 * 7)) + i2_1)] = sinf(ph_0[(((i0 * 35) + (i1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 270; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      DilatedInput[((i0_i1_fused_i2_fused * 19) + i3)] = data[((i0_i1_fused_i2_fused * 19) + i3)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 6, 9, 19), \"float32\"), DilatedInput: T.Buffer((5, 6, 9, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(270):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                DilatedInput_1 = T.Buffer((5130,), data=DilatedInput.data)\n                data_1 = T.Buffer((5130,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [5, 6, 9, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 88; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 22) + (i1 * 2)) + i2)] = asinhf(ph_0[(((i0 * 22) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_1[(((i0_1 * 22) + (i1_1 * 2)) + i2_1)] = cosf(ph_0[(((i0_1 * 22) + (i1_1 * 2)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 11, 2), \"float32\"), ph_3: T.Buffer((4, 11, 2), \"float32\"), T_multiply: T.Buffer((4, 11, 2), \"float32\"), compute: T.Buffer((4, 11, 2), \"float32\"), compute_1: T.Buffer((4, 11, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((88,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(88):\n            T_multiply_1 = T.Buffer((88,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((88,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(11, 2):\n                cse_var_1: T.int32 = i0 * 22 + i1 * 2 + i2\n                compute_2 = T.Buffer((88,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(11, 2):\n                cse_var_2: T.int32 = i0 * 22 + i1 * 2 + i2\n                compute_2 = T.Buffer((88,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])", "op_args": [["multiply", "asinh", "cos"]]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 216; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i0_i1_fused * 160) + (i2 * 20)) + i3)] = asinhf(data[(((i0_i1_fused * 160) + (i2 * 20)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 18, 8, 20), \"float32\"), compute: T.Buffer((12, 18, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(216):\n            for i2, i3 in T.grid(8, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 160 + i2 * 20 + i3\n                compute_1 = T.Buffer((34560,), data=compute.data)\n                data_1 = T.Buffer((34560,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [12, 18, 8, 20]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_add[(((ax0 * 18) + (ax1 * 2)) + ax2)] = (ph_0[(((ax0 * 18) + (ax1 * 2)) + ax2)] + ph_3[(((ax0 * 18) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 360; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf((ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(9, 2):\n                cse_var_1: T.int32 = ax0 * 18 + ax1 * 2 + ax2\n                T_add_1 = T.Buffer((360,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[17];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 272; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_2[i2] = expf(ph_0[((ax1 * 17) + i2)]);\n    }\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_mod[((ax1 * 17) + ax2)] = fmodf(compute_2[ax2], ph_0[((ax1 * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n      compute_1[((i0_i1_fused * 17) + i2_1)] = expf(ph_0[((i0_i1_fused * 17) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 16, 17), \"float32\"), compute: T.Buffer((1, 16, 17), \"float32\"), T_mod: T.Buffer((1, 16, 17), \"float32\"), compute_1: T.Buffer((1, 16, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([17], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((272,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(272):\n            compute_3 = T.Buffer((272,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax1 in range(16):\n            compute_3 = T.Buffer((17,), data=compute_2)\n            for i2 in range(17):\n                compute_3[i2] = T.exp(ph_0_1[ax1 * 17 + i2])\n            for ax2 in range(17):\n                cse_var_1: T.int32 = ax1 * 17 + ax2\n                T_mod_1 = T.Buffer((272,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(compute_3[ax2], ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(16):\n            for i2 in range(17):\n                cse_var_2: T.int32 = i0_i1_fused * 17 + i2\n                compute_3 = T.Buffer((272,), data=compute_1.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])", "op_args": [["cos", "exp", "mod", "exp"]]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 840; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 7; ++i3) {\n      DilatedInput[((i0_i1_fused_i2_fused * 7) + i3)] = data[((i0_i1_fused_i2_fused * 7) + i3)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 3, 20, 7), \"float32\"), DilatedInput: T.Buffer((14, 3, 20, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            for i3 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 7 + i3\n                DilatedInput_1 = T.Buffer((5880,), data=DilatedInput.data)\n                data_1 = T.Buffer((5880,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [14, 3, 20, 7]}{"op_name": "asinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 255) + (i1 * 17)) + i2)] = asinhf(data[(((i0 * 255) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = asinhf(data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 15, 17, 1), \"float32\"), compute: T.Buffer((7, 15, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(15, 17):\n                cse_var_1: T.int32 = i0 * 255 + i1 * 17 + i2\n                compute_1 = T.Buffer((1785,), data=compute.data)\n                data_1 = T.Buffer((1785,), data=data.data)\n                compute_1[cse_var_1] = T.asinh(data_1[cse_var_1])", "op_args": [7, 15, 17, 1]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1620; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1620; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 180) + (i1 * 12)) + i2)] = asinhf((ph_0[(((i0 * 180) + (i1 * 12)) + i2)] - cosf(ph_0[(((i0 * 180) + (i1 * 12)) + i2)])));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1620):\n            T_add_1 = T.Buffer((1620,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(15, 12):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 12 + i2\n                compute_2 = T.Buffer((1620,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1] - T.cos(ph_0_1[cse_var_1]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      DilatedInput[((i0_i1_fused_i2_fused * 6) + i3)] = data[((i0_i1_fused_i2_fused * 6) + i3)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 2, 4, 6), \"float32\"), DilatedInput: T.Buffer((10, 2, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 6 + i3\n                DilatedInput_1 = T.Buffer((480,), data=DilatedInput.data)\n                data_1 = T.Buffer((480,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [10, 2, 4, 6]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 13; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 13) + i3)] = atanhf(data[((i0_i1_fused_i2_fused * 13) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 8, 6, 13), \"float32\"), compute: T.Buffer((18, 8, 6, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            for i3 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 13 + i3\n                compute_1 = T.Buffer((11232,), data=compute.data)\n                data_1 = T.Buffer((11232,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [18, 8, 6, 13]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2240; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 2, 10, 8), \"float32\"), DilatedInput: T.Buffer((14, 2, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2240):\n            DilatedInput_1 = T.Buffer((2240,), data=DilatedInput.data)\n            data_1 = T.Buffer((2240,), data=data.data)\n            DilatedInput_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused]", "op_args": [14, 2, 10, 8]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[315];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 189; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 189; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 189; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 15; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax6 = 0; ax6 < 7; ++ax6) {\n      for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused_1 * 21) + (ax6 * 3)) + ax7)] = ph_3[(((((ax0_ax1_fused_ax2_fused_1 / 5) * 105) + (ax7 * 35)) + (ax6 * 5)) + (ax0_ax1_fused_ax2_fused_1 % 5))];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_i1_outer_fused_i2_outer_fused = 0; i0_outer_i1_outer_fused_i2_outer_fused < 45; ++i0_outer_i1_outer_fused_i2_outer_fused) {\n    float T_batch_matmul_NN[3];\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 3; ++b_outer_inner_init) {\n      T_batch_matmul_NN[b_outer_inner_init] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 7; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 3; ++b_outer_inner) {\n        T_batch_matmul_NN[b_outer_inner] = (T_batch_matmul_NN[b_outer_inner] + (ph_0[(((((i0_outer_i1_outer_fused_i2_outer_fused / 15) * 63) + (b_outer_inner * 21)) + (((i0_outer_i1_outer_fused_i2_outer_fused % 15) / 5) * 7)) + k_outer)] * auto_scheduler_layout_transform[(((((i0_outer_i1_outer_fused_i2_outer_fused / 15) * 105) + ((i0_outer_i1_outer_fused_i2_outer_fused % 5) * 21)) + (k_outer * 3)) + b_outer_inner)]));\n      }\n    }\n    for (int32_t i0_inner = 0; i0_inner < 3; ++i0_inner) {\n      compute_2[((((i0_outer_i1_outer_fused_i2_outer_fused / 15) * 45) + (i0_inner * 15)) + (i0_outer_i1_outer_fused_i2_outer_fused % 15))] = acoshf(T_batch_matmul_NN[i0_inner]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(180) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[2];\n  __shared__ float ph_3_shared[180];\n  for (int i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n    T_batch_matmul_NN[i_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    ph_3_shared[((int)threadIdx.x)] = ph_3[((((((int)threadIdx.x) / 20) * 40) + (k_outer_outer * 20)) + (((int)threadIdx.x) % 20))];\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        for (int i_inner = 0; i_inner < 2; ++i_inner) {\n          T_batch_matmul_NN[i_inner] = (T_batch_matmul_NN[i_inner] + (ph_0[((((((((int)threadIdx.x) / 5) * 16) + (i_inner * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) / 20) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        }\n      }\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 2; ++i1_inner) {\n    compute[((((((int)threadIdx.x) / 5) * 10) + (i1_inner * 5)) + (((int)threadIdx.x) % 5))] = acoshf(T_batch_matmul_NN[i1_inner]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 3, 7), \"float32\"), ph_3: T.Buffer((9, 7, 5), \"float32\"), compute: T.Buffer((9, 3, 7), \"float32\"), compute_1: T.Buffer((9, 3, 7), \"float32\"), T_mod: T.Buffer((9, 3, 7), \"float32\"), compute_2: T.Buffer((9, 3, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([315], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((189,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(189):\n            compute_3 = T.Buffer((189,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(189):\n            compute_3 = T.Buffer((189,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(189):\n            T_mod_1 = T.Buffer((189,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((315,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(15):\n            for ax6, ax7 in T.grid(7, 3):\n                ph_3_1 = T.Buffer((315,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 21 + ax6 * 3 + ax7] = ph_3_1[ax0_ax1_fused_ax2_fused // 5 * 105 + ax7 * 35 + ax6 * 5 + ax0_ax1_fused_ax2_fused % 5]\n        for i0_outer_i1_outer_fused_i2_outer_fused in T.parallel(45):\n            T_batch_matmul_NN = T.allocate([3], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((3,), data=T_batch_matmul_NN, align=8)\n            for b_outer_inner_init in range(3):\n                T_batch_matmul_NN_1[b_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner in T.grid(7, 3):\n                cse_var_1: T.int32 = i0_outer_i1_outer_fused_i2_outer_fused // 15\n                T_batch_matmul_NN_1[b_outer_inner] = T_batch_matmul_NN_1[b_outer_inner] + ph_0_1[cse_var_1 * 63 + b_outer_inner * 21 + i0_outer_i1_outer_fused_i2_outer_fused % 15 // 5 * 7 + k_outer] * auto_scheduler_layout_transform_1[cse_var_1 * 105 + i0_outer_i1_outer_fused_i2_outer_fused % 5 * 21 + k_outer * 3 + b_outer_inner]\n            for i0_inner in range(3):\n                compute_3 = T.Buffer((135,), data=compute_2.data)\n                compute_3[i0_outer_i1_outer_fused_i2_outer_fused // 15 * 45 + i0_inner * 15 + i0_outer_i1_outer_fused_i2_outer_fused % 15] = T.acosh(T_batch_matmul_NN_1[i0_inner])", "op_args": [["batch_matmul", "asin", "acos", "acos", "mod", "acosh"]]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 204; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n        compute[(((i0_i1_fused * 160) + (i2 * 16)) + i3_s)] = atanhf(data[(((i0_i1_fused * 160) + (i2 * 16)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 17, 10, 16), \"float32\"), compute: T.Buffer((12, 17, 10, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(204):\n            for i2, i3_s in T.grid(10, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 160 + i2 * 16 + i3_s\n                compute_1 = T.Buffer((32640,), data=compute.data)\n                data_1 = T.Buffer((32640,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [12, 17, 10, 16]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3762; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 2, 9, 19), \"float32\"), DilatedInput: T.Buffer((11, 2, 9, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3762):\n            DilatedInput_1 = T.Buffer((3762,), data=DilatedInput.data)\n            data_1 = T.Buffer((3762,), data=data.data)\n            DilatedInput_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused]", "op_args": [11, 2, 9, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 112; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_add[((ax0_ax1_fused * 14) + ax2)] = (ph_0[((ax0_ax1_fused * 14) + ax2)] + ph_3[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 14; ++ax2_1) {\n        T_multiply[(((ax0 * 224) + (ax1 * 14)) + ax2_1)] = (ph_0[(((ax0 * 224) + (ax1 * 14)) + ax2_1)] * ph_3[(((ax0 * 224) + (ax1 * 14)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 16, 14), \"float32\"), ph_3: T.Buffer((7, 16, 14), \"float32\"), T_add: T.Buffer((7, 16, 14), \"float32\"), T_multiply: T.Buffer((7, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1568,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1568,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(112):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_add_1 = T.Buffer((1568,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(16, 14):\n                cse_var_2: T.int32 = ax0 * 224 + ax1 * 14 + ax2\n                T_multiply_1 = T.Buffer((1568,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] * ph_3_1[cse_var_2]", "op_args": [["add", "multiply"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 315; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 585; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 3; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 3) + i3)] = atanhf(data[((i0_i1_fused_i2_fused * 3) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 5, 13, 3), \"float32\"), compute: T.Buffer((9, 5, 13, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(585):\n            for i3 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 3 + i3\n                compute_1 = T.Buffer((1755,), data=compute.data)\n                data_1 = T.Buffer((1755,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [9, 5, 13, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = expf((ph_0[(((i0 * 16) + (i1 * 8)) + i2)] + asinhf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 8) + ax2)]) / ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((48,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.asin(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        DilatedInput[(((i0 * 220) + (i1 * 11)) + i3)] = data[(((i0 * 220) + (i1 * 11)) + i3)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 20, 1, 11), \"float32\"), DilatedInput: T.Buffer((19, 20, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(19):\n            for i1, i3 in T.grid(20, 11):\n                cse_var_1: T.int32 = i0 * 220 + i1 * 11 + i3\n                DilatedInput_1 = T.Buffer((4180,), data=DilatedInput.data)\n                data_1 = T.Buffer((4180,), data=data.data)\n                DilatedInput_1[cse_var_1] = data_1[cse_var_1]", "op_args": [19, 20, 1, 11]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 72) + (i1 * 8)) + i2)] = atanhf(atanf(ph_0[(((i0 * 72) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_2[((i0_i1_fused * 8) + i2_1)] = fabsf(ph_0[((i0_i1_fused * 8) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 9, 8), \"float32\"), compute: T.Buffer((12, 9, 8), \"float32\"), compute_1: T.Buffer((12, 9, 8), \"float32\"), compute_2: T.Buffer((12, 9, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((864,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_3 = T.Buffer((864,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(9, 8):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 8 + i2\n                compute_3 = T.Buffer((864,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(108):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((864,), data=compute_2.data)\n                compute_3[cse_var_2] = T.fabs(ph_0_1[cse_var_2])", "op_args": [["exp", "atan", "atanh", "abs"]]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i0_i1_fused * 280) + (i2 * 20)) + i3)] = atanhf(data[(((i0_i1_fused * 280) + (i2 * 20)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 10, 14, 20), \"float32\"), compute: T.Buffer((13, 10, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(130):\n            for i2, i3 in T.grid(14, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 280 + i2 * 20 + i3\n                compute_1 = T.Buffer((36400,), data=compute.data)\n                data_1 = T.Buffer((36400,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [13, 10, 14, 20]}{"op_name": "dilate", "c_code": "void default_function_kernel(float* DilatedInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 8100; ++i0_i1_fused_i2_fused_i3_fused) {\n    DilatedInput[i0_i1_fused_i2_fused_i3_fused] = data[i0_i1_fused_i2_fused_i3_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ DilatedInput, float* __restrict__ data) {\n  DilatedInput[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 9, 6, 15), \"float32\"), DilatedInput: T.Buffer((10, 9, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(8100):\n            DilatedInput_1 = T.Buffer((8100,), data=DilatedInput.data)\n            data_1 = T.Buffer((8100,), data=data.data)\n            DilatedInput_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused]", "op_args": [10, 9, 6, 15]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i1 * 120) + (i2 * 6)) + i3)] = atanhf(data[(((i1 * 120) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 20, 20, 6), \"float32\"), compute: T.Buffer((1, 20, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(20, 20, 6):\n            cse_var_1: T.int32 = i1 * 120 + i2 * 6 + i3\n            compute_1 = T.Buffer((2400,), data=compute.data)\n            data_1 = T.Buffer((2400,), data=data.data)\n            compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [1, 20, 20, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_9) {\n  float auto_scheduler_layout_transform[260];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 364; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 364; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 364; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 5; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax4 = 0; ax4 < 13; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n        for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n          auto_scheduler_layout_transform[((((ax0_ax1_fused_ax2_fused_1 * 52) + (ax4 * 4)) + (ax5 * 2)) + ax8)] = ph_9[((((ax5 * 130) + (ax8 * 65)) + (ax4 * 5)) + ax0_ax1_fused_ax2_fused_1)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 5; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 7; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 70) + (b_inner_init * 35)) + (i_inner_init * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 13; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n          for (int32_t i_inner = 0; i_inner < 7; ++i_inner) {\n            T_batch_matmul_NN[((((b_outer_inner * 70) + (b_inner * 35)) + (i_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[((((b_outer_inner * 70) + (b_inner * 35)) + (i_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (fmodf(ph_0[((((b_outer_inner * 182) + (b_inner * 91)) + (i_inner * 13)) + k_outer)], ph_3[((((b_outer_inner * 182) + (b_inner * 91)) + (i_inner * 13)) + k_outer)]) * auto_scheduler_layout_transform[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 52) + (k_outer * 4)) + (b_outer_inner * 2)) + b_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((int)blockIdx.x)] = (acoshf(ph_0[((int)blockIdx.x)]) + ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(25) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3, float* __restrict__ ph_9) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float T_mod_shared[40];\n  __shared__ float ph_9_shared[40];\n  for (int b_c_inner_init = 0; b_c_inner_init < 8; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 4; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) / 5)) < 8) {\n        T_mod_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 25) + ((int)threadIdx.x))] = fmodf(ph_0[(((ax0_ax1_fused_ax2_fused_outer_outer * 100) + (((int)threadIdx.x) * 4)) + k_outer_outer)], ph_3[(((ax0_ax1_fused_ax2_fused_outer_outer * 100) + (((int)threadIdx.x) * 4)) + k_outer_outer)]);\n      }\n    }\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 2; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 5)) < 8) {\n        ph_9_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 25) + ((int)threadIdx.x))] = ph_9[((((ax0_ax1_fused_ax2_fused_outer_outer_1 * 100) + ((((int)threadIdx.x) / 5) * 20)) + (k_outer_outer * 5)) + (((int)threadIdx.x) % 5))];\n      }\n    }\n    __syncthreads();\n    for (int b_c_inner = 0; b_c_inner < 8; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (T_mod_shared[((b_c_inner * 5) + (((int)threadIdx.x) / 5))] * ph_9_shared[((b_c_inner * 5) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 8; ++b_inner) {\n    T_batch_matmul_NN[((b_inner * 25) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 7, 13), \"float32\"), ph_3: T.Buffer((4, 7, 13), \"float32\"), ph_9: T.Buffer((4, 13, 5), \"float32\"), compute: T.Buffer((4, 7, 13), \"float32\"), compute_1: T.Buffer((4, 7, 13), \"float32\"), T_add: T.Buffer((4, 7, 13), \"float32\"), T_batch_matmul_NN: T.Buffer((4, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([260], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((364,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(364):\n            compute_2 = T.Buffer((364,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(364):\n            compute_2 = T.Buffer((364,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(364):\n            T_add_1 = T.Buffer((364,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((260,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax4, ax5, ax8 in T.grid(13, 2, 2):\n                ph_9_1 = T.Buffer((260,), data=ph_9.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 52 + ax4 * 4 + ax5 * 2 + ax8] = ph_9_1[ax5 * 130 + ax8 * 65 + ax4 * 5 + ax0_ax1_fused_ax2_fused]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(5):\n            T_batch_matmul_NN_1 = T.Buffer((140,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, b_inner_init, i_inner_init in T.grid(2, 2, 7):\n                T_batch_matmul_NN_1[b_outer_inner_init * 70 + b_inner_init * 35 + i_inner_init * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for k_outer, b_outer_inner, b_inner, i_inner in T.grid(13, 2, 2, 7):\n                cse_var_2: T.int32 = b_outer_inner * 70 + b_inner * 35 + i_inner * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                cse_var_1: T.int32 = b_outer_inner * 182 + b_inner * 91 + i_inner * 13 + k_outer\n                ph_3_1 = T.Buffer((364,), data=ph_3.data)\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]) * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 52 + k_outer * 4 + b_outer_inner * 2 + b_inner]", "op_args": [["mod", "acosh", "acosh", "asinh", "add", "batch_matmul"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 280; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 2) + ax2)] = (ph_0[((ax0_ax1_fused * 2) + ax2)] * ph_3[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 40) + (i1 * 2)) + i2)] = atanhf(ph_0[(((i0 * 40) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 20, 2), \"float32\"), ph_3: T.Buffer((14, 20, 2), \"float32\"), T_multiply: T.Buffer((14, 20, 2), \"float32\"), compute: T.Buffer((14, 20, 2), \"float32\"), compute_1: T.Buffer((14, 20, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((560,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(280):\n            for ax2 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_multiply_1 = T.Buffer((560,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((560,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(20, 2):\n                cse_var_2: T.int32 = i0 * 40 + i1 * 2 + i2\n                compute_2 = T.Buffer((560,), data=compute.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(560):\n            compute_2 = T.Buffer((560,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(T.cos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["multiply", "atanh", "cos", "sin"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 19494; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 19, 19, 3), \"float32\"), compute: T.Buffer((18, 1083), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(19494):\n            compute_1 = T.Buffer((19494,), data=compute.data)\n            data_1 = T.Buffer((19494,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [18, 19, 19, 3]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 594; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = atanhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 9, 3, 2), \"float32\"), compute: T.Buffer((11, 9, 3, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(594):\n            compute_1 = T.Buffer((594,), data=compute.data)\n            data_1 = T.Buffer((594,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.atanh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [11, 9, 3, 2]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 44; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = atanf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 132; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_multiply[(((ax0 * 12) + (ax1 * 3)) + ax2)] = (sinf(ph_0[(((ax0 * 12) + (ax1 * 3)) + ax2)]) * ph_0[(((ax0 * 12) + (ax1 * 3)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 132; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(__sinf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 4, 3), \"float32\"), ph_3: T.Buffer((11, 4, 3), \"float32\"), compute: T.Buffer((11, 4, 3), \"float32\"), compute_1: T.Buffer((11, 4, 3), \"float32\"), T_multiply: T.Buffer((11, 4, 3), \"float32\"), T_divide: T.Buffer((11, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((132,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(44):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((132,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(132):\n            compute_2 = T.Buffer((132,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(4, 3):\n                cse_var_2: T.int32 = ax0 * 12 + ax1 * 3 + ax2\n                T_multiply_1 = T.Buffer((132,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.sin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(132):\n            T_divide_1 = T.Buffer((132,), data=T_divide.data)\n            ph_3_1 = T.Buffer((132,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["mod", "atan", "sin", "acos", "multiply", "divide"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 14; ++i) {\n    for (int32_t j = 0; j < 1620; ++j) {\n      compute[((i * 1620) + j)] = data[((i * 1620) + j)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 2835) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 15, 6, 18), \"float32\"), compute: T.Buffer((14, 1620), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(14):\n            for j in range(1620):\n                cse_var_1: T.int32 = i * 1620 + j\n                compute_1 = T.Buffer((22680,), data=compute.data)\n                data_1 = T.Buffer((22680,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]", "op_args": [14, 15, 6, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 304; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_mod[((ax0_ax1_fused * 19) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 19) + ax2)], ph_3[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 304; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = ceilf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5776; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 19, 19), \"float32\"), ph_3: T.Buffer((16, 19, 19), \"float32\"), T_mod: T.Buffer((16, 19, 19), \"float32\"), compute: T.Buffer((16, 19, 19), \"float32\"), compute_1: T.Buffer((16, 19, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5776,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_mod_1 = T.Buffer((5776,), data=T_mod.data)\n                ph_3_1 = T.Buffer((5776,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(304):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((5776,), data=compute.data)\n                compute_2[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(5776):\n            compute_2 = T.Buffer((5776,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "ceil", "acosh", "atan"]]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 6; ++i3_s) {\n        compute[(((i0_i1_fused * 102) + (i2 * 6)) + i3_s)] = atanhf(data[(((i0_i1_fused * 102) + (i2 * 6)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 17, 6), \"float32\"), compute: T.Buffer((6, 12, 17, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(72):\n            for i2, i3_s in T.grid(17, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 102 + i2 * 6 + i3_s\n                compute_1 = T.Buffer((7344,), data=compute.data)\n                data_1 = T.Buffer((7344,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [6, 12, 17, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 99) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 99) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_mod[(((ax0 * 99) + (ax1 * 9)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]), ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(11, 9):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_2: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 308; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 308; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 44) + (i1 * 4)) + i2)] = acosf(sinf(ph_0[(((i0 * 44) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_multiply[(((ax0 * 44) + (ax1 * 4)) + ax2)] = (sinf(ph_0[(((ax0 * 44) + (ax1 * 4)) + ax2)]) * ph_0[(((ax0 * 44) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((int)blockIdx.x)] = fmodf(ph_0[((int)blockIdx.x)], ph_3[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 4), \"float32\"), ph_3: T.Buffer((7, 11, 4), \"float32\"), T_mod: T.Buffer((7, 11, 4), \"float32\"), compute: T.Buffer((7, 11, 4), \"float32\"), compute_1: T.Buffer((7, 11, 4), \"float32\"), T_multiply: T.Buffer((7, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((308,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_mod_1 = T.Buffer((308,), data=T_mod.data)\n            ph_3_1 = T.Buffer((308,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(11, 4):\n                cse_var_1: T.int32 = i0 * 44 + i1 * 4 + i2\n                compute_2 = T.Buffer((308,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(T.sin(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(11, 4):\n                cse_var_2: T.int32 = ax0 * 44 + ax1 * 4 + ax2\n                T_multiply_1 = T.Buffer((308,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.sin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]", "op_args": [["mod", "atan", "sin", "acos", "multiply"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_divide[(((ax0 * 320) + (ax1 * 20)) + ax2)] = (ph_0[(((ax0 * 320) + (ax1 * 20)) + ax2)] / ceilf(ph_0[(((ax0 * 320) + (ax1 * 20)) + ax2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 16, 20), \"float32\"), T_divide: T.Buffer((11, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(16, 20):\n                cse_var_1: T.int32 = ax0 * 320 + ax1 * 20 + ax2\n                T_divide_1 = T.Buffer((3520,), data=T_divide.data)\n                ph_0_1 = T.Buffer((3520,), data=ph_0.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.ceil(ph_0_1[cse_var_1])", "op_args": [["ceil", "divide"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 6664; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 14, 17, 14), \"float32\"), compute: T.Buffer((2, 3332), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(6664):\n            compute_1 = T.Buffer((6664,), data=compute.data)\n            data_1 = T.Buffer((6664,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [2, 14, 17, 14]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1224; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 6; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 6) + i3_s)] = atanhf(data[((i0_i1_fused_i2_fused * 6) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 459) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 18, 4, 6), \"float32\"), compute: T.Buffer((17, 18, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1224):\n            for i3_s in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 6 + i3_s\n                compute_1 = T.Buffer((7344,), data=compute.data)\n                data_1 = T.Buffer((7344,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [17, 18, 4, 6]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2880; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_add[(((ax0 * 240) + (ax1 * 20)) + ax2)] = (ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)] + ph_3[(((ax0 * 240) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 12; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n        T_subtract_1[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)] = (asinhf(ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)]) - ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] + ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 12, 20), \"float32\"), ph_3: T.Buffer((12, 12, 20), \"float32\"), T_subtract: T.Buffer((12, 12, 20), \"float32\"), T_add: T.Buffer((12, 12, 20), \"float32\"), T_subtract_1: T.Buffer((12, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2880,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_subtract_2 = T.Buffer((2880,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_add_1 = T.Buffer((2880,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_2: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_subtract_2 = T.Buffer((2880,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]", "op_args": [["subtract", "add", "asinh", "subtract"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 48960; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 17, 8, 20), \"float32\"), compute: T.Buffer((18, 2720), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(48960):\n            compute_1 = T.Buffer((48960,), data=compute.data)\n            data_1 = T.Buffer((48960,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [18, 17, 8, 20]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 99) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 99) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 154; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_mod[((ax0_ax1_fused * 9) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 9) + ax2)]), ph_0[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(11, 9):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(154):\n            for ax2 in range(9):\n                cse_var_2: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_mod[((ax0_ax1_fused * 7) + ax2)] = fmodf(acosf(ph_0[((ax0_ax1_fused * 7) + ax2)]), ph_0[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 315; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.acos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 198; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 16; ++i3_s) {\n        compute[(((i0_i1_fused * 208) + (i2 * 16)) + i3_s)] = atanhf(data[(((i0_i1_fused * 208) + (i2 * 16)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 11, 13, 16), \"float32\"), compute: T.Buffer((18, 11, 13, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(198):\n            for i2, i3_s in T.grid(13, 16):\n                cse_var_1: T.int32 = i0_i1_fused * 208 + i2 * 16 + i3_s\n                compute_1 = T.Buffer((41184,), data=compute.data)\n                data_1 = T.Buffer((41184,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [18, 11, 13, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 52; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = cosf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 468; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 52; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 9) + i2_1)] = cosf(ph_0[((i0_i1_fused_1 * 9) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 13, 9), \"float32\"), compute: T.Buffer((4, 13, 9), \"float32\"), T_add: T.Buffer((4, 13, 9), \"float32\"), compute_1: T.Buffer((4, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(52):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((468,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(468):\n            T_add_1 = T.Buffer((468,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(52):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((468,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])", "op_args": [["cos", "acosh", "add", "cos"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 1056; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 11, 8), \"float32\"), compute: T.Buffer((3, 352), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(1056):\n            compute_1 = T.Buffer((1056,), data=compute.data)\n            data_1 = T.Buffer((1056,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [3, 4, 11, 8]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* T_subtract_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_subtract[(((ax0 * 240) + (ax1 * 20)) + ax2)] = (ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)] - ph_3[(((ax0 * 240) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2880; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2880; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused_1] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 12, 20), \"float32\"), ph_3: T.Buffer((12, 12, 20), \"float32\"), T_subtract: T.Buffer((12, 12, 20), \"float32\"), T_add: T.Buffer((12, 12, 20), \"float32\"), T_subtract_1: T.Buffer((12, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2880,), data=ph_3.data)\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_subtract_2 = T.Buffer((2880,), data=T_subtract.data)\n                T_subtract_2[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_add_1 = T.Buffer((2880,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_subtract_2 = T.Buffer((2880,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["subtract", "add", "asinh", "subtract"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = expf((ph_0[((i0_i1_fused * 8) + i2)] + asinhf(ph_0[((i0_i1_fused * 8) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 6; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 8) + i2_1)] = acoshf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "atanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 13; ++i3_s) {\n        compute[(((i0_i1_fused * 156) + (i2 * 13)) + i3_s)] = atanhf(data[(((i0_i1_fused * 156) + (i2 * 13)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = atanhf(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 7, 12, 13), \"float32\"), compute: T.Buffer((12, 7, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(84):\n            for i2, i3_s in T.grid(12, 13):\n                cse_var_1: T.int32 = i0_i1_fused * 156 + i2 * 13 + i3_s\n                compute_1 = T.Buffer((13104,), data=compute.data)\n                data_1 = T.Buffer((13104,), data=data.data)\n                compute_1[cse_var_1] = T.atanh(data_1[cse_var_1])", "op_args": [12, 7, 12, 13]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 176; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_2[(((i0 * 16) + (i1 * 8)) + i2)] = cosf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 8), \"float32\"), compute: T.Buffer((11, 2, 8), \"float32\"), compute_1: T.Buffer((11, 2, 8), \"float32\"), compute_2: T.Buffer((11, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((176,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_3 = T.Buffer((176,), data=compute_2.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])", "op_args": [["asinh", "acos", "exp", "cos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 117) + (i1 * 9)) + i2)] = cosf(ph_0[(((i0 * 117) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_add[(((ax0 * 117) + (ax1 * 9)) + ax2)] = (acoshf(ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)]) + ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 468; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 13, 9), \"float32\"), compute: T.Buffer((4, 13, 9), \"float32\"), T_add: T.Buffer((4, 13, 9), \"float32\"), compute_1: T.Buffer((4, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(13, 9):\n                cse_var_1: T.int32 = i0 * 117 + i1 * 9 + i2\n                compute_2 = T.Buffer((468,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(4):\n            for ax1, ax2 in T.grid(13, 9):\n                cse_var_2: T.int32 = ax0 * 117 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((468,), data=T_add.data)\n                T_add_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_2 = T.Buffer((468,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["cos", "acosh", "add", "cos"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 15; ++i) {\n    for (int32_t j = 0; j < 1890; ++j) {\n      compute[((i * 1890) + j)] = data[((i * 1890) + j)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 6)) < 4725) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 15, 14, 9), \"float32\"), compute: T.Buffer((15, 1890), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(15):\n            for j in range(1890):\n                cse_var_1: T.int32 = i * 1890 + j\n                compute_1 = T.Buffer((28350,), data=compute.data)\n                data_1 = T.Buffer((28350,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]", "op_args": [15, 15, 14, 9]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        compute[(((i0_i1_fused * 36) + (i2 * 12)) + i3)] = ceilf(data[(((i0_i1_fused * 36) + (i2 * 12)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 12, 3, 12), \"float32\"), compute: T.Buffer((10, 12, 3, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(120):\n            for i2, i3 in T.grid(3, 12):\n                cse_var_1: T.int32 = i0_i1_fused * 36 + i2 * 12 + i3\n                compute_1 = T.Buffer((4320,), data=compute.data)\n                data_1 = T.Buffer((4320,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [10, 12, 3, 12]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = asinhf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 176; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 8), \"float32\"), compute: T.Buffer((11, 2, 8), \"float32\"), compute_1: T.Buffer((11, 2, 8), \"float32\"), compute_2: T.Buffer((11, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((176,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_3 = T.Buffer((176,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_3 = T.Buffer((176,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["asinh", "acos", "exp", "cos"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 832; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 13, 1, 4), \"float32\"), compute: T.Buffer((16, 52), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(832):\n            compute_1 = T.Buffer((832,), data=compute.data)\n            data_1 = T.Buffer((832,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [16, 13, 1, 4]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2394; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2394; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    float compute_2[1];\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_2[0] = expf(ph_0[(((i0 * 266) + (i1 * 14)) + i2)]);\n        compute_1[(((i0 * 266) + (i1 * 14)) + i2)] = expf(compute_2[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 19, 14), \"float32\"), ph_3: T.Buffer((9, 19, 14), \"float32\"), T_multiply: T.Buffer((9, 19, 14), \"float32\"), compute: T.Buffer((9, 19, 14), \"float32\"), compute_1: T.Buffer((9, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2394,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2394):\n            T_multiply_1 = T.Buffer((2394,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((2394,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2394):\n            compute_2 = T.Buffer((2394,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            for i1, i2 in T.grid(19, 14):\n                cse_var_1: T.int32 = i0 * 266 + i1 * 14 + i2\n                compute_3 = T.Buffer((1,), data=compute_2, align=4)\n                compute_3[0] = T.exp(ph_0_1[cse_var_1])\n                compute_4 = T.Buffer((2394,), data=compute_1.data)\n                compute_4[cse_var_1] = T.exp(compute_3[0])", "op_args": [["multiply", "cos", "exp", "exp"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 17; ++i) {\n    for (int32_t j = 0; j < 3825; ++j) {\n      compute[((i * 3825) + j)] = data[((i * 3825) + j)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 15, 17, 15), \"float32\"), compute: T.Buffer((17, 3825), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(17):\n            for j in range(3825):\n                cse_var_1: T.int32 = i * 3825 + j\n                compute_1 = T.Buffer((65025,), data=compute.data)\n                data_1 = T.Buffer((65025,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1]", "op_args": [17, 15, 17, 15]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = atanhf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 72; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  for (int32_t i1 = 0; i1 < 4; ++i1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_2[((i1 * 18) + i2_1)] = expf(ph_0[((i1 * 18) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 18), \"float32\"), compute: T.Buffer((1, 4, 18), \"float32\"), compute_1: T.Buffer((1, 4, 18), \"float32\"), compute_2: T.Buffer((1, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((72,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((72,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i1, i2 in T.grid(4, 18):\n            cse_var_2: T.int32 = i1 * 18 + i2\n            compute_3 = T.Buffer((72,), data=compute_2.data)\n            compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])", "op_args": [["atanh", "ceil", "acos", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 19; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinhf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 19; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 5) + i2_1)] = asinhf(ph_0[((i0_i1_fused_1 * 5) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 1, 5), \"float32\"), compute: T.Buffer((19, 1, 5), \"float32\"), compute_1: T.Buffer((19, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((95,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(19):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((95,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(19):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((95,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2])", "op_args": [["asinh", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 256; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = acoshf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4864; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 4864; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 4864; ++ax0_ax1_fused_ax2_fused_2) {\n    T_divide[ax0_ax1_fused_ax2_fused_2] = ((ph_0[ax0_ax1_fused_ax2_fused_2] - ph_3[ax0_ax1_fused_ax2_fused_2]) / ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 16, 19), \"float32\"), ph_3: T.Buffer((16, 16, 19), \"float32\"), compute: T.Buffer((16, 16, 19), \"float32\"), T_subtract: T.Buffer((16, 16, 19), \"float32\"), T_add: T.Buffer((16, 16, 19), \"float32\"), T_divide: T.Buffer((16, 16, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4864,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(256):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_1 = T.Buffer((4864,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4864):\n            T_subtract_1 = T.Buffer((4864,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(4864):\n            T_add_1 = T.Buffer((4864,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(4864):\n            T_divide_1 = T.Buffer((4864,), data=T_divide.data)\n            ph_3_1 = T.Buffer((4864,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["subtract", "acosh", "cos", "subtract", "add", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 171; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 14) + ax2)] = (ph_0[((ax0_ax1_fused * 14) + ax2)] * ph_3[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 171; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = cosf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2394; ++i0_i1_fused_i2_fused) {\n    float compute_2[1];\n    compute_2[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute_1[i0_i1_fused_i2_fused] = expf(compute_2[0]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 19, 14), \"float32\"), ph_3: T.Buffer((9, 19, 14), \"float32\"), T_multiply: T.Buffer((9, 19, 14), \"float32\"), compute: T.Buffer((9, 19, 14), \"float32\"), compute_1: T.Buffer((9, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2394,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(171):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_multiply_1 = T.Buffer((2394,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((2394,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(171):\n            for i2 in range(14):\n                cse_var_2: T.int32 = i0_i1_fused * 14 + i2\n                compute_2 = T.Buffer((2394,), data=compute.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2394):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            compute_3 = T.Buffer((1,), data=compute_2, align=4)\n            compute_3[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_4 = T.Buffer((2394,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(compute_3[0])", "op_args": [["multiply", "cos", "exp", "exp"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 13260; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 3315) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 17, 5, 12), \"float32\"), compute: T.Buffer((13, 1020), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(13260):\n            compute_1 = T.Buffer((13260,), data=compute.data)\n            data_1 = T.Buffer((13260,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [13, 17, 5, 12]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3360; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = ceilf(data[((i0_i1_fused_i2_fused * 19) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 14, 20, 19), \"float32\"), compute: T.Buffer((12, 14, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3360):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((63840,), data=compute.data)\n                data_1 = T.Buffer((63840,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [12, 14, 20, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = atanhf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 72; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 4; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 18) + i2_1)] = expf(ph_0[((i0_i1_fused_1 * 18) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 18), \"float32\"), compute: T.Buffer((1, 4, 18), \"float32\"), compute_1: T.Buffer((1, 4, 18), \"float32\"), compute_2: T.Buffer((1, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((72,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((72,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((72,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])", "op_args": [["atanh", "ceil", "acos", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 308; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 308; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 308; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 308; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (sinf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 4), \"float32\"), ph_3: T.Buffer((7, 11, 4), \"float32\"), T_mod: T.Buffer((7, 11, 4), \"float32\"), compute: T.Buffer((7, 11, 4), \"float32\"), compute_1: T.Buffer((7, 11, 4), \"float32\"), T_multiply: T.Buffer((7, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((308,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_mod_1 = T.Buffer((308,), data=T_mod.data)\n            ph_3_1 = T.Buffer((308,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            compute_2 = T.Buffer((308,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(308):\n            T_multiply_1 = T.Buffer((308,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["mod", "atan", "sin", "acos", "multiply"]]}{"op_name": "flatten", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 3740; ++i_j_fused) {\n    compute[i_j_fused] = data[i_j_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 17, 20, 1), \"float32\"), compute: T.Buffer((11, 340), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(3740):\n            compute_1 = T.Buffer((3740,), data=compute.data)\n            data_1 = T.Buffer((3740,), data=data.data)\n            compute_1[i_j_fused] = data_1[i_j_fused]", "op_args": [11, 17, 20, 1]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 126; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 18) + i3)] = ceilf(data[((i0_i1_fused_i2_fused * 18) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 1, 18, 18), \"float32\"), compute: T.Buffer((7, 1, 18, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            for i3 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                compute_1 = T.Buffer((2268,), data=compute.data)\n                data_1 = T.Buffer((2268,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [7, 1, 18, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 323; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 19, 5), \"float32\"), compute: T.Buffer((17, 19, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(323):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_1 = T.Buffer((1615,), data=compute.data)\n                ph_0_1 = T.Buffer((1615,), data=ph_0.data)\n                compute_1[cse_var_1] = T.asin(ph_0_1[cse_var_1])", "op_args": [["asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 40; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 7) + ax2)] = (ph_0[((ax0_ax1_fused * 7) + ax2)] - ph_3[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 14) + (i1 * 7)) + i2)] = expf(ph_0[(((i0 * 14) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 2, 7), \"float32\"), ph_3: T.Buffer((20, 2, 7), \"float32\"), T_subtract: T.Buffer((20, 2, 7), \"float32\"), compute: T.Buffer((20, 2, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((280,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(40):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_subtract_1 = T.Buffer((280,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((280,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(2, 7):\n                cse_var_2: T.int32 = i0 * 14 + i1 * 7 + i2\n                compute_1 = T.Buffer((280,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(ph_0_1[cse_var_2])", "op_args": [["subtract", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = atanhf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 45; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 7) + i2_1)] = sinf(ph_0[((i0_i1_fused_1 * 7) + i2_1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 17; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 20) + ax2)] = (ph_0[((ax0_ax1_fused * 20) + ax2)] * ph_3[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 17; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i0_i1_fused * 20) + i2)] = expf(fmodf(ph_0[((i0_i1_fused * 20) + i2)], asinf(ph_0[((i0_i1_fused * 20) + i2)])));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 1, 20), \"float32\"), ph_3: T.Buffer((17, 1, 20), \"float32\"), T_multiply: T.Buffer((17, 1, 20), \"float32\"), compute: T.Buffer((17, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((340,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(17):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_multiply_1 = T.Buffer((340,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((340,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(17):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_1 = T.Buffer((340,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(T.truncmod(ph_0_1[cse_var_2], T.asin(ph_0_1[cse_var_2])))", "op_args": [["multiply", "asin", "mod", "exp"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 540; ++i_j_fused_k_fused) {\n    for (int32_t l = 0; l < 19; ++l) {\n      new_buffer[((i_j_fused_k_fused * 19) + l)] = data[((i_j_fused_k_fused * 19) + l)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 20, 3, 19), \"float32\"), buffer: T.Buffer((9, 20, 3, 19), \"float32\"), new_buffer: T.Buffer((9, 20, 3, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused_k_fused in T.parallel(540):\n            for l in range(19):\n                cse_var_1: T.int32 = i_j_fused_k_fused * 19 + l\n                new_buffer_1 = T.Buffer((10260,), data=new_buffer.data)\n                data_1 = T.Buffer((10260,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [9, 20, 3, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = expf((ph_0[((i0_i1_fused * 8) + i2)] + asinhf(ph_0[((i0_i1_fused * 8) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        compute_1[(((i0 * 16) + (i1 * 8)) + i2_1)] = acoshf(ph_0[(((i0 * 16) + (i1 * 8)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_2: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[4760];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4760; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4760; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4760; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(compute_2[ax0_ax1_fused_ax2_fused], ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_1[(((i0 * 280) + (i1 * 14)) + i2)] = acosf(ph_0[(((i0 * 280) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 14), \"float32\"), compute: T.Buffer((17, 20, 14), \"float32\"), T_mod: T.Buffer((17, 20, 14), \"float32\"), compute_1: T.Buffer((17, 20, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([4760], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((4760,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_3 = T.Buffer((4760,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        compute_3 = T.Buffer((4760,), data=compute_2)\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4760):\n            T_mod_1 = T.Buffer((4760,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(compute_3[ax0_ax1_fused_ax2_fused], ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(20, 14):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 14 + i2\n                compute_4 = T.Buffer((4760,), data=compute_1.data)\n                compute_4[cse_var_1] = T.acos(ph_0_1[cse_var_1])", "op_args": [["abs", "exp", "mod", "acos"]]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(data[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12, 10, 1), \"float32\"), compute: T.Buffer((4, 12, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            data_1 = T.Buffer((480,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(data_1[i0_i1_fused_i2_fused])", "op_args": [4, 12, 10, 1]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5460; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ceilf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 6, 13, 14), \"float32\"), compute: T.Buffer((5, 6, 13, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(5460):\n            compute_1 = T.Buffer((5460,), data=compute.data)\n            data_1 = T.Buffer((5460,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.ceil(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [5, 6, 13, 14]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_divide[(((ax0 * 99) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)] / ph_3[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 143; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = acosf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1287; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 11, 9), \"float32\"), ph_3: T.Buffer((13, 11, 9), \"float32\"), T_divide: T.Buffer((13, 11, 9), \"float32\"), compute: T.Buffer((13, 11, 9), \"float32\"), compute_1: T.Buffer((13, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1287,), data=ph_0.data)\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_1: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_divide_1 = T.Buffer((1287,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1287,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(143):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((1287,), data=compute.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1287):\n            compute_2 = T.Buffer((1287,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["divide", "acos", "ceil", "abs"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 117) + (i1 * 13)) + i2)] = sinf(ph_0[(((i0 * 117) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2223; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2223; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2223; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(__sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 9, 13), \"float32\"), compute: T.Buffer((19, 9, 13), \"float32\"), compute_1: T.Buffer((19, 9, 13), \"float32\"), compute_2: T.Buffer((19, 9, 13), \"float32\"), compute_3: T.Buffer((19, 9, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2223,), data=ph_0.data)\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(9, 13):\n                cse_var_1: T.int32 = i0 * 117 + i1 * 13 + i2\n                compute_4 = T.Buffer((2223,), data=compute.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2223):\n            compute_4 = T.Buffer((2223,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2223):\n            compute_4 = T.Buffer((2223,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2223):\n            compute_4 = T.Buffer((2223,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["sin", "sin", "sin", "exp", "asin"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 135; ++i_j_fused_k_fused) {\n    for (int32_t l = 0; l < 2; ++l) {\n      new_buffer[((i_j_fused_k_fused * 2) + l)] = data[((i_j_fused_k_fused * 2) + l)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 15, 1, 2), \"float32\"), buffer: T.Buffer((9, 15, 1, 2), \"float32\"), new_buffer: T.Buffer((9, 15, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused_k_fused in T.parallel(135):\n            for l in range(2):\n                cse_var_1: T.int32 = i_j_fused_k_fused * 2 + l\n                new_buffer_1 = T.Buffer((270,), data=new_buffer.data)\n                data_1 = T.Buffer((270,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [9, 15, 1, 2]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 60) + (i1 * 6)) + i2)] = ceilf(data[(((i0 * 60) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 10, 6, 1), \"float32\"), compute: T.Buffer((17, 10, 6, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(10, 6):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 6 + i2\n                compute_1 = T.Buffer((1020,), data=compute.data)\n                data_1 = T.Buffer((1020,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [17, 10, 6, 1]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_2[((i0_i1_fused * 7) + i2)] = sinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 180; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      compute_1[((i0 * 10) + i1)] = sinf(ph_0[((i0 * 10) + i1)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 10, 1), \"float32\"), ph_3: T.Buffer((18, 10, 1), \"float32\"), T_subtract: T.Buffer((18, 10, 1), \"float32\"), compute: T.Buffer((18, 10, 1), \"float32\"), compute_1: T.Buffer((18, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(180):\n            T_subtract_1 = T.Buffer((180,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((180,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_2 = T.Buffer((180,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(18):\n            for i1 in range(10):\n                cse_var_1: T.int32 = i0 * 10 + i1\n                compute_2 = T.Buffer((180,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": [["subtract", "abs", "sin"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused_k_fused_l_fused = 0; i_j_fused_k_fused_l_fused < 1620; ++i_j_fused_k_fused_l_fused) {\n    new_buffer[i_j_fused_k_fused_l_fused] = data[i_j_fused_k_fused_l_fused];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 2, 3, 18), \"float32\"), buffer: T.Buffer((15, 2, 3, 18), \"float32\"), new_buffer: T.Buffer((15, 2, 3, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused_k_fused_l_fused in T.parallel(1620):\n            new_buffer_1 = T.Buffer((1620,), data=new_buffer.data)\n            data_1 = T.Buffer((1620,), data=data.data)\n            new_buffer_1[i_j_fused_k_fused_l_fused] = data_1[i_j_fused_k_fused_l_fused]", "op_args": [15, 2, 3, 18]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 29172; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ceilf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 13, 12, 17), \"float32\"), compute: T.Buffer((11, 13, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(29172):\n            compute_1 = T.Buffer((29172,), data=compute.data)\n            data_1 = T.Buffer((29172,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.ceil(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [11, 13, 12, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = expf((ph_0[((i0_i1_fused * 8) + i2)] + asinhf(ph_0[((i0_i1_fused * 8) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 6; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 8) + i2_1)] = acoshf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(ph_0[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((48,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 17; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 7; ++i3) {\n        compute[(((i1 * 28) + (i2 * 7)) + i3)] = ceilf(data[(((i1 * 28) + (i2 * 7)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 119) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 17, 4, 7), \"float32\"), compute: T.Buffer((1, 17, 4, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(17, 4, 7):\n            cse_var_1: T.int32 = i1 * 28 + i2 * 7 + i3\n            compute_1 = T.Buffer((476,), data=compute.data)\n            data_1 = T.Buffer((476,), data=data.data)\n            compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [1, 17, 4, 7]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 456; ++i_j_fused_k_fused) {\n    for (int32_t l = 0; l < 17; ++l) {\n      new_buffer[((i_j_fused_k_fused * 17) + l)] = data[((i_j_fused_k_fused * 17) + l)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 969) {\n    new_buffer[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 19, 12, 17), \"float32\"), buffer: T.Buffer((2, 19, 12, 17), \"float32\"), new_buffer: T.Buffer((2, 19, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused_k_fused in T.parallel(456):\n            for l in range(17):\n                cse_var_1: T.int32 = i_j_fused_k_fused * 17 + l\n                new_buffer_1 = T.Buffer((7752,), data=new_buffer.data)\n                data_1 = T.Buffer((7752,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [2, 19, 12, 17]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1190; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1190; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 70) + (i1 * 5)) + i2)] = acosf(ph_0[(((i0 * 70) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(ph_0[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 14, 5), \"float32\"), ph_3: T.Buffer((17, 14, 5), \"float32\"), T_subtract: T.Buffer((17, 14, 5), \"float32\"), compute: T.Buffer((17, 14, 5), \"float32\"), compute_1: T.Buffer((17, 14, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1190,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1190):\n            T_subtract_1 = T.Buffer((1190,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((1190,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1190):\n            compute_2 = T.Buffer((1190,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(14, 5):\n                cse_var_1: T.int32 = i0 * 70 + i1 * 5 + i2\n                compute_2 = T.Buffer((1190,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])", "op_args": [["subtract", "asinh", "acos"]]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          compute[((((i0 * 480) + (i1 * 240)) + (i2 * 15)) + i3)] = ceilf(data[((((i0 * 480) + (i1 * 240)) + (i2 * 15)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 2, 16, 15), \"float32\"), compute: T.Buffer((4, 2, 16, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(2, 16, 15):\n                cse_var_1: T.int32 = i0 * 480 + i1 * 240 + i2 * 15 + i3\n                compute_1 = T.Buffer((1920,), data=compute.data)\n                data_1 = T.Buffer((1920,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [4, 2, 16, 15]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 54; ++i0_i1_fused) {\n    compute[i0_i1_fused] = expf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 54; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(cosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 54; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 54; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 6, 1), \"float32\"), compute: T.Buffer((9, 6, 1), \"float32\"), T_mod: T.Buffer((9, 6, 1), \"float32\"), compute_1: T.Buffer((9, 6, 1), \"float32\"), compute_2: T.Buffer((9, 6, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((54,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(54):\n            compute_3 = T.Buffer((54,), data=compute.data)\n            compute_3[i0_i1_fused] = T.exp(ph_0_1[i0_i1_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(54):\n            T_mod_1 = T.Buffer((54,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(54):\n            compute_3 = T.Buffer((54,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(54):\n            compute_3 = T.Buffer((54,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.sin(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["exp", "cos", "mod", "asinh", "sin", "exp"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 108; ++i_j_fused) {\n    for (int32_t k = 0; k < 6; ++k) {\n      for (int32_t l = 0; l < 3; ++l) {\n        new_buffer[(((i_j_fused * 18) + (k * 3)) + l)] = data[(((i_j_fused * 18) + (k * 3)) + l)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 9, 6, 3), \"float32\"), buffer: T.Buffer((12, 9, 6, 3), \"float32\"), new_buffer: T.Buffer((12, 9, 6, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(108):\n            for k, l in T.grid(6, 3):\n                cse_var_1: T.int32 = i_j_fused * 18 + k * 3 + l\n                new_buffer_1 = T.Buffer((1944,), data=new_buffer.data)\n                data_1 = T.Buffer((1944,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [12, 9, 6, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 64; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 64; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(asinf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_1[((i0_i1_fused * 8) + i2)] = sinf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 2, 8), \"float32\"), compute: T.Buffer((4, 2, 8), \"float32\"), T_mod: T.Buffer((4, 2, 8), \"float32\"), compute_1: T.Buffer((4, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((64,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(64):\n            compute_2 = T.Buffer((64,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(64):\n            T_mod_1 = T.Buffer((64,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(8):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((64,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": [["cos", "asin", "mod", "sin"]]}{"op_name": "ceil", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 56; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = ceilf(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 2, 2, 16), \"float32\"), compute: T.Buffer((14, 2, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(56):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((896,), data=compute.data)\n                data_1 = T.Buffer((896,), data=data.data)\n                compute_1[cse_var_1] = T.ceil(data_1[cse_var_1])", "op_args": [14, 2, 2, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 27) + (i1 * 9)) + i2)] = atanf(ph_0[(((i0 * 27) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 378; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 14; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_2[(((i0_1 * 27) + (i1_1 * 9)) + i2_1)] = acosf(ph_0[(((i0_1 * 27) + (i1_1 * 9)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(ph_0[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 3, 9), \"float32\"), compute: T.Buffer((14, 3, 9), \"float32\"), compute_1: T.Buffer((14, 3, 9), \"float32\"), compute_2: T.Buffer((14, 3, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((378,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(3, 9):\n                cse_var_1: T.int32 = i0 * 27 + i1 * 9 + i2\n                compute_3 = T.Buffer((378,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(378):\n            compute_3 = T.Buffer((378,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(3, 9):\n                cse_var_2: T.int32 = i0 * 27 + i1 * 9 + i2\n                compute_3 = T.Buffer((378,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acos(ph_0_1[cse_var_2])", "op_args": [["atan", "asin", "acosh", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_2[(((i0 * 35) + (i1 * 7)) + i2)] = sinf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 65; ++i_j_fused) {\n    for (int32_t k = 0; k < 5; ++k) {\n      for (int32_t l = 0; l < 5; ++l) {\n        new_buffer[(((i_j_fused * 25) + (k * 5)) + l)] = data[(((i_j_fused * 25) + (k * 5)) + l)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 13, 5, 5), \"float32\"), buffer: T.Buffer((5, 13, 5, 5), \"float32\"), new_buffer: T.Buffer((5, 13, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(65):\n            for k, l in T.grid(5, 5):\n                cse_var_1: T.int32 = i_j_fused * 25 + k * 5 + l\n                new_buffer_1 = T.Buffer((1625,), data=new_buffer.data)\n                data_1 = T.Buffer((1625,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [5, 13, 5, 5]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[180];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 36; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n        for (int32_t ax7 = 0; ax7 < 6; ++ax7) {\n          auto_scheduler_layout_transform[((((ax0_ax1_fused_ax2_fused * 60) + (ax5 * 30)) + (ax6 * 6)) + ax7)] = ph_3[((((ax0_ax1_fused_ax2_fused * 60) + (ax5 * 30)) + (ax7 * 5)) + ax6)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 3; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        T_batch_matmul_NN[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10) + (b_outer_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 6; ++k_inner) {\n          T_batch_matmul_NN[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10) + (b_outer_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10) + (b_outer_inner * 5)) + j_outer_inner)] + (ph_0[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 12) + (b_outer_inner * 6)) + k_inner)] * auto_scheduler_layout_transform[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 60) + (b_outer_inner * 30)) + (j_outer_inner * 6)) + k_inner)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float ph_3_shared[80];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    T_batch_matmul_NN_local[b_c_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_outer_inner_init + 2)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) / 10)) < 8) {\n        ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 50) + ((int)threadIdx.x))] = ph_3[((((ax0_ax1_fused_ax2_fused_outer_outer * 100) + ((((int)threadIdx.x) / 10) * 20)) + (k_outer_outer * 10)) + (((int)threadIdx.x) % 10))];\n      }\n    }\n    __syncthreads();\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_batch_matmul_NN_local[b_c_outer_inner] = (T_batch_matmul_NN_local[b_c_outer_inner] + (ph_0[((((((((int)threadIdx.x) / 25) * 40) + (b_c_outer_inner * 20)) + (((((int)threadIdx.x) % 25) / 5) * 4)) + (k_outer_outer * 2)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) / 25) * 20) + (b_c_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN_local[(b_c_outer_inner + 2)] = (T_batch_matmul_NN_local[(b_c_outer_inner + 2)] + (ph_0[(((((((((int)threadIdx.x) / 25) * 40) + (b_c_outer_inner * 20)) + (((((int)threadIdx.x) % 25) / 5) * 4)) + (k_outer_outer * 2)) + k_inner) + 80)] * ph_3_shared[((((((((int)threadIdx.x) / 25) * 20) + (b_c_outer_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5)) + 40)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[((((((int)threadIdx.x) / 25) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 25))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 25) * 50) + (b_inner * 25)) + (((int)threadIdx.x) % 25)) + 100)] = T_batch_matmul_NN_local[(b_inner + 2)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 1, 6), \"float32\"), ph_3: T.Buffer((6, 6, 5), \"float32\"), compute: T.Buffer((6, 1, 6), \"float32\"), T_batch_matmul_NN: T.Buffer((6, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([180], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((36,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_1 = T.Buffer((36,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((180,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3):\n            for ax5, ax6, ax7 in T.grid(2, 5, 6):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 60 + ax5 * 30\n                ph_3_1 = T.Buffer((180,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax6 * 6 + ax7] = ph_3_1[cse_var_1 + ax7 * 5 + ax6]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(3):\n            T_batch_matmul_NN_1 = T.Buffer((30,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, j_outer_inner_init in T.grid(2, 5):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + b_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n            for b_outer_inner, j_outer_inner, k_inner in T.grid(2, 5, 6):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + b_outer_inner * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 12 + b_outer_inner * 6 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 60 + b_outer_inner * 30 + j_outer_inner * 6 + k_inner]", "op_args": [["asin", "batch_matmul"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 99) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 99) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_mod[(((ax0 * 99) + (ax1 * 9)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]), ph_0[(((ax0 * 99) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(11, 9):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(11, 9):\n                cse_var_2: T.int32 = ax0 * 99 + ax1 * 9 + ax2\n                T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 180; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 2) + ax2)] = (asinhf(ph_0[((ax0_ax1_fused * 2) + ax2)]) - ph_0[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 180; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 2; ++ax2_1) {\n      T_add[((ax0_ax1_fused_1 * 2) + ax2_1)] = (ph_0[((ax0_ax1_fused_1 * 2) + ax2_1)] + ph_3[((ax0_ax1_fused_1 * 2) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 360; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf((ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(180):\n            for ax2 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(180):\n            for ax2 in range(2):\n                cse_var_2: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_add_1 = T.Buffer((360,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] + ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  for (int32_t j = 0; j < 10; ++j) {\n    for (int32_t k = 0; k < 13; ++k) {\n      for (int32_t l = 0; l < 17; ++l) {\n        new_buffer[(((j * 221) + (k * 17)) + l)] = data[(((j * 221) + (k * 17)) + l)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 1105) {\n    new_buffer[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 10, 13, 17), \"float32\"), buffer: T.Buffer((1, 10, 13, 17), \"float32\"), new_buffer: T.Buffer((1, 10, 13, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for j, k, l in T.grid(10, 13, 17):\n            cse_var_1: T.int32 = j * 221 + k * 17 + l\n            new_buffer_1 = T.Buffer((2210,), data=new_buffer.data)\n            data_1 = T.Buffer((2210,), data=data.data)\n            new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [1, 10, 13, 17]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 270; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i0_i1_fused * 340) + (i2 * 20)) + i3)] = erff(data[(((i0_i1_fused * 340) + (i2 * 20)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 15, 17, 20), \"float32\"), compute: T.Buffer((18, 15, 17, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(270):\n            for i2, i3 in T.grid(17, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 340 + i2 * 20 + i3\n                compute_1 = T.Buffer((91800,), data=compute.data)\n                data_1 = T.Buffer((91800,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [18, 15, 17, 20]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 216; ++i_j_fused) {\n    for (int32_t k = 0; k < 7; ++k) {\n      for (int32_t l = 0; l < 19; ++l) {\n        new_buffer[(((i_j_fused * 133) + (k * 19)) + l)] = data[(((i_j_fused * 133) + (k * 19)) + l)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 18, 7, 19), \"float32\"), buffer: T.Buffer((12, 18, 7, 19), \"float32\"), new_buffer: T.Buffer((12, 18, 7, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(216):\n            for k, l in T.grid(7, 19):\n                cse_var_1: T.int32 = i_j_fused * 133 + k * 19 + l\n                new_buffer_1 = T.Buffer((28728,), data=new_buffer.data)\n                data_1 = T.Buffer((28728,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [12, 18, 7, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1620; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_add[(((ax0 * 180) + (ax1 * 12)) + ax2)] = (acoshf(ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]) + ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1620; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] - cosf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(15, 12):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 12 + ax2\n                T_add_1 = T.Buffer((1620,), data=T_add.data)\n                T_add_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] - T.cos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_subtract[(((ax0 * 84) + (ax1 * 7)) + ax2)] = (ph_0[(((ax0 * 84) + (ax1 * 7)) + ax2)] - ph_3[(((ax0 * 84) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 72; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 7; ++ax2_1) {\n      T_subtract_1[((ax0_ax1_fused * 7) + ax2_1)] = (ph_0[((ax0_ax1_fused * 7) + ax2_1)] - ph_3[((ax0_ax1_fused * 7) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 12, 7), \"float32\"), ph_3: T.Buffer((6, 12, 7), \"float32\"), T_subtract: T.Buffer((6, 12, 7), \"float32\"), T_subtract_1: T.Buffer((6, 12, 7), \"float32\"), compute: T.Buffer((6, 12, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((504,), data=ph_0.data)\n        ph_3_1 = T.Buffer((504,), data=ph_3.data)\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(12, 7):\n                cse_var_1: T.int32 = ax0 * 84 + ax1 * 7 + ax2\n                T_subtract_2 = T.Buffer((504,), data=T_subtract.data)\n                T_subtract_2[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(72):\n            for ax2 in range(7):\n                cse_var_2: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_subtract_2 = T.Buffer((504,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_2] = ph_0_1[cse_var_2] - ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_1 = T.Buffer((504,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["subtract", "subtract", "acosh", "acosh"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1045; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 7) + i3_s)] = erff(data[((i0_i1_fused_i2_fused * 7) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(46) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 46) + ((int)threadIdx.x)) < 7315) {\n    compute[((((int)blockIdx.x) * 46) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 46) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 11, 19, 7), \"float32\"), compute: T.Buffer((5, 11, 19, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1045):\n            for i3_s in range(7):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 7 + i3_s\n                compute_1 = T.Buffer((7315,), data=compute.data)\n                data_1 = T.Buffer((7315,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [5, 11, 19, 7]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 105; ++i_j_fused) {\n    for (int32_t k = 0; k < 8; ++k) {\n      for (int32_t l = 0; l < 9; ++l) {\n        new_buffer[(((i_j_fused * 72) + (k * 9)) + l)] = data[(((i_j_fused * 72) + (k * 9)) + l)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 7, 8, 9), \"float32\"), buffer: T.Buffer((15, 7, 8, 9), \"float32\"), new_buffer: T.Buffer((15, 7, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(105):\n            for k, l in T.grid(8, 9):\n                cse_var_1: T.int32 = i_j_fused * 72 + k * 9 + l\n                new_buffer_1 = T.Buffer((7560,), data=new_buffer.data)\n                data_1 = T.Buffer((7560,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [15, 7, 8, 9]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 75; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 75; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 75; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 1, 15), \"float32\"), ph_3: T.Buffer((5, 1, 15), \"float32\"), T_multiply: T.Buffer((5, 1, 15), \"float32\"), T_subtract: T.Buffer((5, 1, 15), \"float32\"), compute: T.Buffer((5, 1, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((75,), data=ph_0.data)\n        ph_3_1 = T.Buffer((75,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(75):\n            T_multiply_1 = T.Buffer((75,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(75):\n            T_subtract_1 = T.Buffer((75,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(75):\n            compute_1 = T.Buffer((75,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["multiply", "subtract", "abs", "asin"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 15; ++i1) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i1 * 65) + (i2 * 13)) + i3)] = erff(data[(((i1 * 65) + (i2 * 13)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) / 3)) < 325) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 15, 5, 13), \"float32\"), compute: T.Buffer((1, 15, 5, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(15, 5, 13):\n            cse_var_1: T.int32 = i1 * 65 + i2 * 13 + i3\n            compute_1 = T.Buffer((975,), data=compute.data)\n            data_1 = T.Buffer((975,), data=data.data)\n            compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [1, 15, 5, 13]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 12; ++i) {\n    for (int32_t j = 0; j < 4; ++j) {\n      for (int32_t k = 0; k < 4; ++k) {\n        for (int32_t l = 0; l < 5; ++l) {\n          new_buffer[((((i * 80) + (j * 20)) + (k * 5)) + l)] = data[((((i * 80) + (j * 20)) + (k * 5)) + l)];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 4, 4, 5), \"float32\"), buffer: T.Buffer((12, 4, 4, 5), \"float32\"), new_buffer: T.Buffer((12, 4, 4, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(12):\n            for j, k, l in T.grid(4, 4, 5):\n                cse_var_1: T.int32 = i * 80 + j * 20 + k * 5 + l\n                new_buffer_1 = T.Buffer((960,), data=new_buffer.data)\n                data_1 = T.Buffer((960,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [12, 4, 4, 5]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 396; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 396; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_1[(((i0 * 36) + (i1 * 18)) + i2)] = asinhf(ph_0[(((i0 * 36) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 396; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 18), \"float32\"), compute: T.Buffer((11, 2, 18), \"float32\"), T_divide: T.Buffer((11, 2, 18), \"float32\"), compute_1: T.Buffer((11, 2, 18), \"float32\"), compute_2: T.Buffer((11, 2, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((396,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(396):\n            compute_3 = T.Buffer((396,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(396):\n            T_divide_1 = T.Buffer((396,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(2, 18):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 18 + i2\n                compute_3 = T.Buffer((396,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(396):\n            compute_3 = T.Buffer((396,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["sin", "atanh", "divide", "asinh", "cos", "acos"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused_k_fused = 0; i_j_fused_k_fused < 748; ++i_j_fused_k_fused) {\n    for (int32_t l = 0; l < 16; ++l) {\n      new_buffer[((i_j_fused_k_fused * 16) + l)] = data[((i_j_fused_k_fused * 16) + l)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 11, 17, 16), \"float32\"), buffer: T.Buffer((4, 11, 17, 16), \"float32\"), new_buffer: T.Buffer((4, 11, 17, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused_k_fused in T.parallel(748):\n            for l in range(16):\n                cse_var_1: T.int32 = i_j_fused_k_fused * 16 + l\n                new_buffer_1 = T.Buffer((11968,), data=new_buffer.data)\n                data_1 = T.Buffer((11968,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [4, 11, 17, 16]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 540; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_1[(((i0 * 108) + (i1 * 6)) + i2)] = asinhf(acoshf(ph_0[(((i0 * 108) + (i1 * 6)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 5; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 18; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n        compute_2[(((i0_1 * 108) + (i1_1 * 6)) + i2_1)] = cosf(ph_0[(((i0_1 * 108) + (i1_1 * 6)) + i2_1)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 18, 6), \"float32\"), compute: T.Buffer((5, 18, 6), \"float32\"), compute_1: T.Buffer((5, 18, 6), \"float32\"), compute_2: T.Buffer((5, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_3 = T.Buffer((540,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(18, 6):\n                cse_var_1: T.int32 = i0 * 108 + i1 * 6 + i2\n                compute_3 = T.Buffer((540,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(T.acosh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(18, 6):\n                cse_var_2: T.int32 = i0 * 108 + i1 * 6 + i2\n                compute_3 = T.Buffer((540,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2])", "op_args": [["asinh", "acosh", "asinh", "cos"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2176; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 4; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3_s)] = erff(data[((i0_i1_fused_i2_fused * 4) + i3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 17, 4), \"float32\"), compute: T.Buffer((8, 16, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2176):\n            for i3_s in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3_s\n                compute_1 = T.Buffer((8704,), data=compute.data)\n                data_1 = T.Buffer((8704,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [8, 16, 17, 4]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = atanf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2156; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] * fabsf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2156; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 14, 14), \"float32\"), compute: T.Buffer((11, 14, 14), \"float32\"), compute_1: T.Buffer((11, 14, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2156,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2156):\n            compute_2 = T.Buffer((2156,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] * T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2156):\n            compute_2 = T.Buffer((2156,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["abs", "multiply", "cos", "acos"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i = 0; i < 3; ++i) {\n    for (int32_t j = 0; j < 4; ++j) {\n      for (int32_t k = 0; k < 14; ++k) {\n        for (int32_t l = 0; l < 4; ++l) {\n          new_buffer[((((i * 224) + (j * 56)) + (k * 4)) + l)] = data[((((i * 224) + (j * 56)) + (k * 4)) + l)];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  new_buffer[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 4, 14, 4), \"float32\"), buffer: T.Buffer((3, 4, 14, 4), \"float32\"), new_buffer: T.Buffer((3, 4, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i in T.parallel(3):\n            for j, k, l in T.grid(4, 14, 4):\n                cse_var_1: T.int32 = i * 224 + j * 56 + k * 4 + l\n                new_buffer_1 = T.Buffer((672,), data=new_buffer.data)\n                data_1 = T.Buffer((672,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [3, 4, 14, 4]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n        compute[(((i0_i1_fused * 135) + (i2 * 15)) + i3_s)] = erff(data[(((i0_i1_fused * 135) + (i2 * 15)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 2025) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 20, 9, 15), \"float32\"), compute: T.Buffer((12, 20, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(240):\n            for i2, i3_s in T.grid(9, 15):\n                cse_var_1: T.int32 = i0_i1_fused * 135 + i2 * 15 + i3_s\n                compute_1 = T.Buffer((32400,), data=compute.data)\n                data_1 = T.Buffer((32400,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [12, 20, 9, 15]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = asinf(asinhf(ph_0[((i0_i1_fused * 2) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 360; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 180; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 2) + i2_1)] = asinf((ph_0[((i0_i1_fused_1 * 2) + i2_1)] - ph_3[((i0_i1_fused_1 * 2) + i2_1)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((360,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(T.asinh(ph_0_1[cse_var_1]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_1 = T.Buffer((360,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((360,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 154; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = atanhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1386; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 9), \"float32\"), compute: T.Buffer((14, 11, 9), \"float32\"), T_mod: T.Buffer((14, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(154):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((1386,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1386):\n            T_mod_1 = T.Buffer((1386,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])", "op_args": [["atanh", "acos", "mod"]]}{"op_name": "fifo_buffer", "c_code": "void default_function_kernel(float* data, float* new_buffer) {\n  #pragma omp parallel for\n  for (int32_t i_j_fused = 0; i_j_fused < 190; ++i_j_fused) {\n    for (int32_t k = 0; k < 20; ++k) {\n      for (int32_t l = 0; l < 19; ++l) {\n        new_buffer[(((i_j_fused * 380) + (k * 19)) + l)] = data[(((i_j_fused * 380) + (k * 19)) + l)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ new_buffer) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9025) {\n    new_buffer[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 19, 20, 19), \"float32\"), buffer: T.Buffer((10, 19, 20, 19), \"float32\"), new_buffer: T.Buffer((10, 19, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i_j_fused in T.parallel(190):\n            for k, l in T.grid(20, 19):\n                cse_var_1: T.int32 = i_j_fused * 380 + k * 19 + l\n                new_buffer_1 = T.Buffer((72200,), data=new_buffer.data)\n                data_1 = T.Buffer((72200,), data=data.data)\n                new_buffer_1[cse_var_1] = data_1[cse_var_1]", "op_args": [10, 19, 20, 19]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n          compute[((((i0 * 2800) + (i1 * 140)) + (i2 * 7)) + i3_s)] = erff(data[((((i0 * 2800) + (i1 * 140)) + (i2 * 7)) + i3_s)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 20, 20, 7), \"float32\"), compute: T.Buffer((16, 20, 20, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            for i1, i2, i3_s in T.grid(20, 20, 7):\n                cse_var_1: T.int32 = i0 * 2800 + i1 * 140 + i2 * 7 + i3_s\n                compute_1 = T.Buffer((44800,), data=compute.data)\n                data_1 = T.Buffer((44800,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [16, 20, 20, 7]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1620; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_add[(((ax0 * 180) + (ax1 * 12)) + ax2)] = (acoshf(ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]) + ph_0[(((ax0 * 180) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 180) + (i1 * 12)) + i2)] = asinhf((ph_0[(((i0 * 180) + (i1 * 12)) + i2)] - cosf(ph_0[(((i0 * 180) + (i1 * 12)) + i2)])));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(15, 12):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 12 + ax2\n                T_add_1 = T.Buffer((1620,), data=T_add.data)\n                T_add_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(15, 12):\n                cse_var_2: T.int32 = i0 * 180 + i1 * 12 + i2\n                compute_2 = T.Buffer((1620,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2] - T.cos(ph_0_1[cse_var_2]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = expf((ph_0[((i0_i1_fused * 19) + i2)] + asinhf(ph_0[((i0_i1_fused * 19) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 12, 19), \"float32\"), compute: T.Buffer((20, 12, 19), \"float32\"), compute_1: T.Buffer((20, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4560,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((4560,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4560):\n            compute_2 = T.Buffer((4560,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["asinh", "add", "exp", "acosh"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      compute[((i0_i1_fused * 10) + i3)] = erff(data[((i0_i1_fused * 10) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 5, 1, 10), \"float32\"), compute: T.Buffer((20, 5, 1, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(100):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i3\n                compute_1 = T.Buffer((1000,), data=compute.data)\n                data_1 = T.Buffer((1000,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [20, 5, 1, 10]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_add[(((ax0 * 108) + (ax1 * 18)) + ax2)] = (ph_0[(((ax0 * 108) + (ax1 * 18)) + ax2)] + acosf((ph_0[(((ax0 * 108) + (ax1 * 18)) + ax2)] / asinf(ph_0[(((ax0 * 108) + (ax1 * 18)) + ax2)]))));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + acosf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 6, 18), \"float32\"), T_add: T.Buffer((20, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(6, 18):\n                cse_var_1: T.int32 = ax0 * 108 + ax1 * 18 + ax2\n                T_add_1 = T.Buffer((2160,), data=T_add.data)\n                ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1] / T.asin(ph_0_1[cse_var_1]))", "op_args": [["asin", "divide", "acos", "add"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_6) {\n  float auto_scheduler_layout_transform[1275];\n  float T_batch_matmul_NN[595];\n  for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 5; ++ax5) {\n      for (int32_t ax9 = 0; ax9 < 15; ++ax9) {\n        auto_scheduler_layout_transform[(((ax4 * 75) + (ax5 * 15)) + ax9)] = ph_6[(((ax4 * 75) + (ax9 * 5)) + ax5)];\n      }\n    }\n  }\n  for (int32_t b_outer_outer_inner = 0; b_outer_outer_inner < 17; ++b_outer_outer_inner) {\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 7; ++i_outer_outer_inner) {\n      for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 5; ++j_outer_outer_inner) {\n        T_batch_matmul_NN[(((b_outer_outer_inner * 35) + (i_outer_outer_inner * 5)) + j_outer_outer_inner)] = 0.000000e+00f;\n        for (int32_t k_inner = 0; k_inner < 15; ++k_inner) {\n          T_batch_matmul_NN[(((b_outer_outer_inner * 35) + (i_outer_outer_inner * 5)) + j_outer_outer_inner)] = (T_batch_matmul_NN[(((b_outer_outer_inner * 35) + (i_outer_outer_inner * 5)) + j_outer_outer_inner)] + (ph_0[(((b_outer_outer_inner * 105) + (i_outer_outer_inner * 15)) + k_inner)] * auto_scheduler_layout_transform[(((b_outer_outer_inner * 75) + (j_outer_outer_inner * 15)) + k_inner)]));\n        }\n      }\n    }\n  }\n  for (int32_t i0_inner = 0; i0_inner < 17; ++i0_inner) {\n    for (int32_t i1_inner = 0; i1_inner < 7; ++i1_inner) {\n      for (int32_t i2_inner_s = 0; i2_inner_s < 5; ++i2_inner_s) {\n        compute[(((i0_inner * 35) + (i1_inner * 5)) + i2_inner_s)] = atanf(T_batch_matmul_NN[(((i0_inner * 35) + (i1_inner * 5)) + i2_inner_s)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1785; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1785; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused], ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ compute) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(T_batch_matmul_NN[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[24];\n  __shared__ float T_mod_shared[192];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 3; ++b_c_outer_inner_init) {\n    for (int j_c_outer_inner_init = 0; j_c_outer_inner_init < 8; ++j_c_outer_inner_init) {\n      T_batch_matmul_NN_local[((b_c_outer_inner_init * 8) + j_c_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 96; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    T_mod_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x))] = fmodf(ph_0[((((((int)blockIdx.x) >> 2) * 192) + (ax0_ax1_fused_ax2_fused_outer_outer * 2)) + ((int)threadIdx.x))], ph_3[((((((int)blockIdx.x) >> 2) * 192) + (ax0_ax1_fused_ax2_fused_outer_outer * 2)) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  for (int b_c_outer_inner = 0; b_c_outer_inner < 3; ++b_c_outer_inner) {\n    for (int j_c_outer_inner = 0; j_c_outer_inner < 8; ++j_c_outer_inner) {\n      for (int k_inner = 0; k_inner < 8; ++k_inner) {\n        T_batch_matmul_NN_local[((b_c_outer_inner * 8) + j_c_outer_inner)] = (T_batch_matmul_NN_local[((b_c_outer_inner * 8) + j_c_outer_inner)] + (ph_0[((((((((int)blockIdx.x) >> 2) * 192) + (b_c_outer_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) * 8)) + k_inner)] * T_mod_shared[(((b_c_outer_inner * 64) + (k_inner * 8)) + j_c_outer_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 3; ++b_inner) {\n    for (int j_inner = 0; j_inner < 8; ++j_inner) {\n      T_batch_matmul_NN[((((((((int)blockIdx.x) >> 2) * 192) + (b_inner * 64)) + ((((int)blockIdx.x) & 3) * 16)) + (((int)threadIdx.x) * 8)) + j_inner)] = T_batch_matmul_NN_local[((b_inner * 8) + j_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ compute) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(T_batch_matmul_NN[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 7, 15), \"float32\"), ph_3: T.Buffer((17, 7, 15), \"float32\"), ph_6: T.Buffer((17, 15, 5), \"float32\"), T_mod: T.Buffer((17, 7, 15), \"float32\"), compute: T.Buffer((17, 7, 5), \"float32\"), compute_1: T.Buffer((17, 7, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1275], \"float32\", \"global\")\n        T_batch_matmul_NN = T.allocate([595], \"float32\", \"global\")\n        auto_scheduler_layout_transform_1 = T.Buffer((1275,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax9 in T.grid(17, 5, 15):\n            cse_var_1: T.int32 = ax4 * 75\n            ph_6_1 = T.Buffer((1275,), data=ph_6.data)\n            auto_scheduler_layout_transform_1[cse_var_1 + ax5 * 15 + ax9] = ph_6_1[cse_var_1 + ax9 * 5 + ax5]\n        T_batch_matmul_NN_1 = T.Buffer((595,), data=T_batch_matmul_NN)\n        ph_0_1 = T.Buffer((1785,), data=ph_0.data)\n        for b_outer_outer_inner, i_outer_outer_inner, j_outer_outer_inner in T.grid(17, 7, 5):\n            T_batch_matmul_NN_1[b_outer_outer_inner * 35 + i_outer_outer_inner * 5 + j_outer_outer_inner] = T.float32(0)\n            for k_inner in range(15):\n                cse_var_2: T.int32 = b_outer_outer_inner * 35 + i_outer_outer_inner * 5 + j_outer_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[b_outer_outer_inner * 105 + i_outer_outer_inner * 15 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_inner * 75 + j_outer_outer_inner * 15 + k_inner]\n        for i0_inner, i1_inner, i2_inner_s in T.grid(17, 7, 5):\n            cse_var_3: T.int32 = i0_inner * 35 + i1_inner * 5 + i2_inner_s\n            compute_2 = T.Buffer((595,), data=compute.data)\n            compute_2[cse_var_3] = T.atan(T_batch_matmul_NN_1[cse_var_3])\n        ph_3_1 = T.Buffer((1785,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1785):\n            T_mod_1 = T.Buffer((1785,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1785):\n            compute_2 = T.Buffer((1785,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))", "op_args": [["mod", "mod", "batch_matmul", "atan", "acosh"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 187; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n        compute[(((i0_i1_fused * 18) + (i2 * 3)) + i3_s)] = erff(data[(((i0_i1_fused * 18) + (i2 * 3)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) / 6)) < 561) {\n    compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 11, 6, 3), \"float32\"), compute: T.Buffer((17, 11, 6, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(187):\n            for i2, i3_s in T.grid(6, 3):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2 * 3 + i3_s\n                compute_1 = T.Buffer((3366,), data=compute.data)\n                data_1 = T.Buffer((3366,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [17, 11, 6, 3]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_1[(((i0 * 35) + (i1 * 7)) + i2)] = sinf(ph_0[(((i0 * 35) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), ph_3: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            ph_3_1 = T.Buffer((315,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(5, 7):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 7 + i2\n                compute_2 = T.Buffer((315,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])", "op_args": [["mod", "atan", "sin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 315; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 143; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 7; ++i3_s) {\n        compute[(((i0_i1_fused * 70) + (i2 * 7)) + i3_s)] = erff(data[(((i0_i1_fused * 70) + (i2 * 7)) + i3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 13, 10, 7), \"float32\"), compute: T.Buffer((11, 13, 10, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(143):\n            for i2, i3_s in T.grid(10, 7):\n                cse_var_1: T.int32 = i0_i1_fused * 70 + i2 * 7 + i3_s\n                compute_1 = T.Buffer((10010,), data=compute.data)\n                data_1 = T.Buffer((10010,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [11, 13, 10, 7]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_divide_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 208; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_add[((ax0_ax1_fused * 15) + ax2)] = (ph_0[((ax0_ax1_fused * 15) + ax2)] + ph_3[((ax0_ax1_fused * 15) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 15; ++ax2_1) {\n        T_divide[(((ax0 * 195) + (ax1 * 15)) + ax2_1)] = (ph_0[(((ax0 * 195) + (ax1 * 15)) + ax2_1)] / ph_3[(((ax0 * 195) + (ax1 * 15)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3120; ++ax0_ax1_fused_ax2_fused) {\n    T_divide_1[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] / ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((int)blockIdx.x)] = (__sinf(ph_0[((int)blockIdx.x)]) / ph_0[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 13, 15), \"float32\"), ph_3: T.Buffer((16, 13, 15), \"float32\"), T_add: T.Buffer((16, 13, 15), \"float32\"), T_divide: T.Buffer((16, 13, 15), \"float32\"), T_divide_1: T.Buffer((16, 13, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3120,), data=ph_0.data)\n        ph_3_1 = T.Buffer((3120,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(208):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_add_1 = T.Buffer((3120,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(13, 15):\n                cse_var_2: T.int32 = ax0 * 195 + ax1 * 15 + ax2\n                T_divide_2 = T.Buffer((3120,), data=T_divide.data)\n                T_divide_2[cse_var_2] = ph_0_1[cse_var_2] / ph_3_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(3120):\n            T_divide_2 = T.Buffer((3120,), data=T_divide_1.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["add", "divide", "sin", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_5) {\n  float auto_scheduler_layout_transform[120];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 120; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 120; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n        for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n          for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n            auto_scheduler_layout_transform[(((((ax0_ax1_fused_ax2_fused_1 * 60) + (ax4 * 30)) + (ax5 * 15)) + (ax6 * 3)) + ax7)] = ph_5[(((((ax0_ax1_fused_ax2_fused_1 * 60) + (ax5 * 30)) + (ax4 * 15)) + (ax7 * 5)) + ax6)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 50) + (b_outer_inner_init * 25)) + (i_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t k_inner = 0; k_inner < 3; ++k_inner) {\n            for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n              T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 50) + (b_outer_inner * 25)) + (i_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 50) + (b_outer_inner * 25)) + (i_inner * 5)) + j_outer_inner)] + (ph_0[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 60) + (b_outer_inner * 30)) + (i_inner * 6)) + (k_outer * 3)) + k_inner)] * auto_scheduler_layout_transform[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 60) + (k_outer * 30)) + (b_outer_inner * 15)) + (j_outer_inner * 3)) + k_inner)]));\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float ph_5_shared[80];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_outer_inner_init * 2) + b_c_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) / 10)) < 8) {\n        ph_5_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 50) + ((int)threadIdx.x))] = ph_5[((((ax0_ax1_fused_ax2_fused_outer_outer * 100) + ((((int)threadIdx.x) / 10) * 20)) + (k_outer_outer * 10)) + (((int)threadIdx.x) % 10))];\n      }\n    }\n    __syncthreads();\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n          T_batch_matmul_NN_local[((b_c_outer_inner * 2) + b_c_inner)] = (T_batch_matmul_NN_local[((b_c_outer_inner * 2) + b_c_inner)] + (ph_0[(((((((((int)threadIdx.x) / 25) * 80) + (b_c_outer_inner * 40)) + (b_c_inner * 20)) + (((((int)threadIdx.x) % 25) / 5) * 4)) + (k_outer_outer * 2)) + k_inner)] * ph_5_shared[((((((((int)threadIdx.x) / 25) * 40) + (b_c_outer_inner * 20)) + (b_c_inner * 10)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        }\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    T_batch_matmul_NN[((((((int)threadIdx.x) / 25) * 100) + (b_inner * 25)) + (((int)threadIdx.x) % 25))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 5, 6), \"float32\"), ph_5: T.Buffer((4, 6, 5), \"float32\"), compute: T.Buffer((4, 5, 6), \"float32\"), T_add: T.Buffer((4, 5, 6), \"float32\"), T_batch_matmul_NN: T.Buffer((4, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([120], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((120,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(120):\n            compute_1 = T.Buffer((120,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(120):\n            T_add_1 = T.Buffer((120,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((120,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2):\n            for ax4, ax5, ax6, ax7 in T.grid(2, 2, 5, 3):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 60\n                ph_5_1 = T.Buffer((120,), data=ph_5.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 30 + ax5 * 15 + ax6 * 3 + ax7] = ph_5_1[cse_var_1 + ax5 * 30 + ax4 * 15 + ax7 * 5 + ax6]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((100,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, j_outer_inner_init, i_inner_init in T.grid(2, 5, 5):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 50 + b_outer_inner_init * 25 + i_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, j_outer_inner, k_inner, i_inner in T.grid(2, 2, 5, 3, 5):\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 60\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 50 + b_outer_inner * 25 + i_inner * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_3 + b_outer_inner * 30 + i_inner * 6 + k_outer * 3 + k_inner] * auto_scheduler_layout_transform_1[cse_var_3 + k_outer * 30 + b_outer_inner * 15 + j_outer_inner * 3 + k_inner]", "op_args": [["ceil", "abs", "add", "batch_matmul"]]}{"op_name": "erf", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 120; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = erff(data[((i0_i1_fused_i2_fused * 19) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = erff(data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 5, 6, 19), \"float32\"), compute: T.Buffer((4, 5, 6, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(120):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((2280,), data=compute.data)\n                data_1 = T.Buffer((2280,), data=data.data)\n                compute_1[cse_var_1] = T.erf(data_1[cse_var_1])", "op_args": [4, 5, 6, 19]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 360; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_1[(((i0 * 18) + (i1 * 2)) + i2)] = asinf((ph_0[(((i0 * 18) + (i1 * 2)) + i2)] - ph_3[(((i0 * 18) + (i1 * 2)) + i2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 2), \"float32\"), ph_3: T.Buffer((20, 9, 2), \"float32\"), T_add: T.Buffer((20, 9, 2), \"float32\"), T_subtract: T.Buffer((20, 9, 2), \"float32\"), compute: T.Buffer((20, 9, 2), \"float32\"), compute_1: T.Buffer((20, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_1 = T.Buffer((360,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(9, 2):\n                cse_var_1: T.int32 = i0 * 18 + i1 * 2 + i2\n                compute_2 = T.Buffer((360,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1] - ph_3_1[cse_var_1])", "op_args": [["subtract", "add", "asinh", "subtract", "asin", "asin"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 385; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 77) + (i1 * 11)) + i2)] = atanf(ph_0[(((i0 * 77) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 5; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n        compute_1[(((i0_1 * 77) + (i1_1 * 11)) + i2_1)] = acosf(sinf(ph_0[(((i0_1 * 77) + (i1_1 * 11)) + i2_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 11), \"float32\"), ph_3: T.Buffer((5, 7, 11), \"float32\"), T_mod: T.Buffer((5, 7, 11), \"float32\"), compute: T.Buffer((5, 7, 11), \"float32\"), compute_1: T.Buffer((5, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_mod_1 = T.Buffer((385,), data=T_mod.data)\n            ph_3_1 = T.Buffer((385,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(7, 11):\n                cse_var_1: T.int32 = i0 * 77 + i1 * 11 + i2\n                compute_2 = T.Buffer((385,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(7, 11):\n                cse_var_2: T.int32 = i0 * 77 + i1 * 11 + i2\n                compute_2 = T.Buffer((385,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(T.sin(ph_0_1[cse_var_2]))", "op_args": [["mod", "atan", "sin", "acos"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1620; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 135; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_add[((ax0_ax1_fused * 12) + ax2)] = (acoshf(ph_0[((ax0_ax1_fused * 12) + ax2)]) + ph_0[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_1[((i0_i1_fused * 12) + i2)] = asinhf((ph_0[((i0_i1_fused * 12) + i2)] - cosf(ph_0[((i0_i1_fused * 12) + i2)])));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 12), \"float32\"), compute: T.Buffer((9, 15, 12), \"float32\"), T_add: T.Buffer((9, 15, 12), \"float32\"), compute_1: T.Buffer((9, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1620):\n            compute_2 = T.Buffer((1620,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(135):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_add_1 = T.Buffer((1620,), data=T_add.data)\n                T_add_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((1620,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2] - T.cos(ph_0_1[cse_var_2]))", "op_args": [["cos", "acosh", "add", "cos", "subtract", "asinh"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 72; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 72; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t i1 = 0; i1 < 4; ++i1) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_2[((i1 * 18) + i2)] = expf(ph_0[((i1 * 18) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 18), \"float32\"), compute: T.Buffer((1, 4, 18), \"float32\"), compute_1: T.Buffer((1, 4, 18), \"float32\"), compute_2: T.Buffer((1, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((72,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i1, i2 in T.grid(4, 18):\n            cse_var_1: T.int32 = i1 * 18 + i2\n            compute_3 = T.Buffer((72,), data=compute_2.data)\n            compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])", "op_args": [["atanh", "ceil", "acos", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0 * 16) + i2)] = ceilf(ph_0[((i0 * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 32; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 32; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanhf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 2; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_3[((i0_i1_fused * 16) + i2_1)] = expf((ph_0[((i0_i1_fused * 16) + i2_1)] - ph_3[((i0_i1_fused * 16) + i2_1)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 1, 16), \"float32\"), ph_3: T.Buffer((2, 1, 16), \"float32\"), compute: T.Buffer((2, 1, 16), \"float32\"), compute_1: T.Buffer((2, 1, 16), \"float32\"), compute_2: T.Buffer((2, 1, 16), \"float32\"), compute_3: T.Buffer((2, 1, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0 * 16 + i2\n                compute_4 = T.Buffer((32,), data=compute.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_4 = T.Buffer((32,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_4 = T.Buffer((32,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(2):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((32,), data=compute_3.data)\n                ph_3_1 = T.Buffer((32,), data=ph_3.data)\n                compute_4[cse_var_2] = T.exp(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])", "op_args": [["subtract", "ceil", "atanh", "atan", "atanh", "exp"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 17; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 20) + ax2)] = (ph_0[((ax0_ax1_fused * 20) + ax2)] * ph_3[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i0 * 20) + i2)] = expf(fmodf(ph_0[((i0 * 20) + i2)], asinf(ph_0[((i0 * 20) + i2)])));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 1, 20), \"float32\"), ph_3: T.Buffer((17, 1, 20), \"float32\"), T_multiply: T.Buffer((17, 1, 20), \"float32\"), compute: T.Buffer((17, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((340,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(17):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_multiply_1 = T.Buffer((340,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((340,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0 in T.parallel(17):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0 * 20 + i2\n                compute_1 = T.Buffer((340,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(T.truncmod(ph_0_1[cse_var_2], T.asin(ph_0_1[cse_var_2])))", "op_args": [["multiply", "asin", "mod", "exp"]]}{"op_name": "exp", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 52; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = expf(data[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(26) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 26) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 13, 8, 1), \"float32\"), compute: T.Buffer((4, 13, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(52):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_1 = T.Buffer((416,), data=compute.data)\n                data_1 = T.Buffer((416,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])", "op_args": [4, 13, 8, 1]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = atanhf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_mod[(((ax0 * 35) + (ax1 * 7)) + ax2)] = fmodf(acosf(ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)]), ph_0[(((ax0 * 35) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(5, 7):\n                cse_var_2: T.int32 = ax0 * 35 + ax1 * 7 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.acos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "exp", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4160; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 18) + i3)] = expf(data[((i0_i1_fused_i2_fused * 18) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __expf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 20, 16, 18), \"float32\"), compute: T.Buffer((13, 20, 16, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4160):\n            for i3 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                compute_1 = T.Buffer((74880,), data=compute.data)\n                data_1 = T.Buffer((74880,), data=data.data)\n                compute_1[cse_var_1] = T.exp(data_1[cse_var_1])", "op_args": [13, 20, 16, 18]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] + asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 48; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 8), \"float32\"), compute: T.Buffer((3, 2, 8), \"float32\"), compute_1: T.Buffer((3, 2, 8), \"float32\"), T_divide: T.Buffer((3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] + T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_2 = T.Buffer((48,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_divide_1 = T.Buffer((48,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]", "op_args": [["asinh", "add", "exp", "acosh", "asin", "divide"]]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = expf(fmodf(ph_0[((i0_i1_fused * 15) + i2)], (ph_0[((i0_i1_fused * 15) + i2)] + acosf((ph_0[((i0_i1_fused * 15) + i2)] / asinf(ph_0[((i0_i1_fused * 15) + i2)]))))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + acosf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))))));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 15), \"float32\"), compute: T.Buffer((8, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_1 = T.Buffer((720,), data=compute.data)\n                ph_0_1 = T.Buffer((720,), data=ph_0.data)\n                compute_1[cse_var_1] = T.exp(T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1] / T.asin(ph_0_1[cse_var_1]))))", "op_args": [["asin", "divide", "acos", "add", "mod", "exp"]]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 560; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 7, 10, 1), \"float32\"), T_fast_erf: T.Buffer((8, 7, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(560):\n            T_fast_erf_1 = T.Buffer((560,), data=T_fast_erf.data)\n            data_1 = T.Buffer((560,), data=data.data)\n            T_fast_erf_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [8, 7, 10, 1]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 798; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 133; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0_i1_fused * 6) + i2)] = cosf(ceilf(ph_0[((i0_i1_fused * 6) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n        compute_2[(((i0 * 114) + (i1 * 6)) + i2_1)] = asinf(ph_0[(((i0 * 114) + (i1 * 6)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 798; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 19, 6), \"float32\"), compute: T.Buffer((7, 19, 6), \"float32\"), compute_1: T.Buffer((7, 19, 6), \"float32\"), compute_2: T.Buffer((7, 19, 6), \"float32\"), compute_3: T.Buffer((7, 19, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((798,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(798):\n            compute_4 = T.Buffer((798,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(133):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_4 = T.Buffer((798,), data=compute_1.data)\n                compute_4[cse_var_1] = T.cos(T.ceil(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(19, 6):\n                cse_var_2: T.int32 = i0 * 114 + i1 * 6 + i2\n                compute_4 = T.Buffer((798,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(798):\n            compute_4 = T.Buffer((798,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))", "op_args": [["sin", "ceil", "cos", "asin", "atan", "atanh"]]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        T_fast_erf[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)] = ((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 72) + (ax2 * 12)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 12, 6, 12), \"float32\"), T_fast_erf: T.Buffer((7, 12, 6, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(84):\n            for ax2, ax3 in T.grid(6, 12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 72 + ax2 * 12 + ax3\n                T_fast_erf_1 = T.Buffer((6048,), data=T_fast_erf.data)\n                data_1 = T.Buffer((6048,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [7, 12, 6, 12]}{"op_name": "topology_expansion", "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 315; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 45; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = atanf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 315; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 5, 7), \"float32\"), compute: T.Buffer((9, 5, 7), \"float32\"), T_mod: T.Buffer((9, 5, 7), \"float32\"), compute_1: T.Buffer((9, 5, 7), \"float32\"), compute_2: T.Buffer((9, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(315):\n            T_mod_1 = T.Buffer((315,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(45):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((315,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_3 = T.Buffer((315,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])", "op_args": [["atanh", "acos", "mod", "atan", "sin"]]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_fast_erf[(((ax0 * 54) + (ax1 * 9)) + ax2)] = ((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0 * 54) + (ax1 * 9)) + ax2)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 6, 9, 1), \"float32\"), T_fast_erf: T.Buffer((5, 6, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(6, 9):\n                cse_var_1: T.int32 = ax0 * 54 + ax1 * 9 + ax2\n                T_fast_erf_1 = T.Buffer((270,), data=T_fast_erf.data)\n                data_1 = T.Buffer((270,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [5, 6, 9, 1]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          T_fast_erf[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)] = ((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((ax0 * 260) + (ax1 * 20)) + (ax2 * 10)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 13, 2, 10), \"float32\"), T_fast_erf: T.Buffer((4, 13, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            for ax1, ax2, ax3 in T.grid(13, 2, 10):\n                cse_var_1: T.int32 = ax0 * 260 + ax1 * 20 + ax2 * 10 + ax3\n                T_fast_erf_1 = T.Buffer((1040,), data=T_fast_erf.data)\n                data_1 = T.Buffer((1040,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [4, 13, 2, 10]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 51; ++ax0_ax1_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_fast_erf[((ax0_ax1_fused * 16) + ax3)] = ((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[((ax0_ax1_fused * 16) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 3, 1, 16), \"float32\"), T_fast_erf: T.Buffer((17, 3, 1, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(51):\n            for ax3 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax3\n                T_fast_erf_1 = T.Buffer((816,), data=T_fast_erf.data)\n                data_1 = T.Buffer((816,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [17, 3, 1, 16]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 4536; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_erf[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f) * max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 18, 3, 6), \"float32\"), T_fast_erf: T.Buffer((14, 18, 3, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(4536):\n            T_fast_erf_1 = T.Buffer((4536,), data=T_fast_erf.data)\n            data_1 = T.Buffer((4536,), data=data.data)\n            T_fast_erf_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [14, 18, 3, 6]}{"op_name": "fast_erf", "c_code": "void default_function_kernel(float* T_fast_erf, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 38; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n        T_fast_erf[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)] = ((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f) * max(min(data[(((ax0_ax1_fused * 198) + (ax2 * 18)) + ax3)], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ T_fast_erf, float* __restrict__ data) {\n  T_fast_erf[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = ((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -2.726142e-10f) + 2.770681e-08f)) + -2.101024e-06f)) + -5.692506e-05f)) + -7.349906e-04f)) + -2.954600e-03f)) + -1.609603e-02f)) / (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * (((max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f) * max(min(data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))], 4.000000e+00f), -4.000000e+00f)) * -1.456607e-05f) + -2.133740e-04f)) + -1.682827e-03f)) + -7.373329e-03f)) + -1.426474e-02f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 19, 11, 18), \"float32\"), T_fast_erf: T.Buffer((2, 19, 11, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(38):\n            for ax2, ax3 in T.grid(11, 18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 198 + ax2 * 18 + ax3\n                T_fast_erf_1 = T.Buffer((7524,), data=T_fast_erf.data)\n                data_1 = T.Buffer((7524,), data=data.data)\n                T_fast_erf_1[cse_var_1] = T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-2.7261423674040941e-10) + T.float32(2.7706814620387377e-08)) + T.float32(-2.101023937939317e-06)) + T.float32(-5.6925062381196767e-05)) + T.float32(-0.00073499063728377223)) + T.float32(-0.0029545999132096767)) + T.float32(-0.016096033155918121)) / (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * (T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.max(T.min(data_1[cse_var_1], T.float32(4)), T.float32(-4)) * T.float32(-1.4566071513399947e-05) + T.float32(-0.00021337404905352741)) + T.float32(-0.001682827016338706)) + T.float32(-0.0073733292520046234)) + T.float32(-0.014264739118516445))", "op_args": [2, 19, 11, 18]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 126; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      for (int32_t ax3_s = 0; ax3_s < 14; ++ax3_s) {\n          int32_t v_ = ((int32_t)(floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_fast_exp[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[(((ax0_ax1_fused * 280) + (ax2 * 14)) + ax3_s)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 14, 20, 14), \"float32\"), T_fast_exp: T.Buffer((9, 14, 20, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(126):\n            for ax2, ax3_s in T.grid(20, 14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 280 + ax2 * 14 + ax3_s\n                T_fast_exp_1 = T.Buffer((35280,), data=T_fast_exp.data)\n                data_1 = T.Buffer((35280,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [9, 14, 20, 14]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 156; ++ax0_ax1_fused_ax2_fused) {\n      int32_t v_ = ((int32_t)(floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[ax0_ax1_fused_ax2_fused] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 12)) < 13) {\n      int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 13, 1), \"float32\"), T_fast_exp: T.Buffer((6, 2, 13, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(156):\n            T_fast_exp_1 = T.Buffer((156,), data=T_fast_exp.data)\n            data_1 = T.Buffer((156,), data=data.data)\n            T_fast_exp_1[ax0_ax1_fused_ax2_fused] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[ax0_ax1_fused_ax2_fused])", "op_args": [6, 2, 13, 1]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n            int32_t v_ = ((int32_t)(floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_fast_exp[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((ax0 * 4400) + (ax1 * 220)) + (ax2 * 20)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 3025) {\n      int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 20, 11, 20), \"float32\"), T_fast_exp: T.Buffer((11, 20, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(11):\n            for ax1, ax2, ax3 in T.grid(20, 11, 20):\n                cse_var_1: T.int32 = ax0 * 4400 + ax1 * 220 + ax2 * 20 + ax3\n                T_fast_exp_1 = T.Buffer((48400,), data=T_fast_exp.data)\n                data_1 = T.Buffer((48400,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [11, 20, 11, 20]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 75; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused * 7) + ax2)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 7) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 5)) < 105) {\n      int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 5, 7, 1), \"float32\"), T_fast_exp: T.Buffer((15, 5, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(75):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_fast_exp_1 = T.Buffer((525,), data=T_fast_exp.data)\n                data_1 = T.Buffer((525,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [15, 5, 7, 1]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2240; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3_s = 0; ax3_s < 10; ++ax3_s) {\n        int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused_ax2_fused * 10) + ax3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 14, 10), \"float32\"), T_fast_exp: T.Buffer((8, 20, 14, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(2240):\n            for ax3_s in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 10 + ax3_s\n                T_fast_exp_1 = T.Buffer((22400,), data=T_fast_exp.data)\n                data_1 = T.Buffer((22400,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [8, 20, 14, 10]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2548; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3_s = 0; ax3_s < 14; ++ax3_s) {\n        int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused_ax2_fused * 14) + ax3_s)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 4459) {\n      int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 14, 14, 14), \"float32\"), T_fast_exp: T.Buffer((13, 14, 14, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(2548):\n            for ax3_s in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 14 + ax3_s\n                T_fast_exp_1 = T.Buffer((35672,), data=T_fast_exp.data)\n                data_1 = T.Buffer((35672,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [13, 14, 14, 14]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1008; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n      int32_t v_ = ((int32_t)(floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[ax0_ax1_fused_ax2_fused_ax3_fused] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 3, 3, 8), \"float32\"), T_fast_exp: T.Buffer((14, 3, 3, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1008):\n            T_fast_exp_1 = T.Buffer((1008,), data=T_fast_exp.data)\n            data_1 = T.Buffer((1008,), data=data.data)\n            T_fast_exp_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [14, 3, 3, 8]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 2002; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n      int32_t v_ = ((int32_t)(floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_fast_exp[ax0_ax1_fused_ax2_fused_ax3_fused] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[ax0_ax1_fused_ax2_fused_ax3_fused], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[ax0_ax1_fused_ax2_fused_ax3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 1, 11, 13), \"float32\"), T_fast_exp: T.Buffer((14, 1, 11, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(2002):\n            T_fast_exp_1 = T.Buffer((2002,), data=T_fast_exp.data)\n            data_1 = T.Buffer((2002,), data=data.data)\n            T_fast_exp_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[ax0_ax1_fused_ax2_fused_ax3_fused], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [14, 1, 11, 13]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 85; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        int32_t v_ = ((int32_t)(floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_fast_exp[((ax0_ax1_fused * 16) + ax2)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((ax0_ax1_fused * 16) + ax2)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 5, 16, 1), \"float32\"), T_fast_exp: T.Buffer((17, 5, 16, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(85):\n            for ax2 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_fast_exp_1 = T.Buffer((1360,), data=T_fast_exp.data)\n                data_1 = T.Buffer((1360,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [17, 5, 16, 1]}{"op_name": "fast_exp", "c_code": "void default_function_kernel(float* T_fast_exp, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 11; ++ax3) {\n            int32_t v_ = ((int32_t)(floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_fast_exp[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((ax0 * 924) + (ax1 * 154)) + (ax2 * 11)) + ax3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ T_fast_exp, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_fast_exp[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min(data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))], 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 6, 14, 11), \"float32\"), T_fast_exp: T.Buffer((6, 6, 14, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            for ax1, ax2, ax3 in T.grid(6, 14, 11):\n                cse_var_1: T.int32 = ax0 * 924 + ax1 * 154 + ax2 * 11 + ax3\n                T_fast_exp_1 = T.Buffer((5544,), data=T_fast_exp.data)\n                data_1 = T.Buffer((5544,), data=data.data)\n                T_fast_exp_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1])", "op_args": [6, 6, 14, 11]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n        T_fast_tanh[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 168) + (ax2 * 14)) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 12, 14), \"float32\"), T_fast_tanh: T.Buffer((4, 16, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2, ax3 in T.grid(12, 14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 168 + ax2 * 14 + ax3\n                T_fast_tanh_1 = T.Buffer((10752,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((10752,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [4, 16, 12, 14]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 224; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 2) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 14, 2, 2), \"float32\"), T_fast_tanh: T.Buffer((8, 14, 2, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(224):\n            for ax3 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 2 + ax3\n                T_fast_tanh_1 = T.Buffer((448,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((448,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [8, 14, 2, 2]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      depth_to_space[((i0_i1_fused_i2_fused * 10) + i3)] = data[(((((((i0_i1_fused_i2_fused / 42) * 490) + (((i0_i1_fused_i2_fused % 14) % 2) * 210)) + ((i3 % 2) * 105)) + (((i0_i1_fused_i2_fused % 42) / 14) * 35)) + (((i0_i1_fused_i2_fused % 14) / 2) * 5)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 12) * 490) + ((((((((int)blockIdx.x) & 3) * 7) + (((int)threadIdx.x) / 5)) >> 1) % 2) * 210)) + (((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 10) % 2) * 105)) + (((((int)blockIdx.x) % 12) >> 2) * 35)) + ((((((((int)blockIdx.x) & 3) * 7) + (((int)threadIdx.x) / 5)) >> 1) / 2) * 5)) + ((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 10) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 14, 7, 5), \"float32\"), depth_to_space: T.Buffer((12, 3, 14, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 14\n                depth_to_space_1 = T.Buffer((5040,), data=depth_to_space.data)\n                data_1 = T.Buffer((5880,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 10 + i3] = data_1[i0_i1_fused_i2_fused // 42 * 490 + T.truncmod(cse_var_1, 2) * 210 + T.truncmod(i3, 2) * 105 + i0_i1_fused_i2_fused % 42 // 14 * 35 + T.Div(cse_var_1, 2) * 5 + T.Div(i3, 2)]", "op_args": [12, 14, 7, 5]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 75; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 2) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 2) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 1, 5, 2), \"float32\"), T_fast_tanh: T.Buffer((15, 1, 5, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(75):\n            for ax3 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 2 + ax3\n                T_fast_tanh_1 = T.Buffer((150,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((150,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [15, 1, 5, 2]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 26; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 22; ++i3) {\n      depth_to_space[((i0_i1_fused_i2_fused * 22) + i3)] = data[(((((i0_i1_fused_i2_fused >> 1) * 66) + ((i0_i1_fused_i2_fused & 1) * 22)) + ((i3 % 2) * 11)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 143) {\n    depth_to_space[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 11) * 66) + (((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 22) / 11) * 22)) + (((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22) % 2) * 11)) + ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22) / 2))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 6, 1, 11), \"float32\"), depth_to_space: T.Buffer((13, 1, 2, 22), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(26):\n            for i3 in range(22):\n                depth_to_space_1 = T.Buffer((572,), data=depth_to_space.data)\n                data_1 = T.Buffer((858,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 22 + i3] = data_1[i0_i1_fused_i2_fused // 2 * 66 + i0_i1_fused_i2_fused % 2 * 22 + T.truncmod(i3, 2) * 11 + T.Div(i3, 2)]", "op_args": [13, 6, 1, 11]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 40; ++i2) {\n      for (int32_t i3 = 0; i3 < 32; ++i3) {\n        depth_to_space[(((i0_i1_fused * 1280) + (i2 * 32)) + i3)] = data[(((((i0_i1_fused * 2240) + ((i2 % 2) * 640)) + ((i3 % 2) * 320)) + ((i2 / 2) * 16)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) >> 6) * 2240) + ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) % 320) >> 3) % 2) * 640)) + (((((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) & 31) % 2) * 320)) + ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 2)) % 320) >> 3) / 2) * 16)) + ((((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) & 31) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 7, 20, 16), \"float32\"), depth_to_space: T.Buffer((18, 1, 40, 32), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(18):\n            for i2, i3 in T.grid(40, 32):\n                depth_to_space_1 = T.Buffer((23040,), data=depth_to_space.data)\n                data_1 = T.Buffer((40320,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 1280 + i2 * 32 + i3] = data_1[i0_i1_fused * 2240 + T.truncmod(i2, 2) * 640 + T.truncmod(i3, 2) * 320 + T.Div(i2, 2) * 16 + T.Div(i3, 2)]", "op_args": [18, 7, 20, 16]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 342; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_fast_tanh[((ax0_ax1_fused * 4) + ax2)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused * 4) + ax2)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 18, 4, 1), \"float32\"), T_fast_tanh: T.Buffer((19, 18, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(342):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_fast_tanh_1 = T.Buffer((1368,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((1368,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [19, 18, 4, 1]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 512; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused >> 8) * 256) + ((((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) % 2) * 128)) + ((i0_i1_fused_i2_fused_i3_fused & 1) * 64)) + (((i0_i1_fused_i2_fused_i3_fused & 255) >> 6) * 16)) + (((i0_i1_fused_i2_fused_i3_fused & 63) >> 1) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 3) * 256) + (((((int)threadIdx.x) >> 1) % 2) * 128)) + ((((int)threadIdx.x) & 1) * 64)) + ((((int)blockIdx.x) & 7) * 8)) + ((((int)threadIdx.x) >> 1) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 16, 1), \"float32\"), depth_to_space: T.Buffer((2, 4, 32, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(512):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 64 // 2\n            depth_to_space_1 = T.Buffer((512,), data=depth_to_space.data)\n            data_1 = T.Buffer((512,), data=data.data)\n            depth_to_space_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 256 * 256 + T.truncmod(cse_var_1, 2) * 128 + i0_i1_fused_i2_fused_i3_fused % 2 * 64 + i0_i1_fused_i2_fused_i3_fused % 256 // 64 * 16 + T.Div(cse_var_1, 2)]", "op_args": [2, 16, 16, 1]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2016; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 112) * 140) + ((((i0_i1_fused_i2_fused_i3_fused % 112) >> 3) % 2) * 56)) + (((i0_i1_fused_i2_fused_i3_fused & 7) % 2) * 28)) + ((((i0_i1_fused_i2_fused_i3_fused % 112) >> 3) / 2) * 4)) + ((i0_i1_fused_i2_fused_i3_fused & 7) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) / 7) * 140) + ((((int)threadIdx.x) >> 3) * 56)) + (((((int)threadIdx.x) & 7) % 2) * 28)) + ((((int)blockIdx.x) % 7) * 4)) + ((((int)threadIdx.x) & 7) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 5, 7, 4), \"float32\"), depth_to_space: T.Buffer((18, 1, 14, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2016):\n            cse_var_2: T.int32 = i0_i1_fused_i2_fused_i3_fused % 8\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 112 // 8\n            depth_to_space_1 = T.Buffer((2016,), data=depth_to_space.data)\n            data_1 = T.Buffer((2520,), data=data.data)\n            depth_to_space_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 112 * 140 + T.truncmod(cse_var_1, 2) * 56 + T.truncmod(cse_var_2, 2) * 28 + T.Div(cse_var_1, 2) * 4 + T.Div(cse_var_2, 2)]", "op_args": [18, 5, 7, 4]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2166; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 18) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 19, 6, 18), \"float32\"), T_fast_tanh: T.Buffer((19, 19, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(2166):\n            for ax3 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 18 + ax3\n                T_fast_tanh_1 = T.Buffer((38988,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((38988,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [19, 19, 6, 18]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 22; ++i2) {\n        for (int32_t i3 = 0; i3 < 8; ++i3) {\n          depth_to_space[((((i0 * 528) + (i1 * 176)) + (i2 * 8)) + i3)] = data[((((((i0 * 660) + ((i2 % 2) * 264)) + ((i3 % 2) * 132)) + (i1 * 44)) + ((i2 / 2) * 4)) + (i3 / 2))];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) >> 4) * 660) + ((((((((int)blockIdx.x) * 33) + ((int)threadIdx.x)) % 176) >> 3) % 2) * 264)) + ((((((int)blockIdx.x) + ((int)threadIdx.x)) & 7) % 2) * 132)) + (((((((int)blockIdx.x) & 15) * 3) + (((int)threadIdx.x) / 11)) >> 4) * 44)) + ((((((((int)blockIdx.x) * 33) + ((int)threadIdx.x)) % 176) >> 3) / 2) * 4)) + (((((int)blockIdx.x) + ((int)threadIdx.x)) & 7) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 15, 11, 4), \"float32\"), depth_to_space: T.Buffer((4, 3, 22, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(3, 22, 8):\n                depth_to_space_1 = T.Buffer((2112,), data=depth_to_space.data)\n                data_1 = T.Buffer((2640,), data=data.data)\n                depth_to_space_1[i0 * 528 + i1 * 176 + i2 * 8 + i3] = data_1[i0 * 660 + T.truncmod(i2, 2) * 264 + T.truncmod(i3, 2) * 132 + i1 * 44 + T.Div(i2, 2) * 4 + T.Div(i3, 2)]", "op_args": [4, 15, 11, 4]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1372; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 7, 14, 20), \"float32\"), T_fast_tanh: T.Buffer((14, 7, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1372):\n            for ax3 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_fast_tanh_1 = T.Buffer((27440,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((27440,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [14, 7, 14, 20]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3648; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused / 228) * 228) + (((i0_i1_fused_i2_fused_i3_fused % 76) / 38) * 114)) + (((i0_i1_fused_i2_fused_i3_fused % 38) % 2) * 57)) + (((i0_i1_fused_i2_fused_i3_fused % 228) / 76) * 19)) + ((i0_i1_fused_i2_fused_i3_fused % 38) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 2) * 228) + (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 19)) & 3) >> 1) * 114)) + (((((((int)blockIdx.x) * 19) + ((int)threadIdx.x)) % 38) % 2) * 57)) + (((((((int)blockIdx.x) & 3) * 3) + (((int)threadIdx.x) / 19)) >> 2) * 19)) + ((((((int)blockIdx.x) * 19) + ((int)threadIdx.x)) % 38) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 12, 1, 19), \"float32\"), depth_to_space: T.Buffer((16, 3, 2, 38), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3648):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 38\n            depth_to_space_1 = T.Buffer((3648,), data=depth_to_space.data)\n            data_1 = T.Buffer((3648,), data=data.data)\n            depth_to_space_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 228 * 228 + i0_i1_fused_i2_fused_i3_fused % 76 // 38 * 114 + T.truncmod(cse_var_1, 2) * 57 + i0_i1_fused_i2_fused_i3_fused % 228 // 76 * 19 + T.Div(cse_var_1, 2)]", "op_args": [16, 12, 1, 19]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_fast_tanh[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 16, 4, 20), \"float32\"), T_fast_tanh: T.Buffer((15, 16, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            for ax3 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_fast_tanh_1 = T.Buffer((19200,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((19200,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [15, 16, 4, 20]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 61200; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[(((((((i0_i1_fused_i2_fused_i3_fused / 4080) * 4335) + ((((i0_i1_fused_i2_fused_i3_fused % 1020) / 30) % 2) * 2040)) + (((i0_i1_fused_i2_fused_i3_fused % 30) % 2) * 1020)) + (((i0_i1_fused_i2_fused_i3_fused % 4080) / 1020) * 255)) + ((((i0_i1_fused_i2_fused_i3_fused % 1020) / 30) / 2) * 15)) + ((i0_i1_fused_i2_fused_i3_fused % 30) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 3825) {\n    depth_to_space[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) / 255) * 4335) + ((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 510) / 15) % 2) * 2040)) + (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 30) % 2) * 1020)) + (((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) % 1020) / 255) * 255)) + ((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 510) / 15) / 2) * 15)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 30) / 2))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 17, 15), \"float32\"), depth_to_space: T.Buffer((15, 4, 34, 30), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(61200):\n            cse_var_2: T.int32 = i0_i1_fused_i2_fused_i3_fused % 30\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 1020 // 30\n            depth_to_space_1 = T.Buffer((61200,), data=depth_to_space.data)\n            data_1 = T.Buffer((65025,), data=data.data)\n            depth_to_space_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 4080 * 4335 + T.truncmod(cse_var_1, 2) * 2040 + T.truncmod(cse_var_2, 2) * 1020 + i0_i1_fused_i2_fused_i3_fused % 4080 // 1020 * 255 + T.Div(cse_var_1, 2) * 15 + T.Div(cse_var_2, 2)]", "op_args": [15, 17, 17, 15]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        depth_to_space[(((i0_i1_fused * 216) + (i2 * 12)) + i3)] = data[(((((i0_i1_fused * 378) + ((i2 % 2) * 108)) + ((i3 % 2) * 54)) + ((i2 / 2) * 6)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) / 18) * 378) + (((((int)blockIdx.x) % 18) % 2) * 108)) + ((((int)threadIdx.x) % 2) * 54)) + (((((int)blockIdx.x) % 18) / 2) * 6)) + (((int)threadIdx.x) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 7, 9, 6), \"float32\"), depth_to_space: T.Buffer((3, 1, 18, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(3):\n            for i2, i3 in T.grid(18, 12):\n                depth_to_space_1 = T.Buffer((648,), data=depth_to_space.data)\n                data_1 = T.Buffer((1134,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 216 + i2 * 12 + i3] = data_1[i0_i1_fused * 378 + T.truncmod(i2, 2) * 108 + T.truncmod(i3, 2) * 54 + T.Div(i2, 2) * 6 + T.Div(i3, 2)]", "op_args": [3, 7, 9, 6]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 3888; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_tanh[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 9, 12, 18), \"float32\"), T_fast_tanh: T.Buffer((2, 9, 12, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(3888):\n            T_fast_tanh_1 = T.Buffer((3888,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((3888,), data=data.data)\n            T_fast_tanh_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [2, 9, 12, 18]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 32; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        depth_to_space[(((i0_i1_fused * 360) + (i2 * 20)) + i3)] = data[(((((((i0_i1_fused >> 1) * 990) + ((i2 % 2) * 360)) + ((i3 % 2) * 180)) + ((i0_i1_fused & 1) * 90)) + ((i2 / 2) * 10)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 24) * 990) + ((((((((int)blockIdx.x) % 12) * 3) + (((int)threadIdx.x) / 10)) >> 1) % 2) * 360)) + (((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 20) % 2) * 180)) + (((((int)blockIdx.x) % 24) / 12) * 90)) + ((((((((int)blockIdx.x) % 12) * 3) + (((int)threadIdx.x) / 10)) >> 1) / 2) * 10)) + ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 20) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 11, 9, 10), \"float32\"), depth_to_space: T.Buffer((16, 2, 18, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(32):\n            for i2, i3 in T.grid(18, 20):\n                depth_to_space_1 = T.Buffer((11520,), data=depth_to_space.data)\n                data_1 = T.Buffer((15840,), data=data.data)\n                depth_to_space_1[i0_i1_fused * 360 + i2 * 20 + i3] = data_1[i0_i1_fused // 2 * 990 + T.truncmod(i2, 2) * 360 + T.truncmod(i3, 2) * 180 + i0_i1_fused % 2 * 90 + T.Div(i2, 2) * 10 + T.Div(i3, 2)]", "op_args": [16, 11, 9, 10]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 24960; ++i0_i1_fused_i2_fused_i3_fused) {\n    depth_to_space[i0_i1_fused_i2_fused_i3_fused] = data[(((((((i0_i1_fused_i2_fused_i3_fused / 1920) * 2040) + ((((i0_i1_fused_i2_fused_i3_fused % 480) / 24) % 2) * 960)) + (((i0_i1_fused_i2_fused_i3_fused % 24) % 2) * 480)) + (((i0_i1_fused_i2_fused_i3_fused % 1920) / 480) * 120)) + ((((i0_i1_fused_i2_fused_i3_fused % 480) / 24) / 2) * 12)) + ((i0_i1_fused_i2_fused_i3_fused % 24) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  depth_to_space[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 96) * 2040) + ((((((((int)blockIdx.x) % 24) * 5) + (((int)threadIdx.x) >> 2)) / 6) % 2) * 960)) + (((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 24) % 2) * 480)) + (((((int)blockIdx.x) % 96) / 24) * 120)) + ((((((((int)blockIdx.x) % 24) * 5) + (((int)threadIdx.x) >> 2)) / 6) / 2) * 12)) + ((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 24) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 17, 10, 12), \"float32\"), depth_to_space: T.Buffer((13, 4, 20, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(24960):\n            cse_var_2: T.int32 = i0_i1_fused_i2_fused_i3_fused % 24\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 480 // 24\n            depth_to_space_1 = T.Buffer((24960,), data=depth_to_space.data)\n            data_1 = T.Buffer((26520,), data=data.data)\n            depth_to_space_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 1920 * 2040 + T.truncmod(cse_var_1, 2) * 960 + T.truncmod(cse_var_2, 2) * 480 + i0_i1_fused_i2_fused_i3_fused % 1920 // 480 * 120 + T.Div(cse_var_1, 2) * 12 + T.Div(cse_var_2, 2)]", "op_args": [13, 17, 10, 12]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 14; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 7; ++ax3) {\n        T_fast_tanh[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[(((ax0_ax1_fused * 84) + (ax2 * 7)) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 147) {\n    T_fast_tanh[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 1, 12, 7), \"float32\"), T_fast_tanh: T.Buffer((14, 1, 12, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(14):\n            for ax2, ax3 in T.grid(12, 7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 84 + ax2 * 7 + ax3\n                T_fast_tanh_1 = T.Buffer((1176,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((1176,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [14, 1, 12, 7]}{"op_name": "depth_to_space", "c_code": "void default_function_kernel(float* data, float* depth_to_space) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2312; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 22; ++i3) {\n      depth_to_space[((i0_i1_fused_i2_fused * 22) + i3)] = data[(((((((i0_i1_fused_i2_fused / 136) * 3553) + (((i0_i1_fused_i2_fused % 34) % 2) * 1496)) + ((i3 % 2) * 748)) + (((i0_i1_fused_i2_fused % 136) / 34) * 187)) + (((i0_i1_fused_i2_fused % 34) / 2) * 11)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ depth_to_space) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 3179) {\n    depth_to_space[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 187) * 3553) + ((((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 374) / 11) % 2) * 1496)) + (((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22) % 2) * 748)) + (((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) % 748) / 187) * 187)) + ((((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) % 374) / 11) / 2) * 11)) + ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22) / 2))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 19, 17, 11), \"float32\"), depth_to_space: T.Buffer((17, 4, 34, 22), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2312):\n            for i3 in range(22):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 34\n                depth_to_space_1 = T.Buffer((50864,), data=depth_to_space.data)\n                data_1 = T.Buffer((60401,), data=data.data)\n                depth_to_space_1[i0_i1_fused_i2_fused * 22 + i3] = data_1[i0_i1_fused_i2_fused // 136 * 3553 + T.truncmod(cse_var_1, 2) * 1496 + T.truncmod(i3, 2) * 748 + i0_i1_fused_i2_fused % 136 // 34 * 187 + T.Div(cse_var_1, 2) * 11 + T.Div(i3, 2)]", "op_args": [17, 19, 17, 11]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 7920; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_fast_tanh[ax0_ax1_fused_ax2_fused_ax3_fused] = ((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused])) * max(-9.000000e+00f, min(9.000000e+00f, data[ax0_ax1_fused_ax2_fused_ax3_fused]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 10, 9, 11), \"float32\"), T_fast_tanh: T.Buffer((8, 10, 9, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(7920):\n            T_fast_tanh_1 = T.Buffer((7920,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((7920,), data=data.data)\n            T_fast_tanh_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[ax0_ax1_fused_ax2_fused_ax3_fused])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [8, 10, 9, 11]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 15; ++ax3) {\n          T_fast_tanh[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((ax0 * 240) + (ax1 * 60)) + (ax2 * 15)) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 4, 15), \"float32\"), T_fast_tanh: T.Buffer((6, 4, 4, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            for ax1, ax2, ax3 in T.grid(4, 4, 15):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 60 + ax2 * 15 + ax3\n                T_fast_tanh_1 = T.Buffer((1440,), data=T_fast_tanh.data)\n                data_1 = T.Buffer((1440,), data=data.data)\n                T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [6, 4, 4, 15]}{"op_name": "fast_tanh", "c_code": "void default_function_kernel(float* T_fast_tanh, float* data) {\n  for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n    for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n      T_fast_tanh[((ax2 * 17) + ax3)] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)])) * max(-9.000000e+00f, min(9.000000e+00f, data[((ax2 * 17) + ax3)]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_fast_tanh, float* __restrict__ data) {\n  T_fast_tanh[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * -2.760768e-16f) + 2.000188e-13f)) + -8.604672e-11f)) + 5.122297e-08f)) + 1.485722e-05f)) + 6.372619e-04f)) + 4.893525e-03f)) / (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * (((max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])) * max(-9.000000e+00f, min(9.000000e+00f, data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))) * 1.198258e-06f) + 1.185347e-04f)) + 2.268435e-03f)) + 4.893525e-03f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 1, 20, 17), \"float32\"), T_fast_tanh: T.Buffer((1, 1, 20, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax2, ax3 in T.grid(20, 17):\n            cse_var_1: T.int32 = ax2 * 17 + ax3\n            T_fast_tanh_1 = T.Buffer((340,), data=T_fast_tanh.data)\n            data_1 = T.Buffer((340,), data=data.data)\n            T_fast_tanh_1[cse_var_1] = T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(-2.76076847742355e-16) + T.float32(2.0001879048247699e-13)) + T.float32(-8.60467152213735e-11)) + T.float32(5.1222970903711401e-08)) + T.float32(1.4857223571797901e-05)) + T.float32(0.00063726192887543596)) + T.float32(0.0048935245589178597)) / (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * (T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.max(T.float32(-9), T.min(T.float32(9), data_1[cse_var_1])) * T.float32(1.1982583946670199e-06) + T.float32(0.000118534705686654)) + T.float32(0.0022684346324390002)) + T.float32(0.0048935251855438504))", "op_args": [1, 1, 20, 17]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 21280; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_reverse_sequence[ax0_ax1_fused_ax2_fused_ax3_fused] = data[(((ax0_ax1_fused_ax2_fused_ax3_fused % 2660) + 18620) - ((ax0_ax1_fused_ax2_fused_ax3_fused / 2660) * 2660))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 56) + ((int)threadIdx.x)) % 2660) + 18620) - ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 28)) / 95) * 2660))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 7, 19, 20), \"float32\"), T_reverse_sequence: T.Buffer((8, 7, 19, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(21280):\n            T_reverse_sequence_1 = T.Buffer((21280,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((21280,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_1[ax0_ax1_fused_ax2_fused_ax3_fused % 2660 + 18620 - ax0_ax1_fused_ax2_fused_ax3_fused // 2660 * 2660]", "op_args": [8, 7, 19, 20]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 30; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 14) + ax3)] = data[(((((ax0_ax1_fused_ax2_fused % 5) * 14) + ax3) + 350) - ((ax0_ax1_fused_ax2_fused / 5) * 70))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 70) + 350) - ((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 1)) / 35) * 70))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 5, 1, 14), \"float32\"), T_reverse_sequence: T.Buffer((6, 5, 1, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(30):\n            for ax3 in range(14):\n                T_reverse_sequence_1 = T.Buffer((420,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((420,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 14 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 5 * 14 + ax3 + 350 - ax0_ax1_fused_ax2_fused // 5 * 70]", "op_args": [6, 5, 1, 14]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 7020; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_reverse_sequence[ax0_ax1_fused_ax2_fused_ax3_fused] = data[(((ax0_ax1_fused_ax2_fused_ax3_fused % 1170) + 5850) - ((ax0_ax1_fused_ax2_fused_ax3_fused / 1170) * 1170))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 1755) {\n    T_reverse_sequence[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 1170) + 5850) - ((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 585) * 1170))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 9, 10, 13), \"float32\"), T_reverse_sequence: T.Buffer((6, 9, 10, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(7020):\n            T_reverse_sequence_1 = T.Buffer((7020,), data=T_reverse_sequence.data)\n            data_1 = T.Buffer((7020,), data=data.data)\n            T_reverse_sequence_1[ax0_ax1_fused_ax2_fused_ax3_fused] = data_1[ax0_ax1_fused_ax2_fused_ax3_fused % 1170 + 5850 - ax0_ax1_fused_ax2_fused_ax3_fused // 1170 * 1170]", "op_args": [6, 9, 10, 13]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 90; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_reverse_sequence[((ax0_ax1_fused * 11) + ax2)] = data[(((((ax0_ax1_fused % 18) * 11) + ax2) + 792) - ((ax0_ax1_fused / 18) * 198))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 495) {\n    T_reverse_sequence[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 198) + 792) - ((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 99) * 198))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 18, 11, 1), \"float32\"), T_reverse_sequence: T.Buffer((5, 18, 11, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(90):\n            for ax2 in range(11):\n                T_reverse_sequence_1 = T.Buffer((990,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((990,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 11 + ax2] = data_1[ax0_ax1_fused % 18 * 11 + ax2 + 792 - ax0_ax1_fused // 18 * 198]", "op_args": [5, 18, 11, 1]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 210; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 2) + ax3)] = data[(((((ax0_ax1_fused_ax2_fused % 42) * 2) + ax3) + 336) - ((ax0_ax1_fused_ax2_fused / 42) * 84))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) * 10) + (((int)threadIdx.x) >> 1)) % 42) * 2) + (((int)threadIdx.x) & 1)) + 336) - ((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 2)) / 21) * 84))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 7, 6, 2), \"float32\"), T_reverse_sequence: T.Buffer((5, 7, 6, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(210):\n            for ax3 in range(2):\n                T_reverse_sequence_1 = T.Buffer((420,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((420,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 2 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 42 * 2 + ax3 + 336 - ax0_ax1_fused_ax2_fused // 42 * 84]", "op_args": [5, 7, 6, 2]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n          T_reverse_sequence[((((ax0 * 480) + (ax1 * 120)) + (ax2 * 12)) + ax3)] = data[(((((ax1 * 120) + (ax2 * 12)) + ax3) + 8160) - (ax0 * 480))];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 480) + 8160) - ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 5)) / 15) * 480))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 4, 10, 12), \"float32\"), T_reverse_sequence: T.Buffer((18, 4, 10, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(18):\n            for ax1, ax2, ax3 in T.grid(4, 10, 12):\n                cse_var_3: T.int32 = ax1 * 120\n                cse_var_2: T.int32 = ax2 * 12\n                cse_var_1: T.int32 = ax0 * 480\n                T_reverse_sequence_1 = T.Buffer((8640,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((8640,), data=data.data)\n                T_reverse_sequence_1[cse_var_1 + cse_var_3 + cse_var_2 + ax3] = data_1[cse_var_3 + cse_var_2 + ax3 + 8160 - cse_var_1]", "op_args": [18, 4, 10, 12]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 105; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n        T_reverse_sequence[(((ax0_ax1_fused * 112) + (ax2 * 16)) + ax3)] = data[((((((ax0_ax1_fused % 7) * 112) + (ax2 * 16)) + ax3) + 10976) - ((ax0_ax1_fused / 7) * 784))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) * 40) + ((int)threadIdx.x)) % 784) + 10976) - ((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 3)) / 98) * 784))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 7, 7, 16), \"float32\"), T_reverse_sequence: T.Buffer((15, 7, 7, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(105):\n            for ax2, ax3 in T.grid(7, 16):\n                cse_var_1: T.int32 = ax2 * 16\n                T_reverse_sequence_1 = T.Buffer((11760,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((11760,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 112 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 7 * 112 + cse_var_1 + ax3 + 10976 - ax0_ax1_fused // 7 * 784]", "op_args": [15, 7, 7, 16]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 288; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 19; ++ax3) {\n      T_reverse_sequence[((ax0_ax1_fused_ax2_fused * 19) + ax3)] = data[(((((ax0_ax1_fused_ax2_fused % 144) * 19) + ax3) + 2736) - ((ax0_ax1_fused_ax2_fused / 144) * 2736))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 144) * 19) + ((int)threadIdx.x)) + 2736) - ((((int)blockIdx.x) / 144) * 2736))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 16, 9, 19), \"float32\"), T_reverse_sequence: T.Buffer((2, 16, 9, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(288):\n            for ax3 in range(19):\n                T_reverse_sequence_1 = T.Buffer((5472,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((5472,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused_ax2_fused * 19 + ax3] = data_1[ax0_ax1_fused_ax2_fused % 144 * 19 + ax3 + 2736 - ax0_ax1_fused_ax2_fused // 144 * 2736]", "op_args": [2, 16, 9, 19]}{"op_name": "flip", "c_code": "void default_function_kernel(float* T_reverse_sequence, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 56; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n        T_reverse_sequence[(((ax0_ax1_fused * 340) + (ax2 * 17)) + ax3)] = data[((((((ax0_ax1_fused % 14) * 340) + (ax2 * 17)) + ax3) + 14280) - ((ax0_ax1_fused / 14) * 4760))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_reverse_sequence, float* __restrict__ data) {\n  T_reverse_sequence[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) % 85) * 56) + ((int)threadIdx.x)) + 14280) - ((((int)blockIdx.x) / 85) * 4760))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 14, 20, 17), \"float32\"), T_reverse_sequence: T.Buffer((4, 14, 20, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(56):\n            for ax2, ax3 in T.grid(20, 17):\n                cse_var_1: T.int32 = ax2 * 17\n                T_reverse_sequence_1 = T.Buffer((19040,), data=T_reverse_sequence.data)\n                data_1 = T.Buffer((19040,), data=data.data)\n                T_reverse_sequence_1[ax0_ax1_fused * 340 + cse_var_1 + ax3] = data_1[ax0_ax1_fused % 14 * 340 + cse_var_1 + ax3 + 14280 - ax0_ax1_fused // 14 * 4760]", "op_args": [4, 14, 20, 17]}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7920; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = floorf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 6, 11, 20), \"float32\"), compute: T.Buffer((6, 6, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(7920):\n            compute_1 = T.Buffer((7920,), data=compute.data)\n            data_1 = T.Buffer((7920,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.floor(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [6, 6, 11, 20]}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 10; ++i1) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i1 * 180) + (i2 * 20)) + i3)] = floorf(data[(((i1 * 180) + (i2 * 20)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 10, 9, 20), \"float32\"), compute: T.Buffer((1, 10, 9, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(10, 9, 20):\n            cse_var_1: T.int32 = i1 * 180 + i2 * 20 + i3\n            compute_1 = T.Buffer((1800,), data=compute.data)\n            data_1 = T.Buffer((1800,), data=data.data)\n            compute_1[cse_var_1] = T.floor(data_1[cse_var_1])", "op_args": [1, 10, 9, 20]}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = floorf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 1, 20, 6), \"float32\"), compute: T.Buffer((11, 1, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1320):\n            compute_1 = T.Buffer((1320,), data=compute.data)\n            data_1 = T.Buffer((1320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.floor(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [11, 1, 20, 6]}{"op_name": "floor", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 9; ++i3) {\n      compute[((i0_i1_fused * 9) + i3)] = floorf(data[((i0_i1_fused * 9) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 6)) < 9) {\n    compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = floorf(data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 2, 1, 9), \"float32\"), compute: T.Buffer((3, 2, 1, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(6):\n            for i3 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i3\n                compute_1 = T.Buffer((54,), data=compute.data)\n                data_1 = T.Buffer((54,), data=data.data)\n                compute_1[cse_var_1] = T.floor(data_1[cse_var_1])", "op_args": [3, 2, 1, 9]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 8; ++i3_s) {\n          compute[((((i0 * 1728) + (i1 * 144)) + (i2 * 8)) + i3_s)] = ((int8_t)(data[((((i0 * 1728) + (i1 * 144)) + (i2 * 8)) + i3_s)] != data[((((i0 * 1728) + (i1 * 144)) + (i2 * 8)) + i3_s)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 12, 18, 8), \"float32\"), compute: T.Buffer((14, 12, 18, 8), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(14):\n            for i1, i2, i3_s in T.grid(12, 18, 8):\n                cse_var_1: T.int32 = i0 * 1728 + i1 * 144 + i2 * 8 + i3_s\n                compute_1 = T.Buffer((24192,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((24192,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [14, 12, 18, 8]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 9; ++i3) {\n        compute[(((i1 * 108) + (i2 * 9)) + i3)] = ((int8_t)(data[(((i1 * 108) + (i2 * 9)) + i3)] != data[(((i1 * 108) + (i2 * 9)) + i3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 27) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 2, 12, 9), \"float32\"), compute: T.Buffer((1, 2, 12, 9), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(2, 12, 9):\n            cse_var_1: T.int32 = i1 * 108 + i2 * 9 + i3\n            compute_1 = T.Buffer((216,), \"int8\", data=compute.data)\n            data_1 = T.Buffer((216,), data=data.data)\n            compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [1, 2, 12, 9]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6885; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 5)) < 1377) {\n    compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * 5.000000e-01f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 3, 9), \"float32\"), compute: T.Buffer((15, 17, 3, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6885):\n            compute_1 = T.Buffer((6885,), data=compute.data)\n            data_1 = T.Buffer((6885,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))", "op_args": [15, 17, 3, 9]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 18; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 4; ++i3) {\n        compute[(((i1 * 12) + (i2 * 4)) + i3)] = ((int8_t)(data[(((i1 * 12) + (i2 * 4)) + i3)] != data[(((i1 * 12) + (i2 * 4)) + i3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 18, 3, 4), \"float32\"), compute: T.Buffer((1, 18, 3, 4), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(18, 3, 4):\n            cse_var_1: T.int32 = i1 * 12 + i2 * 4 + i3\n            compute_1 = T.Buffer((216,), \"int8\", data=compute.data)\n            data_1 = T.Buffer((216,), data=data.data)\n            compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [1, 18, 3, 4]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        compute[(((i1 * 55) + (i2 * 11)) + i3)] = ((0.000000e+00f < data[(((i1 * 55) + (i2 * 11)) + i3)]) ? data[(((i1 * 55) + (i2 * 11)) + i3)] : (data[(((i1 * 55) + (i2 * 11)) + i3)] * 5.000000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 27) + (((int)threadIdx.x) >> 1)) < 55) {\n    compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * 5.000000e-01f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 2, 5, 11), \"float32\"), compute: T.Buffer((1, 2, 5, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(2, 5, 11):\n            cse_var_1: T.int32 = i1 * 55 + i2 * 11 + i3\n            compute_1 = T.Buffer((110,), data=compute.data)\n            data_1 = T.Buffer((110,), data=data.data)\n            compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [1, 2, 5, 11]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3_s = 0; i3_s < 3; ++i3_s) {\n          compute[((((i0 * 462) + (i1 * 33)) + (i2 * 3)) + i3_s)] = ((int8_t)(data[((((i0 * 462) + (i1 * 33)) + (i2 * 3)) + i3_s)] != data[((((i0 * 462) + (i1 * 33)) + (i2 * 3)) + i3_s)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 14, 11, 3), \"float32\"), compute: T.Buffer((7, 14, 11, 3), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3_s in T.grid(14, 11, 3):\n                cse_var_1: T.int32 = i0 * 462 + i1 * 33 + i2 * 3 + i3_s\n                compute_1 = T.Buffer((3234,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((3234,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [7, 14, 11, 3]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 13; ++i3) {\n          compute[((((i0 * 156) + (i1 * 78)) + (i2 * 13)) + i3)] = ((0.000000e+00f < data[((((i0 * 156) + (i1 * 78)) + (i2 * 13)) + i3)]) ? data[((((i0 * 156) + (i1 * 78)) + (i2 * 13)) + i3)] : (data[((((i0 * 156) + (i1 * 78)) + (i2 * 13)) + i3)] * 5.000000e-01f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 2, 6, 13), \"float32\"), compute: T.Buffer((12, 2, 6, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(12):\n            for i1, i2, i3 in T.grid(2, 6, 13):\n                cse_var_1: T.int32 = i0 * 156 + i1 * 78 + i2 * 13 + i3\n                compute_1 = T.Buffer((1872,), data=compute.data)\n                data_1 = T.Buffer((1872,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [12, 2, 6, 13]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1001; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_s = 0; i3_s < 2; ++i3_s) {\n      compute[((i0_i1_fused_i2_fused * 2) + i3_s)] = ((int8_t)(data[((i0_i1_fused_i2_fused * 2) + i3_s)] != data[((i0_i1_fused_i2_fused * 2) + i3_s)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 10) + (((int)threadIdx.x) >> 1)) < 1001) {\n    compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 11, 7, 2), \"float32\"), compute: T.Buffer((13, 11, 7, 2), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1001):\n            for i3_s in range(2):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 2 + i3_s\n                compute_1 = T.Buffer((2002,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((2002,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [13, 11, 7, 2]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 120) + (i1 * 20)) + (i2 * 4)) + i3)] = ((int8_t)(data[((((i0 * 120) + (i1 * 20)) + (i2 * 4)) + i3)] != data[((((i0 * 120) + (i1 * 20)) + (i2 * 4)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 6, 5, 4), \"float32\"), compute: T.Buffer((8, 6, 5, 4), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(6, 5, 4):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 20 + i2 * 4 + i3\n                compute_1 = T.Buffer((960,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((960,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [8, 6, 5, 4]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        for (int32_t i3 = 0; i3 < 7; ++i3) {\n          compute[((((i0 * 532) + (i1 * 133)) + (i2 * 7)) + i3)] = ((0.000000e+00f < data[((((i0 * 532) + (i1 * 133)) + (i2 * 7)) + i3)]) ? data[((((i0 * 532) + (i1 * 133)) + (i2 * 7)) + i3)] : (data[((((i0 * 532) + (i1 * 133)) + (i2 * 7)) + i3)] * 5.000000e-01f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 19, 7), \"float32\"), compute: T.Buffer((8, 4, 19, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(4, 19, 7):\n                cse_var_1: T.int32 = i0 * 532 + i1 * 133 + i2 * 7 + i3\n                compute_1 = T.Buffer((4256,), data=compute.data)\n                data_1 = T.Buffer((4256,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [8, 4, 19, 7]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 5400) + (i1 * 360)) + (i2 * 20)) + i3)] = ((int8_t)(data[((((i0 * 5400) + (i1 * 360)) + (i2 * 20)) + i3)] != data[((((i0 * 5400) + (i1 * 360)) + (i2 * 20)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 15, 18, 20), \"float32\"), compute: T.Buffer((10, 15, 18, 20), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(15, 18, 20):\n                cse_var_1: T.int32 = i0 * 5400 + i1 * 360 + i2 * 20 + i3\n                compute_1 = T.Buffer((54000,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((54000,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [10, 15, 18, 20]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 32; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = ((0.000000e+00f < data[((i0_i1_fused * 7) + i2)]) ? data[((i0_i1_fused * 7) + i2)] : (data[((i0_i1_fused * 7) + i2)] * 5.000000e-01f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 7, 1), \"float32\"), compute: T.Buffer((8, 4, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(32):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_1 = T.Buffer((224,), data=compute.data)\n                data_1 = T.Buffer((224,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [8, 4, 7, 1]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 9; ++i3) {\n          compute[((((i0 * 1404) + (i1 * 108)) + (i2 * 9)) + i3)] = ((int8_t)(data[((((i0 * 1404) + (i1 * 108)) + (i2 * 9)) + i3)] != data[((((i0 * 1404) + (i1 * 108)) + (i2 * 9)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 351) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 13, 12, 9), \"float32\"), compute: T.Buffer((2, 13, 12, 9), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(2):\n            for i1, i2, i3 in T.grid(13, 12, 9):\n                cse_var_1: T.int32 = i0 * 1404 + i1 * 108 + i2 * 9 + i3\n                compute_1 = T.Buffer((2808,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((2808,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [2, 13, 12, 9]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 800; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 2, 8, 5), \"float32\"), compute: T.Buffer((10, 2, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(800):\n            compute_1 = T.Buffer((800,), data=compute.data)\n            data_1 = T.Buffer((800,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))", "op_args": [10, 2, 8, 5]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6750; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((int8_t)(data[i0_i1_fused_i2_fused_i3_fused] != data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 15, 15, 15), \"float32\"), compute: T.Buffer((2, 15, 15, 15), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6750):\n            compute_1 = T.Buffer((6750,), \"int8\", data=compute.data)\n            data_1 = T.Buffer((6750,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.Cast(\"int8\", T.isnan(data_1[i0_i1_fused_i2_fused_i3_fused]))", "op_args": [2, 15, 15, 15]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4275; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 15, 5, 19), \"float32\"), compute: T.Buffer((3, 15, 5, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4275):\n            compute_1 = T.Buffer((4275,), data=compute.data)\n            data_1 = T.Buffer((4275,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))", "op_args": [3, 15, 5, 19]}{"op_name": "isnan", "c_code": "void default_function_kernel(int8_t* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 20) + i3)] = ((int8_t)(data[((i0_i1_fused_i2_fused * 20) + i3)] != data[((i0_i1_fused_i2_fused * 20) + i3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(signed char* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((signed char)(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] != data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 5, 4, 20), \"float32\"), compute: T.Buffer((18, 5, 4, 20), \"bool\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((7200,), \"int8\", data=compute.data)\n                data_1 = T.Buffer((7200,), data=data.data)\n                compute_1[cse_var_1] = T.Cast(\"int8\", T.isnan(data_1[cse_var_1]))", "op_args": [18, 5, 4, 20]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 660) + (i1 * 66)) + (i2 * 11)) + i3)] = logf(data[((((i0 * 660) + (i1 * 66)) + (i2 * 11)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 10, 6, 11), \"float32\"), compute: T.Buffer((14, 10, 6, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(14):\n            for i1, i2, i3 in T.grid(10, 6, 11):\n                cse_var_1: T.int32 = i0 * 660 + i1 * 66 + i2 * 11 + i3\n                compute_1 = T.Buffer((9240,), data=compute.data)\n                data_1 = T.Buffer((9240,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [14, 10, 6, 11]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 816; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 17, 4, 3), \"float32\"), compute: T.Buffer((4, 17, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(816):\n            compute_1 = T.Buffer((816,), data=compute.data)\n            data_1 = T.Buffer((816,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))", "op_args": [4, 17, 4, 3]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 99; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 5; ++i3) {\n      compute[((i0_i1_fused * 5) + i3)] = logf(data[((i0_i1_fused * 5) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 495) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 9, 1, 5), \"float32\"), compute: T.Buffer((11, 9, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(99):\n            for i3 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i3\n                compute_1 = T.Buffer((495,), data=compute.data)\n                data_1 = T.Buffer((495,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [11, 9, 1, 5]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i2 = 0; i2 < 17; ++i2) {\n    compute[i2] = ((0.000000e+00f < data[i2]) ? data[i2] : (data[i2] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(17) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((int)threadIdx.x)] = ((0.000000e+00f < data[((int)threadIdx.x)]) ? data[((int)threadIdx.x)] : (data[((int)threadIdx.x)] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 1, 17, 1), \"float32\"), compute: T.Buffer((1, 1, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i2 in range(17):\n            compute_1 = T.Buffer((17,), data=compute.data)\n            data_1 = T.Buffer((17,), data=data.data)\n            compute_1[i2] = T.if_then_else(T.float32(0) < data_1[i2], data_1[i2], data_1[i2] * T.float32(0.5))", "op_args": [1, 1, 17, 1]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i3 = 0; i3 < 2; ++i3) {\n      compute[((i0 * 2) + i3)] = logf(data[((i0 * 2) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((int)threadIdx.x)] = __logf(data[((int)threadIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 1, 1, 2), \"float32\"), compute: T.Buffer((5, 1, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(5):\n            for i3 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i3\n                compute_1 = T.Buffer((10,), data=compute.data)\n                data_1 = T.Buffer((10,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [5, 1, 1, 2]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3825; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 5, 5, 17), \"float32\"), compute: T.Buffer((9, 5, 5, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3825):\n            compute_1 = T.Buffer((3825,), data=compute.data)\n            data_1 = T.Buffer((3825,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))", "op_args": [9, 5, 5, 17]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 15; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 15) + i3)] = ((0.000000e+00f < data[((i0_i1_fused_i2_fused * 15) + i3)]) ? data[((i0_i1_fused_i2_fused * 15) + i3)] : (data[((i0_i1_fused_i2_fused * 15) + i3)] * 5.000000e-01f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 15, 8, 15), \"float32\"), compute: T.Buffer((2, 15, 8, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            for i3 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 15 + i3\n                compute_1 = T.Buffer((3600,), data=compute.data)\n                data_1 = T.Buffer((3600,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * T.float32(0.5))", "op_args": [2, 15, 8, 15]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          compute[((((i0 * 330) + (i1 * 165)) + (i2 * 15)) + i3)] = logf(data[((((i0 * 330) + (i1 * 165)) + (i2 * 15)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 2, 11, 15), \"float32\"), compute: T.Buffer((10, 2, 11, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(2, 11, 15):\n                cse_var_1: T.int32 = i0 * 330 + i1 * 165 + i2 * 15 + i3\n                compute_1 = T.Buffer((3300,), data=compute.data)\n                data_1 = T.Buffer((3300,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [10, 2, 11, 15]}{"op_name": "leaky_relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * 5.000000e-01f));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * 5.000000e-01f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 2, 4, 5), \"float32\"), compute: T.Buffer((8, 2, 4, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(320):\n            compute_1 = T.Buffer((320,), data=compute.data)\n            data_1 = T.Buffer((320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(0.5))", "op_args": [8, 2, 4, 5]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 1292) + (i1 * 323)) + (i2 * 17)) + i3)] = logf(data[((((i0 * 1292) + (i1 * 323)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 1615) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 4, 19, 17), \"float32\"), compute: T.Buffer((20, 4, 19, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1, i2, i3 in T.grid(4, 19, 17):\n                cse_var_1: T.int32 = i0 * 1292 + i1 * 323 + i2 * 17 + i3\n                compute_1 = T.Buffer((25840,), data=compute.data)\n                data_1 = T.Buffer((25840,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [20, 4, 19, 17]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    float T_softmax_maxelem[16];\n    float compute_1[4];\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        T_softmax_maxelem[((i1 * 4) + i2)] = -3.402823e+38f;\n        T_softmax_maxelem[((i1 * 4) + i2)] = max(T_softmax_maxelem[((i1 * 4) + i2)], data[(((i0 * 16) + (i1 * 4)) + i2)]);\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 4; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_1[i2_1] = 0.000000e+00f;\n        compute_1[i2_1] = (compute_1[i2_1] + expf((data[(((i0 * 16) + (i1_1 * 4)) + i2_1)] - T_softmax_maxelem[((i1_1 * 4) + i2_1)])));\n      }\n      for (int32_t i2_2 = 0; i2_2 < 4; ++i2_2) {\n        compute[(((i0 * 16) + (i1_1 * 4)) + i2_2)] = ((data[(((i0 * 16) + (i1_1 * 4)) + i2_2)] - T_softmax_maxelem[((i1_1 * 4) + i2_2)]) - logf(compute_1[i2_2]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __expf((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]) - __logf(compute_1[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 4, 4, 1), \"float32\"), compute: T.Buffer((6, 4, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(6):\n            T_softmax_maxelem = T.allocate([16], \"float32\", \"global\")\n            compute_1 = T.allocate([4], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((16,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((96,), data=data.data)\n            for i1, i2 in T.grid(4, 4):\n                cse_var_2: T.int32 = i1 * 4\n                cse_var_1: T.int32 = cse_var_2 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.float32(-3.4028234663852886e+38)\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 16 + cse_var_2 + i2])\n            for i1 in range(4):\n                compute_2 = T.Buffer((4,), data=compute_1, align=16)\n                for i2 in range(4):\n                    cse_var_3: T.int32 = i1 * 4\n                    compute_2[i2] = T.float32(0)\n                    compute_2[i2] = compute_2[i2] + T.exp(data_1[i0 * 16 + cse_var_3 + i2] - T_softmax_maxelem_1[cse_var_3 + i2])\n                for i2 in range(4):\n                    cse_var_5: T.int32 = i1 * 4\n                    cse_var_4: T.int32 = i0 * 16 + cse_var_5 + i2\n                    compute_3 = T.Buffer((96,), data=compute.data)\n                    compute_3[cse_var_4] = data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5 + i2] - T.log(compute_2[i2])", "op_args": [6, 4, 4, 1]}{"op_name": "log", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 3264) + (i1 * 204)) + (i2 * 17)) + i3)] = logf(data[((((i0 * 3264) + (i1 * 204)) + (i2 * 17)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __logf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 16, 12, 17), \"float32\"), compute: T.Buffer((3, 16, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i1, i2, i3 in T.grid(16, 12, 17):\n                cse_var_1: T.int32 = i0 * 3264 + i1 * 204 + i2 * 17 + i3\n                compute_1 = T.Buffer((9792,), data=compute.data)\n                data_1 = T.Buffer((9792,), data=data.data)\n                compute_1[cse_var_1] = T.log(data_1[cse_var_1])", "op_args": [3, 16, 12, 17]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused < 600; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused) {\n    float T_softmax_maxelem[3];\n    float compute_1[3];\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      T_softmax_maxelem[i1] = -3.402823e+38f;\n      for (int32_t k = 0; k < 15; ++k) {\n        T_softmax_maxelem[i1] = max(T_softmax_maxelem[i1], data[((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 200) * 1800) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10) * 180)) + (i1 * 60)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200) / 50) * 15)) + k)]);\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      compute_1[i1_1] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n        compute_1[i1_1] = (compute_1[i1_1] + expf((data[((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 200) * 1800) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10) * 180)) + (i1_1 * 60)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200) / 50) * 15)) + k_1)] - T_softmax_maxelem[i1_1])));\n      }\n    }\n    for (int32_t i3_outer_inner = 0; i3_outer_inner < 3; ++i3_outer_inner) {\n      for (int32_t i1_inner = 0; i1_inner < 3; ++i1_inner) {\n        compute[((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 200) * 1800) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10) * 180)) + (i1_inner * 60)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200) / 10) * 3)) + i3_outer_inner)] = ((data[((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 200) * 1800) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10) * 180)) + (i1_inner * 60)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200) / 10) * 3)) + i3_outer_inner)] - T_softmax_maxelem[i1_inner]) - logf(compute_1[i1_inner]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 15; ++k) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 120) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 675) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]) - __logf(compute_1[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 15; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 360) + (((int)threadIdx.x) * 15)) + k)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 15, 4, 15), \"float32\"), compute: T.Buffer((6, 15, 4, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(600):\n            T_softmax_maxelem = T.allocate([3], \"float32\", \"global\")\n            compute_1 = T.allocate([3], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((3,), data=T_softmax_maxelem, align=8)\n            data_1 = T.Buffer((5400,), data=data.data)\n            for i1 in range(3):\n                T_softmax_maxelem_1[i1] = T.float32(-3.4028234663852886e+38)\n                for k in range(15):\n                    T_softmax_maxelem_1[i1] = T.max(T_softmax_maxelem_1[i1], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 200 * 1800 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10 * 180 + i1 * 60 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200 // 50 * 15 + k])\n            compute_2 = T.Buffer((3,), data=compute_1, align=8)\n            for i1 in range(3):\n                compute_2[i1] = T.float32(0)\n                for k in range(15):\n                    compute_2[i1] = compute_2[i1] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 200 * 1800 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10 * 180 + i1 * 60 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200 // 50 * 15 + k] - T_softmax_maxelem_1[i1])\n            for i3_outer_inner, i1_inner in T.grid(3, 3):\n                cse_var_1: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 200 * 1800 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 10 * 180 + i1_inner * 60 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 200 // 10 * 3 + i3_outer_inner\n                compute_3 = T.Buffer((5400,), data=compute.data)\n                compute_3[cse_var_1] = data_1[cse_var_1] - T_softmax_maxelem_1[i1_inner] - T.log(compute_2[i1_inner])", "op_args": [6, 15, 4, 15]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 50; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      for (int32_t i3 = 0; i3 < 2; ++i3) {\n        compute[(((i0_i1_fused * 20) + (i2 * 2)) + i3)] = log10f(data[(((i0_i1_fused * 20) + (i2 * 2)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 10, 10, 2), \"float32\"), compute: T.Buffer((5, 10, 10, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(50):\n            for i2, i3 in T.grid(10, 2):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2 * 2 + i3\n                compute_1 = T.Buffer((1000,), data=compute.data)\n                data_1 = T.Buffer((1000,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [5, 10, 10, 2]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        for (int32_t i3 = 0; i3 < 19; ++i3) {\n          compute[((((i0 * 1615) + (i1 * 95)) + (i2 * 19)) + i3)] = log10f(data[((((i0 * 1615) + (i1 * 95)) + (i2 * 19)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 17, 5, 19), \"float32\"), compute: T.Buffer((14, 17, 5, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(14):\n            for i1, i2, i3 in T.grid(17, 5, 19):\n                cse_var_1: T.int32 = i0 * 1615 + i1 * 95 + i2 * 19 + i3\n                compute_1 = T.Buffer((22610,), data=compute.data)\n                data_1 = T.Buffer((22610,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [14, 17, 5, 19]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused < 40; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused) {\n    float T_softmax_maxelem[204];\n    float compute_1[204];\n    for (int32_t i0 = 0; i0 < 3; ++i0) {\n      for (int32_t i1 = 0; i1 < 17; ++i1) {\n        for (int32_t i2 = 0; i2 < 4; ++i2) {\n          T_softmax_maxelem[(((i0 * 68) + (i1 * 4)) + i2)] = -3.402823e+38f;\n          for (int32_t k = 0; k < 20; ++k) {\n            T_softmax_maxelem[(((i0 * 68) + (i1 * 4)) + i2)] = max(T_softmax_maxelem[(((i0 * 68) + (i1 * 4)) + i2)], data[(((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused & 1) * 8160) + (i0 * 2720)) + (i1 * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused / 20) * 80)) + (i2 * 20)) + k)]);\n          }\n        }\n      }\n    }\n    for (int32_t i0_1 = 0; i0_1 < 3; ++i0_1) {\n      for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n        for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n          compute_1[(((i0_1 * 68) + (i1_1 * 4)) + i2_1)] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n            compute_1[(((i0_1 * 68) + (i1_1 * 4)) + i2_1)] = (compute_1[(((i0_1 * 68) + (i1_1 * 4)) + i2_1)] + expf((data[(((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused & 1) * 8160) + (i0_1 * 2720)) + (i1_1 * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused / 20) * 80)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[(((i0_1 * 68) + (i1_1 * 4)) + i2_1)])));\n          }\n        }\n      }\n    }\n    for (int32_t i2_outer_inner = 0; i2_outer_inner < 4; ++i2_outer_inner) {\n      for (int32_t i3_outer_inner = 0; i3_outer_inner < 2; ++i3_outer_inner) {\n        for (int32_t i0_inner = 0; i0_inner < 3; ++i0_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 17; ++i1_inner) {\n            compute[((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused & 1) * 8160) + (i0_inner * 2720)) + (i1_inner * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused / 20) * 80)) + (i2_outer_inner * 20)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused % 20) >> 1) * 2)) + i3_outer_inner)] = ((data[((((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused & 1) * 8160) + (i0_inner * 2720)) + (i1_inner * 160)) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused / 20) * 80)) + (i2_outer_inner * 20)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused % 20) >> 1) * 2)) + i3_outer_inner)] - T_softmax_maxelem[(((i0_inner * 68) + (i1_inner * 4)) + i2_outer_inner)]) - logf(compute_1[(((i0_inner * 68) + (i1_inner * 4)) + i2_outer_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 20; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 20)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]) - __logf(compute_1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 5)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 51) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 20; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 51) {\n      compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 17, 8, 20), \"float32\"), compute: T.Buffer((6, 17, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused in T.parallel(40):\n            T_softmax_maxelem = T.allocate([204], \"float32\", \"global\")\n            compute_1 = T.allocate([204], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((204,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((16320,), data=data.data)\n            for i0, i1, i2 in T.grid(3, 17, 4):\n                T_softmax_maxelem_1[i0 * 68 + i1 * 4 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(20):\n                    cse_var_1: T.int32 = i0 * 68 + i1 * 4 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused % 2 * 8160 + i0 * 2720 + i1 * 160 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused // 20 * 80 + i2 * 20 + k])\n            compute_2 = T.Buffer((204,), data=compute_1)\n            for i0, i1, i2 in T.grid(3, 17, 4):\n                compute_2[i0 * 68 + i1 * 4 + i2] = T.float32(0)\n                for k in range(20):\n                    cse_var_2: T.int32 = i0 * 68 + i1 * 4 + i2\n                    compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused % 2 * 8160 + i0 * 2720 + i1 * 160 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused // 20 * 80 + i2 * 20 + k] - T_softmax_maxelem_1[cse_var_2])\n            for i2_outer_inner, i3_outer_inner, i0_inner, i1_inner in T.grid(4, 2, 3, 17):\n                cse_var_4: T.int32 = i0_inner * 68 + i1_inner * 4 + i2_outer_inner\n                cse_var_3: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused % 2 * 8160 + i0_inner * 2720 + i1_inner * 160 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused // 20 * 80 + i2_outer_inner * 20 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused_i3_outer_outer_inner_fused_i0_outer_inner_fused % 20 // 2 * 2 + i3_outer_inner\n                compute_3 = T.Buffer((16320,), data=compute.data)\n                compute_3[cse_var_3] = data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4] - T.log(compute_2[cse_var_4])", "op_args": [6, 17, 8, 20]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 594; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log10f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 11, 9, 1), \"float32\"), compute: T.Buffer((6, 11, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(594):\n            compute_1 = T.Buffer((594,), data=compute.data)\n            data_1 = T.Buffer((594,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log10(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [6, 11, 9, 1]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[2223];\n  float compute_1[2223];\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        T_softmax_maxelem[(((i0 * 247) + (i1 * 19)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 18; ++k) {\n          T_softmax_maxelem[(((i0 * 247) + (i1 * 19)) + i2)] = max(T_softmax_maxelem[(((i0 * 247) + (i1 * 19)) + i2)], data[((((i0 * 4446) + (i1 * 342)) + (i2 * 18)) + k)]);\n        }\n      }\n    }\n  }\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 13; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_1[(((i0_1 * 247) + (i1_1 * 19)) + i2_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 18; ++k_1) {\n          compute_1[(((i0_1 * 247) + (i1_1 * 19)) + i2_1)] = (compute_1[(((i0_1 * 247) + (i1_1 * 19)) + i2_1)] + expf((data[((((i0_1 * 4446) + (i1_1 * 342)) + (i2_1 * 18)) + k_1)] - T_softmax_maxelem[(((i0_1 * 247) + (i1_1 * 19)) + i2_1)])));\n        }\n      }\n    }\n  }\n  for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 6; ++i3_outer_outer_inner) {\n    for (int32_t i0_outer_inner = 0; i0_outer_inner < 9; ++i0_outer_inner) {\n      for (int32_t i2_outer_inner = 0; i2_outer_inner < 19; ++i2_outer_inner) {\n        for (int32_t i3_outer_inner = 0; i3_outer_inner < 3; ++i3_outer_inner) {\n          for (int32_t i1_inner = 0; i1_inner < 13; ++i1_inner) {\n            compute[(((((i0_outer_inner * 4446) + (i1_inner * 342)) + (i2_outer_inner * 18)) + (i3_outer_outer_inner * 3)) + i3_outer_inner)] = ((data[(((((i0_outer_inner * 4446) + (i1_inner * 342)) + (i2_outer_inner * 18)) + (i3_outer_outer_inner * 3)) + i3_outer_inner)] - T_softmax_maxelem[(((i0_outer_inner * 247) + (i1_inner * 19)) + i2_outer_inner)]) - logf(compute_1[(((i0_outer_inner * 247) + (i1_inner * 19)) + i2_outer_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 2223) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 2223) {\n      compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 20007) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]) - __logf(compute_1[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 20) + (((int)threadIdx.x) / 3)) < 741) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 20) + (((int)threadIdx.x) / 3)) < 741) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 1080) + (((int)threadIdx.x) * 18)) + k)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 13, 19, 18), \"float32\"), compute: T.Buffer((9, 13, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([2223], \"float32\", \"global\")\n        compute_1 = T.allocate([2223], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((2223,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((40014,), data=data.data)\n        for i0, i1, i2 in T.grid(9, 13, 19):\n            T_softmax_maxelem_1[i0 * 247 + i1 * 19 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(18):\n                cse_var_1: T.int32 = i0 * 247 + i1 * 19 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 4446 + i1 * 342 + i2 * 18 + k])\n        compute_2 = T.Buffer((2223,), data=compute_1)\n        for i0, i1, i2 in T.grid(9, 13, 19):\n            compute_2[i0 * 247 + i1 * 19 + i2] = T.float32(0)\n            for k in range(18):\n                cse_var_2: T.int32 = i0 * 247 + i1 * 19 + i2\n                compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0 * 4446 + i1 * 342 + i2 * 18 + k] - T_softmax_maxelem_1[cse_var_2])\n        for i3_outer_outer_inner, i0_outer_inner, i2_outer_inner, i3_outer_inner, i1_inner in T.grid(6, 9, 19, 3, 13):\n            cse_var_4: T.int32 = i0_outer_inner * 247 + i1_inner * 19 + i2_outer_inner\n            cse_var_3: T.int32 = i0_outer_inner * 4446 + i1_inner * 342 + i2_outer_inner * 18 + i3_outer_outer_inner * 3 + i3_outer_inner\n            compute_3 = T.Buffer((40014,), data=compute.data)\n            compute_3[cse_var_3] = data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4] - T.log(compute_2[cse_var_4])", "op_args": [9, 13, 19, 18]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2688; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log10f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 8, 4, 6), \"float32\"), compute: T.Buffer((14, 8, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2688):\n            compute_1 = T.Buffer((2688,), data=compute.data)\n            data_1 = T.Buffer((2688,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log10(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [14, 8, 4, 6]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[595];\n  float compute_1[1];\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        T_softmax_maxelem[(((i0 * 119) + (i1 * 17)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 19; ++k) {\n          T_softmax_maxelem[(((i0 * 119) + (i1 * 17)) + i2)] = max(T_softmax_maxelem[(((i0 * 119) + (i1 * 17)) + i2)], data[((((i0 * 2261) + (i1 * 323)) + (i2 * 19)) + k)]);\n        }\n      }\n    }\n  }\n  for (int32_t i1_outer_outer_inner = 0; i1_outer_outer_inner < 7; ++i1_outer_outer_inner) {\n    for (int32_t i2_outer_outer_inner = 0; i2_outer_outer_inner < 17; ++i2_outer_outer_inner) {\n      for (int32_t i0_outer_inner = 0; i0_outer_inner < 5; ++i0_outer_inner) {\n        for (int32_t i3_outer_inner = 0; i3_outer_inner < 19; ++i3_outer_inner) {\n          compute_1[0] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 19; ++k_1) {\n            compute_1[0] = (compute_1[0] + expf((data[((((i0_outer_inner * 2261) + (i1_outer_outer_inner * 323)) + (i2_outer_outer_inner * 19)) + k_1)] - T_softmax_maxelem[(((i0_outer_inner * 119) + (i1_outer_outer_inner * 17)) + i2_outer_outer_inner)])));\n          }\n          compute[((((i0_outer_inner * 2261) + (i1_outer_outer_inner * 323)) + (i2_outer_outer_inner * 19)) + i3_outer_inner)] = ((data[((((i0_outer_inner * 2261) + (i1_outer_outer_inner * 323)) + (i2_outer_outer_inner * 19)) + i3_outer_inner)] - T_softmax_maxelem[(((i0_outer_inner * 119) + (i1_outer_outer_inner * 17)) + i2_outer_outer_inner)]) - logf(compute_1[0]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 595) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 19; ++k) {\n    if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 595) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 1216) + (((int)threadIdx.x) * 19)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 595) {\n    compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 19; ++k) {\n    if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 595) {\n      compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 76) + (((int)threadIdx.x) * 19)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 11305) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 19)]) - __logf(compute_1[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 19)]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 7, 17, 19), \"float32\"), compute: T.Buffer((5, 7, 17, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([595], \"float32\", \"global\")\n        compute_1 = T.allocate([1], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((595,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((11305,), data=data.data)\n        for i0, i1, i2 in T.grid(5, 7, 17):\n            T_softmax_maxelem_1[i0 * 119 + i1 * 17 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(19):\n                cse_var_1: T.int32 = i0 * 119 + i1 * 17 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 2261 + i1 * 323 + i2 * 19 + k])\n        for i1_outer_outer_inner, i2_outer_outer_inner, i0_outer_inner, i3_outer_inner in T.grid(7, 17, 5, 19):\n            cse_var_2: T.int32 = i0_outer_inner * 2261 + i1_outer_outer_inner * 323 + i2_outer_outer_inner * 19 + i3_outer_inner\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.float32(0)\n            for k in range(19):\n                compute_2[0] = compute_2[0] + T.exp(data_1[i0_outer_inner * 2261 + i1_outer_outer_inner * 323 + i2_outer_outer_inner * 19 + k] - T_softmax_maxelem_1[i0_outer_inner * 119 + i1_outer_outer_inner * 17 + i2_outer_outer_inner])\n            compute_3 = T.Buffer((11305,), data=compute.data)\n            compute_3[cse_var_2] = data_1[cse_var_2] - T_softmax_maxelem_1[i0_outer_inner * 119 + i1_outer_outer_inner * 17 + i2_outer_outer_inner] - T.log(compute_2[0])", "op_args": [5, 7, 17, 19]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 272; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = log10f(data[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 16, 8, 1), \"float32\"), compute: T.Buffer((17, 16, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(272):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_1 = T.Buffer((2176,), data=compute.data)\n                data_1 = T.Buffer((2176,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [17, 16, 8, 1]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        for (int32_t i3 = 0; i3 < 5; ++i3) {\n          compute[((((i0 * 900) + (i1 * 75)) + (i2 * 5)) + i3)] = log10f(data[((((i0 * 900) + (i1 * 75)) + (i2 * 5)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12, 15, 5), \"float32\"), compute: T.Buffer((4, 12, 15, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(12, 15, 5):\n                cse_var_1: T.int32 = i0 * 900 + i1 * 75 + i2 * 5 + i3\n                compute_1 = T.Buffer((3600,), data=compute.data)\n                data_1 = T.Buffer((3600,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [4, 12, 15, 5]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[80];\n  float compute_1[80];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    T_softmax_maxelem[i0_i1_fused_i2_fused] = -3.402823e+38f;\n    for (int32_t k = 0; k < 16; ++k) {\n      T_softmax_maxelem[i0_i1_fused_i2_fused] = max(T_softmax_maxelem[i0_i1_fused_i2_fused], data[((i0_i1_fused_i2_fused * 16) + k)]);\n    }\n  }\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_1[(((i0 * 16) + (i1 * 2)) + i2)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 16; ++k_1) {\n          compute_1[(((i0 * 16) + (i1 * 2)) + i2)] = (compute_1[(((i0 * 16) + (i1 * 2)) + i2)] + expf((data[((((i0 * 256) + (i1 * 32)) + (i2 * 16)) + k_1)] - T_softmax_maxelem[(((i0 * 16) + (i1 * 2)) + i2)])));\n        }\n      }\n    }\n  }\n  for (int32_t i3_outer_outer_outer = 0; i3_outer_outer_outer < 4; ++i3_outer_outer_outer) {\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 4; ++i3_outer_outer_inner) {\n      for (int32_t i0_outer_inner = 0; i0_outer_inner < 5; ++i0_outer_inner) {\n        for (int32_t i1_outer_inner = 0; i1_outer_inner < 8; ++i1_outer_inner) {\n          for (int32_t i2_inner = 0; i2_inner < 2; ++i2_inner) {\n            compute[(((((i0_outer_inner * 256) + (i1_outer_inner * 32)) + (i2_inner * 16)) + (i3_outer_outer_outer * 4)) + i3_outer_outer_inner)] = ((data[(((((i0_outer_inner * 256) + (i1_outer_inner * 32)) + (i2_inner * 16)) + (i3_outer_outer_outer * 4)) + i3_outer_outer_inner)] - T_softmax_maxelem[(((i0_outer_inner * 16) + (i1_outer_inner * 2)) + i2_inner)]) - logf(compute_1[(((i0_outer_inner * 16) + (i1_outer_inner * 2)) + i2_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]) - __logf(compute_1[((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 5) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 16; ++k) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 5) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 512) + (((int)threadIdx.x) * 16)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 5) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 16; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 5) {\n      compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 1024) + (((int)threadIdx.x) * 16)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 2, 16), \"float32\"), compute: T.Buffer((5, 8, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([80], \"float32\", \"global\")\n        compute_1 = T.allocate([80], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((80,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((1280,), data=data.data)\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.float32(-3.4028234663852886e+38)\n            for k in range(16):\n                T_softmax_maxelem_1[i0_i1_fused_i2_fused] = T.max(T_softmax_maxelem_1[i0_i1_fused_i2_fused], data_1[i0_i1_fused_i2_fused * 16 + k])\n        compute_2 = T.Buffer((80,), data=compute_1)\n        for i0, i1, i2 in T.grid(5, 8, 2):\n            compute_2[i0 * 16 + i1 * 2 + i2] = T.float32(0)\n            for k in range(16):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_2[cse_var_1] = compute_2[cse_var_1] + T.exp(data_1[i0 * 256 + i1 * 32 + i2 * 16 + k] - T_softmax_maxelem_1[cse_var_1])\n        for i3_outer_outer_outer, i3_outer_outer_inner, i0_outer_inner, i1_outer_inner, i2_inner in T.grid(4, 4, 5, 8, 2):\n            cse_var_3: T.int32 = i0_outer_inner * 16 + i1_outer_inner * 2 + i2_inner\n            cse_var_2: T.int32 = i0_outer_inner * 256 + i1_outer_inner * 32 + i2_inner * 16 + i3_outer_outer_outer * 4 + i3_outer_outer_inner\n            compute_3 = T.Buffer((1280,), data=compute.data)\n            compute_3[cse_var_2] = data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3] - T.log(compute_2[cse_var_3])", "op_args": [5, 8, 2, 16]}{"op_name": "log10", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 234; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = log10f(data[((i0_i1_fused_i2_fused * 16) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __log10f(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 9, 13, 16), \"float32\"), compute: T.Buffer((2, 9, 13, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(234):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((3744,), data=compute.data)\n                data_1 = T.Buffer((3744,), data=data.data)\n                compute_1[cse_var_1] = T.log10(data_1[cse_var_1])", "op_args": [2, 9, 13, 16]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[357];\n  float compute_1[357];\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        T_softmax_maxelem[(((i0 * 51) + (i1 * 17)) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 10; ++k) {\n          T_softmax_maxelem[(((i0 * 51) + (i1 * 17)) + i2)] = max(T_softmax_maxelem[(((i0 * 51) + (i1 * 17)) + i2)], data[((((i0 * 510) + (i1 * 170)) + (i2 * 10)) + k)]);\n        }\n      }\n    }\n  }\n  for (int32_t i0_1 = 0; i0_1 < 7; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n        compute_1[(((i0_1 * 51) + (i1_1 * 17)) + i2_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 10; ++k_1) {\n          compute_1[(((i0_1 * 51) + (i1_1 * 17)) + i2_1)] = (compute_1[(((i0_1 * 51) + (i1_1 * 17)) + i2_1)] + expf((data[((((i0_1 * 510) + (i1_1 * 170)) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[(((i0_1 * 51) + (i1_1 * 17)) + i2_1)])));\n        }\n      }\n    }\n  }\n  for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 2; ++i3_outer_outer_inner) {\n    for (int32_t i0_outer_inner = 0; i0_outer_inner < 7; ++i0_outer_inner) {\n      for (int32_t i1_outer_inner = 0; i1_outer_inner < 3; ++i1_outer_inner) {\n        for (int32_t i2_outer_inner = 0; i2_outer_inner < 17; ++i2_outer_inner) {\n          for (int32_t i3_outer_inner = 0; i3_outer_inner < 5; ++i3_outer_inner) {\n            compute[(((((i0_outer_inner * 510) + (i1_outer_inner * 170)) + (i2_outer_inner * 10)) + (i3_outer_outer_inner * 5)) + i3_outer_inner)] = ((data[(((((i0_outer_inner * 510) + (i1_outer_inner * 170)) + (i2_outer_inner * 10)) + (i3_outer_outer_inner * 5)) + i3_outer_inner)] - T_softmax_maxelem[(((i0_outer_inner * 51) + (i1_outer_inner * 17)) + i2_outer_inner)]) - logf(compute_1[(((i0_outer_inner * 51) + (i1_outer_inner * 17)) + i2_outer_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 357) {\n    compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 10; ++k) {\n    if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 357) {\n      compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 40) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(17) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 17) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 17) + ((int)threadIdx.x)) / 10)]) - __logf(compute_1[(((((int)blockIdx.x) * 17) + ((int)threadIdx.x)) / 10)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 357) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 10; ++k) {\n    if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 357) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) * 10)) + k)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 3, 17, 10), \"float32\"), compute: T.Buffer((7, 3, 17, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([357], \"float32\", \"global\")\n        compute_1 = T.allocate([357], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((357,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((3570,), data=data.data)\n        for i0, i1, i2 in T.grid(7, 3, 17):\n            T_softmax_maxelem_1[i0 * 51 + i1 * 17 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(10):\n                cse_var_1: T.int32 = i0 * 51 + i1 * 17 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 510 + i1 * 170 + i2 * 10 + k])\n        compute_2 = T.Buffer((357,), data=compute_1)\n        for i0, i1, i2 in T.grid(7, 3, 17):\n            compute_2[i0 * 51 + i1 * 17 + i2] = T.float32(0)\n            for k in range(10):\n                cse_var_2: T.int32 = i0 * 51 + i1 * 17 + i2\n                compute_2[cse_var_2] = compute_2[cse_var_2] + T.exp(data_1[i0 * 510 + i1 * 170 + i2 * 10 + k] - T_softmax_maxelem_1[cse_var_2])\n        for i3_outer_outer_inner, i0_outer_inner, i1_outer_inner, i2_outer_inner, i3_outer_inner in T.grid(2, 7, 3, 17, 5):\n            cse_var_4: T.int32 = i0_outer_inner * 51 + i1_outer_inner * 17 + i2_outer_inner\n            cse_var_3: T.int32 = i0_outer_inner * 510 + i1_outer_inner * 170 + i2_outer_inner * 10 + i3_outer_outer_inner * 5 + i3_outer_inner\n            compute_3 = T.Buffer((3570,), data=compute.data)\n            compute_3[cse_var_3] = data_1[cse_var_3] - T_softmax_maxelem_1[cse_var_4] - T.log(compute_2[cse_var_4])", "op_args": [7, 3, 17, 10]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      for (int32_t i3 = 0; i3 < 9; ++i3) {\n        compute[(((i0 * 45) + (i2 * 9)) + i3)] = log2f(data[(((i0 * 45) + (i2 * 9)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 1, 5, 9), \"float32\"), compute: T.Buffer((8, 1, 5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i2, i3 in T.grid(5, 9):\n                cse_var_1: T.int32 = i0 * 45 + i2 * 9 + i3\n                compute_1 = T.Buffer((360,), data=compute.data)\n                data_1 = T.Buffer((360,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [8, 1, 5, 9]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  float T_softmax_maxelem[187];\n  float compute_1[17];\n  for (int32_t i1 = 0; i1 < 17; ++i1) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      T_softmax_maxelem[((i1 * 11) + i2)] = -3.402823e+38f;\n      for (int32_t k = 0; k < 4; ++k) {\n        T_softmax_maxelem[((i1 * 11) + i2)] = max(T_softmax_maxelem[((i1 * 11) + i2)], data[(((i1 * 44) + (i2 * 4)) + k)]);\n      }\n    }\n  }\n  for (int32_t i2_outer_outer_outer = 0; i2_outer_outer_outer < 11; ++i2_outer_outer_outer) {\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      compute_1[i1_1] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n        compute_1[i1_1] = (compute_1[i1_1] + expf((data[(((i1_1 * 44) + (i2_outer_outer_outer * 4)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_outer_outer_outer)])));\n      }\n    }\n    for (int32_t i3_outer_outer_inner = 0; i3_outer_outer_inner < 4; ++i3_outer_outer_inner) {\n      for (int32_t i1_outer_inner = 0; i1_outer_inner < 17; ++i1_outer_inner) {\n        compute[(((i1_outer_inner * 44) + (i2_outer_outer_outer * 4)) + i3_outer_outer_inner)] = ((data[(((i1_outer_inner * 44) + (i2_outer_outer_outer * 4)) + i3_outer_outer_inner)] - T_softmax_maxelem[((i1_outer_inner * 11) + i2_outer_outer_outer)]) - logf(compute_1[i1_outer_inner]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) < 187) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) < 187) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 64) + (((int)threadIdx.x) * 4)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 187) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]) - __logf(compute_1[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 187) {\n    compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 187) {\n      compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 17, 11, 4), \"float32\"), compute: T.Buffer((1, 17, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_maxelem = T.allocate([187], \"float32\", \"global\")\n        compute_1 = T.allocate([17], \"float32\", \"global\")\n        T_softmax_maxelem_1 = T.Buffer((187,), data=T_softmax_maxelem)\n        data_1 = T.Buffer((748,), data=data.data)\n        for i1, i2 in T.grid(17, 11):\n            T_softmax_maxelem_1[i1 * 11 + i2] = T.float32(-3.4028234663852886e+38)\n            for k in range(4):\n                cse_var_1: T.int32 = i1 * 11 + i2\n                T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i1 * 44 + i2 * 4 + k])\n        for i2_outer_outer_outer in range(11):\n            compute_2 = T.Buffer((17,), data=compute_1)\n            for i1 in range(17):\n                compute_2[i1] = T.float32(0)\n                for k in range(4):\n                    compute_2[i1] = compute_2[i1] + T.exp(data_1[i1 * 44 + i2_outer_outer_outer * 4 + k] - T_softmax_maxelem_1[i1 * 11 + i2_outer_outer_outer])\n            for i3_outer_outer_inner, i1_outer_inner in T.grid(4, 17):\n                cse_var_2: T.int32 = i1_outer_inner * 44 + i2_outer_outer_outer * 4 + i3_outer_outer_inner\n                compute_3 = T.Buffer((748,), data=compute.data)\n                compute_3[cse_var_2] = data_1[cse_var_2] - T_softmax_maxelem_1[i1_outer_inner * 11 + i2_outer_outer_outer] - T.log(compute_2[i1_outer_inner])", "op_args": [1, 17, 11, 4]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 21420; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log2f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 18, 10, 17), \"float32\"), compute: T.Buffer((7, 18, 10, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(21420):\n            compute_1 = T.Buffer((21420,), data=compute.data)\n            data_1 = T.Buffer((21420,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log2(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [7, 18, 10, 17]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 15; ++i3) {\n        compute[(((i0 * 90) + (i2 * 15)) + i3)] = log2f(data[(((i0 * 90) + (i2 * 15)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 1, 6, 15), \"float32\"), compute: T.Buffer((4, 1, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i2, i3 in T.grid(6, 15):\n                cse_var_1: T.int32 = i0 * 90 + i2 * 15 + i3\n                compute_1 = T.Buffer((360,), data=compute.data)\n                data_1 = T.Buffer((360,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [4, 1, 6, 15]}{"op_name": "log_softmax", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused = 0; i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused < 76; ++i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused) {\n    float T_softmax_maxelem[9];\n    float compute_1[3];\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 4; ++k) {\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19) * 72) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 38) * 36)) + (i2 * 4)) + k)]);\n      }\n    }\n    for (int32_t i2_outer_inner = 0; i2_outer_inner < 3; ++i2_outer_inner) {\n      for (int32_t i3_outer_inner = 0; i3_outer_inner < 2; ++i3_outer_inner) {\n        for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n          compute_1[i2_1] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 4; ++k_1) {\n            compute_1[i2_1] = (compute_1[i2_1] + expf((data[((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19) * 72) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 38) * 36)) + (i2_outer_inner * 12)) + (i2_1 * 4)) + k_1)] - T_softmax_maxelem[((i2_outer_inner * 3) + i2_1)])));\n          }\n        }\n        for (int32_t i2_inner = 0; i2_inner < 3; ++i2_inner) {\n          compute[(((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19) * 72) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 38) * 36)) + (i2_outer_inner * 12)) + (i2_inner * 4)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 38) / 19) * 2)) + i3_outer_inner)] = ((data[(((((((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19) * 72) + ((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused / 38) * 36)) + (i2_outer_inner * 12)) + (i2_inner * 4)) + (((i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 38) / 19) * 2)) + i3_outer_inner)] - T_softmax_maxelem[((i2_outer_inner * 3) + i2_inner)]) - logf(compute_1[i2_inner]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ compute_1, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 171) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]) - __logf(compute_1[((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_maxelem, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) < 171) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 4; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) < 171) {\n      compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 32) + (((int)threadIdx.x) * 4)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 4; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 76) + (((int)threadIdx.x) * 4)) + k)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 19, 18, 4), \"float32\"), compute: T.Buffer((1, 19, 18, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused in T.parallel(76):\n            T_softmax_maxelem = T.allocate([9], \"float32\", \"global\")\n            compute_1 = T.allocate([3], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((9,), data=T_softmax_maxelem, align=32)\n            data_1 = T.Buffer((1368,), data=data.data)\n            for i2 in range(9):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(4):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19 * 72 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 38 * 36 + i2 * 4 + k])\n            for i2_outer_inner, i3_outer_inner in T.grid(3, 2):\n                compute_2 = T.Buffer((3,), data=compute_1, align=8)\n                for i2 in range(3):\n                    compute_2[i2] = T.float32(0)\n                    for k in range(4):\n                        compute_2[i2] = compute_2[i2] + T.exp(data_1[i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19 * 72 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 38 * 36 + i2_outer_inner * 12 + i2 * 4 + k] - T_softmax_maxelem_1[i2_outer_inner * 3 + i2])\n                for i2_inner in range(3):\n                    cse_var_1: T.int32 = i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 19 * 72 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused // 38 * 36 + i2_outer_inner * 12 + i2_inner * 4 + i0_outer_outer_outer_i1_outer_outer_outer_fused_i2_outer_outer_outer_fused_i3_outer_outer_outer_fused_i0_outer_outer_inner_fused_i1_outer_outer_inner_fused_i2_outer_outer_inner_fused % 38 // 19 * 2 + i3_outer_inner\n                    compute_3 = T.Buffer((1368,), data=compute.data)\n                    compute_3[cse_var_1] = data_1[cse_var_1] - T_softmax_maxelem_1[i2_outer_inner * 3 + i2_inner] - T.log(compute_2[i2_inner])", "op_args": [1, 19, 18, 4]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 476; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 15; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 15) + i3)] = log2f(data[((i0_i1_fused_i2_fused * 15) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 14, 17, 15), \"float32\"), compute: T.Buffer((2, 14, 17, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(476):\n            for i3 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 15 + i3\n                compute_1 = T.Buffer((7140,), data=compute.data)\n                data_1 = T.Buffer((7140,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [2, 14, 17, 15]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  float tensor[195];\n  for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 13; ++ax3) {\n        tensor[((ax2 * 13) + ax3)] = 0.000000e+00f;\n        tensor[((ax2 * 13) + ax3)] = (tensor[((ax2 * 13) + ax3)] + (data[(((ax1 * 195) + (ax2 * 13)) + ax3)] * data[(((ax1 * 195) + (ax2 * 13)) + ax3)]));\n      }\n    }\n    for (int32_t ax2_1 = 0; ax2_1 < 15; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 13; ++ax3_1) {\n        T_divide[(((ax1 * 195) + (ax2_1 * 13)) + ax3_1)] = (data[(((ax1 * 195) + (ax2_1 * 13)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax2_1 * 13) + ax3_1)])), 7.500000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  if (((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) < 2145) {\n    T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])), 7.500000e-01f));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 11, 15, 13), \"float32\"), T_divide: T.Buffer((1, 11, 15, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        tensor = T.allocate([195], \"float32\", \"global\")\n        for ax1 in range(11):\n            tensor_1 = T.Buffer((195,), data=tensor)\n            data_1 = T.Buffer((2145,), data=data.data)\n            for ax2, ax3 in T.grid(15, 13):\n                cse_var_3: T.int32 = ax2 * 13\n                cse_var_2: T.int32 = cse_var_3 + ax3\n                cse_var_1: T.int32 = ax1 * 195 + cse_var_3 + ax3\n                tensor_1[cse_var_2] = T.float32(0)\n                tensor_1[cse_var_2] = tensor_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax2, ax3 in T.grid(15, 13):\n                cse_var_5: T.int32 = ax2 * 13\n                cse_var_4: T.int32 = ax1 * 195 + cse_var_5 + ax3\n                T_divide_1 = T.Buffer((2145,), data=T_divide.data)\n                T_divide_1[cse_var_4] = data_1[cse_var_4] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[cse_var_5 + ax3], T.float32(0.75))", "op_args": [1, 11, 15, 13]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 11; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3)] = log2f(data[((i0_i1_fused_i2_fused * 11) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 2, 7, 11), \"float32\"), compute: T.Buffer((8, 2, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            for i3 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 11 + i3\n                compute_1 = T.Buffer((1232,), data=compute.data)\n                data_1 = T.Buffer((1232,), data=data.data)\n                compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [8, 2, 7, 11]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 23760; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log2f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 1485) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 11, 10, 18), \"float32\"), compute: T.Buffer((12, 11, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(23760):\n            compute_1 = T.Buffer((23760,), data=compute.data)\n            data_1 = T.Buffer((23760,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log2(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [12, 11, 10, 18]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    float tensor[256];\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n          tensor[((ax2 * 16) + ax3)] = 0.000000e+00f;\n          tensor[((ax2 * 16) + ax3)] = (tensor[((ax2 * 16) + ax3)] + (data[((((ax0 * 3328) + (ax1 * 256)) + (ax2 * 16)) + ax3)] * data[((((ax0 * 3328) + (ax1 * 256)) + (ax2 * 16)) + ax3)]));\n        }\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 16; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 16; ++ax3_1) {\n          T_divide[((((ax0 * 3328) + (ax1 * 256)) + (ax2_1 * 16)) + ax3_1)] = (data[((((ax0 * 3328) + (ax1 * 256)) + (ax2_1 * 16)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax2_1 * 16) + ax3_1)])), 7.500000e-01f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 13, 16, 16), \"float32\"), T_divide: T.Buffer((17, 13, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(17):\n            tensor = T.allocate([256], \"float32\", \"global\")\n            for ax1 in range(13):\n                tensor_1 = T.Buffer((256,), data=tensor)\n                data_1 = T.Buffer((56576,), data=data.data)\n                for ax2, ax3 in T.grid(16, 16):\n                    cse_var_3: T.int32 = ax2 * 16\n                    cse_var_2: T.int32 = cse_var_3 + ax3\n                    cse_var_1: T.int32 = ax0 * 3328 + ax1 * 256 + cse_var_3 + ax3\n                    tensor_1[cse_var_2] = T.float32(0)\n                    tensor_1[cse_var_2] = tensor_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax2, ax3 in T.grid(16, 16):\n                    cse_var_5: T.int32 = ax2 * 16\n                    cse_var_4: T.int32 = ax0 * 3328 + ax1 * 256 + cse_var_5 + ax3\n                    T_divide_1 = T.Buffer((56576,), data=T_divide.data)\n                    T_divide_1[cse_var_4] = data_1[cse_var_4] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[cse_var_5 + ax3], T.float32(0.75))", "op_args": [17, 13, 16, 16]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1920; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = log2f(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __log2f(data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 6, 10, 8), \"float32\"), compute: T.Buffer((4, 6, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1920):\n            compute_1 = T.Buffer((1920,), data=compute.data)\n            data_1 = T.Buffer((1920,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.log2(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [4, 6, 10, 8]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    float tensor[13];\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 13; ++ax3) {\n        tensor[ax3] = 0.000000e+00f;\n        tensor[ax3] = (tensor[ax3] + (data[(((ax0_ax1_fused * 52) + (ax2 * 13)) + ax3)] * data[(((ax0_ax1_fused * 52) + (ax2 * 13)) + ax3)]));\n      }\n      for (int32_t ax3_1 = 0; ax3_1 < 13; ++ax3_1) {\n        T_divide[(((ax0_ax1_fused * 52) + (ax2 * 13)) + ax3_1)] = (data[(((ax0_ax1_fused * 52) + (ax2 * 13)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 3, 4, 13), \"float32\"), T_divide: T.Buffer((2, 3, 4, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(6):\n            tensor = T.allocate([13], \"float32\", \"global\")\n            for ax2 in range(4):\n                tensor_1 = T.Buffer((13,), data=tensor, align=32)\n                data_1 = T.Buffer((312,), data=data.data)\n                for ax3 in range(13):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 52 + ax2 * 13 + ax3\n                    tensor_1[ax3] = T.float32(0)\n                    tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(13):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 52 + ax2 * 13 + ax3\n                    T_divide_1 = T.Buffer((312,), data=T_divide.data)\n                    T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))", "op_args": [2, 3, 4, 13]}{"op_name": "log2", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 3; ++i1) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      for (int32_t i3 = 0; i3 < 3; ++i3) {\n        compute[(((i1 * 6) + (i2 * 3)) + i3)] = log2f(data[(((i1 * 6) + (i2 * 3)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((int)threadIdx.x)] = __log2f(data[((int)threadIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 3, 2, 3), \"float32\"), compute: T.Buffer((1, 3, 2, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(3, 2, 3):\n            cse_var_1: T.int32 = i1 * 6 + i2 * 3 + i3\n            compute_1 = T.Buffer((18,), data=compute.data)\n            data_1 = T.Buffer((18,), data=data.data)\n            compute_1[cse_var_1] = T.log2(data_1[cse_var_1])", "op_args": [1, 3, 2, 3]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 4; ++ax0_ax1_fused) {\n    float tensor[1];\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      tensor[0] = 0.000000e+00f;\n      tensor[0] = (tensor[0] + (data[((ax0_ax1_fused * 4) + ax2)] * data[((ax0_ax1_fused * 4) + ax2)]));\n      T_divide[((ax0_ax1_fused * 4) + ax2)] = (data[((ax0_ax1_fused * 4) + ax2)] / powf((2.000000e+00f + (1.000000e-04f * tensor[0])), 7.500000e-01f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((int)threadIdx.x)] = (data[((int)threadIdx.x)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((int)threadIdx.x)])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((int)threadIdx.x)] = 0.000000e+00f;\n  tensor[((int)threadIdx.x)] = (tensor[((int)threadIdx.x)] + (data[((int)threadIdx.x)] * data[((int)threadIdx.x)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 1, 4, 1), \"float32\"), T_divide: T.Buffer((4, 1, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(4):\n            tensor = T.allocate([1], \"float32\", \"global\")\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                tensor_1 = T.Buffer((1,), data=tensor, align=4)\n                tensor_1[0] = T.float32(0)\n                data_1 = T.Buffer((16,), data=data.data)\n                tensor_1[0] = tensor_1[0] + data_1[cse_var_1] * data_1[cse_var_1]\n                T_divide_1 = T.Buffer((16,), data=T_divide.data)\n                T_divide_1[cse_var_1] = data_1[cse_var_1] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[0], T.float32(0.75))", "op_args": [4, 1, 4, 1]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 238; ++ax0_ax1_fused) {\n    float tensor[117];\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 13; ++ax3) {\n        tensor[((ax2 * 13) + ax3)] = 0.000000e+00f;\n        tensor[((ax2 * 13) + ax3)] = (tensor[((ax2 * 13) + ax3)] + (data[(((ax0_ax1_fused * 117) + (ax2 * 13)) + ax3)] * data[(((ax0_ax1_fused * 117) + (ax2 * 13)) + ax3)]));\n      }\n    }\n    for (int32_t ax2_1 = 0; ax2_1 < 9; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 13; ++ax3_1) {\n        T_divide[(((ax0_ax1_fused * 117) + (ax2_1 * 13)) + ax3_1)] = (data[(((ax0_ax1_fused * 117) + (ax2_1 * 13)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax2_1 * 13) + ax3_1)])), 7.500000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) < 13923) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), 7.500000e-01f));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 17, 9, 13), \"float32\"), T_divide: T.Buffer((14, 17, 9, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(238):\n            tensor = T.allocate([117], \"float32\", \"global\")\n            tensor_1 = T.Buffer((117,), data=tensor)\n            data_1 = T.Buffer((27846,), data=data.data)\n            for ax2, ax3 in T.grid(9, 13):\n                cse_var_3: T.int32 = ax2 * 13\n                cse_var_2: T.int32 = cse_var_3 + ax3\n                cse_var_1: T.int32 = ax0_ax1_fused * 117 + cse_var_3 + ax3\n                tensor_1[cse_var_2] = T.float32(0)\n                tensor_1[cse_var_2] = tensor_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax2, ax3 in T.grid(9, 13):\n                cse_var_5: T.int32 = ax2 * 13\n                cse_var_4: T.int32 = ax0_ax1_fused * 117 + cse_var_5 + ax3\n                T_divide_1 = T.Buffer((27846,), data=T_divide.data)\n                T_divide_1[cse_var_4] = data_1[cse_var_4] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[cse_var_5 + ax3], T.float32(0.75))", "op_args": [14, 17, 9, 13]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 858; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1073; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 2145) {\n      normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 13, 11, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(858, 40):\n            data_1 = T.Buffer((34320,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(40):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [12, 13, 11, 20]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 360; ++ax0_ax1_fused) {\n    float tensor[20];\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        tensor[ax3] = 0.000000e+00f;\n        tensor[ax3] = (tensor[ax3] + (data[(((ax0_ax1_fused * 160) + (ax2 * 20)) + ax3)] * data[(((ax0_ax1_fused * 160) + (ax2 * 20)) + ax3)]));\n      }\n      for (int32_t ax3_1 = 0; ax3_1 < 20; ++ax3_1) {\n        T_divide[(((ax0_ax1_fused * 160) + (ax2 * 20)) + ax3_1)] = (data[(((ax0_ax1_fused * 160) + (ax2 * 20)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 20, 8, 20), \"float32\"), T_divide: T.Buffer((18, 20, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(360):\n            tensor = T.allocate([20], \"float32\", \"global\")\n            for ax2 in range(8):\n                tensor_1 = T.Buffer((20,), data=tensor)\n                data_1 = T.Buffer((57600,), data=data.data)\n                for ax3 in range(20):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 160 + ax2 * 20 + ax3\n                    tensor_1[ax3] = T.float32(0)\n                    tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(20):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 160 + ax2 * 20 + ax3\n                    T_divide_1 = T.Buffer((57600,), data=T_divide.data)\n                    T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))", "op_args": [18, 20, 8, 20]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 85; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 107; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 425) {\n      normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 5, 10, 17), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(85, 40):\n            data_1 = T.Buffer((3400,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(40):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [4, 5, 10, 17]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    float tensor[49];\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax3 = 0; ax3 < 7; ++ax3) {\n        tensor[((ax1 * 7) + ax3)] = 0.000000e+00f;\n        tensor[((ax1 * 7) + ax3)] = (tensor[((ax1 * 7) + ax3)] + (data[(((ax0 * 49) + (ax1 * 7)) + ax3)] * data[(((ax0 * 49) + (ax1 * 7)) + ax3)]));\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 7; ++ax1_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 7; ++ax3_1) {\n        T_divide[(((ax0 * 49) + (ax1_1 * 7)) + ax3_1)] = (data[(((ax0 * 49) + (ax1_1 * 7)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[((ax1_1 * 7) + ax3_1)])), 7.500000e-01f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  if (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) < 245) {\n    T_divide[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])), 7.500000e-01f));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 7, 1, 7), \"float32\"), T_divide: T.Buffer((5, 7, 1, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            tensor = T.allocate([49], \"float32\", \"global\")\n            tensor_1 = T.Buffer((49,), data=tensor)\n            data_1 = T.Buffer((245,), data=data.data)\n            for ax1, ax3 in T.grid(7, 7):\n                cse_var_3: T.int32 = ax1 * 7\n                cse_var_2: T.int32 = cse_var_3 + ax3\n                cse_var_1: T.int32 = ax0 * 49 + cse_var_3 + ax3\n                tensor_1[cse_var_2] = T.float32(0)\n                tensor_1[cse_var_2] = tensor_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax1, ax3 in T.grid(7, 7):\n                cse_var_5: T.int32 = ax1 * 7\n                cse_var_4: T.int32 = ax0 * 49 + cse_var_5 + ax3\n                T_divide_1 = T.Buffer((245,), data=T_divide.data)\n                T_divide_1[cse_var_4] = data_1[cse_var_4] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[cse_var_5 + ax3], T.float32(0.75))", "op_args": [5, 7, 1, 7]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[20];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 507; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 20; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 20) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 13; ++k0) {\n    for (int k1 = 0; k1 < 3; ++k1) {\n      for (int k2 = 0; k2 < 20; ++k2) {\n        for (int k3 = 0; k3 < 13; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 780) + (k1 * 260)) + (k2 * 13)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 3, 20, 13), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([20], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((20,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(20):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(507, 20):\n            data_1 = T.Buffer((10140,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 20 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(20):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [13, 3, 20, 13]}{"op_name": "lrn", "c_code": "void default_function_kernel(float* T_divide, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    float tensor[6];\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n          tensor[ax3] = 0.000000e+00f;\n          tensor[ax3] = (tensor[ax3] + (data[((((ax0 * 936) + (ax1 * 78)) + (ax2 * 6)) + ax3)] * data[((((ax0 * 936) + (ax1 * 78)) + (ax2 * 6)) + ax3)]));\n        }\n        for (int32_t ax3_1 = 0; ax3_1 < 6; ++ax3_1) {\n          T_divide[((((ax0 * 936) + (ax1 * 78)) + (ax2 * 6)) + ax3_1)] = (data[((((ax0 * 936) + (ax1 * 78)) + (ax2 * 6)) + ax3_1)] / powf((2.000000e+00f + (1.000000e-04f * tensor[ax3_1])), 7.500000e-01f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ tensor) {\n  T_divide[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / powf((2.000000e+00f + (1.000000e-04f * tensor[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])), 7.500000e-01f));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ data, float* __restrict__ tensor) {\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (tensor[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 12, 13, 6), \"float32\"), T_divide: T.Buffer((6, 12, 13, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            tensor = T.allocate([6], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(12, 13):\n                tensor_1 = T.Buffer((6,), data=tensor, align=16)\n                data_1 = T.Buffer((5616,), data=data.data)\n                for ax3 in range(6):\n                    cse_var_1: T.int32 = ax0 * 936 + ax1 * 78 + ax2 * 6 + ax3\n                    tensor_1[ax3] = T.float32(0)\n                    tensor_1[ax3] = tensor_1[ax3] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(6):\n                    cse_var_2: T.int32 = ax0 * 936 + ax1 * 78 + ax2 * 6 + ax3\n                    T_divide_1 = T.Buffer((5616,), data=T_divide.data)\n                    T_divide_1[cse_var_2] = data_1[cse_var_2] / T.pow(T.float32(2) + T.float32(9.9999997473787516e-05) * tensor_1[ax3], T.float32(0.75))", "op_args": [6, 12, 13, 6]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 270; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 6; ++k1) {\n      for (int k2 = 0; k2 < 15; ++k2) {\n        for (int k3 = 0; k3 < 10; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 900) + (k1 * 150)) + (k2 * 10)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 6, 15, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(270, 40):\n            data_1 = T.Buffer((10800,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(40):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [12, 6, 15, 10]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[17];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 17; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2250; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 17; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 17) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 17; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 15; ++k0) {\n    for (int k1 = 0; k1 < 17; ++k1) {\n      for (int k2 = 0; k2 < 15; ++k2) {\n        for (int k3 = 0; k3 < 10; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 2550) + (k1 * 150)) + (k2 * 10)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 17, 15, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([17], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((17,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(17):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2250, 17):\n            data_1 = T.Buffer((38250,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 17 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(17):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [15, 17, 15, 10]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      MirrorPadInput[((i0 * 12) + i1)] = data[((((14 <= i0) ? (26 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 9) + ((i1 == 11) ? (19 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = data[((((21 <= ((int)blockIdx.x)) ? (26 - (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) / 3)) : ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 3) ? 0 : ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) / 3) - 1))) * 9) + (((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12) == 11) ? (19 - (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12)) : (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) % 6) < 1) ? (1 - (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12)) : ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12) - 2))))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 9), \"float32\"), MirrorPadInput: T.Buffer((16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            for i1 in range(12):\n                MirrorPadInput_1 = T.Buffer((192,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((117,), data=data.data)\n                MirrorPadInput_1[i0 * 12 + i1] = data_1[T.if_then_else(14 <= i0, 26 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 9 + T.if_then_else(i1 == 11, 19 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]", "op_args": [18, 6, 13, 9]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 418; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 11; ++k0) {\n    for (int k1 = 0; k1 < 16; ++k1) {\n      for (int k2 = 0; k2 < 5; ++k2) {\n        for (int k3 = 0; k3 < 19; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 1520) + (k1 * 95)) + (k2 * 19)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 16, 5, 19), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(418, 40):\n            data_1 = T.Buffer((16720,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(40):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [11, 16, 5, 19]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[56];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 266; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 56; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 56) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 466; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 2) + (((int)threadIdx.x) >> 4)) < 931) {\n      normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 16, 7, 7), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([56], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((56,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(56):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(266, 56):\n            data_1 = T.Buffer((14896,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 56 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(56):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [19, 16, 7, 7]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 23; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      MirrorPadInput[((i0 * 18) + i1)] = data[((((21 <= i0) ? (40 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 15) + ((i1 == 17) ? (31 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(23) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 23) + ((int)threadIdx.x))] = data[((((378 <= ((((int)blockIdx.x) * 23) + ((int)threadIdx.x))) ? (40 - (((((int)blockIdx.x) * 23) + ((int)threadIdx.x)) / 18)) : ((((((int)blockIdx.x) * 23) + ((int)threadIdx.x)) < 18) ? 0 : ((((((int)blockIdx.x) * 23) + ((int)threadIdx.x)) / 18) - 1))) * 15) + (((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 18) == 17) ? (31 - (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 18)) : (((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 18) < 2) ? (1 - (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 18)) : ((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 18) - 2))))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 15), \"float32\"), MirrorPadInput: T.Buffer((23, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(23):\n            for i1 in range(18):\n                MirrorPadInput_1 = T.Buffer((414,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((300,), data=data.data)\n                MirrorPadInput_1[i0 * 18 + i1] = data_1[T.if_then_else(21 <= i0, 40 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 15 + T.if_then_else(i1 == 17, 31 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]", "op_args": [12, 13, 20, 15]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[56];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 96; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 56; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 56) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 168; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 12, 7), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([56], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((56,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(56):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(96, 56):\n            data_1 = T.Buffer((5376,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 56 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(56):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [4, 16, 12, 7]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 23; ++i1) {\n      MirrorPadInput[((i0 * 23) + i1)] = data[((((7 <= i0) ? (12 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 20) + ((i1 == 22) ? (41 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) < 207) {\n    MirrorPadInput[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = data[((((161 <= ((((int)blockIdx.x) * 20) + ((int)threadIdx.x))) ? (12 - (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) / 23)) : ((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) < 23) ? 0 : ((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) / 23) - 1))) * 20) + (((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 23) == 22) ? (41 - (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 23)) : (((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 23) < 2) ? (1 - (((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 23)) : ((((((int)blockIdx.x) * 20) + ((int)threadIdx.x)) % 23) - 2))))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 20), \"float32\"), MirrorPadInput: T.Buffer((9, 23), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1 in range(23):\n                MirrorPadInput_1 = T.Buffer((207,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((120,), data=data.data)\n                MirrorPadInput_1[i0 * 23 + i1] = data_1[T.if_then_else(7 <= i0, 12 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 20 + T.if_then_else(i1 == 22, 41 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]", "op_args": [16, 5, 6, 20]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[30];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 30; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 105; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 30; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 30) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 30; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 7; ++k0) {\n    for (int k1 = 0; k1 < 2; ++k1) {\n      for (int k2 = 0; k2 < 15; ++k2) {\n        for (int k3 = 0; k3 < 15; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 450) + (k1 * 225)) + (k2 * 15)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 15, 15), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([30], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((30,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(30):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(105, 30):\n            data_1 = T.Buffer((3150,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 30 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(30):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [7, 2, 15, 15]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 396; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((352 <= i0_i1_fused) ? (30 - (i0_i1_fused / 22)) : ((i0_i1_fused < 22) ? 0 : ((i0_i1_fused / 22) - 1))) * 19) + (((i0_i1_fused % 22) == 21) ? 18 : (((i0_i1_fused % 22) < 2) ? (1 - (i0_i1_fused % 22)) : ((i0_i1_fused % 22) - 2))))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = data[((((88 <= ((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 2))) ? (30 - (((((int)blockIdx.x) * 18) + (((int)threadIdx.x) >> 1)) / 11)) : ((((((int)blockIdx.x) * 18) + (((int)threadIdx.x) >> 1)) < 11) ? 0 : ((((((int)blockIdx.x) * 18) + (((int)threadIdx.x) >> 1)) / 11) - 1))) * 19) + (((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 22) == 21) ? (39 - (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 22)) : (((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) % 11) < 1) ? (1 - (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 22)) : ((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 22) - 2))))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 19), \"float32\"), MirrorPadInput: T.Buffer((18, 22), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(396):\n            cse_var_2: T.int32 = i0_i1_fused // 22\n            cse_var_1: T.int32 = i0_i1_fused % 22\n            MirrorPadInput_1 = T.Buffer((396,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((285,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(352 <= i0_i1_fused, 30 - cse_var_2, T.if_then_else(i0_i1_fused < 22, 0, cse_var_2 - 1)) * 19 + T.if_then_else(cse_var_1 == 21, 18, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]", "op_args": [14, 7, 15, 19]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 63; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 3; ++k0) {\n    for (int k1 = 0; k1 < 14; ++k1) {\n      for (int k2 = 0; k2 < 6; ++k2) {\n        for (int k3 = 0; k3 < 16; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 1344) + (k1 * 96)) + (k2 * 16)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 14, 6, 16), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(63, 64):\n            data_1 = T.Buffer((4032,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [3, 14, 6, 16]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[18];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 660; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 18; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 18) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 6; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 15; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 990) + (k1 * 165)) + (k2 * 15)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 6, 11, 15), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([18], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((18,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(18):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(660, 18):\n            data_1 = T.Buffer((11880,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 18 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(18):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [12, 6, 11, 15]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 23; ++i0) {\n    for (int32_t i1 = 0; i1 < 22; ++i1) {\n      MirrorPadInput[((i0 * 22) + i1)] = data[((((21 <= i0) ? (40 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 19) + ((i1 == 21) ? (39 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 253) {\n    MirrorPadInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((231 <= ((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))) ? (40 - (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 11)) : ((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 11) ? 0 : ((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 11) - 1))) * 19) + (((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22) == 21) ? (39 - (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22)) : (((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 1)) % 11) < 1) ? (1 - (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22)) : ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 22) - 2))))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 19), \"float32\"), MirrorPadInput: T.Buffer((23, 22), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(23):\n            for i1 in range(22):\n                MirrorPadInput_1 = T.Buffer((506,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((380,), data=data.data)\n                MirrorPadInput_1[i0 * 22 + i1] = data_1[T.if_then_else(21 <= i0, 40 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 19 + T.if_then_else(i1 == 21, 39 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]", "op_args": [8, 15, 20, 19]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[28];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 28; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 39; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 28; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 28) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 28; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 35; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 8) + (((int)threadIdx.x) >> 2)) < 273) {\n      normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 3, 13, 7), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([28], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((28,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(28):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(39, 28):\n            data_1 = T.Buffer((1092,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 28 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(28):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [4, 3, 13, 7]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 70; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((60 <= i0_i1_fused) ? (22 - (i0_i1_fused / 5)) : ((i0_i1_fused < 5) ? 0 : ((i0_i1_fused / 5) - 1))) * 2) + (((i0_i1_fused % 5) == 4) ? 1 : (((i0_i1_fused % 5) < 2) ? (1 - (i0_i1_fused % 5)) : ((i0_i1_fused % 5) - 2))))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) < 35) {\n    MirrorPadInput[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = data[((((15 <= ((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2))) ? (22 - (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 5)) : ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 5) ? 0 : ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 5) - 1))) * 2) + (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 5) == 4) ? (5 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 5)) : (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 5) < 2) ? (1 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 5)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 5) - 2))))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 2), \"float32\"), MirrorPadInput: T.Buffer((14, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(70):\n            cse_var_2: T.int32 = i0_i1_fused // 5\n            cse_var_1: T.int32 = i0_i1_fused % 5\n            MirrorPadInput_1 = T.Buffer((70,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((22,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(60 <= i0_i1_fused, 22 - cse_var_2, T.if_then_else(i0_i1_fused < 5, 0, cse_var_2 - 1)) * 2 + T.if_then_else(cse_var_1 == 4, 1, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]", "op_args": [11, 3, 11, 2]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[33];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 33; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 130; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 33; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 33) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 33; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 11; ++k0) {\n    for (int k1 = 0; k1 < 6; ++k1) {\n      for (int k2 = 0; k2 < 13; ++k2) {\n        for (int k3 = 0; k3 < 5; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 390) + (k1 * 65)) + (k2 * 5)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 6, 13, 5), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([33], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((33,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(33):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(130, 33):\n            data_1 = T.Buffer((4290,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 33 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(33):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [11, 6, 13, 5]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 414; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((378 <= i0_i1_fused) ? (40 - (i0_i1_fused / 18)) : ((i0_i1_fused < 18) ? 0 : ((i0_i1_fused / 18) - 1))) * 15) + (((i0_i1_fused % 18) == 17) ? 14 : (((i0_i1_fused % 18) < 2) ? (1 - (i0_i1_fused % 18)) : ((i0_i1_fused % 18) - 2))))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 207) {\n    MirrorPadInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = data[((((189 <= ((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))) ? (40 - (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)) : ((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 9) ? 0 : ((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9) - 1))) * 15) + (((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 18) == 17) ? (31 - (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 18)) : (((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) % 9) < 1) ? (1 - (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 18)) : ((((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 18) - 2))))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 15), \"float32\"), MirrorPadInput: T.Buffer((23, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(414):\n            cse_var_2: T.int32 = i0_i1_fused // 18\n            cse_var_1: T.int32 = i0_i1_fused % 18\n            MirrorPadInput_1 = T.Buffer((414,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((300,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(378 <= i0_i1_fused, 40 - cse_var_2, T.if_then_else(i0_i1_fused < 18, 0, cse_var_2 - 1)) * 15 + T.if_then_else(cse_var_1 == 17, 14, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]", "op_args": [7, 19, 20, 15]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[15];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 15; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 117; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 15; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 15) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 15; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 3; ++k0) {\n    for (int k1 = 0; k1 < 13; ++k1) {\n      for (int k2 = 0; k2 < 15; ++k2) {\n        for (int k3 = 0; k3 < 3; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 585) + (k1 * 45)) + (k2 * 3)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 13, 15, 3), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([15], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((15,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(15):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(117, 15):\n            data_1 = T.Buffer((1755,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 15 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(15):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [3, 13, 15, 3]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[30];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 30; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 42; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 30; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 30) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 30; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 7; ++k0) {\n    for (int k1 = 0; k1 < 20; ++k1) {\n      for (int k2 = 0; k2 < 9; ++k2) {\n        data_red[0] = max(data_red[0], data[(((k0 * 180) + (k1 * 9)) + k2)]);\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 9, 1), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([30], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((30,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(30):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(42, 30):\n            data_1 = T.Buffer((1260,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 30 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(30):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [7, 20, 9, 1]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    MirrorPadInput[i0_i1_fused] = data[((((140 <= i0_i1_fused) ? (12 - (i0_i1_fused / 20)) : ((i0_i1_fused < 20) ? 0 : ((i0_i1_fused / 20) - 1))) * 17) + (((i0_i1_fused % 20) == 19) ? 16 : (((i0_i1_fused % 20) < 2) ? (1 - (i0_i1_fused % 20)) : ((i0_i1_fused % 20) - 2))))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = data[((((140 <= ((((int)blockIdx.x) * 3) + ((int)threadIdx.x))) ? (12 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) / 20)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) < 20) ? 0 : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) / 20) - 1))) * 17) + (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 20) == 19) ? (35 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 20)) : (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 20) < 2) ? (1 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 20)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 20) - 2))))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 17), \"float32\"), MirrorPadInput: T.Buffer((9, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(180):\n            cse_var_2: T.int32 = i0_i1_fused // 20\n            cse_var_1: T.int32 = i0_i1_fused % 20\n            MirrorPadInput_1 = T.Buffer((180,), data=MirrorPadInput.data)\n            data_1 = T.Buffer((102,), data=data.data)\n            MirrorPadInput_1[i0_i1_fused] = data_1[T.if_then_else(140 <= i0_i1_fused, 12 - cse_var_2, T.if_then_else(i0_i1_fused < 20, 0, cse_var_2 - 1)) * 17 + T.if_then_else(cse_var_1 == 19, 16, T.if_then_else(cse_var_1 < 2, 1 - cse_var_1, cse_var_1 - 2))]", "op_args": [12, 14, 6, 17]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[24];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 24; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 182; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 24; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 24) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 24; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 13; ++k0) {\n    for (int k1 = 0; k1 < 14; ++k1) {\n      for (int k2 = 0; k2 < 8; ++k2) {\n        for (int k3 = 0; k3 < 3; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 336) + (k1 * 24)) + (k2 * 3)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 14, 8, 3), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([24], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((24,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(24):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(182, 24):\n            data_1 = T.Buffer((4368,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 24 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(24):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [13, 14, 8, 3]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 23; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      MirrorPadInput[((i0 * 13) + i1)] = data[((((21 <= i0) ? (40 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 10) + ((i1 == 12) ? (21 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 42) + ((int)threadIdx.x)) < 299) {\n    MirrorPadInput[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = data[((((13 <= ((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 21))) ? (40 - (((((int)blockIdx.x) * 42) + ((int)threadIdx.x)) / 13)) : ((((((int)blockIdx.x) * 42) + ((int)threadIdx.x)) < 13) ? 0 : ((((((int)blockIdx.x) * 42) + ((int)threadIdx.x)) / 13) - 1))) * 10) + (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 13) == 12) ? (21 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 13)) : (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 13) < 2) ? (1 - (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 13)) : ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 13) - 2))))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 10), \"float32\"), MirrorPadInput: T.Buffer((23, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(23):\n            for i1 in range(13):\n                MirrorPadInput_1 = T.Buffer((299,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((200,), data=data.data)\n                MirrorPadInput_1[i0 * 13 + i1] = data_1[T.if_then_else(21 <= i0, 40 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 10 + T.if_then_else(i1 == 12, 21 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]", "op_args": [2, 4, 20, 10]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[57];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 57; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 400; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 57; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 57) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 57; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 5; ++k0) {\n    for (int k1 = 0; k1 < 16; ++k1) {\n      for (int k2 = 0; k2 < 15; ++k2) {\n        for (int k3 = 0; k3 < 19; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 4560) + (k1 * 285)) + (k2 * 19)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 15, 19), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([57], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((57,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(57):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(400, 57):\n            data_1 = T.Buffer((22800,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 57 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(57):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [5, 16, 15, 19]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 175; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = -3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 350; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = max(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = max(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 4, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(175, 64):\n            data_1 = T.Buffer((11200,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [7, 20, 4, 20]}{"op_name": "mirror_pad", "c_code": "void default_function_kernel(float* MirrorPadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 22; ++i1) {\n      MirrorPadInput[((i0 * 22) + i1)] = data[((((6 <= i0) ? (10 - i0) : ((i0 < 1) ? (0 - i0) : (i0 - 1))) * 19) + ((i1 == 21) ? (39 - i1) : ((i1 < 2) ? (1 - i1) : (i1 - 2))))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(22) default_function_kernel(float* __restrict__ MirrorPadInput, float* __restrict__ data) {\n  MirrorPadInput[((((int)blockIdx.x) * 22) + ((int)threadIdx.x))] = data[((((6 <= ((int)blockIdx.x)) ? (10 - ((int)blockIdx.x)) : ((((int)blockIdx.x) < 1) ? (0 - ((int)blockIdx.x)) : (((int)blockIdx.x) - 1))) * 19) + ((((int)threadIdx.x) == 21) ? 18 : ((((int)threadIdx.x) < 2) ? (1 - ((int)threadIdx.x)) : (((int)threadIdx.x) - 2))))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 19), \"float32\"), MirrorPadInput: T.Buffer((8, 22), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1 in range(22):\n                MirrorPadInput_1 = T.Buffer((176,), data=MirrorPadInput.data)\n                data_1 = T.Buffer((95,), data=data.data)\n                MirrorPadInput_1[i0 * 22 + i1] = data_1[T.if_then_else(6 <= i0, 10 - i0, T.if_then_else(i0 < 1, 0 - i0, i0 - 1)) * 19 + T.if_then_else(i1 == 21, 39 - i1, T.if_then_else(i1 < 2, 1 - i1, i1 - 2))]", "op_args": [9, 14, 5, 19]}{"op_name": "max", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[64];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = -3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 75; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 64; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = max(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 64) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = -3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 64; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = max(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = -3.402823e+38f;\n  for (int k0 = 0; k0 < 20; ++k0) {\n    for (int k1 = 0; k1 < 6; ++k1) {\n      for (int k2 = 0; k2 < 4; ++k2) {\n        for (int k3 = 0; k3 < 10; ++k3) {\n          data_red[0] = max(data_red[0], data[((((k0 * 240) + (k1 * 40)) + (k2 * 10)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 6, 4, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([64], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((64,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(64):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(75, 64):\n            data_1 = T.Buffer((4800,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.max(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 64 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(-3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(64):\n            data_red_1[0] = T.max(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [20, 6, 4, 10]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[33];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 33; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 420; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 33; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 33) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 33; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 434; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 8) + (((int)threadIdx.x) >> 2)) < 3465) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 14, 5, 18), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([33], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((33,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(33):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(420, 33):\n            data_1 = T.Buffer((13860,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 33 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(33):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [11, 14, 5, 18]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      PadInput[((i0 * 16) + i1)] = (((((1 <= i0) && (i0 < 16)) && (2 <= i1)) && (i1 < 15)) ? data[(((i0 * 13) + i1) - 15)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (((((1 <= ((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4))) && (((int)blockIdx.x) < 8)) && (2 <= (((int)threadIdx.x) & 15))) && ((((int)threadIdx.x) & 15) < 15)) ? data[((((((int)blockIdx.x) * 26) + ((((int)threadIdx.x) >> 4) * 13)) + (((int)threadIdx.x) & 15)) - 15)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 13), \"float32\"), PadInput: T.Buffer((18, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            for i1 in range(16):\n                PadInput_1 = T.Buffer((288,), data=PadInput.data)\n                data_1 = T.Buffer((195,), data=data.data)\n                PadInput_1[i0 * 16 + i1] = T.if_then_else(1 <= i0 and i0 < 16 and 2 <= i1 and i1 < 15, data_1[i0 * 13 + i1 - 15], T.float32(0))", "op_args": [2, 5, 15, 13]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[17];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 17; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 312; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 17; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 17) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 17; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 166; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 663) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 13, 2, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([17], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((17,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(17):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(312, 17):\n            data_1 = T.Buffer((5304,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 17 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(17):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [17, 13, 2, 12]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      PadInput[((i0 * 16) + i1)] = (((((1 <= i0) && (i0 < 10)) && (2 <= i1)) && (i1 < 15)) ? data[(((i0 * 13) + i1) - 15)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (((((1 <= ((int)blockIdx.x)) && (((int)blockIdx.x) < 10)) && (2 <= ((int)threadIdx.x))) && (((int)threadIdx.x) < 15)) ? data[(((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) - 15)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 13), \"float32\"), PadInput: T.Buffer((12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(12):\n            for i1 in range(16):\n                PadInput_1 = T.Buffer((192,), data=PadInput.data)\n                data_1 = T.Buffer((117,), data=data.data)\n                PadInput_1[i0 * 16 + i1] = T.if_then_else(1 <= i0 and i0 < 10 and 2 <= i1 and i1 < 15, data_1[i0 * 13 + i1 - 15], T.float32(0))", "op_args": [15, 5, 9, 13]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[36];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 36; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 275; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 36; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 36) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 36; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 5; ++k0) {\n    for (int k1 = 0; k1 < 11; ++k1) {\n      for (int k2 = 0; k2 < 20; ++k2) {\n        for (int k3 = 0; k3 < 9; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 1980) + (k1 * 180)) + (k2 * 9)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 11, 20, 9), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([36], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((36,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(36):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(275, 36):\n            data_1 = T.Buffer((9900,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 36 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(36):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [5, 11, 20, 9]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 138; ++i0_i1_fused) {\n    PadInput[i0_i1_fused] = (((((6 <= i0_i1_fused) && (i0_i1_fused < 126)) && (2 <= (i0_i1_fused % 6))) && ((i0_i1_fused % 6) < 5)) ? data[((((i0_i1_fused / 6) * 3) + (i0_i1_fused % 6)) - 5)] : 0.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (((((1 <= ((int)blockIdx.x)) && (((int)blockIdx.x) < 21)) && (2 <= ((int)threadIdx.x))) && (((int)threadIdx.x) < 5)) ? data[(((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) - 5)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 3), \"float32\"), PadInput: T.Buffer((23, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(138):\n            cse_var_1: T.int32 = i0_i1_fused % 6\n            PadInput_1 = T.Buffer((138,), data=PadInput.data)\n            data_1 = T.Buffer((60,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(6 <= i0_i1_fused and i0_i1_fused < 126 and 2 <= cse_var_1 and cse_var_1 < 5, data_1[i0_i1_fused // 6 * 3 + cse_var_1 - 5], T.float32(0))", "op_args": [20, 11, 20, 3]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[20];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 374; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 20; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 20) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 234; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 935) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 11, 8, 5), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([20], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((20,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(20):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(374, 20):\n            data_1 = T.Buffer((7480,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 20 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(20):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [17, 11, 8, 5]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[44];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 44; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 864; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 44; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 44) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 44; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 12; ++k0) {\n    for (int k1 = 0; k1 < 18; ++k1) {\n      for (int k2 = 0; k2 < 16; ++k2) {\n        for (int k3 = 0; k3 < 11; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 3168) + (k1 * 176)) + (k2 * 11)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 18, 16, 11), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([44], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((44,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(44):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(864, 44):\n            data_1 = T.Buffer((38016,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 44 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(44):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [12, 18, 16, 11]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      PadInput[((i0 * 12) + i1)] = (((((1 <= i0) && (i0 < 16)) && (2 <= i1)) && (i1 < 11)) ? data[(((i0 * 9) + i1) - 11)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (((((3 <= ((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2))) && (((int)blockIdx.x) < 24)) && (1 <= (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) % 6))) && ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12) < 11)) ? data[((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) / 3) * 9) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12)) - 11)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 9), \"float32\"), PadInput: T.Buffer((18, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            for i1 in range(12):\n                PadInput_1 = T.Buffer((216,), data=PadInput.data)\n                data_1 = T.Buffer((135,), data=data.data)\n                PadInput_1[i0 * 12 + i1] = T.if_then_else(1 <= i0 and i0 < 16 and 2 <= i1 and i1 < 11, data_1[i0 * 9 + i1 - 11], T.float32(0))", "op_args": [15, 8, 15, 9]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[18];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2964; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 18; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 18) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 18; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 18; ++k0) {\n    for (int k1 = 0; k1 < 19; ++k1) {\n      for (int k2 = 0; k2 < 12; ++k2) {\n        for (int k3 = 0; k3 < 13; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 2964) + (k1 * 156)) + (k2 * 13)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 19, 12, 13), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([18], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((18,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(18):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(2964, 18):\n            data_1 = T.Buffer((53352,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 18 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(18):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [18, 19, 12, 13]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 105; ++i0_i1_fused) {\n    PadInput[i0_i1_fused] = (((((15 <= i0_i1_fused) && (i0_i1_fused < 75)) && (2 <= (i0_i1_fused % 15))) && ((i0_i1_fused % 15) < 14)) ? data[((((i0_i1_fused / 15) * 12) + (i0_i1_fused % 15)) - 14)] : 0.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 105) {\n    PadInput[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (((((15 <= ((((int)blockIdx.x) * 4) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 75)) && (2 <= (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 15))) && ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 15) < 14)) ? data[((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) / 15) * 12) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 15)) - 14)] : 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 12), \"float32\"), PadInput: T.Buffer((7, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(105):\n            cse_var_1: T.int32 = i0_i1_fused % 15\n            PadInput_1 = T.Buffer((105,), data=PadInput.data)\n            data_1 = T.Buffer((48,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(15 <= i0_i1_fused and i0_i1_fused < 75 and 2 <= cse_var_1 and cse_var_1 < 14, data_1[i0_i1_fused // 15 * 12 + cse_var_1 - 14], T.float32(0))", "op_args": [5, 11, 4, 12]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[40];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 20; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 40; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 40) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 40; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 4; ++k0) {\n    for (int k2 = 0; k2 < 10; ++k2) {\n      for (int k3 = 0; k3 < 20; ++k3) {\n        data_red[0] = min(data_red[0], data[(((k0 * 200) + (k2 * 20)) + k3)]);\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 1, 10, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([40], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((40,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(40):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(20, 40):\n            data_1 = T.Buffer((800,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 40 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(40):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [4, 1, 10, 20]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[54];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 54; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 160; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 54; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 54) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 54; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 270; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 9, 8, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([54], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((54,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(54):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(160, 54):\n            data_1 = T.Buffer((8640,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 54 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(54):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [10, 9, 8, 12]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      PadInput[((i0 * 19) + i1)] = (((((1 <= i0) && (i0 < 18)) && (2 <= i1)) && (i1 < 18)) ? data[(((i0 * 16) + i1) - 18)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = (((((19 <= ((((int)blockIdx.x) * 10) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 5) + (((int)threadIdx.x) >> 1)) < 171)) && (2 <= (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 19))) && ((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 19) < 18)) ? data[((((((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) / 19) * 16) + (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 19)) - 18)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 16), \"float32\"), PadInput: T.Buffer((20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1 in range(19):\n                PadInput_1 = T.Buffer((380,), data=PadInput.data)\n                data_1 = T.Buffer((272,), data=data.data)\n                PadInput_1[i0 * 19 + i1] = T.if_then_else(1 <= i0 and i0 < 18 and 2 <= i1 and i1 < 18, data_1[i0 * 16 + i1 - 18], T.float32(0))", "op_args": [14, 20, 17, 16]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[56];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 114; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 56; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 56) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 56; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 3.402823e+38f;\n  for (int k0 = 0; k0 < 7; ++k0) {\n    for (int k1 = 0; k1 < 16; ++k1) {\n      for (int k2 = 0; k2 < 19; ++k2) {\n        for (int k3 = 0; k3 < 3; ++k3) {\n          data_red[0] = min(data_red[0], data[((((k0 * 912) + (k1 * 57)) + (k2 * 3)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 16, 19, 3), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([56], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((56,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(56):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(114, 56):\n            data_1 = T.Buffer((6384,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 56 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(56):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [7, 16, 19, 3]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[32];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 425; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 32; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 32; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 425; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 17, 10, 20), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([32], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((32,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(32):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(425, 32):\n            data_1 = T.Buffer((13600,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 32 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(32):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [4, 17, 10, 20]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 23; ++i1) {\n      PadInput[((i0 * 23) + i1)] = (((((1 <= i0) && (i0 < 7)) && (2 <= i1)) && (i1 < 22)) ? data[(((i0 * 20) + i1) - 22)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 207) {\n    PadInput[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (((((23 <= ((((int)blockIdx.x) * 4) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 161)) && (2 <= (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 23))) && ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 23) < 22)) ? data[((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) / 23) * 20) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 23)) - 22)] : 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 20), \"float32\"), PadInput: T.Buffer((9, 23), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1 in range(23):\n                PadInput_1 = T.Buffer((207,), data=PadInput.data)\n                data_1 = T.Buffer((120,), data=data.data)\n                PadInput_1[i0 * 23 + i1] = T.if_then_else(1 <= i0 and i0 < 7 and 2 <= i1 and i1 < 22, data_1[i0 * 20 + i1 - 22], T.float32(0))", "op_args": [1, 6, 6, 20]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[60];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 90; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 60; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 60) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 60; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 169; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 675) {\n      normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 5, 10, 6), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([60], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((60,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(60):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(90, 60):\n            data_1 = T.Buffer((5400,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 60 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(60):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [18, 5, 10, 6]}{"op_name": "min", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[34];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 34; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 3.402823e+38f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 480; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 34; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = min(data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner], data[((k0_k1_fused_k2_fused_k3_fused_outer * 34) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 3.402823e+38f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 34; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = min(data_red[0], data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 3.402823e+38f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 510; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = min(normal_reduce_temp0[0], data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = min(red_buf0[0], t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 8, 12, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([34], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((34,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(34):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(480, 34):\n            data_1 = T.Buffer((16320,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = T.min(data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner], data_1[k0_k1_fused_k2_fused_k3_fused_outer * 34 + k0_k1_fused_k2_fused_k3_fused_inner])\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(3.4028234663852886e+38)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(34):\n            data_red_1[0] = T.min(data_red_1[0], data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v])", "op_args": [17, 8, 12, 10]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      PadInput[((i0 * 14) + i1)] = (((((1 <= i0) && (i0 < 16)) && (2 <= i1)) && (i1 < 13)) ? data[(((i0 * 11) + i1) - 13)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (((((7 <= ((((int)blockIdx.x) * 18) + (((int)threadIdx.x) >> 1))) && (((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 2)) < 56)) && (1 <= (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) % 7))) && ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 14) < 13)) ? data[((((((((int)blockIdx.x) * 18) + (((int)threadIdx.x) >> 1)) / 7) * 11) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 14)) - 13)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 11), \"float32\"), PadInput: T.Buffer((18, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            for i1 in range(14):\n                PadInput_1 = T.Buffer((252,), data=PadInput.data)\n                data_1 = T.Buffer((165,), data=data.data)\n                PadInput_1[i0 * 14 + i1] = T.if_then_else(1 <= i0 and i0 < 16 and 2 <= i1 and i1 < 13, data_1[i0 * 11 + i1 - 13], T.float32(0))", "op_args": [18, 16, 15, 11]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1980; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 17) + i3)] = (data[((i0_i1_fused_i2_fused * 17) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 9, 20, 17), \"float32\"), compute: T.Buffer((11, 9, 20, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1980):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 17 + i3\n                compute_1 = T.Buffer((33660,), data=compute.data)\n                data_1 = T.Buffer((33660,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [11, 9, 20, 17]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      PadInput[((i0 * 17) + i1)] = (((((1 <= i0) && (i0 < 5)) && (2 <= i1)) && (i1 < 16)) ? data[(((i0 * 14) + i1) - 16)] : 0.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 119) {\n    PadInput[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (((((17 <= ((((int)blockIdx.x) * 4) + ((int)threadIdx.x))) && (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 85)) && (2 <= (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 17))) && ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 17) < 16)) ? data[((((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) / 17) * 14) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 17)) - 16)] : 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 14), \"float32\"), PadInput: T.Buffer((7, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1 in range(17):\n                PadInput_1 = T.Buffer((119,), data=PadInput.data)\n                data_1 = T.Buffer((56,), data=data.data)\n                PadInput_1[i0 * 17 + i1] = T.if_then_else(1 <= i0 and i0 < 5 and 2 <= i1 and i1 < 16, data_1[i0 * 14 + i1 - 16], T.float32(0))", "op_args": [12, 15, 4, 14]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 608; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 9; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 9) + i3)] = (data[((i0_i1_fused_i2_fused * 9) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 19, 8, 9), \"float32\"), compute: T.Buffer((4, 19, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(608):\n            for i3 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 9 + i3\n                compute_1 = T.Buffer((5472,), data=compute.data)\n                data_1 = T.Buffer((5472,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [4, 19, 8, 9]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        compute[(((i0 * 240) + (i1 * 12)) + i3)] = (data[(((i0 * 240) + (i1 * 12)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 20, 1, 12), \"float32\"), compute: T.Buffer((10, 20, 1, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i3 in T.grid(20, 12):\n                cse_var_1: T.int32 = i0 * 240 + i1 * 12 + i3\n                compute_1 = T.Buffer((2400,), data=compute.data)\n                data_1 = T.Buffer((2400,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [10, 20, 1, 12]}{"op_name": "pad", "c_code": "void default_function_kernel(float* PadInput, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    PadInput[i0_i1_fused] = (((((9 <= i0_i1_fused) && (i0_i1_fused < 117)) && (2 <= (i0_i1_fused % 9))) && ((i0_i1_fused % 9) < 8)) ? data[((((i0_i1_fused / 9) * 6) + (i0_i1_fused % 9)) - 8)] : 0.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ PadInput, float* __restrict__ data) {\n  PadInput[((int)blockIdx.x)] = (((((9 <= ((int)blockIdx.x)) && (((int)blockIdx.x) < 117)) && (2 <= (((int)blockIdx.x) % 9))) && ((((int)blockIdx.x) % 9) < 8)) ? data[((((((int)blockIdx.x) / 9) * 6) + (((int)blockIdx.x) % 9)) - 8)] : 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 6), \"float32\"), PadInput: T.Buffer((15, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(135):\n            cse_var_1: T.int32 = i0_i1_fused % 9\n            PadInput_1 = T.Buffer((135,), data=PadInput.data)\n            data_1 = T.Buffer((72,), data=data.data)\n            PadInput_1[i0_i1_fused] = T.if_then_else(9 <= i0_i1_fused and i0_i1_fused < 117 and 2 <= cse_var_1 and cse_var_1 < 8, data_1[i0_i1_fused // 9 * 6 + cse_var_1 - 8], T.float32(0))", "op_args": [1, 2, 12, 6]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 85; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 19; ++i3) {\n        compute[(((i0_i1_fused * 57) + (i2 * 19)) + i3)] = (data[(((i0_i1_fused * 57) + (i2 * 19)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 4845) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * -1.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 17, 3, 19), \"float32\"), compute: T.Buffer((5, 17, 3, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(85):\n            for i2, i3 in T.grid(3, 19):\n                cse_var_1: T.int32 = i0_i1_fused * 57 + i2 * 19 + i3\n                compute_1 = T.Buffer((4845,), data=compute.data)\n                data_1 = T.Buffer((4845,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [5, 17, 3, 19]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1944; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[3];\n    for (int32_t ax1_ax2_fused_s = 0; ax1_ax2_fused_s < 3; ++ax1_ax2_fused_s) {\n      pad_temp[ax1_ax2_fused_s] = (((1 <= (((ax0_ax1_fused_ax2_fused % 6) * 2) + ax1_ax2_fused_s)) && (((ax1_ax2_fused_s >> 1) + (ax0_ax1_fused_ax2_fused % 6)) < 6)) ? data[(((((ax0_ax1_fused_ax2_fused / 6) * 11) + ((ax0_ax1_fused_ax2_fused % 6) * 2)) + ax1_ax2_fused_s) - 1)] : -3.402823e+38f);\n    }\n    pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      pool_max[ax0_ax1_fused_ax2_fused] = max(pool_max[ax0_ax1_fused_ax2_fused], pad_temp[rv0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 243) {\n    pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 243) {\n      pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (((1 <= (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 6) * 2) + rv0)) && (((rv0 >> 1) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 6)) < 6)) ? data[(((((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) / 3) * 11) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 6) * 2)) + rv0) - 1)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 18, 11), \"float32\"), pool_max: T.Buffer((18, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1944):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n            for ax1_ax2_fused_s in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused % 6\n                cse_var_1: T.int32 = cse_var_2 * 2\n                data_1 = T.Buffer((3564,), data=data.data)\n                pad_temp_1[ax1_ax2_fused_s] = T.if_then_else(1 <= cse_var_1 + ax1_ax2_fused_s and ax1_ax2_fused_s // 2 + cse_var_2 < 6, data_1[ax0_ax1_fused_ax2_fused // 6 * 11 + cse_var_1 + ax1_ax2_fused_s - 1], T.float32(-3.4028234663852886e+38))\n            pool_max_1 = T.Buffer((1944,), data=pool_max.data)\n            pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(pool_max_1[ax0_ax1_fused_ax2_fused], pad_temp_1[rv0])", "op_args": [18, 18, 6, 11]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 320; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 4, 2), \"float32\"), compute: T.Buffer((2, 20, 4, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(320):\n            compute_1 = T.Buffer((320,), data=compute.data)\n            data_1 = T.Buffer((320,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [2, 20, 4, 2]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 224; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[3];\n    for (int32_t ax1_ax2_fused_s = 0; ax1_ax2_fused_s < 3; ++ax1_ax2_fused_s) {\n      pad_temp[ax1_ax2_fused_s] = (((1 <= (((ax0_ax1_fused_ax2_fused & 3) * 2) + ax1_ax2_fused_s)) && (((ax1_ax2_fused_s >> 1) + (ax0_ax1_fused_ax2_fused & 3)) < 4)) ? data[(((((ax0_ax1_fused_ax2_fused >> 2) * 7) + ((ax0_ax1_fused_ax2_fused & 3) * 2)) + ax1_ax2_fused_s) - 1)] : -3.402823e+38f);\n    }\n    pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      pool_max[ax0_ax1_fused_ax2_fused] = max(pool_max[ax0_ax1_fused_ax2_fused], pad_temp[rv0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], (((1 <= (((((int)threadIdx.x) & 3) * 2) + rv0)) && (((rv0 >> 1) + (((int)threadIdx.x) & 3)) < 4)) ? data[(((((((int)blockIdx.x) * 98) + ((((int)threadIdx.x) >> 2) * 7)) + ((((int)threadIdx.x) & 3) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 7, 7), \"float32\"), pool_max: T.Buffer((8, 7, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(224):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n            for ax1_ax2_fused_s in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused % 4\n                cse_var_1: T.int32 = cse_var_2 * 2\n                data_1 = T.Buffer((392,), data=data.data)\n                pad_temp_1[ax1_ax2_fused_s] = T.if_then_else(1 <= cse_var_1 + ax1_ax2_fused_s and ax1_ax2_fused_s // 2 + cse_var_2 < 4, data_1[ax0_ax1_fused_ax2_fused // 4 * 7 + cse_var_1 + ax1_ax2_fused_s - 1], T.float32(-3.4028234663852886e+38))\n            pool_max_1 = T.Buffer((224,), data=pool_max.data)\n            pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(pool_max_1[ax0_ax1_fused_ax2_fused], pad_temp_1[rv0])", "op_args": [8, 7, 2, 7]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 400) + (i1 * 200)) + (i2 * 20)) + i3)] = (data[((((i0 * 400) + (i1 * 200)) + (i2 * 20)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 2, 10, 20), \"float32\"), compute: T.Buffer((20, 2, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1, i2, i3 in T.grid(2, 10, 20):\n                cse_var_1: T.int32 = i0 * 400 + i1 * 200 + i2 * 20 + i3\n                compute_1 = T.Buffer((8000,), data=compute.data)\n                data_1 = T.Buffer((8000,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [20, 2, 10, 20]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 144; ++ax0_ax1_fused) {\n    float pad_temp[11];\n    for (int32_t ax1_ax2_fused_s = 0; ax1_ax2_fused_s < 11; ++ax1_ax2_fused_s) {\n      pad_temp[ax1_ax2_fused_s] = ((1 <= ax1_ax2_fused_s) ? data[(((ax0_ax1_fused * 10) + ax1_ax2_fused_s) - 1)] : -3.402823e+38f);\n    }\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      pool_max[((ax0_ax1_fused * 5) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        pool_max[((ax0_ax1_fused * 5) + ax2)] = max(pool_max[((ax0_ax1_fused * 5) + ax2)], pad_temp[((ax2 * 2) + rv0)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], ((1 <= (((((int)threadIdx.x) % 5) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 120) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 12, 10), \"float32\"), pool_max: T.Buffer((12, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(144):\n            pad_temp = T.allocate([11], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((11,), data=pad_temp, align=32)\n            for ax1_ax2_fused_s in range(11):\n                data_1 = T.Buffer((1440,), data=data.data)\n                pad_temp_1[ax1_ax2_fused_s] = T.if_then_else(1 <= ax1_ax2_fused_s, data_1[ax0_ax1_fused * 10 + ax1_ax2_fused_s - 1], T.float32(-3.4028234663852886e+38))\n            for ax2 in range(5):\n                pool_max_1 = T.Buffer((720,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 5 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 2 + rv0])", "op_args": [12, 12, 14, 10]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 308; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 4; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 4) + i3)] = (data[((i0_i1_fused_i2_fused * 4) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 4, 7, 4), \"float32\"), compute: T.Buffer((11, 4, 7, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(308):\n            for i3 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 4 + i3\n                compute_1 = T.Buffer((1232,), data=compute.data)\n                data_1 = T.Buffer((1232,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [11, 4, 7, 4]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 108; ++ax0_ax1_fused) {\n    float pad_temp[3];\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      for (int32_t ax1_ax2_fused_s = 0; ax1_ax2_fused_s < 3; ++ax1_ax2_fused_s) {\n        pad_temp[ax1_ax2_fused_s] = (((1 <= ((ax2 * 2) + ax1_ax2_fused_s)) && (((ax1_ax2_fused_s >> 1) + ax2) < 9)) ? data[((((ax0_ax1_fused * 17) + (ax2 * 2)) + ax1_ax2_fused_s) - 1)] : -3.402823e+38f);\n      }\n      pool_max[((ax0_ax1_fused * 9) + ax2)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        pool_max[((ax0_ax1_fused * 9) + ax2)] = max(pool_max[((ax0_ax1_fused * 9) + ax2)], pad_temp[rv0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 243) {\n    pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 243) {\n      pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], (((1 <= (((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 9) * 2) + rv0)) && (((rv0 >> 1) + (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 9)) < 9)) ? data[(((((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 9) * 17) + ((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 9) * 2)) + rv0) - 1)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 6, 17), \"float32\"), pool_max: T.Buffer((18, 6, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(108):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            for ax2 in range(9):\n                pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n                for ax1_ax2_fused_s in range(3):\n                    cse_var_1: T.int32 = ax2 * 2\n                    data_1 = T.Buffer((1836,), data=data.data)\n                    pad_temp_1[ax1_ax2_fused_s] = T.if_then_else(1 <= cse_var_1 + ax1_ax2_fused_s and ax1_ax2_fused_s // 2 + ax2 < 9, data_1[ax0_ax1_fused * 17 + cse_var_1 + ax1_ax2_fused_s - 1], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((972,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 9 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 9 + ax2\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0])", "op_args": [18, 6, 14, 17]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        for (int32_t i3 = 0; i3 < 4; ++i3) {\n          compute[((((i0 * 616) + (i1 * 56)) + (i2 * 4)) + i3)] = (data[((((i0 * 616) + (i1 * 56)) + (i2 * 4)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 77) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 11, 14, 4), \"float32\"), compute: T.Buffer((2, 11, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(2):\n            for i1, i2, i3 in T.grid(11, 14, 4):\n                cse_var_1: T.int32 = i0 * 616 + i1 * 56 + i2 * 4 + i3\n                compute_1 = T.Buffer((1232,), data=compute.data)\n                data_1 = T.Buffer((1232,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [2, 11, 14, 4]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  float pad_temp[24];\n  for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      pad_temp[((ax1 * 3) + ax2)] = ((1 <= ax2) ? data[(((ax1 * 2) + ax2) - 1)] : -3.402823e+38f);\n    }\n  }\n  for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n    pool_max[ax1_1] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      pool_max[ax1_1] = max(pool_max[ax1_1], pad_temp[((ax1_1 * 3) + rv0)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((int)threadIdx.x)] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((int)threadIdx.x)] = max(pool_max[((int)threadIdx.x)], ((1 <= rv0) ? data[(((((int)threadIdx.x) * 2) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 2), \"float32\"), pool_max: T.Buffer((1, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([24], \"float32\", \"global\")\n        pad_temp_1 = T.Buffer((24,), data=pad_temp)\n        for ax1, ax2 in T.grid(8, 3):\n            data_1 = T.Buffer((16,), data=data.data)\n            pad_temp_1[ax1 * 3 + ax2] = T.if_then_else(1 <= ax2, data_1[ax1 * 2 + ax2 - 1], T.float32(-3.4028234663852886e+38))\n        for ax1 in range(8):\n            pool_max_1 = T.Buffer((8,), data=pool_max.data)\n            pool_max_1[ax1] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                pool_max_1[ax1] = T.max(pool_max_1[ax1], pad_temp_1[ax1 * 3 + rv0])", "op_args": [1, 8, 11, 2]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        for (int32_t i3 = 0; i3 < 14; ++i3) {\n          compute[((((i0 * 140) + (i1 * 70)) + (i2 * 14)) + i3)] = (data[((((i0 * 140) + (i1 * 70)) + (i2 * 14)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 5, 14), \"float32\"), compute: T.Buffer((16, 2, 5, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            for i1, i2, i3 in T.grid(2, 5, 14):\n                cse_var_1: T.int32 = i0 * 140 + i1 * 70 + i2 * 14 + i3\n                compute_1 = T.Buffer((2240,), data=compute.data)\n                data_1 = T.Buffer((2240,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [16, 2, 5, 14]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 16; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 16) + i3)] = (data[((i0_i1_fused_i2_fused * 16) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 16, 6, 16), \"float32\"), compute: T.Buffer((9, 16, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            for i3 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 16 + i3\n                compute_1 = T.Buffer((13824,), data=compute.data)\n                data_1 = T.Buffer((13824,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [9, 16, 6, 16]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 24; ++ax0_ax1_fused) {\n    float pad_temp[21];\n    for (int32_t ax2 = 0; ax2 < 21; ++ax2) {\n      pad_temp[ax2] = ((1 <= ax2) ? data[(((ax0_ax1_fused * 20) + ax2) - 1)] : -3.402823e+38f);\n    }\n    for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n      pool_max[((ax0_ax1_fused * 10) + ax2_1)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        pool_max[((ax0_ax1_fused * 10) + ax2_1)] = max(pool_max[((ax0_ax1_fused * 10) + ax2_1)], pad_temp[((ax2_1 * 2) + rv0)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], ((1 <= (((((int)threadIdx.x) % 10) * 2) + rv0)) ? data[((((((int)blockIdx.x) * 80) + (((int)threadIdx.x) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 20), \"float32\"), pool_max: T.Buffer((2, 12, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(24):\n            pad_temp = T.allocate([21], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((21,), data=pad_temp)\n            for ax2 in range(21):\n                data_1 = T.Buffer((480,), data=data.data)\n                pad_temp_1[ax2] = T.if_then_else(1 <= ax2, data_1[ax0_ax1_fused * 20 + ax2 - 1], T.float32(-3.4028234663852886e+38))\n            for ax2 in range(10):\n                pool_max_1 = T.Buffer((240,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 10 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 2 + rv0])", "op_args": [2, 12, 7, 20]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1881; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 11, 1, 9), \"float32\"), compute: T.Buffer((19, 11, 1, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1881):\n            compute_1 = T.Buffer((1881,), data=compute.data)\n            data_1 = T.Buffer((1881,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [19, 11, 1, 9]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    float pad_temp[3];\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n          pad_temp[ax2_s] = (((1 <= ((ax2 * 2) + ax2_s)) && (((ax2_s >> 1) + ax2) < 2)) ? data[(((((ax0 * 42) + (ax1 * 3)) + (ax2 * 2)) + ax2_s) - 1)] : -3.402823e+38f);\n        }\n        pool_max[(((ax0 * 28) + (ax1 * 2)) + ax2)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          pool_max[(((ax0 * 28) + (ax1 * 2)) + ax2)] = max(pool_max[(((ax0 * 28) + (ax1 * 2)) + ax2)], pad_temp[rv0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], (((1 <= (((((int)threadIdx.x) & 1) * 2) + rv0)) && (((rv0 >> 1) + (((int)threadIdx.x) & 1)) < 2)) ? data[(((((((int)blockIdx.x) * 12) + ((((int)threadIdx.x) >> 1) * 3)) + ((((int)threadIdx.x) & 1) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 14, 3), \"float32\"), pool_max: T.Buffer((16, 14, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(16):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(14, 2):\n                pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n                for ax2_s in range(3):\n                    cse_var_1: T.int32 = ax2 * 2\n                    data_1 = T.Buffer((672,), data=data.data)\n                    pad_temp_1[ax2_s] = T.if_then_else(1 <= cse_var_1 + ax2_s and ax2_s // 2 + ax2 < 2, data_1[ax0 * 42 + ax1 * 3 + cse_var_1 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((448,), data=pool_max.data)\n                pool_max_1[ax0 * 28 + ax1 * 2 + ax2] = T.float32(-3.4028234663852886e+38)\n                for rv0 in range(3):\n                    cse_var_2: T.int32 = ax0 * 28 + ax1 * 2 + ax2\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0])", "op_args": [16, 14, 9, 3]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      for (int32_t i3 = 0; i3 < 3; ++i3) {\n        compute[(((i0 * 36) + (i2 * 3)) + i3)] = (data[(((i0 * 36) + (i2 * 3)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 1, 12, 3), \"float32\"), compute: T.Buffer((9, 1, 12, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i2, i3 in T.grid(12, 3):\n                cse_var_1: T.int32 = i0 * 36 + i2 * 3 + i3\n                compute_1 = T.Buffer((324,), data=compute.data)\n                data_1 = T.Buffer((324,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [9, 1, 12, 3]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 40; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[3];\n    for (int32_t ax1_ax2_fused_s = 0; ax1_ax2_fused_s < 3; ++ax1_ax2_fused_s) {\n      pad_temp[ax1_ax2_fused_s] = (((1 <= ax1_ax2_fused_s) && (ax1_ax2_fused_s < 2)) ? data[((ax0_ax1_fused_ax2_fused + ax1_ax2_fused_s) - 1)] : -3.402823e+38f);\n    }\n    pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      pool_max[ax0_ax1_fused_ax2_fused] = max(pool_max[ax0_ax1_fused_ax2_fused], pad_temp[rv0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], (((1 <= rv0) && (rv0 < 2)) ? data[((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 2, 1), \"float32\"), pool_max: T.Buffer((20, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(40):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n            for ax1_ax2_fused_s in range(3):\n                data_1 = T.Buffer((40,), data=data.data)\n                pad_temp_1[ax1_ax2_fused_s] = T.if_then_else(1 <= ax1_ax2_fused_s and ax1_ax2_fused_s < 2, data_1[ax0_ax1_fused_ax2_fused + ax1_ax2_fused_s - 1], T.float32(-3.4028234663852886e+38))\n            pool_max_1 = T.Buffer((40,), data=pool_max.data)\n            pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(pool_max_1[ax0_ax1_fused_ax2_fused], pad_temp_1[rv0])", "op_args": [20, 2, 15, 1]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 480; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 10, 4), \"float32\"), compute: T.Buffer((6, 2, 10, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            data_1 = T.Buffer((480,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [6, 2, 10, 4]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    float pad_temp[19];\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        pad_temp[ax2] = (((1 <= ax2) && (ax2 < 18)) ? data[((((ax0 * 272) + (ax1 * 17)) + ax2) - 1)] : -3.402823e+38f);\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 9; ++ax2_1) {\n        pool_max[(((ax0 * 144) + (ax1 * 9)) + ax2_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          pool_max[(((ax0 * 144) + (ax1 * 9)) + ax2_1)] = max(pool_max[(((ax0 * 144) + (ax1 * 9)) + ax2_1)], pad_temp[((ax2_1 * 2) + rv0)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))], (((1 <= (((((int)threadIdx.x) % 9) * 2) + rv0)) && (((rv0 >> 1) + (((int)threadIdx.x) % 9)) < 9)) ? data[(((((((int)blockIdx.x) * 85) + ((((int)threadIdx.x) / 9) * 17)) + ((((int)threadIdx.x) % 9) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 16, 17), \"float32\"), pool_max: T.Buffer((15, 16, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(15):\n            pad_temp = T.allocate([19], \"float32\", \"global\")\n            for ax1 in range(16):\n                pad_temp_1 = T.Buffer((19,), data=pad_temp)\n                for ax2 in range(19):\n                    data_1 = T.Buffer((4080,), data=data.data)\n                    pad_temp_1[ax2] = T.if_then_else(1 <= ax2 and ax2 < 18, data_1[ax0 * 272 + ax1 * 17 + ax2 - 1], T.float32(-3.4028234663852886e+38))\n                for ax2 in range(9):\n                    pool_max_1 = T.Buffer((2160,), data=pool_max.data)\n                    pool_max_1[ax0 * 144 + ax1 * 9 + ax2] = T.float32(-3.4028234663852886e+38)\n                    for rv0 in range(3):\n                        cse_var_1: T.int32 = ax0 * 144 + ax1 * 9 + ax2\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 2 + rv0])", "op_args": [15, 16, 2, 17]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 7; ++i3) {\n        compute[(((i0 * 42) + (i2 * 7)) + i3)] = (data[(((i0 * 42) + (i2 * 7)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 1, 6, 7), \"float32\"), compute: T.Buffer((13, 1, 6, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i2, i3 in T.grid(6, 7):\n                cse_var_1: T.int32 = i0 * 42 + i2 * 7 + i3\n                compute_1 = T.Buffer((546,), data=compute.data)\n                data_1 = T.Buffer((546,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [13, 1, 6, 7]}{"op_name": "pool1d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[3];\n    for (int32_t ax2_s = 0; ax2_s < 3; ++ax2_s) {\n      pad_temp[ax2_s] = (((1 <= (((ax0_ax1_fused_ax2_fused % 5) * 2) + ax2_s)) && (((ax2_s >> 1) + (ax0_ax1_fused_ax2_fused % 5)) < 5)) ? data[(((((ax0_ax1_fused_ax2_fused / 5) * 9) + ((ax0_ax1_fused_ax2_fused % 5) * 2)) + ax2_s) - 1)] : -3.402823e+38f);\n    }\n    pool_max[ax0_ax1_fused_ax2_fused] = -3.402823e+38f;\n    for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n      pool_max[ax0_ax1_fused_ax2_fused] = max(pool_max[ax0_ax1_fused_ax2_fused], pad_temp[rv0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    pool_max[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], (((1 <= (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5) * 2) + rv0)) && (((rv0 >> 1) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5)) < 5)) ? data[(((((((((int)blockIdx.x) * 24) + ((int)threadIdx.x)) / 5) * 9) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5) * 2)) + rv0) - 1)] : -3.402823e+38f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 8, 9), \"float32\"), pool_max: T.Buffer((12, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            pad_temp = T.allocate([3], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((3,), data=pad_temp, align=8)\n            for ax2_s in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused % 5\n                cse_var_1: T.int32 = cse_var_2 * 2\n                data_1 = T.Buffer((864,), data=data.data)\n                pad_temp_1[ax2_s] = T.if_then_else(1 <= cse_var_1 + ax2_s and ax2_s // 2 + cse_var_2 < 5, data_1[ax0_ax1_fused_ax2_fused // 5 * 9 + cse_var_1 + ax2_s - 1], T.float32(-3.4028234663852886e+38))\n            pool_max_1 = T.Buffer((480,), data=pool_max.data)\n            pool_max_1[ax0_ax1_fused_ax2_fused] = T.float32(-3.4028234663852886e+38)\n            for rv0 in range(3):\n                pool_max_1[ax0_ax1_fused_ax2_fused] = T.max(pool_max_1[ax0_ax1_fused_ax2_fused], pad_temp_1[rv0])", "op_args": [12, 8, 16, 9]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1792; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 20) + i3)] = (data[((i0_i1_fused_i2_fused * 20) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 14, 16, 20), \"float32\"), compute: T.Buffer((8, 14, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(1792):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((35840,), data=compute.data)\n                data_1 = T.Buffer((35840,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [8, 14, 16, 20]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      for (int32_t i3 = 0; i3 < 2; ++i3) {\n        compute[(((i1 * 18) + (i2 * 2)) + i3)] = (data[(((i1 * 18) + (i2 * 2)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 8, 9, 2), \"float32\"), compute: T.Buffer((1, 8, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(8, 9, 2):\n            cse_var_1: T.int32 = i1 * 18 + i2 * 2 + i3\n            compute_1 = T.Buffer((144,), data=compute.data)\n            data_1 = T.Buffer((144,), data=data.data)\n            compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [1, 8, 9, 2]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float pad_temp[209];\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        for (int32_t ax3_s = 0; ax3_s < 19; ++ax3_s) {\n          pad_temp[((ax2 * 19) + ax3_s)] = (((1 <= ax2) && (1 <= ax3_s)) ? data[(((((ax0 * 3420) + (ax1 * 180)) + (ax2 * 18)) + ax3_s) - 19)] : -3.402823e+38f);\n        }\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 5; ++ax2_1) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          pool_max[((((ax0 * 855) + (ax1 * 45)) + (ax2_1 * 9)) + ax3)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0 * 855) + (ax1 * 45)) + (ax2_1 * 9)) + ax3)] = max(pool_max[((((ax0 * 855) + (ax1 * 45)) + (ax2_1 * 9)) + ax3)], pad_temp[((((ax2_1 * 38) + (rv0 * 19)) + (ax3 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], (((1 <= (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 9)) % 5) * 2) + rv0)) && (1 <= (((((int)threadIdx.x) % 9) * 2) + rv1))) ? data[((((((((int)blockIdx.x) * 144) + ((((int)threadIdx.x) / 9) * 36)) + (rv0 * 18)) + ((((int)threadIdx.x) % 9) * 2)) + rv1) - 19)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 19, 10, 18), \"float32\"), pool_max: T.Buffer((4, 19, 5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            pad_temp = T.allocate([209], \"float32\", \"global\")\n            for ax1 in range(19):\n                pad_temp_1 = T.Buffer((209,), data=pad_temp)\n                for ax2, ax3_s in T.grid(11, 19):\n                    data_1 = T.Buffer((13680,), data=data.data)\n                    pad_temp_1[ax2 * 19 + ax3_s] = T.if_then_else(1 <= ax2 and 1 <= ax3_s, data_1[ax0 * 3420 + ax1 * 180 + ax2 * 18 + ax3_s - 19], T.float32(-3.4028234663852886e+38))\n                for ax2, ax3 in T.grid(5, 9):\n                    pool_max_1 = T.Buffer((3420,), data=pool_max.data)\n                    pool_max_1[ax0 * 855 + ax1 * 45 + ax2 * 9 + ax3] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_1: T.int32 = ax0 * 855 + ax1 * 45 + ax2 * 9 + ax3\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 38 + rv0 * 19 + ax3 * 2 + rv1])", "op_args": [4, 19, 10, 6]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 3; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 16; ++i3) {\n        compute[(((i1 * 64) + (i2 * 16)) + i3)] = (data[(((i1 * 64) + (i2 * 16)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 3, 4, 16), \"float32\"), compute: T.Buffer((1, 3, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(3, 4, 16):\n            cse_var_1: T.int32 = i1 * 64 + i2 * 16 + i3\n            compute_1 = T.Buffer((192,), data=compute.data)\n            data_1 = T.Buffer((192,), data=data.data)\n            compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [1, 3, 4, 16]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 19; ++i3) {\n        compute[(((i0 * 114) + (i2 * 19)) + i3)] = (data[(((i0 * 114) + (i2 * 19)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 1, 6, 19), \"float32\"), compute: T.Buffer((3, 1, 6, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(3):\n            for i2, i3 in T.grid(6, 19):\n                cse_var_1: T.int32 = i0 * 114 + i2 * 19 + i3\n                compute_1 = T.Buffer((342,), data=compute.data)\n                data_1 = T.Buffer((342,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [3, 1, 6, 19]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 144; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[15];\n    for (int32_t ax2_ax3_fused_s = 0; ax2_ax3_fused_s < 15; ++ax2_ax3_fused_s) {\n      pad_temp[ax2_ax3_fused_s] = (((((1 <= (((ax0_ax1_fused_ax2_fused % 6) * 2) + (ax2_ax3_fused_s / 5))) && (((ax2_ax3_fused_s / 10) + (ax0_ax1_fused_ax2_fused % 6)) < 6)) && (1 <= (ax2_ax3_fused_s % 5))) && ((ax2_ax3_fused_s % 5) < 4)) ? data[((((((ax0_ax1_fused_ax2_fused / 6) * 33) + ((ax0_ax1_fused_ax2_fused % 6) * 6)) + ((ax2_ax3_fused_s / 5) * 3)) + (ax2_ax3_fused_s % 5)) - 4)] : -3.402823e+38f);\n    }\n    for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n      pool_max[((ax0_ax1_fused_ax2_fused * 2) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          pool_max[((ax0_ax1_fused_ax2_fused * 2) + ax3)] = max(pool_max[((ax0_ax1_fused_ax2_fused * 2) + ax3)], pad_temp[(((rv0 * 5) + (ax3 * 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))], (((((1 <= (((((int)threadIdx.x) >> 1) * 2) + rv0)) && (((((int)threadIdx.x) >> 1) + (rv0 >> 1)) < 6)) && (1 <= (((((int)threadIdx.x) & 1) * 2) + rv1))) && (((rv1 >> 1) + (((int)threadIdx.x) & 1)) < 2)) ? data[((((((((int)blockIdx.x) * 33) + ((((int)threadIdx.x) >> 1) * 6)) + (rv0 * 3)) + ((((int)threadIdx.x) & 1) * 2)) + rv1) - 4)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 11, 3), \"float32\"), pool_max: T.Buffer((2, 12, 6, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            pad_temp = T.allocate([15], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((15,), data=pad_temp, align=32)\n            for ax2_ax3_fused_s in range(15):\n                cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused % 6\n                cse_var_2: T.int32 = ax2_ax3_fused_s // 5\n                cse_var_1: T.int32 = ax2_ax3_fused_s % 5\n                data_1 = T.Buffer((792,), data=data.data)\n                pad_temp_1[ax2_ax3_fused_s] = T.if_then_else(1 <= cse_var_3 * 2 + cse_var_2 and ax2_ax3_fused_s // 10 + cse_var_3 < 6 and 1 <= cse_var_1 and cse_var_1 < 4, data_1[ax0_ax1_fused_ax2_fused // 6 * 33 + cse_var_3 * 6 + cse_var_2 * 3 + cse_var_1 - 4], T.float32(-3.4028234663852886e+38))\n            for ax3 in range(2):\n                pool_max_1 = T.Buffer((288,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 2 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused * 2 + ax3\n                    pool_max_1[cse_var_4] = T.max(pool_max_1[cse_var_4], pad_temp_1[rv0 * 5 + ax3 * 2 + rv1])", "op_args": [2, 12, 11, 1]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 162; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 7; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 7) + i3)] = (data[((i0_i1_fused_i2_fused * 7) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 6)) < 189) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * -1.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 18, 1, 7), \"float32\"), compute: T.Buffer((9, 18, 1, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(162):\n            for i3 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 7 + i3\n                compute_1 = T.Buffer((1134,), data=compute.data)\n                data_1 = T.Buffer((1134,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [9, 18, 1, 7]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 224; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 1, 16, 1), \"float32\"), compute: T.Buffer((14, 1, 16, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(224):\n            compute_1 = T.Buffer((224,), data=compute.data)\n            data_1 = T.Buffer((224,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [14, 1, 16, 1]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 975; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[9];\n    for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n      for (int32_t ax2_ax3_fused_s = 0; ax2_ax3_fused_s < 9; ++ax2_ax3_fused_s) {\n        pad_temp[ax2_ax3_fused_s] = (((((1 <= (((ax0_ax1_fused_ax2_fused % 5) * 2) + (ax2_ax3_fused_s / 3))) && (((ax2_ax3_fused_s / 6) + (ax0_ax1_fused_ax2_fused % 5)) < 5)) && (1 <= ((ax3 * 2) + (ax2_ax3_fused_s % 3)))) && ((((ax2_ax3_fused_s % 3) >> 1) + ax3) < 14)) ? data[(((((((ax0_ax1_fused_ax2_fused / 5) * 243) + ((ax0_ax1_fused_ax2_fused % 5) * 54)) + ((ax2_ax3_fused_s / 3) * 27)) + (ax3 * 2)) + (ax2_ax3_fused_s % 3)) - 28)] : -3.402823e+38f);\n      }\n      pool_max[((ax0_ax1_fused_ax2_fused * 14) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          pool_max[((ax0_ax1_fused_ax2_fused * 14) + ax3)] = max(pool_max[((ax0_ax1_fused_ax2_fused * 14) + ax3)], pad_temp[((rv0 * 3) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))], (((((1 <= ((((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) % 35) / 7) * 2) + rv0)) && ((((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) % 35) / 7) + (rv0 >> 1)) < 5)) && (1 <= (((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 14) * 2) + rv1))) && (((rv1 >> 1) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 14)) < 14)) ? data[(((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) / 7) * 243) + (((((((int)blockIdx.x) * 25) + (((int)threadIdx.x) >> 1)) % 35) / 7) * 54)) + (rv0 * 27)) + ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 14) * 2)) + rv1) - 28)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 15, 9, 27), \"float32\"), pool_max: T.Buffer((13, 15, 5, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(975):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax3 in range(14):\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                for ax2_ax3_fused_s in range(9):\n                    cse_var_4: T.int32 = ax0_ax1_fused_ax2_fused % 5\n                    cse_var_3: T.int32 = ax2_ax3_fused_s // 3\n                    cse_var_2: T.int32 = ax3 * 2\n                    cse_var_1: T.int32 = ax2_ax3_fused_s % 3\n                    data_1 = T.Buffer((47385,), data=data.data)\n                    pad_temp_1[ax2_ax3_fused_s] = T.if_then_else(1 <= cse_var_4 * 2 + cse_var_3 and ax2_ax3_fused_s // 6 + cse_var_4 < 5 and 1 <= cse_var_2 + cse_var_1 and cse_var_1 // 2 + ax3 < 14, data_1[ax0_ax1_fused_ax2_fused // 5 * 243 + cse_var_4 * 54 + cse_var_3 * 27 + cse_var_2 + cse_var_1 - 28], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((13650,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 14 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_5: T.int32 = ax0_ax1_fused_ax2_fused * 14 + ax3\n                    pool_max_1[cse_var_5] = T.max(pool_max_1[cse_var_5], pad_temp_1[rv0 * 3 + rv1])", "op_args": [13, 15, 9, 9]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        for (int32_t i3 = 0; i3 < 17; ++i3) {\n          compute[((((i0 * 867) + (i1 * 289)) + (i2 * 17)) + i3)] = (data[((((i0 * 867) + (i1 * 289)) + (i2 * 17)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 3, 17, 17), \"float32\"), compute: T.Buffer((12, 3, 17, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(12):\n            for i1, i2, i3 in T.grid(3, 17, 17):\n                cse_var_1: T.int32 = i0 * 867 + i1 * 289 + i2 * 17 + i3\n                compute_1 = T.Buffer((10404,), data=compute.data)\n                data_1 = T.Buffer((10404,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [12, 3, 17, 17]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 432; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 20; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 20) + i3)] = (data[((i0_i1_fused_i2_fused * 20) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 18, 2, 20), \"float32\"), compute: T.Buffer((12, 18, 2, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(432):\n            for i3 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 20 + i3\n                compute_1 = T.Buffer((8640,), data=compute.data)\n                data_1 = T.Buffer((8640,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [12, 18, 2, 20]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 220; ++ax0_ax1_fused) {\n    float pad_temp[183];\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n        for (int32_t ax3_s = 0; ax3_s < 61; ++ax3_s) {\n          pad_temp[((ax2_1 * 61) + ax3_s)] = (((1 <= ((ax2 * 2) + ax2_1)) && (1 <= ax3_s)) ? data[(((((ax0_ax1_fused * 1200) + (ax2 * 120)) + (ax2_1 * 60)) + ax3_s) - 61)] : -3.402823e+38f);\n        }\n      }\n      for (int32_t ax3 = 0; ax3 < 30; ++ax3) {\n        pool_max[(((ax0_ax1_fused * 300) + (ax2 * 30)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            pool_max[(((ax0_ax1_fused * 300) + (ax2 * 30)) + ax3)] = max(pool_max[(((ax0_ax1_fused * 300) + (ax2 * 30)) + ax3)], pad_temp[(((rv0 * 61) + (ax3 * 2)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], (((1 <= ((((((int)blockIdx.x) % 5) * 4) + ((((int)threadIdx.x) / 30) * 2)) + rv0)) && (1 <= (((((int)threadIdx.x) % 30) * 2) + rv1))) ? data[((((((((int)blockIdx.x) * 240) + ((((int)threadIdx.x) / 30) * 120)) + (rv0 * 60)) + ((((int)threadIdx.x) % 30) * 2)) + rv1) - 61)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 20, 20, 60), \"float32\"), pool_max: T.Buffer((11, 20, 10, 30), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(220):\n            pad_temp = T.allocate([183], \"float32\", \"global\")\n            for ax2 in range(10):\n                pad_temp_1 = T.Buffer((183,), data=pad_temp)\n                for ax2_1, ax3_s in T.grid(3, 61):\n                    data_1 = T.Buffer((264000,), data=data.data)\n                    pad_temp_1[ax2_1 * 61 + ax3_s] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and 1 <= ax3_s, data_1[ax0_ax1_fused * 1200 + ax2 * 120 + ax2_1 * 60 + ax3_s - 61], T.float32(-3.4028234663852886e+38))\n                for ax3 in range(30):\n                    pool_max_1 = T.Buffer((66000,), data=pool_max.data)\n                    pool_max_1[ax0_ax1_fused * 300 + ax2 * 30 + ax3] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1 in T.grid(3, 3):\n                        cse_var_1: T.int32 = ax0_ax1_fused * 300 + ax2 * 30 + ax3\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 61 + ax3 * 2 + rv1])", "op_args": [11, 20, 20, 20]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 14; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3)] = (data[((i0_i1_fused_i2_fused * 14) + i3)] * -1.000000e+00f);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 2, 7, 14), \"float32\"), compute: T.Buffer((8, 2, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            for i3 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3\n                compute_1 = T.Buffer((1568,), data=compute.data)\n                data_1 = T.Buffer((1568,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [8, 2, 7, 14]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    float pad_temp[2142];\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n          pad_temp[(((ax1 * 153) + (ax2 * 17)) + ax3)] = ((((1 <= ax2) && (1 <= ax3)) && (ax3 < 16)) ? data[(((((ax0 * 1680) + (ax1 * 120)) + (ax2 * 15)) + ax3) - 16)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 14; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 8; ++ax3_1) {\n          pool_max[((((ax0 * 448) + (ax1_1 * 32)) + (ax2_1 * 8)) + ax3_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0 * 448) + (ax1_1 * 32)) + (ax2_1 * 8)) + ax3_1)] = max(pool_max[((((ax0 * 448) + (ax1_1 * 32)) + (ax2_1 * 8)) + ax3_1)], pad_temp[(((((ax1_1 * 153) + (ax2_1 * 34)) + (rv0 * 17)) + (ax3_1 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))], ((((1 <= (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) & 3) * 2) + rv0)) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv1))) && (((rv1 >> 1) + (((int)threadIdx.x) & 7)) < 8)) ? data[((((((((int)blockIdx.x) * 210) + ((((int)threadIdx.x) >> 3) * 30)) + (rv0 * 15)) + ((((int)threadIdx.x) & 7) * 2)) + rv1) - 16)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 14, 8, 15), \"float32\"), pool_max: T.Buffer((12, 14, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(12):\n            pad_temp = T.allocate([2142], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((2142,), data=pad_temp)\n            for ax1, ax2, ax3 in T.grid(14, 9, 17):\n                data_1 = T.Buffer((20160,), data=data.data)\n                pad_temp_1[ax1 * 153 + ax2 * 17 + ax3] = T.if_then_else(1 <= ax2 and 1 <= ax3 and ax3 < 16, data_1[ax0 * 1680 + ax1 * 120 + ax2 * 15 + ax3 - 16], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2, ax3 in T.grid(14, 4, 8):\n                pool_max_1 = T.Buffer((5376,), data=pool_max.data)\n                pool_max_1[ax0 * 448 + ax1 * 32 + ax2 * 8 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0 * 448 + ax1 * 32 + ax2 * 8 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 153 + ax2 * 34 + rv0 * 17 + ax3 * 2 + rv1])", "op_args": [12, 14, 8, 5]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 9996; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (data[i0_i1_fused_i2_fused_i3_fused] * -1.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 2499) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * -1.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 6, 7, 17), \"float32\"), compute: T.Buffer((14, 6, 7, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(9996):\n            compute_1 = T.Buffer((9996,), data=compute.data)\n            data_1 = T.Buffer((9996,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused] * T.float32(-1)", "op_args": [14, 6, 7, 17]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    float pad_temp[9];\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n        for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n          for (int32_t ax3_s = 0; ax3_s < 3; ++ax3_s) {\n            pad_temp[((ax2 * 3) + ax3_s)] = ((((1 <= ax2) && (1 <= ((ax3 * 2) + ax3_s))) && (((ax3_s >> 1) + ax3) < 14)) ? data[((((((ax0 * 486) + (ax1 * 54)) + (ax2 * 27)) + (ax3 * 2)) + ax3_s) - 28)] : -3.402823e+38f);\n          }\n        }\n        pool_max[(((ax0 * 126) + (ax1 * 14)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            pool_max[(((ax0 * 126) + (ax1 * 14)) + ax3)] = max(pool_max[(((ax0 * 126) + (ax1 * 14)) + ax3)], pad_temp[((rv0 * 3) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], ((((1 <= rv0) && (1 <= (((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 14) * 2) + rv1))) && (((rv1 >> 1) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 14)) < 14)) ? data[((((((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 1)) / 7) * 54) + (rv0 * 27)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 14) * 2)) + rv1) - 28)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 9, 2, 27), \"float32\"), pool_max: T.Buffer((3, 9, 1, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            pad_temp = T.allocate([9], \"float32\", \"global\")\n            for ax1, ax3 in T.grid(9, 14):\n                pad_temp_1 = T.Buffer((9,), data=pad_temp, align=32)\n                for ax2, ax3_s in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax3 * 2\n                    data_1 = T.Buffer((1458,), data=data.data)\n                    pad_temp_1[ax2 * 3 + ax3_s] = T.if_then_else(1 <= ax2 and 1 <= cse_var_1 + ax3_s and ax3_s // 2 + ax3 < 14, data_1[ax0 * 486 + ax1 * 54 + ax2 * 27 + cse_var_1 + ax3_s - 28], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((378,), data=pool_max.data)\n                pool_max_1[ax0 * 126 + ax1 * 14 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_2: T.int32 = ax0 * 126 + ax1 * 14 + ax3\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 3 + rv1])", "op_args": [3, 9, 2, 9]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 13; ++i3) {\n          compute[((((i0 * 936) + (i1 * 78)) + (i2 * 13)) + i3)] = (data[((((i0 * 936) + (i1 * 78)) + (i2 * 13)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 117) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * -1.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 12, 6, 13), \"float32\"), compute: T.Buffer((2, 12, 6, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(2):\n            for i1, i2, i3 in T.grid(12, 6, 13):\n                cse_var_1: T.int32 = i0 * 936 + i1 * 78 + i2 * 13 + i3\n                compute_1 = T.Buffer((1872,), data=compute.data)\n                data_1 = T.Buffer((1872,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [2, 12, 6, 13]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    float pad_temp[1350];\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3_s = 0; ax3_s < 25; ++ax3_s) {\n          pad_temp[(((ax1 * 75) + (ax2 * 25)) + ax3_s)] = ((((1 <= ax2) && (ax2 < 2)) && (1 <= ax3_s)) ? data[(((((ax0 * 432) + (ax1 * 24)) + (ax2 * 24)) + ax3_s) - 25)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 18; ++ax1_1) {\n      for (int32_t ax3 = 0; ax3 < 12; ++ax3) {\n        pool_max[(((ax0 * 216) + (ax1_1 * 12)) + ax3)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            pool_max[(((ax0 * 216) + (ax1_1 * 12)) + ax3)] = max(pool_max[(((ax0 * 216) + (ax1_1 * 12)) + ax3)], pad_temp[((((ax1_1 * 75) + (rv0 * 25)) + (ax3 * 2)) + rv1)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))], ((((1 <= rv0) && (rv0 < 2)) && (1 <= (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 12) * 2) + rv1))) ? data[(((((((int)blockIdx.x) * 54) + (rv0 * 24)) + (((int)threadIdx.x) * 2)) + rv1) - 25)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 18, 1, 24), \"float32\"), pool_max: T.Buffer((7, 18, 1, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(7):\n            pad_temp = T.allocate([1350], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1350,), data=pad_temp)\n            for ax1, ax2, ax3_s in T.grid(18, 3, 25):\n                data_1 = T.Buffer((3024,), data=data.data)\n                pad_temp_1[ax1 * 75 + ax2 * 25 + ax3_s] = T.if_then_else(1 <= ax2 and ax2 < 2 and 1 <= ax3_s, data_1[ax0 * 432 + ax1 * 24 + ax2 * 24 + ax3_s - 25], T.float32(-3.4028234663852886e+38))\n            for ax1, ax3 in T.grid(18, 12):\n                pool_max_1 = T.Buffer((1512,), data=pool_max.data)\n                pool_max_1[ax0 * 216 + ax1 * 12 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0 * 216 + ax1 * 12 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 75 + rv0 * 25 + ax3 * 2 + rv1])", "op_args": [7, 18, 1, 8]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      for (int32_t i3 = 0; i3 < 5; ++i3) {\n        compute[(((i0_i1_fused * 65) + (i2 * 5)) + i3)] = (data[(((i0_i1_fused * 65) + (i2 * 5)) + i3)] * -1.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 7, 13, 5), \"float32\"), compute: T.Buffer((5, 7, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(35):\n            for i2, i3 in T.grid(13, 5):\n                cse_var_1: T.int32 = i0_i1_fused * 65 + i2 * 5 + i3\n                compute_1 = T.Buffer((2275,), data=compute.data)\n                data_1 = T.Buffer((2275,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [5, 7, 13, 5]}{"op_name": "negative", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        for (int32_t i3 = 0; i3 < 18; ++i3) {\n          compute[((((i0 * 2736) + (i1 * 342)) + (i2 * 18)) + i3)] = (data[((((i0 * 2736) + (i1 * 342)) + (i2 * 18)) + i3)] * -1.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * -1.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 19, 18), \"float32\"), compute: T.Buffer((7, 8, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3 in T.grid(8, 19, 18):\n                cse_var_1: T.int32 = i0 * 2736 + i1 * 342 + i2 * 18 + i3\n                compute_1 = T.Buffer((19152,), data=compute.data)\n                data_1 = T.Buffer((19152,), data=data.data)\n                compute_1[cse_var_1] = data_1[cse_var_1] * T.float32(-1)", "op_args": [7, 8, 19, 18]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    float pad_temp[12915];\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 21; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 41; ++ax3) {\n          pad_temp[(((ax1 * 861) + (ax2 * 41)) + ax3)] = ((((1 <= ax2) && (1 <= ax3)) && (ax3 < 40)) ? data[(((((ax0 * 11700) + (ax1 * 780)) + (ax2 * 39)) + ax3) - 40)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 15; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 20; ++ax3_1) {\n          pool_max[((((ax0 * 3000) + (ax1_1 * 200)) + (ax2_1 * 20)) + ax3_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0 * 3000) + (ax1_1 * 200)) + (ax2_1 * 20)) + ax3_1)] = max(pool_max[((((ax0 * 3000) + (ax1_1 * 200)) + (ax2_1 * 20)) + ax3_1)], pad_temp[(((((ax1_1 * 861) + (ax2_1 * 82)) + (rv0 * 41)) + (ax3_1 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], ((((1 <= (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 20)) % 10) * 2) + rv0)) && (1 <= (((((int)threadIdx.x) % 20) * 2) + rv1))) && (((rv1 >> 1) + (((int)threadIdx.x) % 20)) < 20)) ? data[((((((((int)blockIdx.x) * 234) + ((((int)threadIdx.x) / 20) * 78)) + (rv0 * 39)) + ((((int)threadIdx.x) % 20) * 2)) + rv1) - 40)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 15, 20, 39), \"float32\"), pool_max: T.Buffer((10, 15, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(10):\n            pad_temp = T.allocate([12915], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((12915,), data=pad_temp)\n            for ax1, ax2, ax3 in T.grid(15, 21, 41):\n                data_1 = T.Buffer((117000,), data=data.data)\n                pad_temp_1[ax1 * 861 + ax2 * 41 + ax3] = T.if_then_else(1 <= ax2 and 1 <= ax3 and ax3 < 40, data_1[ax0 * 11700 + ax1 * 780 + ax2 * 39 + ax3 - 40], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2, ax3 in T.grid(15, 10, 20):\n                pool_max_1 = T.Buffer((30000,), data=pool_max.data)\n                pool_max_1[ax0 * 3000 + ax1 * 200 + ax2 * 20 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0 * 3000 + ax1 * 200 + ax2 * 20 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 861 + ax2 * 82 + rv0 * 41 + ax3 * 2 + rv1])", "op_args": [10, 15, 20, 13]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[26];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 26; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 864; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 26; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 26) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 26; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 702; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 16, 18, 6), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([26], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((26,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(26):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(864, 26):\n            data_1 = T.Buffer((22464,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 26 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(26):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [13, 16, 18, 6]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 40; ++ax0_ax1_fused) {\n    float pad_temp[177];\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3_s = 0; ax3_s < 59; ++ax3_s) {\n        pad_temp[((ax2 * 59) + ax3_s)] = (((((1 <= ax2) && (ax2 < 2)) && (1 <= ax3_s)) && (ax3_s < 58)) ? data[((((ax0_ax1_fused * 57) + (ax2 * 57)) + ax3_s) - 58)] : -3.402823e+38f);\n      }\n    }\n    for (int32_t ax3 = 0; ax3 < 29; ++ax3) {\n      pool_max[((ax0_ax1_fused * 29) + ax3)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          pool_max[((ax0_ax1_fused * 29) + ax3)] = max(pool_max[((ax0_ax1_fused * 29) + ax3)], pad_temp[(((rv0 * 59) + (ax3 * 2)) + rv1)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], (((((1 <= rv0) && (rv0 < 2)) && (1 <= (((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 29) * 2) + rv1))) && (((rv1 >> 1) + (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 29)) < 29)) ? data[((((((((((int)blockIdx.x) * 40) + ((int)threadIdx.x)) / 29) * 57) + (rv0 * 57)) + ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 29) * 2)) + rv1) - 58)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 8, 1, 57), \"float32\"), pool_max: T.Buffer((5, 8, 1, 29), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(40):\n            pad_temp = T.allocate([177], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((177,), data=pad_temp)\n            for ax2, ax3_s in T.grid(3, 59):\n                data_1 = T.Buffer((2280,), data=data.data)\n                pad_temp_1[ax2 * 59 + ax3_s] = T.if_then_else(1 <= ax2 and ax2 < 2 and 1 <= ax3_s and ax3_s < 58, data_1[ax0_ax1_fused * 57 + ax2 * 57 + ax3_s - 58], T.float32(-3.4028234663852886e+38))\n            for ax3 in range(29):\n                pool_max_1 = T.Buffer((1160,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 29 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 29 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 59 + ax3 * 2 + rv1])", "op_args": [5, 8, 1, 19]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k1 = 0; k1 < 19; ++k1) {\n    for (int32_t k3 = 0; k3 < 2; ++k3) {\n      data_red[0] = (data_red[0] * data[((k1 * 2) + k3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 2; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 16) + (((int)threadIdx.x) >> 1)) < 19) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 19, 1, 2), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k1, k3 in T.grid(19, 2):\n            data_1 = T.Buffer((38,), data=data.data)\n            data_red_1[0] = data_red_1[0] * data_1[k1 * 2 + k3]", "op_args": [1, 19, 1, 2]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[52];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 52; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 152; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 52; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 52) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 52; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 247; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 19, 13), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([52], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((52,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(52):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(152, 52):\n            data_1 = T.Buffer((7904,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 52 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(52):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [16, 2, 19, 13]}{"op_name": "pool2d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    float pad_temp[2360];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 59; ++ax3) {\n          pad_temp[(((ax1 * 295) + (ax2 * 59)) + ax3)] = (((((1 <= ax2) && (ax2 < 4)) && (1 <= ax3)) && (ax3 < 58)) ? data[(((((ax0 * 1368) + (ax1 * 171)) + (ax2 * 57)) + ax3) - 58)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 2; ++ax2_1) {\n        for (int32_t ax3_1 = 0; ax3_1 < 29; ++ax3_1) {\n          pool_max[((((ax0 * 464) + (ax1_1 * 58)) + (ax2_1 * 29)) + ax3_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              pool_max[((((ax0 * 464) + (ax1_1 * 58)) + (ax2_1 * 29)) + ax3_1)] = max(pool_max[((((ax0 * 464) + (ax1_1 * 58)) + (ax2_1 * 29)) + ax3_1)], pad_temp[(((((ax1_1 * 295) + (ax2_1 * 118)) + (rv0 * 59)) + (ax3_1 * 2)) + rv1)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], (((((1 <= ((((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 58) / 29) * 2) + rv0)) && ((((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 58) / 29) + (rv0 >> 1)) < 2)) && (1 <= (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 29) * 2) + rv1))) && (((rv1 >> 1) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 29)) < 29)) ? data[(((((((((((int)blockIdx.x) * 30) + (((int)threadIdx.x) >> 1)) / 29) * 171) + (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 58) / 29) * 114)) + (rv0 * 57)) + ((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 29) * 2)) + rv1) - 58)] : -3.402823e+38f));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 8, 3, 57), \"float32\"), pool_max: T.Buffer((15, 8, 2, 29), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(15):\n            pad_temp = T.allocate([2360], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((2360,), data=pad_temp)\n            for ax1, ax2, ax3 in T.grid(8, 5, 59):\n                data_1 = T.Buffer((20520,), data=data.data)\n                pad_temp_1[ax1 * 295 + ax2 * 59 + ax3] = T.if_then_else(1 <= ax2 and ax2 < 4 and 1 <= ax3 and ax3 < 58, data_1[ax0 * 1368 + ax1 * 171 + ax2 * 57 + ax3 - 58], T.float32(-3.4028234663852886e+38))\n            for ax1, ax2, ax3 in T.grid(8, 2, 29):\n                pool_max_1 = T.Buffer((6960,), data=pool_max.data)\n                pool_max_1[ax0 * 464 + ax1 * 58 + ax2 * 29 + ax3] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1 in T.grid(3, 3):\n                    cse_var_1: T.int32 = ax0 * 464 + ax1 * 58 + ax2 * 29 + ax3\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 295 + ax2 * 118 + rv0 * 59 + ax3 * 2 + rv1])", "op_args": [15, 8, 3, 19]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[39];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 39; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 216; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 39; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 39) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 39; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 18; ++k0) {\n    for (int k1 = 0; k1 < 13; ++k1) {\n      for (int k2 = 0; k2 < 9; ++k2) {\n        for (int k3 = 0; k3 < 4; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 468) + (k1 * 36)) + (k2 * 4)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 13, 9, 4), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([39], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((39,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(39):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(216, 39):\n            data_1 = T.Buffer((8424,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 39 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(39):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [18, 13, 9, 4]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    float pad_temp[1785];\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 15; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          pad_temp[(((ax2 * 255) + (ax3 * 17)) + ax4)] = ((((1 <= ax2) && (1 <= ax3)) && (1 <= ax4)) ? data[(((((ax0_ax1_fused * 1344) + (ax2 * 224)) + (ax3 * 16)) + ax4) - 241)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 7; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          pool_max[((((ax0_ax1_fused * 168) + (ax2_1 * 56)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                pool_max[((((ax0_ax1_fused * 168) + (ax2_1 * 56)) + (ax3_1 * 8)) + ax4_1)] = max(pool_max[((((ax0_ax1_fused * 168) + (ax2_1 * 56)) + (ax3_1 * 8)) + ax4_1)], pad_temp[((((((ax2_1 * 510) + (rv0 * 255)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) % 21) / 7) * 2) + rv0)) && (1 <= (((((((int)threadIdx.x) >> 3) + ((int)blockIdx.x)) % 7) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) / 7) * 448) + (rv0 * 224)) + ((((((int)threadIdx.x) >> 3) + ((int)blockIdx.x)) % 7) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 241)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 4, 6, 14, 16), \"float32\"), pool_max: T.Buffer((8, 4, 3, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(32):\n            pad_temp = T.allocate([1785], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1785,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(7, 15, 17):\n                data_1 = T.Buffer((43008,), data=data.data)\n                pad_temp_1[ax2 * 255 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused * 1344 + ax2 * 224 + ax3 * 16 + ax4 - 241], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3, ax4 in T.grid(3, 7, 8):\n                pool_max_1 = T.Buffer((5376,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 168 + ax2 * 56 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 168 + ax2 * 56 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 510 + rv0 * 255 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [8, 4, 6, 14]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[49];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 49; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 136; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 49; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 49) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 49; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 17; ++k0) {\n    for (int k1 = 0; k1 < 7; ++k1) {\n      for (int k2 = 0; k2 < 14; ++k2) {\n        for (int k3 = 0; k3 < 4; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 392) + (k1 * 56)) + (k2 * 4)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 7, 14, 4), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([49], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((49,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(49):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(136, 49):\n            data_1 = T.Buffer((6664,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 49 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(49):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [17, 7, 14, 4]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[15];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 15; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1452; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 15; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 15) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 15; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 11; ++k0) {\n    for (int k1 = 0; k1 < 10; ++k1) {\n      for (int k2 = 0; k2 < 11; ++k2) {\n        for (int k3 = 0; k3 < 18; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 1980) + (k1 * 198)) + (k2 * 18)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 10, 11, 18), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([15], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((15,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(15):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1452, 15):\n            data_1 = T.Buffer((21780,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 15 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(15):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [11, 10, 11, 18]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 5700; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float pad_temp[153];\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          pad_temp[(((ax2 * 51) + (ax3 * 17)) + ax4)] = ((((1 <= ((((ax0_ax1_fused_ax2_fused_ax3_fused % 100) / 10) * 2) + ax2)) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused % 10) * 2) + ax3))) && (1 <= ax4)) ? data[(((((((ax0_ax1_fused_ax2_fused_ax3_fused / 10) * 640) + (ax2 * 320)) + ((ax0_ax1_fused_ax2_fused_ax3_fused % 10) * 32)) + (ax3 * 16)) + ax4) - 337)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n      pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4_1)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n            pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4_1)] = max(pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4_1)], pad_temp[((((rv0 * 51) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) % 25) * 2) + (((int)threadIdx.x) >> 4)) / 5) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 10) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 5) * 640) + (rv0 * 320)) + ((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 10) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 337)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 19, 20, 20, 16), \"float32\"), pool_max: T.Buffer((3, 19, 10, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(5700):\n            pad_temp = T.allocate([153], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((153,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(3, 3, 17):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 10\n                data_1 = T.Buffer((364800,), data=data.data)\n                pad_temp_1[ax2 * 51 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused_ax3_fused % 100 // 10 * 2 + ax2 and 1 <= cse_var_1 * 2 + ax3 and 1 <= ax4, data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 10 * 640 + ax2 * 320 + cse_var_1 * 32 + ax3 * 16 + ax4 - 337], T.float32(-3.4028234663852886e+38))\n            for ax4 in range(8):\n                pool_max_1 = T.Buffer((45600,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused * 8 + ax4\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 51 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [3, 19, 20, 20]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[20];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 231; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 20; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 20) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 20; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 145; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 8) + (((int)threadIdx.x) >> 2)) < 1155) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 7, 20, 11), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([20], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((20,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(20):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(231, 20):\n            data_1 = T.Buffer((4620,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 20 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(20):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [3, 7, 20, 11]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[8];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 8; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 585; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 8; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 8) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 8; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  float normal_reduce_temp0[1];\n  float red_buf0[1];\n  normal_reduce_temp0[0] = 1.000000e+00f;\n  for (int k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 147; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    if (((k0_k1_fused_k2_fused_k3_fused_outer * 4) + (((int)threadIdx.x) >> 3)) < 585) {\n      normal_reduce_temp0[0] = (normal_reduce_temp0[0] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 32) + ((int)threadIdx.x))]);\n    }\n  }\n  uint mask[1];\n  float t0[1];\n  red_buf0[0] = normal_reduce_temp0[0];\n  mask[0] = __activemask();\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 16, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 8, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 4, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 2, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  t0[0] = __shfl_down_sync(mask[0], red_buf0[0], 1, 32);\n  red_buf0[0] = (red_buf0[0] * t0[0]);\n  red_buf0[0] = __shfl_sync(mask[0], red_buf0[0], 0, 32);\n  if (((int)threadIdx.x) == 0) {\n    data_red[0] = red_buf0[0];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 13, 6, 12), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([8], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((8,), data=data_red_rf, align=32)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(8):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(585, 8):\n            data_1 = T.Buffer((4680,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 8 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(8):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [5, 13, 6, 12]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float pad_temp[10710];\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 21; ++ax3) {\n          for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n            pad_temp[((((ax1 * 1071) + (ax2 * 357)) + (ax3 * 17)) + ax4)] = ((((((1 <= ax2) && (ax2 < 2)) && (1 <= ax3)) && (ax3 < 20)) && (1 <= ax4)) ? data[((((((ax0 * 3040) + (ax1 * 304)) + (ax2 * 304)) + (ax3 * 16)) + ax4) - 321)] : -3.402823e+38f);\n          }\n        }\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 10; ++ax1_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 10; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          pool_max[((((ax0 * 800) + (ax1_1 * 80)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                pool_max[((((ax0 * 800) + (ax1_1 * 80)) + (ax3_1 * 8)) + ax4_1)] = max(pool_max[((((ax0 * 800) + (ax1_1 * 80)) + (ax3_1 * 8)) + ax4_1)], pad_temp[((((((ax1_1 * 1071) + (rv0 * 357)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], ((((((1 <= rv0) && (rv0 < 2)) && (1 <= ((((((int)blockIdx.x) & 1) * 10) + ((((int)threadIdx.x) >> 3) * 2)) + rv1))) && (((((((int)threadIdx.x) >> 3) + (rv1 >> 1)) / 5) + (((int)blockIdx.x) & 1)) < 2)) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[(((((((((((int)blockIdx.x) >> 1) * 304) + (rv0 * 304)) + ((((int)blockIdx.x) & 1) * 160)) + ((((int)threadIdx.x) >> 3) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 321)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 10, 1, 19, 16), \"float32\"), pool_max: T.Buffer((4, 10, 1, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            pad_temp = T.allocate([10710], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((10710,), data=pad_temp)\n            for ax1, ax2, ax3, ax4 in T.grid(10, 3, 21, 17):\n                data_1 = T.Buffer((12160,), data=data.data)\n                pad_temp_1[ax1 * 1071 + ax2 * 357 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and ax2 < 2 and 1 <= ax3 and ax3 < 20 and 1 <= ax4, data_1[ax0 * 3040 + ax1 * 304 + ax2 * 304 + ax3 * 16 + ax4 - 321], T.float32(-3.4028234663852886e+38))\n            for ax1, ax3, ax4 in T.grid(10, 10, 8):\n                pool_max_1 = T.Buffer((3200,), data=pool_max.data)\n                pool_max_1[ax0 * 800 + ax1 * 80 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0 * 800 + ax1 * 80 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax1 * 1071 + rv0 * 357 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [4, 10, 1, 19]}{"op_name": "prod", "c_code": "void default_function_kernel(float* data, float* data_red) {\n  float data_red_rf[17];\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_init = 0; k0_k1_fused_k2_fused_k3_fused_inner_init < 17; ++k0_k1_fused_k2_fused_k3_fused_inner_init) {\n    data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_init] = 1.000000e+00f;\n  }\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_outer = 0; k0_k1_fused_k2_fused_k3_fused_outer < 1620; ++k0_k1_fused_k2_fused_k3_fused_outer) {\n    for (int32_t k0_k1_fused_k2_fused_k3_fused_inner = 0; k0_k1_fused_k2_fused_k3_fused_inner < 17; ++k0_k1_fused_k2_fused_k3_fused_inner) {\n      data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] = (data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner] * data[((k0_k1_fused_k2_fused_k3_fused_outer * 17) + k0_k1_fused_k2_fused_k3_fused_inner)]);\n    }\n  }\n  data_red[0] = 1.000000e+00f;\n  for (int32_t k0_k1_fused_k2_fused_k3_fused_inner_v = 0; k0_k1_fused_k2_fused_k3_fused_inner_v < 17; ++k0_k1_fused_k2_fused_k3_fused_inner_v) {\n    data_red[0] = (data_red[0] * data_red_rf[k0_k1_fused_k2_fused_k3_fused_inner_v]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ data, float* __restrict__ data_red) {\n  data_red[0] = 1.000000e+00f;\n  for (int k0 = 0; k0 < 9; ++k0) {\n    for (int k1 = 0; k1 < 17; ++k1) {\n      for (int k2 = 0; k2 < 18; ++k2) {\n        for (int k3 = 0; k3 < 10; ++k3) {\n          data_red[0] = (data_red[0] * data[((((k0 * 3060) + (k1 * 180)) + (k2 * 10)) + k3)]);\n        }\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 17, 18, 10), \"float32\"), data_red: T.Buffer((), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_red_rf = T.allocate([17], \"float32\", \"global\")\n        data_red_rf_1 = T.Buffer((17,), data=data_red_rf)\n        for k0_k1_fused_k2_fused_k3_fused_inner_init in range(17):\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_init] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_outer, k0_k1_fused_k2_fused_k3_fused_inner in T.grid(1620, 17):\n            data_1 = T.Buffer((27540,), data=data.data)\n            data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] = data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner] * data_1[k0_k1_fused_k2_fused_k3_fused_outer * 17 + k0_k1_fused_k2_fused_k3_fused_inner]\n        data_red_1 = T.Buffer((1,), data=data_red.data)\n        data_red_1[0] = T.float32(1)\n        for k0_k1_fused_k2_fused_k3_fused_inner_v in range(17):\n            data_red_1[0] = data_red_1[0] * data_red_rf_1[k0_k1_fused_k2_fused_k3_fused_inner_v]", "op_args": [9, 17, 18, 10]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 54; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    float pad_temp[153];\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          pad_temp[(((ax2 * 51) + (ax3 * 17)) + ax4)] = (((((1 <= ((((ax0_ax1_fused_ax2_fused_ax3_fused % 9) / 3) * 2) + ax2)) && ((((ax0_ax1_fused_ax2_fused_ax3_fused % 9) / 3) + (ax2 >> 1)) < 3)) && (1 <= (((ax0_ax1_fused_ax2_fused_ax3_fused % 3) * 2) + ax3))) && (1 <= ax4)) ? data[((((((((ax0_ax1_fused_ax2_fused_ax3_fused / 9) * 480) + (((ax0_ax1_fused_ax2_fused_ax3_fused % 9) / 3) * 192)) + (ax2 * 96)) + ((ax0_ax1_fused_ax2_fused_ax3_fused % 3) * 32)) + (ax3 * 16)) + ax4) - 113)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n      pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4_1)] = -3.402823e+38f;\n      for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n        for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n          for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n            pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4_1)] = max(pool_max[((ax0_ax1_fused_ax2_fused_ax3_fused * 8) + ax4_1)], pad_temp[((((rv0 * 51) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))], (((((1 <= ((((((int)blockIdx.x) % 24) >> 3) * 2) + rv0)) && ((((((int)blockIdx.x) % 24) >> 3) + (rv0 >> 1)) < 3)) && (1 <= ((((((((int)blockIdx.x) & 7) * 3) + ((int)threadIdx.x)) >> 3) * 2) + rv1))) && (1 <= (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[(((((((((((int)blockIdx.x) / 24) * 480) + (((((int)blockIdx.x) % 24) >> 3) * 192)) + (rv0 * 96)) + (((((((int)blockIdx.x) & 7) * 3) + ((int)threadIdx.x)) >> 3) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 113)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 2, 5, 6, 16), \"float32\"), pool_max: T.Buffer((3, 2, 3, 3, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(54):\n            pad_temp = T.allocate([153], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((153,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(3, 3, 17):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 3\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused % 9 // 3\n                data_1 = T.Buffer((2880,), data=data.data)\n                pad_temp_1[ax2 * 51 + ax3 * 17 + ax4] = T.if_then_else(1 <= cse_var_1 * 2 + ax2 and cse_var_1 + ax2 // 2 < 3 and 1 <= cse_var_2 * 2 + ax3 and 1 <= ax4, data_1[ax0_ax1_fused_ax2_fused_ax3_fused // 9 * 480 + cse_var_1 * 192 + ax2 * 96 + cse_var_2 * 32 + ax3 * 16 + ax4 - 113], T.float32(-3.4028234663852886e+38))\n            for ax4 in range(8):\n                pool_max_1 = T.Buffer((432,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused_ax3_fused * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_3: T.int32 = ax0_ax1_fused_ax2_fused_ax3_fused * 8 + ax4\n                    pool_max_1[cse_var_3] = T.max(pool_max_1[cse_var_3], pad_temp_1[rv0 * 51 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [3, 2, 5, 6]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2625; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = roundf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 15, 7, 5), \"float32\"), compute: T.Buffer((5, 15, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2625):\n            compute_1 = T.Buffer((2625,), data=compute.data)\n            data_1 = T.Buffer((2625,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [5, 15, 7, 5]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        for (int32_t i3 = 0; i3 < 12; ++i3) {\n          compute[((((i0 * 288) + (i1 * 96)) + (i2 * 12)) + i3)] = roundf(data[((((i0 * 288) + (i1 * 96)) + (i2 * 12)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 3, 8, 12), \"float32\"), compute: T.Buffer((10, 3, 8, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(3, 8, 12):\n                cse_var_1: T.int32 = i0 * 288 + i1 * 96 + i2 * 12 + i3\n                compute_1 = T.Buffer((2880,), data=compute.data)\n                data_1 = T.Buffer((2880,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [10, 3, 8, 12]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 7; ++i1) {\n    for (int32_t i3 = 0; i3 < 5; ++i3) {\n      compute[((i1 * 5) + i3)] = roundf(data[((i1 * 5) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((int)blockIdx.x)] = roundf(data[((int)blockIdx.x)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 7, 1, 5), \"float32\"), compute: T.Buffer((1, 7, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i3 in T.grid(7, 5):\n            cse_var_1: T.int32 = i1 * 5 + i3\n            compute_1 = T.Buffer((35,), data=compute.data)\n            data_1 = T.Buffer((35,), data=data.data)\n            compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [1, 7, 1, 5]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  float pad_temp[153];\n  for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n        for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n          for (int32_t ax3_1 = 0; ax3_1 < 3; ++ax3_1) {\n            for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n              pad_temp[(((ax2_1 * 51) + (ax3_1 * 17)) + ax4)] = (((((1 <= ((ax2 * 2) + ax2_1)) && (((ax2_1 >> 1) + ax2) < 4)) && (1 <= ((ax3 * 2) + ax3_1))) && (1 <= ax4)) ? data[(((((((ax1 * 1120) + (ax2 * 320)) + (ax2_1 * 160)) + (ax3 * 32)) + (ax3_1 * 16)) + ax4) - 177)] : -3.402823e+38f);\n            }\n          }\n        }\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          pool_max[((((ax1 * 160) + (ax2 * 40)) + (ax3 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                pool_max[((((ax1 * 160) + (ax2 * 40)) + (ax3 * 8)) + ax4_1)] = max(pool_max[((((ax1 * 160) + (ax2 * 40)) + (ax3 * 8)) + ax4_1)], pad_temp[((((rv0 * 51) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))], (((((1 <= ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) & 15) >> 2) * 2) + rv0)) && ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) & 15) >> 2) + (rv0 >> 1)) < 4)) && (1 <= ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 20) >> 2) * 2) + rv1))) && (1 <= (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[(((((((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) >> 4) * 1120) + (((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 10)) & 15) >> 2) * 320)) + (rv0 * 160)) + (((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 20) >> 2) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 177)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 7, 10, 16), \"float32\"), pool_max: T.Buffer((1, 12, 4, 5, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        pad_temp = T.allocate([153], \"float32\", \"global\")\n        for ax1, ax2, ax3 in T.grid(12, 4, 5):\n            pad_temp_1 = T.Buffer((153,), data=pad_temp)\n            for ax2_1, ax3_1, ax4 in T.grid(3, 3, 17):\n                data_1 = T.Buffer((13440,), data=data.data)\n                pad_temp_1[ax2_1 * 51 + ax3_1 * 17 + ax4] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and ax2_1 // 2 + ax2 < 4 and 1 <= ax3 * 2 + ax3_1 and 1 <= ax4, data_1[ax1 * 1120 + ax2 * 320 + ax2_1 * 160 + ax3 * 32 + ax3_1 * 16 + ax4 - 177], T.float32(-3.4028234663852886e+38))\n            for ax4 in range(8):\n                pool_max_1 = T.Buffer((1920,), data=pool_max.data)\n                pool_max_1[ax1 * 160 + ax2 * 40 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax1 * 160 + ax2 * 40 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 51 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [1, 12, 7, 10]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          compute[((((i0 * 900) + (i1 * 45)) + (i2 * 15)) + i3)] = roundf(data[((((i0 * 900) + (i1 * 45)) + (i2 * 15)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 20, 3, 15), \"float32\"), compute: T.Buffer((9, 20, 3, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2, i3 in T.grid(20, 3, 15):\n                cse_var_1: T.int32 = i0 * 900 + i1 * 45 + i2 * 15 + i3\n                compute_1 = T.Buffer((8100,), data=compute.data)\n                data_1 = T.Buffer((8100,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [9, 20, 3, 15]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 442; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 8) + i3)] = roundf(data[((i0_i1_fused_i2_fused * 8) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 221) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 13, 2, 8), \"float32\"), compute: T.Buffer((17, 13, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(442):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((3536,), data=compute.data)\n                data_1 = T.Buffer((3536,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [17, 13, 2, 8]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 900; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[867];\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          pad_temp[(((ax2 * 289) + (ax3 * 17)) + ax4)] = ((((((1 <= (((ax0_ax1_fused_ax2_fused % 9) * 2) + ax2)) && (((ax2 >> 1) + (ax0_ax1_fused_ax2_fused % 9)) < 9)) && (1 <= ax3)) && (ax3 < 16)) && (1 <= ax4)) ? data[(((((((ax0_ax1_fused_ax2_fused / 9) * 4080) + ((ax0_ax1_fused_ax2_fused % 9) * 480)) + (ax2 * 240)) + (ax3 * 16)) + ax4) - 257)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax3_1 = 0; ax3_1 < 8; ++ax3_1) {\n      for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n        pool_max[(((ax0_ax1_fused_ax2_fused * 64) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              pool_max[(((ax0_ax1_fused_ax2_fused * 64) + (ax3_1 * 8)) + ax4_1)] = max(pool_max[(((ax0_ax1_fused_ax2_fused * 64) + (ax3_1 * 8)) + ax4_1)], pad_temp[(((((rv0 * 289) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))], ((((((1 <= ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 288) >> 5) * 2) + rv0)) && ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 288) >> 5) + (rv0 >> 1)) < 9)) && (1 <= ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) & 31) >> 2) * 2) + rv1))) && ((((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) & 31) >> 2) + (rv1 >> 1)) < 8)) && (1 <= (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[(((((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 6)) / 96) * 4080) + (((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) % 288) >> 5) * 480)) + (rv0 * 240)) + (((((((int)blockIdx.x) * 15) + (((int)threadIdx.x) >> 1)) & 31) >> 2) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 257)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 20, 17, 15, 16), \"float32\"), pool_max: T.Buffer((5, 20, 9, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            pad_temp = T.allocate([867], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((867,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(3, 17, 17):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused % 9\n                data_1 = T.Buffer((408000,), data=data.data)\n                pad_temp_1[ax2 * 289 + ax3 * 17 + ax4] = T.if_then_else(1 <= cse_var_1 * 2 + ax2 and ax2 // 2 + cse_var_1 < 9 and 1 <= ax3 and ax3 < 16 and 1 <= ax4, data_1[ax0_ax1_fused_ax2_fused // 9 * 4080 + cse_var_1 * 480 + ax2 * 240 + ax3 * 16 + ax4 - 257], T.float32(-3.4028234663852886e+38))\n            for ax3, ax4 in T.grid(8, 8):\n                pool_max_1 = T.Buffer((57600,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 64 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 64 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 289 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [5, 20, 17, 15]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      for (int32_t i3 = 0; i3 < 15; ++i3) {\n        compute[(((i0_i1_fused * 285) + (i2 * 15)) + i3)] = roundf(data[(((i0_i1_fused * 285) + (i2 * 15)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 3, 19, 15), \"float32\"), compute: T.Buffer((14, 3, 19, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(42):\n            for i2, i3 in T.grid(19, 15):\n                cse_var_1: T.int32 = i0_i1_fused * 285 + i2 * 15 + i3\n                compute_1 = T.Buffer((11970,), data=compute.data)\n                data_1 = T.Buffer((11970,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [14, 3, 19, 15]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 784; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = roundf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(7) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 7) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 4, 7, 14), \"float32\"), compute: T.Buffer((2, 4, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(784):\n            compute_1 = T.Buffer((784,), data=compute.data)\n            data_1 = T.Buffer((784,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [2, 4, 7, 14]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2352; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = roundf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 7, 14, 4), \"float32\"), compute: T.Buffer((6, 7, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2352):\n            compute_1 = T.Buffer((2352,), data=compute.data)\n            data_1 = T.Buffer((2352,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [6, 7, 14, 4]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 105; ++ax0_ax1_fused) {\n    float pad_temp[2805];\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 15; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          pad_temp[(((ax2 * 255) + (ax3 * 17)) + ax4)] = ((((((1 <= ax2) && (ax2 < 10)) && (1 <= ax3)) && (ax3 < 14)) && (1 <= ax4)) ? data[(((((ax0_ax1_fused * 1872) + (ax2 * 208)) + (ax3 * 16)) + ax4) - 225)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax2_1 = 0; ax2_1 < 5; ++ax2_1) {\n      for (int32_t ax3_1 = 0; ax3_1 < 7; ++ax3_1) {\n        for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n          pool_max[((((ax0_ax1_fused * 280) + (ax2_1 * 56)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n          for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n            for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n              for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                pool_max[((((ax0_ax1_fused * 280) + (ax2_1 * 56)) + (ax3_1 * 8)) + ax4_1)] = max(pool_max[((((ax0_ax1_fused * 280) + (ax2_1 * 56)) + (ax3_1 * 8)) + ax4_1)], pad_temp[((((((ax2_1 * 510) + (rv0 * 255)) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(49) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 49) + ((int)threadIdx.x))], ((((((1 <= ((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) / 7)) % 40) >> 3) * 2) + rv0)) && ((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) / 7)) % 40) >> 3) + (rv0 >> 1)) < 5)) && (1 <= ((((((((int)blockIdx.x) * 49) + ((int)threadIdx.x)) % 56) >> 3) * 2) + rv1))) && ((((((((int)blockIdx.x) * 49) + ((int)threadIdx.x)) % 56) >> 3) + (rv1 >> 1)) < 7)) && (1 <= ((((((int)blockIdx.x) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[(((((((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) / 7)) / 40) * 1872) + (((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) / 7)) % 40) >> 3) * 416)) + (rv0 * 208)) + (((((((int)blockIdx.x) * 49) + ((int)threadIdx.x)) % 56) >> 3) * 32)) + (rv1 * 16)) + (((((int)blockIdx.x) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 225)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 7, 9, 13, 16), \"float32\"), pool_max: T.Buffer((15, 7, 5, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(105):\n            pad_temp = T.allocate([2805], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((2805,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(11, 15, 17):\n                data_1 = T.Buffer((196560,), data=data.data)\n                pad_temp_1[ax2 * 255 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 and ax2 < 10 and 1 <= ax3 and ax3 < 14 and 1 <= ax4, data_1[ax0_ax1_fused * 1872 + ax2 * 208 + ax3 * 16 + ax4 - 225], T.float32(-3.4028234663852886e+38))\n            for ax2, ax3, ax4 in T.grid(5, 7, 8):\n                pool_max_1 = T.Buffer((29400,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused * 280 + ax2 * 56 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 280 + ax2 * 56 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[ax2 * 510 + rv0 * 255 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [15, 7, 9, 13]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 7140; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = roundf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 6, 10, 17), \"float32\"), compute: T.Buffer((7, 6, 10, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(7140):\n            compute_1 = T.Buffer((7140,), data=compute.data)\n            data_1 = T.Buffer((7140,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.round(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [7, 6, 10, 17]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 210; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[27];\n    for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n      for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n        for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n          for (int32_t ax3_1 = 0; ax3_1 < 3; ++ax3_1) {\n            for (int32_t ax4_s = 0; ax4_s < 3; ++ax4_s) {\n              pad_temp[(((ax2 * 9) + (ax3_1 * 3)) + ax4_s)] = ((((1 <= (((ax0_ax1_fused_ax2_fused % 3) * 2) + ax2)) && (1 <= ((ax3 * 2) + ax3_1))) && (1 <= ((ax4 * 2) + ax4_s))) ? data[(((((((ax0_ax1_fused_ax2_fused * 128) + (ax2 * 64)) + (ax3 * 32)) + (ax3_1 * 16)) + (ax4 * 2)) + ax4_s) - 81)] : -3.402823e+38f);\n            }\n          }\n        }\n        pool_max[(((ax0_ax1_fused_ax2_fused * 16) + (ax3 * 8)) + ax4)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              pool_max[(((ax0_ax1_fused_ax2_fused * 16) + (ax3 * 8)) + ax4)] = max(pool_max[(((ax0_ax1_fused_ax2_fused * 16) + (ax3 * 8)) + ax4)], pad_temp[(((rv0 * 9) + (rv1 * 3)) + rv2)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) % 24) >> 3) * 2) + rv0)) && (1 <= ((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) & 7) >> 2) * 2) + rv1))) && (1 <= (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) >> 3) * 128) + (rv0 * 64)) + (((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 1)) & 7) >> 2) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 81)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 10, 6, 4, 16), \"float32\"), pool_max: T.Buffer((7, 10, 3, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(210):\n            pad_temp = T.allocate([27], \"float32\", \"global\")\n            for ax3, ax4 in T.grid(2, 8):\n                pad_temp_1 = T.Buffer((27,), data=pad_temp)\n                for ax2, ax3_1, ax4_s in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax4 * 2\n                    data_1 = T.Buffer((26880,), data=data.data)\n                    pad_temp_1[ax2 * 9 + ax3_1 * 3 + ax4_s] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 3 * 2 + ax2 and 1 <= ax3 * 2 + ax3_1 and 1 <= cse_var_1 + ax4_s, data_1[ax0_ax1_fused_ax2_fused * 128 + ax2 * 64 + ax3 * 32 + ax3_1 * 16 + cse_var_1 + ax4_s - 81], T.float32(-3.4028234663852886e+38))\n                pool_max_1 = T.Buffer((3360,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 16 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 16 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_2] = T.max(pool_max_1[cse_var_2], pad_temp_1[rv0 * 9 + rv1 * 3 + rv2])", "op_args": [7, 10, 6, 4]}{"op_name": "round", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 19; ++i3) {\n        compute[(((i0_i1_fused * 152) + (i2 * 19)) + i3)] = roundf(data[(((i0_i1_fused * 152) + (i2 * 19)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = roundf(data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 1, 8, 19), \"float32\"), compute: T.Buffer((10, 1, 8, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(10):\n            for i2, i3 in T.grid(8, 19):\n                cse_var_1: T.int32 = i0_i1_fused * 152 + i2 * 19 + i3\n                compute_1 = T.Buffer((1520,), data=compute.data)\n                data_1 = T.Buffer((1520,), data=data.data)\n                compute_1[cse_var_1] = T.round(data_1[cse_var_1])", "op_args": [10, 1, 8, 19]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 280; ++ax0_ax1_fused_ax2_fused) {\n    float pad_temp[1071];\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 21; ++ax3) {\n        for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n          pad_temp[(((ax2 * 357) + (ax3 * 17)) + ax4)] = ((((1 <= (((ax0_ax1_fused_ax2_fused % 7) * 2) + ax2)) && (1 <= ax3)) && (1 <= ax4)) ? data[(((((ax0_ax1_fused_ax2_fused * 640) + (ax2 * 320)) + (ax3 * 16)) + ax4) - 337)] : -3.402823e+38f);\n        }\n      }\n    }\n    for (int32_t ax3_1 = 0; ax3_1 < 10; ++ax3_1) {\n      for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n        pool_max[(((ax0_ax1_fused_ax2_fused * 80) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n        for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n          for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n            for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n              pool_max[(((ax0_ax1_fused_ax2_fused * 80) + (ax3_1 * 8)) + ax4_1)] = max(pool_max[(((ax0_ax1_fused_ax2_fused * 80) + (ax3_1 * 8)) + ax4_1)], pad_temp[(((((rv0 * 357) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((((1 <= ((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) % 35) / 5) * 2) + rv0)) && (1 <= (((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 10) * 2) + rv1))) && (1 <= (((((int)threadIdx.x) & 7) * 2) + rv2))) ? data[((((((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 5) * 640) + (rv0 * 320)) + ((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 10) * 32)) + (rv1 * 16)) + ((((int)threadIdx.x) & 7) * 2)) + rv2) - 337)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 20, 14, 20, 16), \"float32\"), pool_max: T.Buffer((2, 20, 7, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(280):\n            pad_temp = T.allocate([1071], \"float32\", \"global\")\n            pad_temp_1 = T.Buffer((1071,), data=pad_temp)\n            for ax2, ax3, ax4 in T.grid(3, 21, 17):\n                data_1 = T.Buffer((179200,), data=data.data)\n                pad_temp_1[ax2 * 357 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax0_ax1_fused_ax2_fused % 7 * 2 + ax2 and 1 <= ax3 and 1 <= ax4, data_1[ax0_ax1_fused_ax2_fused * 640 + ax2 * 320 + ax3 * 16 + ax4 - 337], T.float32(-3.4028234663852886e+38))\n            for ax3, ax4 in T.grid(10, 8):\n                pool_max_1 = T.Buffer((22400,), data=pool_max.data)\n                pool_max_1[ax0_ax1_fused_ax2_fused * 80 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 80 + ax3 * 8 + ax4\n                    pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 357 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [2, 20, 14, 20]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        compute[(((i0 * 192) + (i2 * 12)) + i3)] = (1.000000e+00f / sqrtf(data[(((i0 * 192) + (i2 * 12)) + i3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 1, 16, 12), \"float32\"), compute: T.Buffer((17, 1, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            for i2, i3 in T.grid(16, 12):\n                cse_var_1: T.int32 = i0 * 192 + i2 * 12 + i3\n                compute_1 = T.Buffer((3264,), data=compute.data)\n                data_1 = T.Buffer((3264,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [17, 1, 16, 12]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 132; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      for (int32_t i3 = 0; i3 < 5; ++i3) {\n        compute[(((i0_i1_fused * 10) + (i2 * 5)) + i3)] = (1.000000e+00f / sqrtf(data[(((i0_i1_fused * 10) + (i2 * 5)) + i3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 12, 2, 5), \"float32\"), compute: T.Buffer((11, 12, 2, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(132):\n            for i2, i3 in T.grid(2, 5):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2 * 5 + i3\n                compute_1 = T.Buffer((1320,), data=compute.data)\n                data_1 = T.Buffer((1320,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [11, 12, 2, 5]}{"op_name": "pool3d", "c_code": "void default_function_kernel(float* data, float* pool_max) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    float pad_temp[561];\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n          for (int32_t ax3 = 0; ax3 < 11; ++ax3) {\n            for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n              pad_temp[(((ax2_1 * 187) + (ax3 * 17)) + ax4)] = (((((1 <= ((ax2 * 2) + ax2_1)) && (((ax2_1 >> 1) + ax2) < 7)) && (1 <= ax3)) && (1 <= ax4)) ? data[(((((((ax0 * 6240) + (ax1 * 2080)) + (ax2 * 320)) + (ax2_1 * 160)) + (ax3 * 16)) + ax4) - 177)] : -3.402823e+38f);\n            }\n          }\n        }\n        for (int32_t ax3_1 = 0; ax3_1 < 5; ++ax3_1) {\n          for (int32_t ax4_1 = 0; ax4_1 < 8; ++ax4_1) {\n            pool_max[(((((ax0 * 840) + (ax1 * 280)) + (ax2 * 40)) + (ax3_1 * 8)) + ax4_1)] = -3.402823e+38f;\n            for (int32_t rv0 = 0; rv0 < 3; ++rv0) {\n              for (int32_t rv1 = 0; rv1 < 3; ++rv1) {\n                for (int32_t rv2 = 0; rv2 < 3; ++rv2) {\n                  pool_max[(((((ax0 * 840) + (ax1 * 280)) + (ax2 * 40)) + (ax3_1 * 8)) + ax4_1)] = max(pool_max[(((((ax0 * 840) + (ax1 * 280)) + (ax2 * 40)) + (ax3_1 * 8)) + ax4_1)], pad_temp[(((((rv0 * 187) + (ax3_1 * 34)) + (rv1 * 17)) + (ax4_1 * 2)) + rv2)]);\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ data, float* __restrict__ pool_max) {\n  pool_max[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int rv0 = 0; rv0 < 3; ++rv0) {\n    for (int rv1 = 0; rv1 < 3; ++rv1) {\n      for (int rv2 = 0; rv2 < 3; ++rv2) {\n        pool_max[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = max(pool_max[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))], (((((1 <= ((((((((int)blockIdx.x) & 7) * 7) + (((int)threadIdx.x) / 5)) >> 3) * 2) + rv0)) && ((((((((int)blockIdx.x) & 7) * 7) + (((int)threadIdx.x) / 5)) >> 3) + (rv0 >> 1)) < 7)) && (1 <= ((((((((int)blockIdx.x) * 35) + ((int)threadIdx.x)) % 40) >> 3) * 2) + rv1))) && (1 <= (((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 2) + rv2))) ? data[(((((((((((int)blockIdx.x) >> 3) * 2080) + (((((((int)blockIdx.x) & 7) * 7) + (((int)threadIdx.x) / 5)) >> 3) * 320)) + (rv0 * 160)) + (((((((int)blockIdx.x) * 35) + ((int)threadIdx.x)) % 40) >> 3) * 32)) + (rv1 * 16)) + ((((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) & 7) * 2)) + rv2) - 177)] : -3.402823e+38f));\n      }\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 3, 13, 10, 16), \"float32\"), pool_max: T.Buffer((5, 3, 7, 5, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            pad_temp = T.allocate([561], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(3, 7):\n                pad_temp_1 = T.Buffer((561,), data=pad_temp)\n                for ax2_1, ax3, ax4 in T.grid(3, 11, 17):\n                    data_1 = T.Buffer((31200,), data=data.data)\n                    pad_temp_1[ax2_1 * 187 + ax3 * 17 + ax4] = T.if_then_else(1 <= ax2 * 2 + ax2_1 and ax2_1 // 2 + ax2 < 7 and 1 <= ax3 and 1 <= ax4, data_1[ax0 * 6240 + ax1 * 2080 + ax2 * 320 + ax2_1 * 160 + ax3 * 16 + ax4 - 177], T.float32(-3.4028234663852886e+38))\n                for ax3, ax4 in T.grid(5, 8):\n                    pool_max_1 = T.Buffer((4200,), data=pool_max.data)\n                    pool_max_1[ax0 * 840 + ax1 * 280 + ax2 * 40 + ax3 * 8 + ax4] = T.float32(-3.4028234663852886e+38)\n                    for rv0, rv1, rv2 in T.grid(3, 3, 3):\n                        cse_var_1: T.int32 = ax0 * 840 + ax1 * 280 + ax2 * 40 + ax3 * 8 + ax4\n                        pool_max_1[cse_var_1] = T.max(pool_max_1[cse_var_1], pad_temp_1[rv0 * 187 + ax3 * 34 + rv1 * 17 + ax4 * 2 + rv2])", "op_args": [5, 3, 13, 10]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        for (int32_t i3 = 0; i3 < 11; ++i3) {\n          compute[((((i0 * 1100) + (i1 * 55)) + (i2 * 11)) + i3)] = (1.000000e+00f / sqrtf(data[((((i0 * 1100) + (i1 * 55)) + (i2 * 11)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 5, 11), \"float32\"), compute: T.Buffer((7, 20, 5, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2, i3 in T.grid(20, 5, 11):\n                cse_var_1: T.int32 = i0 * 1100 + i1 * 55 + i2 * 11 + i3\n                compute_1 = T.Buffer((7700,), data=compute.data)\n                data_1 = T.Buffer((7700,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [7, 20, 5, 11]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        for (int32_t i3 = 0; i3 < 10; ++i3) {\n          compute[((((i0 * 800) + (i1 * 50)) + (i2 * 10)) + i3)] = (1.000000e+00f / sqrtf(data[((((i0 * 800) + (i1 * 50)) + (i2 * 10)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 5, 10), \"float32\"), compute: T.Buffer((5, 16, 5, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(5):\n            for i1, i2, i3 in T.grid(16, 5, 10):\n                cse_var_1: T.int32 = i0 * 800 + i1 * 50 + i2 * 10 + i3\n                compute_1 = T.Buffer((4000,), data=compute.data)\n                data_1 = T.Buffer((4000,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [5, 16, 5, 10]}{"op_name": "relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        for (int32_t i3 = 0; i3 < 3; ++i3) {\n          compute[((((i0 * 168) + (i1 * 21)) + (i2 * 3)) + i3)] = max(data[((((i0 * 168) + (i1 * 21)) + (i2 * 3)) + i3)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 315) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 8, 7, 3), \"float32\"), compute: T.Buffer((15, 8, 7, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(15):\n            for i1, i2, i3 in T.grid(8, 7, 3):\n                cse_var_1: T.int32 = i0 * 168 + i1 * 21 + i2 * 3 + i3\n                compute_1 = T.Buffer((2520,), data=compute.data)\n                data_1 = T.Buffer((2520,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))", "op_args": [15, 8, 7, 3]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 8) + i3)] = (1.000000e+00f / sqrtf(data[((i0_i1_fused_i2_fused * 8) + i3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 10, 4, 8), \"float32\"), compute: T.Buffer((2, 10, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 8 + i3\n                compute_1 = T.Buffer((640,), data=compute.data)\n                data_1 = T.Buffer((640,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [2, 10, 4, 8]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        compute[(((i0 * 88) + (i1 * 8)) + i3)] = (1.000000e+00f / sqrtf(data[(((i0 * 88) + (i1 * 8)) + i3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 3)) < 187) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 11, 1, 8), \"float32\"), compute: T.Buffer((17, 11, 1, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            for i1, i3 in T.grid(11, 8):\n                cse_var_1: T.int32 = i0 * 88 + i1 * 8 + i3\n                compute_1 = T.Buffer((1496,), data=compute.data)\n                data_1 = T.Buffer((1496,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [17, 11, 1, 8]}{"op_name": "relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3920; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = max(data[i0_i1_fused_i2_fused_i3_fused], 0.000000e+00f);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 20, 2, 14), \"float32\"), compute: T.Buffer((7, 20, 2, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3920):\n            compute_1 = T.Buffer((3920,), data=compute.data)\n            data_1 = T.Buffer((3920,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.max(data_1[i0_i1_fused_i2_fused_i3_fused], T.float32(0))", "op_args": [7, 20, 2, 14]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 2860; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / sqrtf(data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) < 715) {\n    compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 5, 13, 11), \"float32\"), compute: T.Buffer((4, 5, 13, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(2860):\n            compute_1 = T.Buffer((2860,), data=compute.data)\n            data_1 = T.Buffer((2860,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.rsqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [4, 5, 13, 11]}{"op_name": "relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 14; ++i3) {\n        compute[(((i0_i1_fused * 84) + (i2 * 14)) + i3)] = max(data[(((i0_i1_fused * 84) + (i2 * 14)) + i3)], 0.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 315) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], 0.000000e+00f);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 6, 6, 14), \"float32\"), compute: T.Buffer((10, 6, 6, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(60):\n            for i2, i3 in T.grid(6, 14):\n                cse_var_1: T.int32 = i0_i1_fused * 84 + i2 * 14 + i3\n                compute_1 = T.Buffer((5040,), data=compute.data)\n                data_1 = T.Buffer((5040,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))", "op_args": [10, 6, 6, 14]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        for (int32_t i3 = 0; i3 < 12; ++i3) {\n          compute[((((i0 * 360) + (i1 * 180)) + (i2 * 12)) + i3)] = (1.000000e+00f / sqrtf(data[((((i0 * 360) + (i1 * 180)) + (i2 * 12)) + i3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 2, 15, 12), \"float32\"), compute: T.Buffer((6, 2, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(6):\n            for i1, i2, i3 in T.grid(2, 15, 12):\n                cse_var_1: T.int32 = i0 * 360 + i1 * 180 + i2 * 12 + i3\n                compute_1 = T.Buffer((2160,), data=compute.data)\n                data_1 = T.Buffer((2160,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [6, 2, 15, 12]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      for (int32_t i3_s = 0; i3_s < 9; ++i3_s) {\n        compute[(((i0_i1_fused * 117) + (i2 * 9)) + i3_s)] = (1.000000e+00f / sqrtf(data[(((i0_i1_fused * 117) + (i2 * 9)) + i3_s)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 13, 13, 9), \"float32\"), compute: T.Buffer((10, 13, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(130):\n            for i2, i3_s in T.grid(13, 9):\n                cse_var_1: T.int32 = i0_i1_fused * 117 + i2 * 9 + i3_s\n                compute_1 = T.Buffer((15210,), data=compute.data)\n                data_1 = T.Buffer((15210,), data=data.data)\n                compute_1[cse_var_1] = T.rsqrt(data_1[cse_var_1])", "op_args": [10, 13, 13, 9]}{"op_name": "relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 306; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i0_i1_fused * 260) + (i2 * 20)) + i3)] = max(data[(((i0_i1_fused * 260) + (i2 * 20)) + i3)], 0.000000e+00f);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 18, 13, 20), \"float32\"), compute: T.Buffer((17, 18, 13, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(306):\n            for i2, i3 in T.grid(13, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 260 + i2 * 20 + i3\n                compute_1 = T.Buffer((79560,), data=compute.data)\n                data_1 = T.Buffer((79560,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))", "op_args": [17, 18, 13, 20]}{"op_name": "relu", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        for (int32_t i3 = 0; i3 < 16; ++i3) {\n          compute[((((i0 * 3120) + (i1 * 208)) + (i2 * 16)) + i3)] = max(data[((((i0 * 3120) + (i1 * 208)) + (i2 * 16)) + i3)], 0.000000e+00f);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], 0.000000e+00f);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 15, 13, 16), \"float32\"), compute: T.Buffer((4, 15, 13, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(4):\n            for i1, i2, i3 in T.grid(15, 13, 16):\n                cse_var_1: T.int32 = i0 * 3120 + i1 * 208 + i2 * 16 + i3\n                compute_1 = T.Buffer((12480,), data=compute.data)\n                data_1 = T.Buffer((12480,), data=data.data)\n                compute_1[cse_var_1] = T.max(data_1[cse_var_1], T.float32(0))", "op_args": [4, 15, 13, 16]}{"op_name": "rsqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6750; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = (1.000000e+00f / sqrtf(data[i0_i1_fused_i2_fused_i3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 14) + (((int)threadIdx.x) >> 1)) < 3375) {\n    compute[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = (1.000000e+00f / sqrtf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 9, 5, 10), \"float32\"), compute: T.Buffer((15, 9, 5, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6750):\n            compute_1 = T.Buffer((6750,), data=compute.data)\n            data_1 = T.Buffer((6750,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.rsqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [15, 9, 5, 10]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 19 : ((ax0 == 2) ? 2 : ((ax0 == 1) ? 13 : 14)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 19 : ((((int)threadIdx.x) == 2) ? 2 : ((((int)threadIdx.x) == 1) ? 13 : 14)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 13, 2, 19), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 19, T.if_then_else(ax0 == 2, 2, T.if_then_else(ax0 == 1, 13, 14)))", "op_args": [14, 13, 2, 19]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused < 1596; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused) {\n    for (int32_t c_inner = 0; c_inner < 9; ++c_inner) {\n      ScaleShift[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 84) / 14) * 2394) + (c_inner * 266)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 14) * 19)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 84))] = ((data[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 84) / 14) * 2394) + (c_inner * 266)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 14) * 19)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 84))] * Scale[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 28) / 14) * 9) + c_inner)]) + Shift[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 28) / 14) * 9) + c_inner)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 27) + (((int)threadIdx.x) >> 1)) % 2394) / 133)]) + Shift[((((((int)blockIdx.x) * 27) + (((int)threadIdx.x) >> 1)) % 2394) / 133)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 18, 14, 19), \"float32\"), Scale: T.Buffer((18,), \"float32\"), Shift: T.Buffer((18,), \"float32\"), ScaleShift: T.Buffer((3, 18, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused in T.parallel(1596):\n            for c_inner in range(9):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 28 // 14 * 9 + c_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 84 // 14 * 2394 + c_inner * 266 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 14 * 19 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 84\n                ScaleShift_1 = T.Buffer((14364,), data=ScaleShift.data)\n                data_1 = T.Buffer((14364,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [3, 18, 14, 19]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 16; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t c_inner = 0; c_inner < 6; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 17; ++i_inner) {\n        ScaleShift[(((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 816) + (c_inner * 136)) + (i_inner * 8)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7))] = ((data[(((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 816) + (c_inner * 136)) + (i_inner * 8)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7))] * Scale[(((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 6) + c_inner)]) + Shift[(((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 6) + c_inner)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 17)]) + Shift[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 17)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 17, 8), \"float32\"), Scale: T.Buffer((12,), \"float32\"), Shift: T.Buffer((12,), \"float32\"), ScaleShift: T.Buffer((1, 12, 17, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(16):\n            for c_inner, i_inner in T.grid(6, 17):\n                cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 8\n                cse_var_2: T.int32 = cse_var_3 * 6 + c_inner\n                cse_var_1: T.int32 = cse_var_3 * 816 + c_inner * 136 + i_inner * 8 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 8\n                ScaleShift_1 = T.Buffer((1632,), data=ScaleShift.data)\n                data_1 = T.Buffer((1632,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [1, 12, 17, 8]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused < 375; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused) {\n    for (int32_t c_inner = 0; c_inner < 3; ++c_inner) {\n      for (int32_t i_inner = 0; i_inner < 11; ++i_inner) {\n        ScaleShift[(((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 25) * 495) + (c_inner * 165)) + (i_inner * 15)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 25))] = ((data[(((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 25) * 495) + (c_inner * 165)) + (i_inner * 15)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused / 25))] * Scale[(((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 5) * 3) + c_inner)]) + Shift[(((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 5) * 3) + c_inner)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 12375) {\n    ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 2475) / 165)]) + Shift[((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 2475) / 165)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 15, 11, 15), \"float32\"), Scale: T.Buffer((15,), \"float32\"), Shift: T.Buffer((15,), \"float32\"), ScaleShift: T.Buffer((5, 15, 11, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused in T.parallel(375):\n            for c_inner, i_inner in T.grid(3, 11):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 5 * 3 + c_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused % 25 * 495 + c_inner * 165 + i_inner * 15 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused // 25\n                ScaleShift_1 = T.Buffer((12375,), data=ScaleShift.data)\n                data_1 = T.Buffer((12375,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [5, 15, 11, 15]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 5 : ((ax0 == 2) ? 2 : ((ax0 == 1) ? 11 : 14)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 5 : ((((int)threadIdx.x) == 2) ? 2 : ((((int)threadIdx.x) == 1) ? 11 : 14)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 11, 2, 5), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 5, T.if_then_else(ax0 == 2, 2, T.if_then_else(ax0 == 1, 11, 14)))", "op_args": [14, 11, 2, 5]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  for (int32_t b_outer_outer_inner = 0; b_outer_outer_inner < 2; ++b_outer_outer_inner) {\n    for (int32_t c_outer_outer_inner = 0; c_outer_outer_inner < 8; ++c_outer_outer_inner) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n          for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n            for (int32_t j_inner = 0; j_inner < 5; ++j_inner) {\n              ScaleShift[((((((b_outer_outer_inner * 320) + (b_outer_inner * 160)) + (b_inner * 80)) + (c_outer_outer_inner * 10)) + (c_inner * 5)) + j_inner)] = ((data[((((((b_outer_outer_inner * 320) + (b_outer_inner * 160)) + (b_inner * 80)) + (c_outer_outer_inner * 10)) + (c_inner * 5)) + j_inner)] * Scale[((c_outer_outer_inner * 2) + c_inner)]) + Shift[((c_outer_outer_inner * 2) + c_inner)]);\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 5) * 16) + ((int)threadIdx.x)) / 5)]) + Shift[((((((int)blockIdx.x) % 5) * 16) + ((int)threadIdx.x)) / 5)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 16, 1, 5), \"float32\"), Scale: T.Buffer((16,), \"float32\"), Shift: T.Buffer((16,), \"float32\"), ScaleShift: T.Buffer((8, 16, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_inner, c_outer_outer_inner, b_outer_inner, b_inner, c_inner, j_inner in T.grid(2, 8, 2, 2, 2, 5):\n            cse_var_2: T.int32 = c_outer_outer_inner * 2 + c_inner\n            cse_var_1: T.int32 = b_outer_outer_inner * 320 + b_outer_inner * 160 + b_inner * 80 + c_outer_outer_inner * 10 + c_inner * 5 + j_inner\n            ScaleShift_1 = T.Buffer((640,), data=ScaleShift.data)\n            data_1 = T.Buffer((640,), data=data.data)\n            ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [8, 16, 1, 5]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused < 5; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused) {\n    for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 2; ++j_outer_outer_inner) {\n      for (int32_t c_outer_inner = 0; c_outer_inner < 19; ++c_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 19; ++i_outer_inner) {\n          ScaleShift[((((c_outer_inner * 190) + (i_outer_inner * 10)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused * 2)) + j_outer_outer_inner)] = ((data[((((c_outer_inner * 190) + (i_outer_inner * 10)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused * 2)) + j_outer_outer_inner)] * Scale[c_outer_inner]) + Shift[c_outer_inner]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 6) + (((int)threadIdx.x) / 10)) < 361) {\n    ScaleShift[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 6) + (((int)threadIdx.x) / 10)) / 19)]) + Shift[(((((int)blockIdx.x) * 6) + (((int)threadIdx.x) / 10)) / 19)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 19, 19, 10), \"float32\"), Scale: T.Buffer((19,), \"float32\"), Shift: T.Buffer((19,), \"float32\"), ScaleShift: T.Buffer((1, 19, 19, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused in T.parallel(5):\n            for j_outer_outer_inner, c_outer_inner, i_outer_inner in T.grid(2, 19, 19):\n                cse_var_1: T.int32 = c_outer_inner * 190 + i_outer_inner * 10 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused * 2 + j_outer_outer_inner\n                ScaleShift_1 = T.Buffer((3610,), data=ScaleShift.data)\n                data_1 = T.Buffer((3610,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[c_outer_inner] + Shift[c_outer_inner]", "op_args": [1, 19, 19, 10]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused < 6480; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused) {\n    ScaleShift[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused / 144) * 144) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 48) / 24) * 72)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused & 7) * 9)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 144) / 48) * 3)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 24) >> 3))] = ((data[((((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused / 144) * 144) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 48) / 24) * 72)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused & 7) * 9)) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 144) / 48) * 3)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 24) >> 3))] * Scale[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 1296) / 144) * 2) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 48) / 24))]) + Shift[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 1296) / 144) * 2) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 48) / 24))]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) % 108) / 6)]) + Shift[((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) % 108) / 6)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 18, 8, 9), \"float32\"), Scale: T.Buffer((18,), \"float32\"), Shift: T.Buffer((18,), \"float32\"), ScaleShift: T.Buffer((5, 18, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused in T.parallel(6480):\n            cse_var_3: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 48 // 24\n            cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 1296 // 144 * 2 + cse_var_3\n            cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused // 144 * 144 + cse_var_3 * 72 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 8 * 9 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 144 // 48 * 3 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_b_outer_inner_fused_c_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_b_inner_fused_c_inner_fused_i_inner_fused % 24 // 8\n            ScaleShift_1 = T.Buffer((6480,), data=ScaleShift.data)\n            data_1 = T.Buffer((6480,), data=data.data)\n            ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [5, 18, 8, 9]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  for (int32_t c_outer_outer_inner = 0; c_outer_outer_inner < 4; ++c_outer_outer_inner) {\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 5; ++i_outer_outer_inner) {\n      for (int32_t b_inner = 0; b_inner < 4; ++b_inner) {\n        for (int32_t c_inner = 0; c_inner < 2; ++c_inner) {\n          for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n            ScaleShift[(((((b_inner * 80) + (c_outer_outer_inner * 20)) + (c_inner * 10)) + (i_outer_outer_inner * 2)) + i_inner)] = ((data[(((((b_inner * 80) + (c_outer_outer_inner * 20)) + (c_inner * 10)) + (i_outer_outer_inner * 2)) + i_inner)] * Scale[((c_outer_outer_inner * 2) + c_inner)]) + Shift[((c_outer_outer_inner * 2) + c_inner)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) / 10))]) + Shift[(((((int)blockIdx.x) & 1) * 4) + (((int)threadIdx.x) / 10))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 8, 10, 1), \"float32\"), Scale: T.Buffer((8,), \"float32\"), Shift: T.Buffer((8,), \"float32\"), ScaleShift: T.Buffer((4, 8, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for c_outer_outer_inner, i_outer_outer_inner, b_inner, c_inner, i_inner in T.grid(4, 5, 4, 2, 2):\n            cse_var_2: T.int32 = c_outer_outer_inner * 2 + c_inner\n            cse_var_1: T.int32 = b_inner * 80 + c_outer_outer_inner * 20 + c_inner * 10 + i_outer_outer_inner * 2 + i_inner\n            ScaleShift_1 = T.Buffer((320,), data=ScaleShift.data)\n            data_1 = T.Buffer((320,), data=data.data)\n            ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [4, 8, 10, 1]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 15 : ((ax0 == 2) ? 9 : ((ax0 == 1) ? 12 : 3)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 15 : ((((int)threadIdx.x) == 2) ? 9 : ((((int)threadIdx.x) == 1) ? 12 : 3)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 12, 9, 15), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 15, T.if_then_else(ax0 == 2, 9, T.if_then_else(ax0 == 1, 12, 3)))", "op_args": [3, 12, 9, 15]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 800; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 11; ++b_outer_inner) {\n      ScaleShift[((((b_outer_inner * 800) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 200) / 20) * 80)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 200) * 20)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 20))] = ((data[((((b_outer_inner * 800) + (((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 200) / 20) * 80)) + ((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 200) * 20)) + (b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 20))] * Scale[((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 200) / 20)]) + Shift[((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 200) / 20)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 11) + (((int)threadIdx.x) >> 2)) % 200) / 20)]) + Shift[((((((int)blockIdx.x) * 11) + (((int)threadIdx.x) >> 2)) % 200) / 20)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 10, 20, 4), \"float32\"), Scale: T.Buffer((10,), \"float32\"), Shift: T.Buffer((10,), \"float32\"), ScaleShift: T.Buffer((11, 10, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(800):\n            for b_outer_inner in range(11):\n                cse_var_2: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 200 // 20\n                cse_var_1: T.int32 = b_outer_inner * 800 + cse_var_2 * 80 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 200 * 20 + b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_c_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 20\n                ScaleShift_1 = T.Buffer((8800,), data=ScaleShift.data)\n                data_1 = T.Buffer((8800,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[cse_var_2] + Shift[cse_var_2]", "op_args": [11, 10, 20, 4]}{"op_name": "scale_shift_nchw", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused = 0; b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused < 6; ++b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused) {\n    for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 17; ++j_outer_outer_inner) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 11; ++i_outer_inner) {\n        for (int32_t c_inner = 0; c_inner < 5; ++c_inner) {\n          ScaleShift[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused * 935) + (c_inner * 187)) + (i_outer_inner * 17)) + j_outer_outer_inner)] = ((data[((((b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused * 935) + (c_inner * 187)) + (i_outer_inner * 17)) + j_outer_outer_inner)] * Scale[c_inner]) + Shift[c_inner]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 30) + ((int)threadIdx.x)) % 935) / 187)]) + Shift[((((((int)blockIdx.x) * 30) + ((int)threadIdx.x)) % 935) / 187)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 5, 11, 17), \"float32\"), Scale: T.Buffer((5,), \"float32\"), Shift: T.Buffer((5,), \"float32\"), ScaleShift: T.Buffer((6, 5, 11, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused in T.parallel(6):\n            for j_outer_outer_inner, i_outer_inner, c_inner in T.grid(17, 11, 5):\n                cse_var_1: T.int32 = b_outer_outer_outer_c_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused * 935 + c_inner * 187 + i_outer_inner * 17 + j_outer_outer_inner\n                ScaleShift_1 = T.Buffer((5610,), data=ScaleShift.data)\n                data_1 = T.Buffer((5610,), data=data.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale[c_inner] + Shift[c_inner]", "op_args": [6, 5, 11, 17]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 5 : ((ax0 == 2) ? 9 : ((ax0 == 1) ? 5 : 8)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 5 : ((((int)threadIdx.x) == 2) ? 9 : ((((int)threadIdx.x) == 1) ? 5 : 8)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 5, 9, 5), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 5, T.if_then_else(ax0 == 2, 9, T.if_then_else(ax0 == 1, 5, 8)))", "op_args": [8, 5, 9, 5]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 9; ++i1) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      for (int32_t i3 = 0; i3 < 12; ++i3) {\n        compute[(((i1 * 84) + (i2 * 12)) + i3)] = ((0.000000e+00f < data[(((i1 * 84) + (i2 * 12)) + i3)]) ? data[(((i1 * 84) + (i2 * 12)) + i3)] : (data[(((i1 * 84) + (i2 * 12)) + i3)] * Scale[i3]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 189) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 12)]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 9, 7, 12), \"float32\"), Scale: T.Buffer((12,), \"float32\"), compute: T.Buffer((1, 9, 7, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(9, 7, 12):\n            cse_var_1: T.int32 = i1 * 84 + i2 * 12 + i3\n            compute_1 = T.Buffer((756,), data=compute.data)\n            data_1 = T.Buffer((756,), data=data.data)\n            compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [1, 9, 7, 12]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 64) + (i1 * 8)) + i2)] = ((0.000000e+00f < data[(((i0 * 64) + (i1 * 8)) + i2)]) ? data[(((i0 * 64) + (i1 * 8)) + i2)] : (data[(((i0 * 64) + (i1 * 8)) + i2)] * Scale[0]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[0]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 8, 8, 1), \"float32\"), Scale: T.Buffer((1,), \"float32\"), compute: T.Buffer((20, 8, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(8, 8):\n                cse_var_1: T.int32 = i0 * 64 + i1 * 8 + i2\n                compute_1 = T.Buffer((1280,), data=compute.data)\n                data_1 = T.Buffer((1280,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[0])", "op_args": [20, 8, 8, 1]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 16 : ((ax0 == 2) ? 17 : ((ax0 == 1) ? 9 : 7)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 16 : ((((int)threadIdx.x) == 2) ? 17 : ((((int)threadIdx.x) == 1) ? 9 : 7)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 9, 17, 16), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 16, T.if_then_else(ax0 == 2, 17, T.if_then_else(ax0 == 1, 9, 7)))", "op_args": [7, 9, 17, 16]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 420; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 10; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 10) + i3)] = ((0.000000e+00f < data[((i0_i1_fused_i2_fused * 10) + i3)]) ? data[((i0_i1_fused_i2_fused * 10) + i3)] : (data[((i0_i1_fused_i2_fused * 10) + i3)] * Scale[i3]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 10)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 14, 5, 10), \"float32\"), Scale: T.Buffer((10,), \"float32\"), compute: T.Buffer((6, 14, 5, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(420):\n            for i3 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 10 + i3\n                compute_1 = T.Buffer((4200,), data=compute.data)\n                data_1 = T.Buffer((4200,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [6, 14, 5, 10]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 7 : ((ax0 == 2) ? 20 : ((ax0 == 1) ? 1 : 18)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 7 : ((((int)threadIdx.x) == 2) ? 20 : ((((int)threadIdx.x) == 1) ? 1 : 18)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 1, 20, 7), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 7, T.if_then_else(ax0 == 2, 20, T.if_then_else(ax0 == 1, 1, 18)))", "op_args": [18, 1, 20, 7]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 10752; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[(i0_i1_fused_i2_fused_i3_fused & 15)]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) & 15)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 14, 4, 16), \"float32\"), Scale: T.Buffer((16,), \"float32\"), compute: T.Buffer((12, 14, 4, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(10752):\n            compute_1 = T.Buffer((10752,), data=compute.data)\n            data_1 = T.Buffer((10752,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 16])", "op_args": [12, 14, 4, 16]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 4 : ((ax0 == 2) ? 7 : ((ax0 == 1) ? 1 : 9)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 4 : ((((int)threadIdx.x) == 2) ? 7 : ((((int)threadIdx.x) == 1) ? 1 : 9)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 1, 7, 4), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 4, T.if_then_else(ax0 == 2, 7, T.if_then_else(ax0 == 1, 1, 9)))", "op_args": [9, 1, 7, 4]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 252; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 19) + i3)] = ((0.000000e+00f < data[((i0_i1_fused_i2_fused * 19) + i3)]) ? data[((i0_i1_fused_i2_fused * 19) + i3)] : (data[((i0_i1_fused_i2_fused * 19) + i3)] * Scale[i3]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 17) + ((int)threadIdx.x)) % 19)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 7, 4, 19), \"float32\"), Scale: T.Buffer((19,), \"float32\"), compute: T.Buffer((9, 7, 4, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(252):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 19 + i3\n                compute_1 = T.Buffer((4788,), data=compute.data)\n                data_1 = T.Buffer((4788,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [9, 7, 4, 19]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 5 : ((ax0 == 2) ? 17 : ((ax0 == 1) ? 18 : 11)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 5 : ((((int)threadIdx.x) == 2) ? 17 : ((((int)threadIdx.x) == 1) ? 18 : 11)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 18, 17, 5), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 5, T.if_then_else(ax0 == 2, 17, T.if_then_else(ax0 == 1, 18, 11)))", "op_args": [11, 18, 17, 5]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 11340; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[(i0_i1_fused_i2_fused_i3_fused % 15)]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(42) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 42) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 15)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 12, 9, 15), \"float32\"), Scale: T.Buffer((15,), \"float32\"), compute: T.Buffer((7, 12, 9, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(11340):\n            compute_1 = T.Buffer((11340,), data=compute.data)\n            data_1 = T.Buffer((11340,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 15])", "op_args": [7, 12, 9, 15]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 5; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 5) + i3)] = ((0.000000e+00f < data[((i0_i1_fused_i2_fused * 5) + i3)]) ? data[((i0_i1_fused_i2_fused * 5) + i3)] : (data[((i0_i1_fused_i2_fused * 5) + i3)] * Scale[i3]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * Scale[((((int)blockIdx.x) + ((int)threadIdx.x)) % 5)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 8, 5, 5), \"float32\"), Scale: T.Buffer((5,), \"float32\"), compute: T.Buffer((12, 8, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            for i3 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 5 + i3\n                compute_1 = T.Buffer((2400,), data=compute.data)\n                data_1 = T.Buffer((2400,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [12, 8, 5, 5]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 7 : ((ax0 == 2) ? 2 : ((ax0 == 1) ? 17 : 11)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 7 : ((((int)threadIdx.x) == 2) ? 2 : ((((int)threadIdx.x) == 1) ? 17 : 11)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 17, 2, 7), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 7, T.if_then_else(ax0 == 2, 2, T.if_then_else(ax0 == 1, 17, 11)))", "op_args": [11, 17, 2, 7]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 98; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      for (int32_t i3 = 0; i3 < 11; ++i3) {\n        compute[(((i0_i1_fused * 33) + (i2 * 11)) + i3)] = ((0.000000e+00f < data[(((i0_i1_fused * 33) + (i2 * 11)) + i3)]) ? data[(((i0_i1_fused * 33) + (i2 * 11)) + i3)] : (data[(((i0_i1_fused * 33) + (i2 * 11)) + i3)] * Scale[i3]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(33) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 33) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) % 11)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 14, 3, 11), \"float32\"), Scale: T.Buffer((11,), \"float32\"), compute: T.Buffer((7, 14, 3, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(98):\n            for i2, i3 in T.grid(3, 11):\n                cse_var_1: T.int32 = i0_i1_fused * 33 + i2 * 11 + i3\n                compute_1 = T.Buffer((3234,), data=compute.data)\n                data_1 = T.Buffer((3234,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [7, 14, 3, 11]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      for (int32_t i3 = 0; i3 < 20; ++i3) {\n        compute[(((i0_i1_fused * 380) + (i2 * 20)) + i3)] = ((0.000000e+00f < data[(((i0_i1_fused * 380) + (i2 * 20)) + i3)]) ? data[(((i0_i1_fused * 380) + (i2 * 20)) + i3)] : (data[(((i0_i1_fused * 380) + (i2 * 20)) + i3)] * Scale[i3]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * Scale[(((int)threadIdx.x) % 20)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 8, 19, 20), \"float32\"), Scale: T.Buffer((20,), \"float32\"), compute: T.Buffer((16, 8, 19, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(128):\n            for i2, i3 in T.grid(19, 20):\n                cse_var_1: T.int32 = i0_i1_fused * 380 + i2 * 20 + i3\n                compute_1 = T.Buffer((48640,), data=compute.data)\n                data_1 = T.Buffer((48640,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [16, 8, 19, 20]}{"op_name": "shape", "c_code": "void default_function_kernel(int32_t* T_shape) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    T_shape[ax0] = ((ax0 == 3) ? 20 : ((ax0 == 2) ? 17 : ((ax0 == 1) ? 6 : 20)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(int* __restrict__ T_shape) {\n  T_shape[((int)threadIdx.x)] = ((((int)threadIdx.x) == 3) ? 20 : ((((int)threadIdx.x) == 2) ? 17 : ((((int)threadIdx.x) == 1) ? 6 : 20)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 6, 17, 20), \"float32\"), T_shape: T.Buffer((4,), \"int32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_shape[ax0] = T.if_then_else(ax0 == 3, 20, T.if_then_else(ax0 == 2, 17, T.if_then_else(ax0 == 1, 6, 20)))", "op_args": [20, 6, 17, 20]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 361; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 6) + i3)] = ((0.000000e+00f < data[((i0_i1_fused_i2_fused * 6) + i3)]) ? data[((i0_i1_fused_i2_fused * 6) + i3)] : (data[((i0_i1_fused_i2_fused * 6) + i3)] * Scale[i3]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 10) + (((int)threadIdx.x) >> 1)) < 1083) {\n    compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 6)]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 19, 1, 6), \"float32\"), Scale: T.Buffer((6,), \"float32\"), compute: T.Buffer((19, 19, 1, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(361):\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 6 + i3\n                compute_1 = T.Buffer((2166,), data=compute.data)\n                data_1 = T.Buffer((2166,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [19, 19, 1, 6]}{"op_name": "sigmoid", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i1 * 7) + i2)] = (1.000000e+00f / (1.000000e+00f + expf((0.000000e+00f - data[((i1 * 7) + i2)]))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 35) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (1.000000e+00f / (1.000000e+00f + __expf((0.000000e+00f - data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]))));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 20, 7, 1), \"float32\"), compute: T.Buffer((1, 20, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2 in T.grid(20, 7):\n            cse_var_1: T.int32 = i1 * 7 + i2\n            compute_1 = T.Buffer((140,), data=compute.data)\n            data_1 = T.Buffer((140,), data=data.data)\n            compute_1[cse_var_1] = T.sigmoid(data_1[cse_var_1])", "op_args": [1, 20, 7, 1]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 845; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 12; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 12) + i3)] = ((0.000000e+00f < data[((i0_i1_fused_i2_fused * 12) + i3)]) ? data[((i0_i1_fused_i2_fused * 12) + i3)] : (data[((i0_i1_fused_i2_fused * 12) + i3)] * Scale[i3]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 12)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 13, 13, 12), \"float32\"), Scale: T.Buffer((12,), \"float32\"), compute: T.Buffer((5, 13, 13, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(845):\n            for i3 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 12 + i3\n                compute_1 = T.Buffer((10140,), data=compute.data)\n                data_1 = T.Buffer((10140,), data=data.data)\n                compute_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], data_1[cse_var_1], data_1[cse_var_1] * Scale[i3])", "op_args": [5, 13, 13, 12]}{"op_name": "prelu", "c_code": "void default_function_kernel(float* Scale, float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1020; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = ((0.000000e+00f < data[i0_i1_fused_i2_fused_i3_fused]) ? data[i0_i1_fused_i2_fused_i3_fused] : (data[i0_i1_fused_i2_fused_i3_fused] * Scale[(i0_i1_fused_i2_fused_i3_fused % 17)]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ Scale, float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]) ? data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] : (data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] * Scale[(((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) % 17)]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 2, 15, 17), \"float32\"), Scale: T.Buffer((17,), \"float32\"), compute: T.Buffer((2, 2, 15, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1020):\n            compute_1 = T.Buffer((1020,), data=compute.data)\n            data_1 = T.Buffer((1020,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.if_then_else(T.float32(0) < data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused], data_1[i0_i1_fused_i2_fused_i3_fused] * Scale[i0_i1_fused_i2_fused_i3_fused % 17])", "op_args": [2, 2, 15, 17]}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 14; ++ax3) {\n          T_sign[((((ax0 * 280) + (ax1 * 140)) + (ax2 * 14)) + ax3)] = ((0.000000e+00f < data[((((ax0 * 280) + (ax1 * 140)) + (ax2 * 14)) + ax3)]) ? 1.000000e+00f : ((data[((((ax0 * 280) + (ax1 * 140)) + (ax2 * 14)) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 2, 10, 14), \"float32\"), T_sign: T.Buffer((20, 2, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(20):\n            for ax1, ax2, ax3 in T.grid(2, 10, 14):\n                cse_var_1: T.int32 = ax0 * 280 + ax1 * 140 + ax2 * 14 + ax3\n                T_sign_1 = T.Buffer((5600,), data=T_sign.data)\n                data_1 = T.Buffer((5600,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [20, 2, 10, 14]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused < 16150; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n      for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n        ScaleShift[(((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused / 3230) * 12920) + (b_inner * 6460)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 10) * 646)) + (i_inner * 323)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 170) / 10) * 19)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 3230) / 170))] = ((data[(((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused / 3230) * 12920) + (b_inner * 6460)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 10) * 646)) + (i_inner * 323)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 170) / 10) * 19)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 3230) / 170))] * Scale[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 10) / 5) * 19) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 3230) / 170))]) + Shift[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 10) / 5) * 19) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 3230) / 170))]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) % 646) / 323) * 19) + (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 19))]) + Shift[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) % 646) / 323) * 19) + (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 19))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 2, 10, 17, 19), \"float32\"), Scale: T.Buffer((2, 19), \"float32\"), Shift: T.Buffer((2, 19), \"float32\"), ScaleShift: T.Buffer((10, 2, 10, 17, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused in T.parallel(16150):\n            for b_inner, i_inner in T.grid(2, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 10\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 3230 // 170\n                cse_var_2: T.int32 = cse_var_4 // 5 * 19 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused // 3230 * 12920 + b_inner * 6460 + cse_var_4 * 646 + i_inner * 323 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused_j_outer_inner_fused_cb_outer_inner_fused % 170 // 10 * 19 + cse_var_3\n                ScaleShift_1 = T.Buffer((64600,), data=ScaleShift.data)\n                data_1 = T.Buffer((64600,), data=data.data)\n                Scale_1 = T.Buffer((38,), data=Scale.data)\n                Shift_1 = T.Buffer((38,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [10, 19, 10, 17]}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 896; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 13; ++ax3) {\n      T_sign[((ax0_ax1_fused_ax2_fused * 13) + ax3)] = ((0.000000e+00f < data[((ax0_ax1_fused_ax2_fused * 13) + ax3)]) ? 1.000000e+00f : ((data[((ax0_ax1_fused_ax2_fused * 13) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(52) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 52) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 16, 14, 13), \"float32\"), T_sign: T.Buffer((4, 16, 14, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(896):\n            for ax3 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 13 + ax3\n                T_sign_1 = T.Buffer((11648,), data=T_sign.data)\n                data_1 = T.Buffer((11648,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [4, 16, 14, 13]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused < 680; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 7; ++i_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 3; ++j_outer_inner) {\n          ScaleShift[((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 170) * 7140) + (b_outer_inner * 3570)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 170) / 85) * 1785)) + (i_outer_inner * 255)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 85) / 17) * 51)) + (j_outer_inner * 17)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))] = ((data[((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 170) * 7140) + (b_outer_inner * 3570)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 170) / 85) * 1785)) + (i_outer_inner * 255)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 85) / 17) * 51)) + (j_outer_inner * 17)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))] * Scale[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 170) / 85) * 17) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))]) + Shift[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 170) / 85) * 17) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 1785) {\n    ScaleShift[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 3570) / 1785) * 17) + (((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) % 17))]) + Shift[((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 3570) / 1785) * 17) + (((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) % 17))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 2, 7, 15, 17), \"float32\"), Scale: T.Buffer((2, 17), \"float32\"), Shift: T.Buffer((2, 17), \"float32\"), ScaleShift: T.Buffer((8, 2, 7, 15, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused in T.parallel(680):\n            for b_outer_inner, i_outer_inner, j_outer_inner in T.grid(2, 7, 3):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 170 // 85\n                cse_var_2: T.int32 = cse_var_3 * 17 + cse_var_4\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused // 170 * 7140 + b_outer_inner * 3570 + cse_var_3 * 1785 + i_outer_inner * 255 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 85 // 17 * 51 + j_outer_inner * 17 + cse_var_4\n                ScaleShift_1 = T.Buffer((28560,), data=ScaleShift.data)\n                data_1 = T.Buffer((28560,), data=data.data)\n                Scale_1 = T.Buffer((34,), data=Scale.data)\n                Shift_1 = T.Buffer((34,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [8, 17, 7, 15]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused < 234; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused) {\n    for (int32_t cc_outer_inner = 0; cc_outer_inner < 2; ++cc_outer_inner) {\n      for (int32_t cb_outer_inner = 0; cb_outer_inner < 15; ++cb_outer_inner) {\n        for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n          ScaleShift[(((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 78) / 6) * 1080) + (cc_outer_inner * 540)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 6) >> 1) * 180)) + (i_inner * 90)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 78) * 30)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 1) * 15)) + cb_outer_inner)] = ((data[(((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 78) / 6) * 1080) + (cc_outer_inner * 540)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 6) >> 1) * 180)) + (i_inner * 90)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 78) * 30)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused & 1) * 15)) + cb_outer_inner)] * Scale[((cc_outer_inner * 15) + cb_outer_inner)]) + Shift[((cc_outer_inner * 15) + cb_outer_inner)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 18) / 9) * 15) + (((int)threadIdx.x) % 15))]) + Shift[((((((int)blockIdx.x) % 18) / 9) * 15) + (((int)threadIdx.x) % 15))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 2, 6, 6, 15), \"float32\"), Scale: T.Buffer((2, 15), \"float32\"), Shift: T.Buffer((2, 15), \"float32\"), ScaleShift: T.Buffer((13, 2, 6, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused in T.parallel(234):\n            for cc_outer_inner, cb_outer_inner, i_inner in T.grid(2, 15, 2):\n                cse_var_2: T.int32 = cc_outer_inner * 15 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 78 // 6 * 1080 + cc_outer_inner * 540 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 6 // 2 * 180 + i_inner * 90 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused // 78 * 30 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 2 * 15 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((14040,), data=ScaleShift.data)\n                data_1 = T.Buffer((14040,), data=data.data)\n                Scale_1 = T.Buffer((30,), data=Scale.data)\n                Shift_1 = T.Buffer((30,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [13, 15, 6, 6]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused < 112; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 7; ++b_outer_inner) {\n      for (int32_t cc_outer_inner = 0; cc_outer_inner < 2; ++cc_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 4; ++i_outer_inner) {\n          for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n            ScaleShift[((((((b_outer_inner * 1792) + (cc_outer_inner * 896)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 56) * 448)) + (i_outer_inner * 112)) + (i_inner * 56)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 56))] = ((data[((((((b_outer_inner * 1792) + (cc_outer_inner * 896)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 56) * 448)) + (i_outer_inner * 112)) + (i_inner * 56)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 56))] * Scale[((cc_outer_inner * 8) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused & 7))]) + Shift[((cc_outer_inner * 8) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused & 7))]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) & 63) >> 5) * 8) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7))]) + Shift[((((((int)blockIdx.x) & 63) >> 5) * 8) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 16, 7, 8), \"float32\"), Scale: T.Buffer((2, 8), \"float32\"), Shift: T.Buffer((2, 8), \"float32\"), ScaleShift: T.Buffer((7, 2, 16, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused in T.parallel(112):\n            for b_outer_inner, cc_outer_inner, i_outer_inner, i_inner in T.grid(7, 2, 4, 2):\n                cse_var_2: T.int32 = cc_outer_inner * 8 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 8\n                cse_var_1: T.int32 = b_outer_inner * 1792 + cc_outer_inner * 896 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused // 56 * 448 + i_outer_inner * 112 + i_inner * 56 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 56\n                ScaleShift_1 = T.Buffer((12544,), data=ScaleShift.data)\n                data_1 = T.Buffer((12544,), data=data.data)\n                Scale_1 = T.Buffer((16,), data=Scale.data)\n                Shift_1 = T.Buffer((16,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [7, 8, 16, 7]}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n        T_sign[(((ax0_ax1_fused * 323) + (ax2 * 17)) + ax3)] = ((0.000000e+00f < data[(((ax0_ax1_fused * 323) + (ax2 * 17)) + ax3)]) ? 1.000000e+00f : ((data[(((ax0_ax1_fused * 323) + (ax2 * 17)) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 969) {\n    T_sign[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 3, 19, 17), \"float32\"), T_sign: T.Buffer((4, 3, 19, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2, ax3 in T.grid(19, 17):\n                cse_var_1: T.int32 = ax0_ax1_fused * 323 + ax2 * 17 + ax3\n                T_sign_1 = T.Buffer((3876,), data=T_sign.data)\n                data_1 = T.Buffer((3876,), data=data.data)\n                T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [4, 3, 19, 17]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused < 399; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t cc_inner = 0; cc_inner < 2; ++cc_inner) {\n      for (int32_t i_inner = 0; i_inner < 19; ++i_inner) {\n        for (int32_t j_inner = 0; j_inner < 3; ++j_inner) {\n          ScaleShift[(((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused / 21) * 2394) + (cc_inner * 1197)) + (i_inner * 63)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 21) / 7) * 21)) + (j_inner * 7)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 7))] = ((data[(((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused / 21) * 2394) + (cc_inner * 1197)) + (i_inner * 63)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 21) / 7) * 21)) + (j_inner * 7)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 7))] * Scale[((cc_inner * 7) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 7))]) + Shift[((cc_inner * 7) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 7))]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(63) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 63) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 38) / 19) * 7) + (((int)threadIdx.x) % 7))]) + Shift[((((((int)blockIdx.x) % 38) / 19) * 7) + (((int)threadIdx.x) % 7))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 2, 19, 9, 7), \"float32\"), Scale: T.Buffer((2, 7), \"float32\"), Shift: T.Buffer((2, 7), \"float32\"), ScaleShift: T.Buffer((19, 2, 19, 9, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused in T.parallel(399):\n            for cc_inner, i_inner, j_inner in T.grid(2, 19, 3):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 7\n                cse_var_2: T.int32 = cc_inner * 7 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused // 21 * 2394 + cc_inner * 1197 + i_inner * 63 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 21 // 7 * 21 + j_inner * 7 + cse_var_3\n                ScaleShift_1 = T.Buffer((45486,), data=ScaleShift.data)\n                data_1 = T.Buffer((45486,), data=data.data)\n                Scale_1 = T.Buffer((14,), data=Scale.data)\n                Shift_1 = T.Buffer((14,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [19, 7, 19, 9]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused < 56; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused) {\n    for (int32_t b_inner = 0; b_inner < 11; ++b_inner) {\n      for (int32_t j_inner = 0; j_inner < 4; ++j_inner) {\n        ScaleShift[((((b_inner * 224) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused & 3) * 56)) + (j_inner * 14)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused >> 2))] = ((data[((((b_inner * 224) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused & 3) * 56)) + (j_inner * 14)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused >> 2))] * Scale[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused & 3) >> 1) * 14) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused >> 2))]) + Shift[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused & 3) >> 1) * 14) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused >> 2))]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(11) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 224) / 112) * 14) + (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 14))]) + Shift[((((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 224) / 112) * 14) + (((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 14))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 2, 2, 4, 14), \"float32\"), Scale: T.Buffer((2, 14), \"float32\"), Shift: T.Buffer((2, 14), \"float32\"), ScaleShift: T.Buffer((11, 2, 2, 4, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused in T.parallel(56):\n            for b_inner, j_inner in T.grid(11, 4):\n                cse_var_4: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused % 4\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused_i_outer_inner_fused // 4\n                cse_var_2: T.int32 = cse_var_4 // 2 * 14 + cse_var_3\n                cse_var_1: T.int32 = b_inner * 224 + cse_var_4 * 56 + j_inner * 14 + cse_var_3\n                ScaleShift_1 = T.Buffer((2464,), data=ScaleShift.data)\n                data_1 = T.Buffer((2464,), data=data.data)\n                Scale_1 = T.Buffer((28,), data=Scale.data)\n                Shift_1 = T.Buffer((28,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [11, 14, 2, 4]}{"op_name": "sign", "c_code": "void default_function_kernel(float* T_sign, float* data) {\n  for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 6; ++ax3) {\n        T_sign[(((ax1 * 102) + (ax2 * 6)) + ax3)] = ((0.000000e+00f < data[(((ax1 * 102) + (ax2 * 6)) + ax3)]) ? 1.000000e+00f : ((data[(((ax1 * 102) + (ax2 * 6)) + ax3)] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_sign, float* __restrict__ data) {\n  T_sign[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((0.000000e+00f < data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) ? 1.000000e+00f : ((data[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] < 0.000000e+00f) ? -1.000000e+00f : 0.000000e+00f));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 10, 17, 6), \"float32\"), T_sign: T.Buffer((1, 10, 17, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1, ax2, ax3 in T.grid(10, 17, 6):\n            cse_var_1: T.int32 = ax1 * 102 + ax2 * 6 + ax3\n            T_sign_1 = T.Buffer((1020,), data=T_sign.data)\n            data_1 = T.Buffer((1020,), data=data.data)\n            T_sign_1[cse_var_1] = T.if_then_else(T.float32(0) < data_1[cse_var_1], T.float32(1), T.Select(data_1[cse_var_1] < T.float32(0), T.float32(-1), T.float32(0)))", "op_args": [1, 10, 17, 6]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused < 1444; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n      for (int32_t cc_inner = 0; cc_inner < 2; ++cc_inner) {\n        for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n          ScaleShift[(((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 19) * 608) + (cc_inner * 304)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 361) * 76)) + (i_outer_inner * 38)) + (i_inner * 19)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 361) / 19))] = ((data[(((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 19) * 608) + (cc_inner * 304)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused / 361) * 76)) + (i_outer_inner * 38)) + (i_inner * 19)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 361) / 19))] * Scale[((cc_inner * 19) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 361) / 19))]) + Shift[((cc_inner * 19) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 361) / 19))]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) % 19) * 2) + (((int)threadIdx.x) >> 4)) / 19) * 19) + (((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) % 19))]) + Shift[((((((((int)blockIdx.x) % 19) * 2) + (((int)threadIdx.x) >> 4)) / 19) * 19) + (((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) % 19))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 2, 16, 1, 19), \"float32\"), Scale: T.Buffer((2, 19), \"float32\"), Shift: T.Buffer((2, 19), \"float32\"), ScaleShift: T.Buffer((19, 2, 16, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused in T.parallel(1444):\n            for i_outer_inner, cc_inner, i_inner in T.grid(2, 2, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 361 // 19\n                cse_var_2: T.int32 = cc_inner * 19 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused % 19 * 608 + cc_inner * 304 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused // 361 * 76 + i_outer_inner * 38 + i_inner * 19 + cse_var_3\n                ScaleShift_1 = T.Buffer((11552,), data=ScaleShift.data)\n                data_1 = T.Buffer((11552,), data=data.data)\n                Scale_1 = T.Buffer((38,), data=Scale.data)\n                Shift_1 = T.Buffer((38,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [19, 19, 16, 1]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 13; ++j_outer_outer_inner) {\n    for (int32_t cb_outer_outer_inner = 0; cb_outer_outer_inner < 8; ++cb_outer_outer_inner) {\n      for (int32_t cc_outer_inner = 0; cc_outer_inner < 2; ++cc_outer_inner) {\n        ScaleShift[(((cc_outer_inner * 104) + (j_outer_outer_inner * 8)) + cb_outer_outer_inner)] = ((data[(((cc_outer_inner * 104) + (j_outer_outer_inner * 8)) + cb_outer_outer_inner)] * Scale[((cc_outer_inner * 8) + cb_outer_outer_inner)]) + Shift[((cc_outer_inner * 8) + cb_outer_outer_inner)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) / 26) * 8) + ((((int)blockIdx.x) & 1) * 4)) + ((int)threadIdx.x))]) + Shift[((((((int)blockIdx.x) / 26) * 8) + ((((int)blockIdx.x) & 1) * 4)) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 2, 1, 13, 8), \"float32\"), Scale: T.Buffer((2, 8), \"float32\"), Shift: T.Buffer((2, 8), \"float32\"), ScaleShift: T.Buffer((1, 2, 1, 13, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for j_outer_outer_inner, cb_outer_outer_inner, cc_outer_inner in T.grid(13, 8, 2):\n            cse_var_2: T.int32 = cc_outer_inner * 8 + cb_outer_outer_inner\n            cse_var_1: T.int32 = cc_outer_inner * 104 + j_outer_outer_inner * 8 + cb_outer_outer_inner\n            ScaleShift_1 = T.Buffer((208,), data=ScaleShift.data)\n            data_1 = T.Buffer((208,), data=data.data)\n            Scale_1 = T.Buffer((16,), data=Scale.data)\n            Shift_1 = T.Buffer((16,), data=Shift.data)\n            ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [1, 8, 1, 13]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused < 1632; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused) {\n    for (int32_t cc_outer_inner = 0; cc_outer_inner < 2; ++cc_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n        ScaleShift[((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 544) * 2176) + (cc_outer_inner * 1088)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 544) / 136) * 272)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 34) / 17) * 136)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 136) / 34) * 34)) + (j_outer_inner * 17)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))] = ((data[((((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 544) * 2176) + (cc_outer_inner * 1088)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 544) / 136) * 272)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 34) / 17) * 136)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 136) / 34) * 34)) + (j_outer_inner * 17)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))] * Scale[((cc_outer_inner * 17) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))]) + Shift[((cc_outer_inner * 17) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17))]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) % 136) / 68) * 17) + (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 17))]) + Shift[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) >> 4)) % 136) / 68) * 17) + (((((int)blockIdx.x) * 14) + ((int)threadIdx.x)) % 17))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 2, 8, 8, 17), \"float32\"), Scale: T.Buffer((2, 17), \"float32\"), Shift: T.Buffer((2, 17), \"float32\"), ScaleShift: T.Buffer((3, 2, 8, 8, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused in T.parallel(1632):\n            for cc_outer_inner, j_outer_inner in T.grid(2, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 17\n                cse_var_2: T.int32 = cc_outer_inner * 17 + cse_var_3\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused // 544 * 2176 + cc_outer_inner * 1088 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 544 // 136 * 272 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 34 // 17 * 136 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 136 // 34 * 34 + j_outer_inner * 17 + cse_var_3\n                ScaleShift_1 = T.Buffer((6528,), data=ScaleShift.data)\n                data_1 = T.Buffer((6528,), data=data.data)\n                Scale_1 = T.Buffer((34,), data=Scale.data)\n                Shift_1 = T.Buffer((34,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [3, 17, 8, 8]}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 15; ++i3) {\n        compute[(((i1 * 90) + (i2 * 15)) + i3)] = sinhf(data[(((i1 * 90) + (i2 * 15)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 12, 6, 15), \"float32\"), compute: T.Buffer((1, 12, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(12, 6, 15):\n            cse_var_1: T.int32 = i1 * 90 + i2 * 15 + i3\n            compute_1 = T.Buffer((1080,), data=compute.data)\n            data_1 = T.Buffer((1080,), data=data.data)\n            compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])", "op_args": [1, 12, 6, 15]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused < 8; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused) {\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 17; ++i_outer_outer_inner) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 11; ++b_outer_inner) {\n        for (int32_t cb_outer_inner = 0; cb_outer_inner < 2; ++cb_outer_inner) {\n          for (int32_t cc_inner = 0; cc_inner < 2; ++cc_inner) {\n            ScaleShift[(((((b_outer_inner * 544) + (cc_inner * 272)) + (i_outer_outer_inner * 16)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused * 2)) + cb_outer_inner)] = ((data[(((((b_outer_inner * 544) + (cc_inner * 272)) + (i_outer_outer_inner * 16)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused * 2)) + cb_outer_inner)] * Scale[(((cc_inner * 16) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused * 2)) + cb_outer_inner)]) + Shift[(((cc_inner * 16) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused * 2)) + cb_outer_inner)]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(44) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 44) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 11) + (((int)threadIdx.x) >> 2)) % 136) / 68) * 16) + (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) & 15))]) + Shift[((((((((int)blockIdx.x) * 11) + (((int)threadIdx.x) >> 2)) % 136) / 68) * 16) + (((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) & 15))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 2, 17, 1, 16), \"float32\"), Scale: T.Buffer((2, 16), \"float32\"), Shift: T.Buffer((2, 16), \"float32\"), ScaleShift: T.Buffer((11, 2, 17, 1, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused in T.parallel(8):\n            for i_outer_outer_inner, b_outer_inner, cb_outer_inner, cc_inner in T.grid(17, 11, 2, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused * 2\n                cse_var_2: T.int32 = cc_inner * 16 + cse_var_3 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_inner * 544 + cc_inner * 272 + i_outer_outer_inner * 16 + cse_var_3 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((5984,), data=ScaleShift.data)\n                data_1 = T.Buffer((5984,), data=data.data)\n                Scale_1 = T.Buffer((32,), data=Scale.data)\n                Shift_1 = T.Buffer((32,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [11, 16, 17, 1]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  for (int32_t j_outer_outer_outer = 0; j_outer_outer_outer < 2; ++j_outer_outer_outer) {\n    for (int32_t cb_outer_outer_outer = 0; cb_outer_outer_outer < 2; ++cb_outer_outer_outer) {\n      for (int32_t cc_outer_outer_inner = 0; cc_outer_outer_inner < 2; ++cc_outer_outer_inner) {\n        for (int32_t cb_outer_outer_inner = 0; cb_outer_outer_inner < 2; ++cb_outer_outer_inner) {\n          for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n            for (int32_t b_inner = 0; b_inner < 7; ++b_inner) {\n              for (int32_t i_inner = 0; i_inner < 3; ++i_inner) {\n                ScaleShift[(((((((b_inner * 96) + (cc_outer_outer_inner * 48)) + (i_inner * 16)) + (j_outer_outer_outer * 8)) + (j_outer_inner * 4)) + (cb_outer_outer_outer * 2)) + cb_outer_outer_inner)] = ((data[(((((((b_inner * 96) + (cc_outer_outer_inner * 48)) + (i_inner * 16)) + (j_outer_outer_outer * 8)) + (j_outer_inner * 4)) + (cb_outer_outer_outer * 2)) + cb_outer_outer_inner)] * Scale[(((cc_outer_outer_inner * 4) + (cb_outer_outer_outer * 2)) + cb_outer_outer_inner)]) + Shift[(((cc_outer_outer_inner * 4) + (cb_outer_outer_outer * 2)) + cb_outer_outer_inner)]);\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 6) / 3) * 4) + (((int)threadIdx.x) & 3))]) + Shift[((((((int)blockIdx.x) % 6) / 3) * 4) + (((int)threadIdx.x) & 3))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 3, 4, 4), \"float32\"), Scale: T.Buffer((2, 4), \"float32\"), Shift: T.Buffer((2, 4), \"float32\"), ScaleShift: T.Buffer((7, 2, 3, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for j_outer_outer_outer, cb_outer_outer_outer, cc_outer_outer_inner, cb_outer_outer_inner, j_outer_inner, b_inner, i_inner in T.grid(2, 2, 2, 2, 2, 7, 3):\n            cse_var_3: T.int32 = cb_outer_outer_outer * 2\n            cse_var_2: T.int32 = cc_outer_outer_inner * 4 + cse_var_3 + cb_outer_outer_inner\n            cse_var_1: T.int32 = b_inner * 96 + cc_outer_outer_inner * 48 + i_inner * 16 + j_outer_outer_outer * 8 + j_outer_inner * 4 + cse_var_3 + cb_outer_outer_inner\n            ScaleShift_1 = T.Buffer((672,), data=ScaleShift.data)\n            data_1 = T.Buffer((672,), data=data.data)\n            Scale_1 = T.Buffer((8,), data=Scale.data)\n            Shift_1 = T.Buffer((8,), data=Shift.data)\n            ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [7, 4, 3, 4]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused < 600; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n      for (int32_t cb_outer_inner = 0; cb_outer_inner < 3; ++cb_outer_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 120) * 720) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 360)) + (i_outer_inner * 180)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 120) >> 1) * 3)) + cb_outer_inner)] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 120) * 720) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 360)) + (i_outer_inner * 180)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 120) >> 1) * 3)) + cb_outer_inner)] * Scale[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 9) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 6) >> 1) * 3)) + cb_outer_inner)]) + Shift[((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused & 1) * 9) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 6) >> 1) * 3)) + cb_outer_inner)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] * Scale[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) % 72) / 36) * 9) + (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 9))]) + Shift[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 10)) % 72) / 36) * 9) + (((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 9))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 2, 2, 20, 9), \"float32\"), Scale: T.Buffer((2, 9), \"float32\"), Shift: T.Buffer((2, 9), \"float32\"), ScaleShift: T.Buffer((5, 2, 2, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused in T.parallel(600):\n            for i_outer_inner, cb_outer_inner in T.grid(2, 3):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 9 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 6 // 2 * 3 + cb_outer_inner\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 120 * 720 + cse_var_3 * 360 + i_outer_inner * 180 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 120 // 2 * 3 + cb_outer_inner\n                ScaleShift_1 = T.Buffer((3600,), data=ScaleShift.data)\n                data_1 = T.Buffer((3600,), data=data.data)\n                Scale_1 = T.Buffer((18,), data=Scale.data)\n                Shift_1 = T.Buffer((18,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [5, 9, 2, 20]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused < 600; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n      for (int32_t j_inner = 0; j_inner < 2; ++j_inner) {\n        ScaleShift[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 25) * 100) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 5) * 20)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 25) / 5) * 4)) + (j_outer_inner * 2)) + j_inner)] = ((data[((((((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused / 25) * 100) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 5) * 20)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 25) / 5) * 4)) + (j_outer_inner * 2)) + j_inner)] * Scale[((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 50) / 25)]) + Shift[((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 50) / 25)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 50) / 25)]) + Shift[((((((int)blockIdx.x) * 6) + (((int)threadIdx.x) >> 2)) % 50) / 25)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 2, 5, 20, 1), \"float32\"), Scale: T.Buffer((2, 1), \"float32\"), Shift: T.Buffer((2, 1), \"float32\"), ScaleShift: T.Buffer((12, 2, 5, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused in T.parallel(600):\n            for j_outer_inner, j_inner in T.grid(2, 2):\n                cse_var_2: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 50 // 25\n                cse_var_1: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused // 25 * 100 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 5 * 20 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused_b_outer_inner_fused_cc_outer_inner_fused % 25 // 5 * 4 + j_outer_inner * 2 + j_inner\n                ScaleShift_1 = T.Buffer((2400,), data=ScaleShift.data)\n                data_1 = T.Buffer((2400,), data=data.data)\n                Scale_1 = T.Buffer((2,), data=Scale.data)\n                Shift_1 = T.Buffer((2,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [12, 1, 5, 20]}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        for (int32_t i3 = 0; i3 < 15; ++i3) {\n          compute[((((i0 * 600) + (i1 * 150)) + (i2 * 15)) + i3)] = sinhf(data[((((i0 * 600) + (i1 * 150)) + (i2 * 15)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 4, 10, 15), \"float32\"), compute: T.Buffer((10, 4, 10, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(4, 10, 15):\n                cse_var_1: T.int32 = i0 * 600 + i1 * 150 + i2 * 15 + i3\n                compute_1 = T.Buffer((6000,), data=compute.data)\n                data_1 = T.Buffer((6000,), data=data.data)\n                compute_1[cse_var_1] = T.sinh(data_1[cse_var_1])", "op_args": [10, 4, 10, 15]}{"op_name": "scale_shift_nchwc", "c_code": "void default_function_kernel(float* Scale, float* ScaleShift, float* Shift, float* data) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused = 0; b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused < 156; ++b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 7; ++b_outer_inner) {\n      for (int32_t cc_outer_inner = 0; cc_outer_inner < 2; ++cc_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 3; ++i_outer_inner) {\n          for (int32_t j_outer_inner = 0; j_outer_inner < 2; ++j_outer_inner) {\n            ScaleShift[((((((((b_outer_inner * 1872) + (cc_outer_inner * 936)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 78) / 26) * 312)) + (i_outer_inner * 104)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 78) * 52)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 26) / 13) * 26)) + (j_outer_inner * 13)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 13))] = ((data[((((((((b_outer_inner * 1872) + (cc_outer_inner * 936)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 78) / 26) * 312)) + (i_outer_inner * 104)) + ((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused / 78) * 52)) + (((b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 26) / 13) * 26)) + (j_outer_inner * 13)) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 13))] * Scale[((cc_outer_inner * 13) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 13))]) + Shift[((cc_outer_inner * 13) + (b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 13))]);\n          }\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ Scale, float* __restrict__ ScaleShift, float* __restrict__ Shift, float* __restrict__ data) {\n  ScaleShift[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * Scale[((((((int)blockIdx.x) % 52) / 26) * 13) + (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 13))]) + Shift[((((((int)blockIdx.x) % 52) / 26) * 13) + (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 13))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 2, 9, 8, 13), \"float32\"), Scale: T.Buffer((2, 13), \"float32\"), Shift: T.Buffer((2, 13), \"float32\"), ScaleShift: T.Buffer((7, 2, 9, 8, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused in T.parallel(156):\n            for b_outer_inner, cc_outer_inner, i_outer_inner, j_outer_inner in T.grid(7, 2, 3, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 13\n                cse_var_2: T.int32 = cc_outer_inner * 13 + cse_var_3\n                cse_var_1: T.int32 = b_outer_inner * 1872 + cc_outer_inner * 936 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 78 // 26 * 312 + i_outer_inner * 104 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused // 78 * 52 + b_outer_outer_outer_cc_outer_outer_outer_fused_i_outer_outer_outer_fused_j_outer_outer_outer_fused_cb_outer_outer_outer_fused_b_outer_outer_inner_fused_cc_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused_cb_outer_outer_inner_fused % 26 // 13 * 26 + j_outer_inner * 13 + cse_var_3\n                ScaleShift_1 = T.Buffer((13104,), data=ScaleShift.data)\n                data_1 = T.Buffer((13104,), data=data.data)\n                Scale_1 = T.Buffer((26,), data=Scale.data)\n                Shift_1 = T.Buffer((26,), data=Shift.data)\n                ScaleShift_1[cse_var_1] = data_1[cse_var_1] * Scale_1[cse_var_2] + Shift_1[cse_var_2]", "op_args": [7, 13, 9, 8]}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 3888; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 243) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 2, 18, 6), \"float32\"), compute: T.Buffer((18, 2, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(3888):\n            compute_1 = T.Buffer((3888,), data=compute.data)\n            data_1 = T.Buffer((3888,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [18, 2, 18, 6]}{"op_name": "sinh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6720; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sinhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = sinhf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 7, 8, 10), \"float32\"), compute: T.Buffer((12, 7, 8, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6720):\n            compute_1 = T.Buffer((6720,), data=compute.data)\n            data_1 = T.Buffer((6720,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sinh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [12, 7, 8, 10]}{"op_name": "softmax", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  float T_softmax_exp[342];\n  float T_softmax_maxelem[1];\n  float T_softmax_expsum[1];\n  for (int32_t i1 = 0; i1 < 17; ++i1) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 18; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[(((i1 * 342) + (i2 * 18)) + k)]);\n      }\n      for (int32_t i3 = 0; i3 < 18; ++i3) {\n        T_softmax_exp[((i2 * 18) + i3)] = expf((data[(((i1 * 342) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]));\n      }\n    }\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      for (int32_t i3_1 = 0; i3_1 < 18; ++i3_1) {\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 18; ++k_1) {\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + T_softmax_exp[((i2_1 * 18) + k_1)]);\n        }\n        T_softmax_norm[(((i1 * 342) + (i2_1 * 18)) + i3_1)] = (T_softmax_exp[((i2_1 * 18) + i3_1)] / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 323) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 323) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 323) {\n    T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 323) {\n      T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __expf((data[(((((int)blockIdx.x) * 72) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 2907) {\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 17, 19, 18), \"float32\"), T_softmax_norm: T.Buffer((1, 17, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_softmax_exp = T.allocate([342], \"float32\", \"global\")\n        T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n        T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n        for i1 in range(17):\n            T_softmax_exp_1 = T.Buffer((342,), data=T_softmax_exp)\n            for i2 in range(19):\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((5814,), data=data.data)\n                for k in range(18):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i1 * 342 + i2 * 18 + k])\n                for i3 in range(18):\n                    cse_var_1: T.int32 = i2 * 18\n                    T_softmax_exp_1[cse_var_1 + i3] = T.exp(data_1[i1 * 342 + cse_var_1 + i3] - T_softmax_maxelem_1[0])\n            for i2, i3 in T.grid(19, 18):\n                cse_var_2: T.int32 = i2 * 18\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(18):\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T_softmax_exp_1[cse_var_2 + k]\n                T_softmax_norm_1 = T.Buffer((5814,), data=T_softmax_norm.data)\n                T_softmax_norm_1[i1 * 342 + cse_var_2 + i3] = T_softmax_exp_1[cse_var_2 + i3] / T_softmax_expsum_1[0]", "op_args": [1, 17, 19, 18]}{"op_name": "sqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 880; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 14; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 14) + i3)] = sqrtf(data[((i0_i1_fused_i2_fused * 14) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 11, 10, 14), \"float32\"), compute: T.Buffer((8, 11, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(880):\n            for i3 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 14 + i3\n                compute_1 = T.Buffer((12320,), data=compute.data)\n                data_1 = T.Buffer((12320,), data=data.data)\n                compute_1[cse_var_1] = T.sqrt(data_1[cse_var_1])", "op_args": [8, 11, 10, 14]}{"op_name": "sqrt", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 5824; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = sqrtf(data[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 16, 2, 14), \"float32\"), compute: T.Buffer((13, 16, 2, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(5824):\n            compute_1 = T.Buffer((5824,), data=compute.data)\n            data_1 = T.Buffer((5824,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [13, 16, 2, 14]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    float T_softmax_maxelem[2];\n    float T_softmax_expsum[2];\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        T_softmax_maxelem[i2] = -3.402823e+38f;\n        for (int32_t k = 0; k < 12; ++k) {\n          T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[((((i0 * 288) + (i1 * 24)) + (i2 * 12)) + k)]);\n        }\n      }\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        T_softmax_expsum[i2_1] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 12; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[i2_1] = (T_softmax_expsum[i2_1] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 288) + (i1 * 24)) + (i2_1 * 12)) + k_1)] - T_softmax_maxelem[i2_1])));\n        }\n      }\n      for (int32_t i2_2 = 0; i2_2 < 2; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 12; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 288) + (i1 * 24)) + (i2_2 * 12)) + i3_s)] - T_softmax_maxelem[i2_2])) / T_softmax_expsum[i2_2]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) / 3)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 12; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 24) + (((int)threadIdx.x) * 12)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 12; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 48) + (((int)threadIdx.x) * 12)) + k)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 12, 2, 12), \"float32\"), T_softmax_norm: T.Buffer((13, 12, 2, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            T_softmax_maxelem = T.allocate([2], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([2], \"float32\", \"global\")\n            for i1 in range(12):\n                T_softmax_maxelem_1 = T.Buffer((2,), data=T_softmax_maxelem, align=8)\n                data_1 = T.Buffer((3744,), data=data.data)\n                for i2 in range(2):\n                    T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                    for k in range(12):\n                        T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0 * 288 + i1 * 24 + i2 * 12 + k])\n                T_softmax_expsum_1 = T.Buffer((2,), data=T_softmax_expsum, align=8)\n                for i2 in range(2):\n                    T_softmax_expsum_1[i2] = T.float32(0)\n                    for k in range(12):\n                        cse_var_1: T.int32 = i0 * 288 + i1 * 24 + i2 * 12 + k\n                        T_softmax_expsum_1[i2] = T_softmax_expsum_1[i2] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2])\n                for i2, i3_s in T.grid(2, 12):\n                    cse_var_2: T.int32 = i0 * 288 + i1 * 24 + i2 * 12 + i3_s\n                    T_softmax_norm_1 = T.Buffer((3744,), data=T_softmax_norm.data)\n                    T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[i2]", "op_args": [13, 12, 2, 12]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4900; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 5, 7, 20), \"float32\"), compute: T.Buffer((7, 5, 7, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4900):\n            compute_1 = T.Buffer((4900,), data=compute.data)\n            data_1 = T.Buffer((4900,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tan(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [7, 5, 7, 20]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 39; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      for (int32_t i3 = 0; i3 < 7; ++i3) {\n        compute[(((i0_i1_fused * 42) + (i2 * 7)) + i3)] = tanf(data[(((i0_i1_fused * 42) + (i2 * 7)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 13, 6, 7), \"float32\"), compute: T.Buffer((3, 13, 6, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(39):\n            for i2, i3 in T.grid(6, 7):\n                cse_var_1: T.int32 = i0_i1_fused * 42 + i2 * 7 + i3\n                compute_1 = T.Buffer((1638,), data=compute.data)\n                data_1 = T.Buffer((1638,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [3, 13, 6, 7]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 85; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 8; ++i3) {\n        compute[(((i0_i1_fused * 64) + (i2 * 8)) + i3)] = tanf(data[(((i0_i1_fused * 64) + (i2 * 8)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 17, 8, 8), \"float32\"), compute: T.Buffer((5, 17, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(85):\n            for i2, i3 in T.grid(8, 8):\n                cse_var_1: T.int32 = i0_i1_fused * 64 + i2 * 8 + i3\n                compute_1 = T.Buffer((5440,), data=compute.data)\n                data_1 = T.Buffer((5440,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [5, 17, 8, 8]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3600; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i3 = 0; i3 < 6; ++i3) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 6; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((i0_i1_fused_i2_fused * 6) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 6; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 6) + k_1)] - T_softmax_maxelem[0])));\n      }\n        int32_t v__1 = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[((i0_i1_fused_i2_fused * 6) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 6) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 6; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 270) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 6; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 18) + (((int)threadIdx.x) * 6)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 3)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 15, 15, 6), \"float32\"), T_softmax_norm: T.Buffer((16, 15, 15, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3600):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i3 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 6 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((21600,), data=data.data)\n                for k in range(6):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 6 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(6):\n                    cse_var_2: T.int32 = i0_i1_fused_i2_fused * 6 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((21600,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [16, 15, 15, 6]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 32; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 11; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 11) + i3)] = tanf(data[((i0_i1_fused_i2_fused * 11) + i3)]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 2, 1, 11), \"float32\"), compute: T.Buffer((16, 2, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            for i3 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 11 + i3\n                compute_1 = T.Buffer((352,), data=compute.data)\n                data_1 = T.Buffer((352,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [16, 2, 1, 11]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    float T_softmax_maxelem[126];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        T_softmax_maxelem[((i1 * 18) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 6; ++k) {\n          T_softmax_maxelem[((i1 * 18) + i2)] = max(T_softmax_maxelem[((i1 * 18) + i2)], data[((((i0 * 756) + (i1 * 108)) + (i2 * 6)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        for (int32_t i3 = 0; i3 < 6; ++i3) {\n          T_softmax_expsum[0] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 6; ++k_1) {\n              int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + k_1)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)])));\n          }\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 756) + (i1_1 * 108)) + (i2_1 * 6)) + i3)] - T_softmax_maxelem[((i1_1 * 18) + i2_1)])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 945) {\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 6; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 945) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(60) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))])) / T_softmax_expsum[((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 945) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 6; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 945) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 192) + (((int)threadIdx.x) * 6)) + k)]);\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 7, 18, 6), \"float32\"), T_softmax_norm: T.Buffer((15, 7, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(15):\n            T_softmax_maxelem = T.allocate([126], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((126,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((11340,), data=data.data)\n            for i1, i2 in T.grid(7, 18):\n                T_softmax_maxelem_1[i1 * 18 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(6):\n                    cse_var_1: T.int32 = i1 * 18 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 756 + i1 * 108 + i2 * 6 + k])\n            for i1, i2, i3 in T.grid(7, 18, 6):\n                cse_var_3: T.int32 = i1 * 18 + i2\n                cse_var_2: T.int32 = i0 * 756 + i1 * 108 + i2 * 6 + i3\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(6):\n                    cse_var_4: T.int32 = i0 * 756 + i1 * 108 + i2 * 6 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3])\n                T_softmax_norm_1 = T.Buffer((11340,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3]) / T_softmax_expsum_1[0]", "op_args": [15, 7, 18, 6]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 240) + (i1 * 120)) + (i2 * 20)) + i3)] = tanf(data[((((i0 * 240) + (i1 * 120)) + (i2 * 20)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 2, 6, 20), \"float32\"), compute: T.Buffer((5, 2, 6, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(5):\n            for i1, i2, i3 in T.grid(2, 6, 20):\n                cse_var_1: T.int32 = i0 * 240 + i1 * 120 + i2 * 20 + i3\n                compute_1 = T.Buffer((1200,), data=compute.data)\n                data_1 = T.Buffer((1200,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [5, 2, 6, 20]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 357; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = tanf(data[i0_i1_fused_i2_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) < 357) {\n    compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 3, 7, 1), \"float32\"), compute: T.Buffer((17, 3, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(357):\n            compute_1 = T.Buffer((357,), data=compute.data)\n            data_1 = T.Buffer((357,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused] = T.tan(data_1[i0_i1_fused_i2_fused])", "op_args": [17, 3, 7, 1]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      for (int32_t i3 = 0; i3 < 6; ++i3) {\n        compute[(((i0_i1_fused * 54) + (i2 * 6)) + i3)] = tanf(data[(((i0_i1_fused * 54) + (i2 * 6)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 10, 9, 6), \"float32\"), compute: T.Buffer((8, 10, 9, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(80):\n            for i2, i3 in T.grid(9, 6):\n                cse_var_1: T.int32 = i0_i1_fused * 54 + i2 * 6 + i3\n                compute_1 = T.Buffer((4320,), data=compute.data)\n                data_1 = T.Buffer((4320,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [8, 10, 9, 6]}{"op_name": "tan", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 104; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      for (int32_t i3 = 0; i3 < 10; ++i3) {\n        compute[(((i0_i1_fused * 80) + (i2 * 10)) + i3)] = tanf(data[(((i0_i1_fused * 80) + (i2 * 10)) + i3)]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = tanf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 8, 8, 10), \"float32\"), compute: T.Buffer((13, 8, 8, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(104):\n            for i2, i3 in T.grid(8, 10):\n                cse_var_1: T.int32 = i0_i1_fused * 80 + i2 * 10 + i3\n                compute_1 = T.Buffer((8320,), data=compute.data)\n                data_1 = T.Buffer((8320,), data=data.data)\n                compute_1[cse_var_1] = T.tan(data_1[cse_var_1])", "op_args": [13, 8, 8, 10]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    float T_softmax_maxelem[110];\n    float T_softmax_expsum[110];\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        T_softmax_maxelem[((i1 * 11) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 15; ++k) {\n          T_softmax_maxelem[((i1 * 11) + i2)] = max(T_softmax_maxelem[((i1 * 11) + i2)], data[((((i0 * 1650) + (i1 * 165)) + (i2 * 15)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 10; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n        T_softmax_expsum[((i1_1 * 11) + i2_1)] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 15; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[((i1_1 * 11) + i2_1)] = (T_softmax_expsum[((i1_1 * 11) + i2_1)] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1650) + (i1_1 * 165)) + (i2_1 * 15)) + k_1)] - T_softmax_maxelem[((i1_1 * 11) + i2_1)])));\n        }\n      }\n    }\n    for (int32_t i1_2 = 0; i1_2 < 10; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 11; ++i2_2) {\n        for (int32_t i3_s = 0; i3_s < 15; ++i3_s) {\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1650) + (i1_2 * 165)) + (i2_2 * 15)) + i3_s)] - T_softmax_maxelem[((i1_2 * 11) + i2_2)])) / T_softmax_expsum[((i1_2 * 11) + i2_2)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 1045) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 15; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 1045) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 480) + (((int)threadIdx.x) * 15)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(11) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 15; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 165) + (((int)threadIdx.x) * 15)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 11) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) < 15675) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)])) / T_softmax_expsum[(((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) / 15)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 10, 11, 15), \"float32\"), T_softmax_norm: T.Buffer((19, 10, 11, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(19):\n            T_softmax_maxelem = T.allocate([110], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([110], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((110,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((31350,), data=data.data)\n            for i1, i2 in T.grid(10, 11):\n                T_softmax_maxelem_1[i1 * 11 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(15):\n                    cse_var_1: T.int32 = i1 * 11 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 1650 + i1 * 165 + i2 * 15 + k])\n            T_softmax_expsum_1 = T.Buffer((110,), data=T_softmax_expsum)\n            for i1, i2 in T.grid(10, 11):\n                T_softmax_expsum_1[i1 * 11 + i2] = T.float32(0)\n                for k in range(15):\n                    cse_var_3: T.int32 = i1 * 11 + i2\n                    cse_var_2: T.int32 = i0 * 1650 + i1 * 165 + i2 * 15 + k\n                    T_softmax_expsum_1[cse_var_3] = T_softmax_expsum_1[cse_var_3] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3])\n            for i1, i2, i3_s in T.grid(10, 11, 15):\n                cse_var_5: T.int32 = i1 * 11 + i2\n                cse_var_4: T.int32 = i0 * 1650 + i1 * 165 + i2 * 15 + i3_s\n                T_softmax_norm_1 = T.Buffer((31350,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_4] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_5]) / T_softmax_expsum_1[cse_var_5]", "op_args": [19, 10, 11, 15]}{"op_name": "tanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 918; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = tanhf(data[i0_i1_fused_i2_fused_i3_fused]);\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(51) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 51) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 9, 3, 17), \"float32\"), compute: T.Buffer((2, 9, 3, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(918):\n            compute_1 = T.Buffer((918,), data=compute.data)\n            data_1 = T.Buffer((918,), data=data.data)\n            compute_1[i0_i1_fused_i2_fused_i3_fused] = T.tanh(data_1[i0_i1_fused_i2_fused_i3_fused])", "op_args": [2, 9, 3, 17]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    float T_softmax_maxelem[57];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        T_softmax_maxelem[((i1 * 19) + i2)] = -3.402823e+38f;\n        for (int32_t k = 0; k < 20; ++k) {\n          T_softmax_maxelem[((i1 * 19) + i2)] = max(T_softmax_maxelem[((i1 * 19) + i2)], data[((((i0 * 1140) + (i1 * 380)) + (i2 * 20)) + k)]);\n        }\n      }\n    }\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          T_softmax_expsum[0] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 20; ++k_1) {\n              int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + k_1)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)])));\n          }\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 1140) + (i1_1 * 380)) + (i2_1 * 20)) + i3)] - T_softmax_maxelem[((i1_1 * 19) + i2_1)])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 969) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 20; ++k) {\n    if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 969) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 640) + (((int)threadIdx.x) * 20)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 969) {\n    T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 20; ++k) {\n    if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 969) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 1280) + (((int)threadIdx.x) * 20)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 4845) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 5)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 3, 19, 20), \"float32\"), T_softmax_norm: T.Buffer((17, 3, 19, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(17):\n            T_softmax_maxelem = T.allocate([57], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((57,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((19380,), data=data.data)\n            for i1, i2 in T.grid(3, 19):\n                T_softmax_maxelem_1[i1 * 19 + i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(20):\n                    cse_var_1: T.int32 = i1 * 19 + i2\n                    T_softmax_maxelem_1[cse_var_1] = T.max(T_softmax_maxelem_1[cse_var_1], data_1[i0 * 1140 + i1 * 380 + i2 * 20 + k])\n            for i1, i2, i3 in T.grid(3, 19, 20):\n                cse_var_3: T.int32 = i1 * 19 + i2\n                cse_var_2: T.int32 = i0 * 1140 + i1 * 380 + i2 * 20 + i3\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(20):\n                    cse_var_4: T.int32 = i0 * 1140 + i1 * 380 + i2 * 20 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_4] - T_softmax_maxelem_1[cse_var_3])\n                T_softmax_norm_1 = T.Buffer((19380,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_2] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[cse_var_3]) / T_softmax_expsum_1[0]", "op_args": [17, 3, 19, 20]}{"op_name": "tanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        for (int32_t i3 = 0; i3 < 19; ++i3) {\n          compute[((((i0 * 2964) + (i1 * 228)) + (i2 * 19)) + i3)] = tanhf(data[((((i0 * 2964) + (i1 * 228)) + (i2 * 19)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 13, 12, 19), \"float32\"), compute: T.Buffer((10, 13, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(10):\n            for i1, i2, i3 in T.grid(13, 12, 19):\n                cse_var_1: T.int32 = i0 * 2964 + i1 * 228 + i2 * 19 + i3\n                compute_1 = T.Buffer((29640,), data=compute.data)\n                data_1 = T.Buffer((29640,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])", "op_args": [10, 13, 12, 19]}{"op_name": "tanh", "c_code": "void default_function_kernel(float* compute, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          compute[((((i0 * 900) + (i1 * 180)) + (i2 * 20)) + i3)] = tanhf(data[((((i0 * 900) + (i1 * 180)) + (i2 * 20)) + i3)]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ compute, float* __restrict__ data) {\n  compute[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = tanhf(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 5, 9, 20), \"float32\"), compute: T.Buffer((14, 5, 9, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(14):\n            for i1, i2, i3 in T.grid(5, 9, 20):\n                cse_var_1: T.int32 = i0 * 900 + i1 * 180 + i2 * 20 + i3\n                compute_1 = T.Buffer((12600,), data=compute.data)\n                data_1 = T.Buffer((12600,), data=data.data)\n                compute_1[cse_var_1] = T.tanh(data_1[cse_var_1])", "op_args": [14, 5, 9, 20]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        for (int32_t i3 = 0; i3 < 18; ++i3) {\n          T_softmax_maxelem[0] = -3.402823e+38f;\n          for (int32_t k = 0; k < 18; ++k) {\n            T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k)]);\n          }\n          T_softmax_expsum[0] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 18; ++k_1) {\n              int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + k_1)] - T_softmax_maxelem[0])));\n          }\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 2268) + (i1 * 162)) + (i2 * 18)) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 18; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 108) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 567) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 18; ++k) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 567) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 5103) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 9)]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((18, 14, 9, 18), \"float32\"), T_softmax_norm: T.Buffer((18, 14, 9, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(18):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1, i2, i3 in T.grid(14, 9, 18):\n                cse_var_1: T.int32 = i0 * 2268 + i1 * 162 + i2 * 18 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((40824,), data=data.data)\n                for k in range(18):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0 * 2268 + i1 * 162 + i2 * 18 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(18):\n                    cse_var_2: T.int32 = i0 * 2268 + i1 * 162 + i2 * 18 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((40824,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [18, 14, 9, 18]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 306; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 16; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 16) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 16) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 16) + ax3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 18, 1, 16), \"float32\"), data_1: T.Buffer((17, 18, 1, 16), \"float32\"), T_add: T.Buffer((17, 18, 1, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            for ax3 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 16 + ax3\n                T_add_1 = T.Buffer((4896,), data=T_add.data)\n                data_2 = T.Buffer((4896,), data=data.data)\n                data_3 = T.Buffer((4896,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [17, 18, 1, 16]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 900; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 20) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 20) + ax3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 15, 10, 20), \"float32\"), data_1: T.Buffer((6, 15, 10, 20), \"float32\"), T_add: T.Buffer((6, 15, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            for ax3 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_add_1 = T.Buffer((18000,), data=T_add.data)\n                data_2 = T.Buffer((18000,), data=data.data)\n                data_3 = T.Buffer((18000,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [6, 15, 10, 20]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n        T_add[(((ax1 * 306) + (ax2 * 18)) + ax3)] = (sqrtf(data[(((ax1 * 306) + (ax2 * 18)) + ax3)]) + cosf(data_1[(((ax1 * 306) + (ax2 * 18)) + ax3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(27) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 27) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 3, 17, 18), \"float32\"), data_1: T.Buffer((1, 3, 17, 18), \"float32\"), T_add: T.Buffer((1, 3, 17, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax1, ax2, ax3 in T.grid(3, 17, 18):\n            cse_var_1: T.int32 = ax1 * 306 + ax2 * 18 + ax3\n            T_add_1 = T.Buffer((918,), data=T_add.data)\n            data_2 = T.Buffer((918,), data=data.data)\n            data_3 = T.Buffer((918,), data=data_1.data)\n            T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [1, 3, 17, 18]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    float T_softmax_maxelem[19];\n    float T_softmax_expsum[1];\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      T_softmax_maxelem[i2] = -3.402823e+38f;\n      for (int32_t k = 0; k < 10; ++k) {\n        T_softmax_maxelem[i2] = max(T_softmax_maxelem[i2], data[(((i0_i1_fused * 190) + (i2 * 10)) + k)]);\n      }\n    }\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      for (int32_t i3 = 0; i3 < 10; ++i3) {\n        T_softmax_expsum[0] = 0.000000e+00f;\n        for (int32_t k_1 = 0; k_1 < 10; ++k_1) {\n            int32_t v_ = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 190) + (i2_1 * 10)) + k_1)] - T_softmax_maxelem[i2_1])));\n        }\n          int32_t v__1 = ((int32_t)(floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_norm[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((i0_i1_fused * 190) + (i2_1 * 10)) + i3)] - T_softmax_maxelem[i2_1])) / T_softmax_expsum[0]);\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 1425) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)])) / T_softmax_expsum[(((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1)) / 5)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 285) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 10; ++k) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 285) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 160) + (((int)threadIdx.x) * 10)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 10; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 200) + (((int)threadIdx.x) * 10)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 15, 19, 10), \"float32\"), T_softmax_norm: T.Buffer((4, 15, 19, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(60):\n            T_softmax_maxelem = T.allocate([19], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            T_softmax_maxelem_1 = T.Buffer((19,), data=T_softmax_maxelem)\n            data_1 = T.Buffer((11400,), data=data.data)\n            for i2 in range(19):\n                T_softmax_maxelem_1[i2] = T.float32(-3.4028234663852886e+38)\n                for k in range(10):\n                    T_softmax_maxelem_1[i2] = T.max(T_softmax_maxelem_1[i2], data_1[i0_i1_fused * 190 + i2 * 10 + k])\n            for i2, i3 in T.grid(19, 10):\n                cse_var_1: T.int32 = i0_i1_fused * 190 + i2 * 10 + i3\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(10):\n                    cse_var_2: T.int32 = i0_i1_fused * 190 + i2 * 10 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[i2])\n                T_softmax_norm_1 = T.Buffer((11400,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[i2], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[i2]) / T_softmax_expsum_1[0]", "op_args": [4, 15, 19, 10]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 684; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_add[ax0_ax1_fused_ax2_fused_ax3_fused] = (sqrtf(data[ax0_ax1_fused_ax2_fused_ax3_fused]) + cosf(data_1[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 2)) < 171) {\n    T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 3, 1, 12), \"float32\"), data_1: T.Buffer((19, 3, 1, 12), \"float32\"), T_add: T.Buffer((19, 3, 1, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(684):\n            T_add_1 = T.Buffer((684,), data=T_add.data)\n            data_2 = T.Buffer((684,), data=data.data)\n            data_3 = T.Buffer((684,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.sqrt(data_2[ax0_ax1_fused_ax2_fused_ax3_fused]) + T.cos(data_3[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [19, 3, 1, 12]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 55080; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_add[ax0_ax1_fused_ax2_fused_ax3_fused] = (sqrtf(data[ax0_ax1_fused_ax2_fused_ax3_fused]) + cosf(data_1[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 18, 15, 17), \"float32\"), data_1: T.Buffer((12, 18, 15, 17), \"float32\"), T_add: T.Buffer((12, 18, 15, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(55080):\n            T_add_1 = T.Buffer((55080,), data=T_add.data)\n            data_2 = T.Buffer((55080,), data=data.data)\n            data_3 = T.Buffer((55080,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.sqrt(data_2[ax0_ax1_fused_ax2_fused_ax3_fused]) + T.cos(data_3[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [12, 18, 15, 17]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 800; ++i0_i1_fused_i2_fused) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i3 = 0; i3 < 18; ++i3) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      for (int32_t k = 0; k < 18; ++k) {\n        T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((i0_i1_fused_i2_fused * 18) + k)]);\n      }\n      T_softmax_expsum[0] = 0.000000e+00f;\n      for (int32_t k_1 = 0; k_1 < 18; ++k_1) {\n          int32_t v_ = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n        T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 18) + k_1)] - T_softmax_maxelem[0])));\n      }\n        int32_t v__1 = ((int32_t)(floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[((i0_i1_fused_i2_fused * 18) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0_i1_fused_i2_fused * 18) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k = 0; k < 18; ++k) {\n      int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = -3.402823e+38f;\n  for (int k = 0; k < 18; ++k) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 576) + (((int)threadIdx.x) * 18)) + k)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n    int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n  T_softmax_norm[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - T_softmax_maxelem[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)])) / T_softmax_expsum[(((((int)blockIdx.x) * 20) + (((int)threadIdx.x) >> 1)) / 9)]);\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 16, 10, 18), \"float32\"), T_softmax_norm: T.Buffer((5, 16, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(800):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i3 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 18 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((14400,), data=data.data)\n                for k in range(18):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0_i1_fused_i2_fused * 18 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(18):\n                    cse_var_2: T.int32 = i0_i1_fused_i2_fused * 18 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((14400,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [5, 16, 10, 18]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 35; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n        T_add[(((ax0_ax1_fused * 51) + (ax2 * 3)) + ax3)] = (sqrtf(data[(((ax0_ax1_fused * 51) + (ax2 * 3)) + ax3)]) + cosf(data_1[(((ax0_ax1_fused * 51) + (ax2 * 3)) + ax3)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(35) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 35) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 5, 17, 3), \"float32\"), data_1: T.Buffer((7, 5, 17, 3), \"float32\"), T_add: T.Buffer((7, 5, 17, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(35):\n            for ax2, ax3 in T.grid(17, 3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 51 + ax2 * 3 + ax3\n                T_add_1 = T.Buffer((1785,), data=T_add.data)\n                data_2 = T.Buffer((1785,), data=data.data)\n                data_3 = T.Buffer((1785,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [7, 5, 17, 3]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 1976; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_add[ax0_ax1_fused_ax2_fused_ax3_fused] = (sqrtf(data[ax0_ax1_fused_ax2_fused_ax3_fused]) + cosf(data_1[ax0_ax1_fused_ax2_fused_ax3_fused]));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 247) {\n    T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 19, 2, 4), \"float32\"), data_1: T.Buffer((13, 19, 2, 4), \"float32\"), T_add: T.Buffer((13, 19, 2, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(1976):\n            T_add_1 = T.Buffer((1976,), data=T_add.data)\n            data_2 = T.Buffer((1976,), data=data.data)\n            data_3 = T.Buffer((1976,), data=data_1.data)\n            T_add_1[ax0_ax1_fused_ax2_fused_ax3_fused] = T.sqrt(data_2[ax0_ax1_fused_ax2_fused_ax3_fused]) + T.cos(data_3[ax0_ax1_fused_ax2_fused_ax3_fused])", "op_args": [13, 19, 2, 4]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      T_softmax_maxelem[0] = -3.402823e+38f;\n      T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((i0 * 13) + i1)]);\n      T_softmax_expsum[0] = 0.000000e+00f;\n        int32_t v_ = ((int32_t)(floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0 * 13) + i1)] - T_softmax_maxelem[0])));\n        int32_t v__1 = ((int32_t)(floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_norm[((i0 * 13) + i1)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((i0 * 13) + i1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((i0 * 13) + i1)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 169) {\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 169) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 169) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / T_softmax_expsum[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 169) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 169) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], data[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 13, 1, 1), \"float32\"), T_softmax_norm: T.Buffer((13, 13, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1 in range(13):\n                cse_var_1: T.int32 = i0 * 13 + i1\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((169,), data=data.data)\n                T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[cse_var_1])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((169,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [13, 13, 1, 1]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 18) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 18) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 18) + ax3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 3, 14, 18), \"float32\"), data_1: T.Buffer((12, 3, 14, 18), \"float32\"), T_add: T.Buffer((12, 3, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(504):\n            for ax3 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 18 + ax3\n                T_add_1 = T.Buffer((9072,), data=T_add.data)\n                data_2 = T.Buffer((9072,), data=data.data)\n                data_3 = T.Buffer((9072,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [12, 3, 14, 18]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n          T_add[((((ax0 * 30) + (ax1 * 10)) + (ax2 * 5)) + ax3)] = (sqrtf(data[((((ax0 * 30) + (ax1 * 10)) + (ax2 * 5)) + ax3)]) + cosf(data_1[((((ax0 * 30) + (ax1 * 10)) + (ax2 * 5)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 3, 2, 5), \"float32\"), data_1: T.Buffer((10, 3, 2, 5), \"float32\"), T_add: T.Buffer((10, 3, 2, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(10):\n            for ax1, ax2, ax3 in T.grid(3, 2, 5):\n                cse_var_1: T.int32 = ax0 * 30 + ax1 * 10 + ax2 * 5 + ax3\n                T_add_1 = T.Buffer((300,), data=T_add.data)\n                data_2 = T.Buffer((300,), data=data.data)\n                data_3 = T.Buffer((300,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [10, 3, 2, 5]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_add[((((ax0 * 324) + (ax1 * 54)) + (ax2 * 18)) + ax3)] = (sqrtf(data[((((ax0 * 324) + (ax1 * 54)) + (ax2 * 18)) + ax3)]) + cosf(data_1[((((ax0 * 324) + (ax1 * 54)) + (ax2 * 18)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 405) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 6, 3, 18), \"float32\"), data_1: T.Buffer((20, 6, 3, 18), \"float32\"), T_add: T.Buffer((20, 6, 3, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(20):\n            for ax1, ax2, ax3 in T.grid(6, 3, 18):\n                cse_var_1: T.int32 = ax0 * 324 + ax1 * 54 + ax2 * 18 + ax3\n                T_add_1 = T.Buffer((6480,), data=T_add.data)\n                data_2 = T.Buffer((6480,), data=data.data)\n                data_3 = T.Buffer((6480,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [20, 6, 3, 18]}{"op_name": "softmax_common", "c_code": "void default_function_kernel(float* T_softmax_norm, float* data) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    float T_softmax_maxelem[1];\n    float T_softmax_expsum[1];\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 2; ++i3) {\n          T_softmax_maxelem[0] = -3.402823e+38f;\n          for (int32_t k = 0; k < 2; ++k) {\n            T_softmax_maxelem[0] = max(T_softmax_maxelem[0], data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k)]);\n          }\n          T_softmax_expsum[0] = 0.000000e+00f;\n          for (int32_t k_1 = 0; k_1 < 2; ++k_1) {\n              int32_t v_ = ((int32_t)(floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n            T_softmax_expsum[0] = (T_softmax_expsum[0] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + k_1)] - T_softmax_maxelem[0])));\n          }\n            int32_t v__1 = ((int32_t)(floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n          T_softmax_norm[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] = (max(((*(float *)(&(v__1))) * ((((((((((((((1.987569e-04f * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((i0 * 154) + (i1 * 22)) + (i2 * 2)) + i3)] - T_softmax_maxelem[0])) / T_softmax_expsum[0]);\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 231) {\n    T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = -3.402823e+38f;\n  }\n  for (int k = 0; k < 2; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 231) {\n      T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = max(T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 231) {\n    T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k = 0; k < 2; ++k) {\n    if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 231) {\n        int v_ = ((int)(floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n      T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_softmax_expsum[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[(((((int)blockIdx.x) * 128) + (((int)threadIdx.x) * 2)) + k)] - T_softmax_maxelem[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_softmax_expsum, float* __restrict__ T_softmax_maxelem, float* __restrict__ T_softmax_norm, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 231) {\n      int v_ = ((int)(floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) + 1.270000e+02f)) << 23;\n    T_softmax_norm[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (max(((*(float *)(&(v_))) * ((((((((((((((1.987569e-04f * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.398200e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 8.333452e-03f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 4.166580e-02f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.666667e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 5.000000e-01f) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) * (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + (max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) - (floorf(((max(min((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]), 8.837627e+01f), -8.837626e+01f) * 1.442695e+00f) + 5.000000e-01f)) * 6.931472e-01f))) + 1.000000e+00f)), (data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - T_softmax_maxelem[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))])) / T_softmax_expsum[((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 1))]);\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 7, 11, 2), \"float32\"), T_softmax_norm: T.Buffer((12, 7, 11, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(12):\n            T_softmax_maxelem = T.allocate([1], \"float32\", \"global\")\n            T_softmax_expsum = T.allocate([1], \"float32\", \"global\")\n            for i1, i2, i3 in T.grid(7, 11, 2):\n                cse_var_1: T.int32 = i0 * 154 + i1 * 22 + i2 * 2 + i3\n                T_softmax_maxelem_1 = T.Buffer((1,), data=T_softmax_maxelem, align=4)\n                T_softmax_maxelem_1[0] = T.float32(-3.4028234663852886e+38)\n                data_1 = T.Buffer((1848,), data=data.data)\n                for k in range(2):\n                    T_softmax_maxelem_1[0] = T.max(T_softmax_maxelem_1[0], data_1[i0 * 154 + i1 * 22 + i2 * 2 + k])\n                T_softmax_expsum_1 = T.Buffer((1,), data=T_softmax_expsum, align=4)\n                T_softmax_expsum_1[0] = T.float32(0)\n                for k in range(2):\n                    cse_var_2: T.int32 = i0 * 154 + i1 * 22 + i2 * 2 + k\n                    T_softmax_expsum_1[0] = T_softmax_expsum_1[0] + T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_2] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_2] - T_softmax_maxelem_1[0])\n                T_softmax_norm_1 = T.Buffer((1848,), data=T_softmax_norm.data)\n                T_softmax_norm_1[cse_var_1] = T.max(T.reinterpret(\"float32\", T.shift_left(T.Cast(\"int32\", T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) + T.float32(127)), 23)) * ((((((T.float32(0.00019875691214110702) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.0013981999363750219)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.008333452045917511)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.041665796190500259)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.1666666567325592)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(0.5)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) * (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + (T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) - T.floor(T.max(T.min(data_1[cse_var_1] - T_softmax_maxelem_1[0], T.float32(88.376266479492188)), T.float32(-88.376258850097656)) * T.float32(1.4426950216293335) + T.float32(0.5)) * T.float32(0.69314718246459961)) + T.float32(1)), data_1[cse_var_1] - T_softmax_maxelem_1[0]) / T_softmax_expsum_1[0]", "op_args": [12, 7, 11, 2]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 448; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n      T_add[((ax0_ax1_fused_ax2_fused * 10) + ax3)] = (sqrtf(data[((ax0_ax1_fused_ax2_fused * 10) + ax3)]) + cosf(data_1[((ax0_ax1_fused_ax2_fused * 10) + ax3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 7, 4, 10), \"float32\"), data_1: T.Buffer((16, 7, 4, 10), \"float32\"), T_add: T.Buffer((16, 7, 4, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(448):\n            for ax3 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 10 + ax3\n                T_add_1 = T.Buffer((4480,), data=T_add.data)\n                data_2 = T.Buffer((4480,), data=data.data)\n                data_3 = T.Buffer((4480,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [16, 7, 4, 10]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 143640; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[(((((((i0_i1_fused_i2_fused_i3_fused / 9576) * 9576) + ((((i0_i1_fused_i2_fused_i3_fused % 9576) / 126) % 19) * 504)) + (((i0_i1_fused_i2_fused_i3_fused % 126) / 18) * 72)) + ((((i0_i1_fused_i2_fused_i3_fused % 9576) / 126) / 38) * 36)) + ((i0_i1_fused_i2_fused_i3_fused % 18) * 2)) + ((((i0_i1_fused_i2_fused_i3_fused % 9576) / 126) % 38) / 19))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) / 798) * 9576) + ((((((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6)) % 1596) / 21) % 19) * 504)) + (((((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6)) % 21) / 3) * 72)) + ((((((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6)) % 1596) / 21) / 38) * 36)) + ((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 18) * 2)) + ((((((((int)blockIdx.x) * 10) + (((int)threadIdx.x) / 6)) % 1596) / 21) % 38) / 19))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 19, 14, 36), \"float32\"), space_to_depth: T.Buffer((15, 76, 7, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(143640):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 9576 // 126\n            space_to_depth_1 = T.Buffer((143640,), data=space_to_depth.data)\n            data_1 = T.Buffer((143640,), data=data.data)\n            space_to_depth_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 9576 * 9576 + T.truncmod(cse_var_1, 19) * 504 + i0_i1_fused_i2_fused_i3_fused % 126 // 18 * 72 + T.Div(cse_var_1, 38) * 36 + i0_i1_fused_i2_fused_i3_fused % 18 * 2 + T.Div(T.truncmod(cse_var_1, 38), 19)]", "op_args": [15, 19, 7, 18]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          T_add[((((ax0 * 240) + (ax1 * 20)) + (ax2 * 10)) + ax3)] = (sqrtf(data[((((ax0 * 240) + (ax1 * 20)) + (ax2 * 10)) + ax3)]) + cosf(data_1[((((ax0 * 240) + (ax1 * 20)) + (ax2 * 10)) + ax3)]));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 12, 2, 10), \"float32\"), data_1: T.Buffer((5, 12, 2, 10), \"float32\"), T_add: T.Buffer((5, 12, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(5):\n            for ax1, ax2, ax3 in T.grid(12, 2, 10):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 20 + ax2 * 10 + ax3\n                T_add_1 = T.Buffer((1200,), data=T_add.data)\n                data_2 = T.Buffer((1200,), data=data.data)\n                data_3 = T.Buffer((1200,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [5, 12, 2, 10]}{"op_name": "combination_op", "c_code": "void default_function_kernel(float* T_add, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_add[(((ax0 * 220) + (ax1 * 20)) + ax2)] = (sqrtf(data[(((ax0 * 220) + (ax1 * 20)) + ax2)]) + cosf(data_1[(((ax0 * 220) + (ax1 * 20)) + ax2)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ data, float* __restrict__ data_1) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (sqrtf(data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]) + __cosf(data_1[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 11, 20, 1), \"float32\"), data_1: T.Buffer((6, 11, 20, 1), \"float32\"), T_add: T.Buffer((6, 11, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(11, 20):\n                cse_var_1: T.int32 = ax0 * 220 + ax1 * 20 + ax2\n                T_add_1 = T.Buffer((1320,), data=T_add.data)\n                data_2 = T.Buffer((1320,), data=data.data)\n                data_3 = T.Buffer((1320,), data=data_1.data)\n                T_add_1[cse_var_1] = T.sqrt(data_2[cse_var_1]) + T.cos(data_3[cse_var_1])", "op_args": [6, 11, 20, 1]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2652; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 19; ++i3) {\n      space_to_depth[((i0_i1_fused_i2_fused * 19) + i3)] = data[(((((((i0_i1_fused_i2_fused / 204) * 3876) + ((((i0_i1_fused_i2_fused % 204) / 3) % 17) * 228)) + ((i0_i1_fused_i2_fused % 3) * 76)) + ((((i0_i1_fused_i2_fused % 204) / 3) / 34) * 38)) + (i3 * 2)) + ((((i0_i1_fused_i2_fused % 204) / 3) % 34) / 17))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 12597) {\n    space_to_depth[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[(((((((((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) / 969) * 3876) + ((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 3876) / 57) % 17) * 228)) + (((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 57) / 19) * 76)) + ((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 3876) / 57) / 34) * 38)) + ((((((int)blockIdx.x) * 7) + ((int)threadIdx.x)) % 19) * 2)) + ((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 3876) / 57) % 34) / 17))];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 17, 6, 38), \"float32\"), space_to_depth: T.Buffer((13, 68, 3, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(2652):\n            for i3 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 204 // 3\n                space_to_depth_1 = T.Buffer((50388,), data=space_to_depth.data)\n                data_1 = T.Buffer((50388,), data=data.data)\n                space_to_depth_1[i0_i1_fused_i2_fused * 19 + i3] = data_1[i0_i1_fused_i2_fused // 204 * 3876 + T.truncmod(cse_var_1, 17) * 228 + i0_i1_fused_i2_fused % 3 * 76 + T.Div(cse_var_1, 34) * 38 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 34), 17)]", "op_args": [13, 17, 3, 19]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 17; ++i3) {\n      space_to_depth[((i0_i1_fused_i2_fused * 17) + i3)] = data[((((((i0_i1_fused_i2_fused >> 3) * 136) + (((i0_i1_fused_i2_fused & 7) % 2) * 68)) + (((i0_i1_fused_i2_fused & 7) / 4) * 34)) + (i3 * 2)) + (((i0_i1_fused_i2_fused & 7) % 4) / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 2)) / 34) * 136) + ((((((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) % 136) / 17) % 2) * 68)) + ((((((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) % 136) / 17) / 4) * 34)) + ((((((int)blockIdx.x) * 11) + ((int)threadIdx.x)) % 17) * 2)) + ((((((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) % 136) / 17) % 4) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 2, 2, 34), \"float32\"), space_to_depth: T.Buffer((14, 8, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            for i3 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused % 8\n                space_to_depth_1 = T.Buffer((1904,), data=space_to_depth.data)\n                data_1 = T.Buffer((1904,), data=data.data)\n                space_to_depth_1[i0_i1_fused_i2_fused * 17 + i3] = data_1[i0_i1_fused_i2_fused // 8 * 136 + T.truncmod(cse_var_1, 2) * 68 + T.Div(cse_var_1, 4) * 34 + i3 * 2 + T.Div(T.truncmod(cse_var_1, 4), 2)]", "op_args": [14, 2, 1, 17]}{"op_name": "multi_out_op", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 15; ++i3) {\n      compute[((i0_i1_fused_i2_fused * 15) + i3)] = sqrtf((data[((i0_i1_fused_i2_fused * 15) + i3)] + data_1[((i0_i1_fused_i2_fused * 15) + i3)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 21; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      for (int32_t i3_1 = 0; i3_1 < 15; ++i3_1) {\n        compute_1[(((i0_i1_fused * 225) + (i2 * 15)) + i3_1)] = cosf((data[(((i0_i1_fused * 225) + (i2 * 15)) + i3_1)] + data_1[(((i0_i1_fused * 225) + (i2 * 15)) + i3_1)]));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 4725) {\n    compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))]));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((3, 7, 15, 15), \"float32\"), data_1: T.Buffer((3, 7, 15, 15), \"float32\"), compute: T.Buffer((3, 7, 15, 15), \"float32\"), compute_1: T.Buffer((3, 7, 15, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((4725,), data=data.data)\n        data_3 = T.Buffer((4725,), data=data_1.data)\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            for i3 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused_i2_fused * 15 + i3\n                compute_2 = T.Buffer((4725,), data=compute.data)\n                compute_2[cse_var_1] = T.sqrt(data_2[cse_var_1] + data_3[cse_var_1])\n        for i0_i1_fused in T.parallel(21):\n            for i2, i3 in T.grid(15, 15):\n                cse_var_2: T.int32 = i0_i1_fused * 225 + i2 * 15 + i3\n                compute_2 = T.Buffer((4725,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(data_2[cse_var_2] + data_3[cse_var_2])", "op_args": [3, 7, 15, 15]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 80; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        for (int32_t i3 = 0; i3 < 20; ++i3) {\n          space_to_depth[((((i0 * 17600) + (i1 * 220)) + (i2 * 20)) + i3)] = data[((((((i0 * 17600) + ((i1 % 20) * 880)) + (i2 * 80)) + ((i1 / 40) * 40)) + (i3 * 2)) + ((i1 % 40) / 20))];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(55) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] = data[(((((((((int)blockIdx.x) / 320) * 17600) + ((((((int)blockIdx.x) % 320) >> 2) % 20) * 880)) + (((((((int)blockIdx.x) & 3) * 11) + (((int)threadIdx.x) / 5)) >> 2) * 80)) + ((((((int)blockIdx.x) % 320) >> 2) / 40) * 40)) + ((((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) % 20) * 2)) + ((((((int)blockIdx.x) % 320) >> 2) % 40) / 20))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((8, 20, 22, 40), \"float32\"), space_to_depth: T.Buffer((8, 80, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2, i3 in T.grid(80, 11, 20):\n                cse_var_1: T.int32 = i0 * 17600\n                space_to_depth_1 = T.Buffer((140800,), data=space_to_depth.data)\n                data_1 = T.Buffer((140800,), data=data.data)\n                space_to_depth_1[cse_var_1 + i1 * 220 + i2 * 20 + i3] = data_1[cse_var_1 + T.truncmod(i1, 20) * 880 + i2 * 80 + T.Div(i1, 40) * 40 + i3 * 2 + T.Div(T.truncmod(i1, 40), 20)]", "op_args": [8, 20, 11, 20]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 28; ++i1) {\n      for (int32_t i3 = 0; i3 < 3; ++i3) {\n        space_to_depth[(((i0 * 84) + (i1 * 3)) + i3)] = data[(((((i0 * 84) + ((i1 % 7) * 12)) + ((i1 / 14) * 6)) + (i3 * 2)) + ((i1 % 14) / 7))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(21) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) >> 2) * 84) + ((((int)threadIdx.x) / 3) * 12)) + (((((((int)blockIdx.x) & 3) * 7) + (((int)threadIdx.x) / 3)) / 14) * 6)) + ((((int)threadIdx.x) % 3) * 2)) + (((((((int)blockIdx.x) & 3) * 7) + (((int)threadIdx.x) / 3)) % 14) / 7))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 7, 2, 6), \"float32\"), space_to_depth: T.Buffer((16, 28, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(16):\n            for i1, i3 in T.grid(28, 3):\n                cse_var_1: T.int32 = i0 * 84\n                space_to_depth_1 = T.Buffer((1344,), data=space_to_depth.data)\n                data_1 = T.Buffer((1344,), data=data.data)\n                space_to_depth_1[cse_var_1 + i1 * 3 + i3] = data_1[cse_var_1 + T.truncmod(i1, 7) * 12 + T.Div(i1, 14) * 6 + i3 * 2 + T.Div(T.truncmod(i1, 14), 7)]", "op_args": [16, 7, 1, 3]}{"op_name": "space_to_depth", "c_code": "void default_function_kernel(float* data, float* space_to_depth) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 416; ++i0_i1_fused_i2_fused_i3_fused) {\n    space_to_depth[i0_i1_fused_i2_fused_i3_fused] = data[((((((i0_i1_fused_i2_fused_i3_fused >> 5) * 32) + ((((i0_i1_fused_i2_fused_i3_fused & 31) >> 1) % 4) * 8)) + ((((i0_i1_fused_i2_fused_i3_fused & 31) >> 1) / 8) * 4)) + ((i0_i1_fused_i2_fused_i3_fused & 1) * 2)) + ((((i0_i1_fused_i2_fused_i3_fused & 31) >> 1) % 8) / 4))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ data, float* __restrict__ space_to_depth) {\n  space_to_depth[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = data[((((((((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) >> 5) * 32) + ((((((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) & 31) >> 1) % 4) * 8)) + ((((((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) & 31) >> 1) / 8) * 4)) + (((((int)blockIdx.x) + ((int)threadIdx.x)) & 1) * 2)) + ((((((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) & 31) >> 1) % 8) / 4))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 4, 2, 4), \"float32\"), space_to_depth: T.Buffer((13, 16, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(416):\n            cse_var_1: T.int32 = i0_i1_fused_i2_fused_i3_fused % 32 // 2\n            space_to_depth_1 = T.Buffer((416,), data=space_to_depth.data)\n            data_1 = T.Buffer((416,), data=data.data)\n            space_to_depth_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 32 * 32 + T.truncmod(cse_var_1, 4) * 8 + T.Div(cse_var_1, 8) * 4 + i0_i1_fused_i2_fused_i3_fused % 2 * 2 + T.Div(T.truncmod(cse_var_1, 8), 4)]", "op_args": [13, 4, 1, 2]}{"op_name": "multi_out_op", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 39; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      for (int32_t i3 = 0; i3 < 13; ++i3) {\n        compute[(((i0_i1_fused * 91) + (i2 * 13)) + i3)] = sqrtf((data[(((i0_i1_fused * 91) + (i2 * 13)) + i3)] + data_1[(((i0_i1_fused * 91) + (i2 * 13)) + i3)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 273; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3_1 = 0; i3_1 < 13; ++i3_1) {\n      compute_1[((i0_i1_fused_i2_fused * 13) + i3_1)] = cosf((data[((i0_i1_fused_i2_fused * 13) + i3_1)] + data_1[((i0_i1_fused_i2_fused * 13) + i3_1)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(21) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 21) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) < 3549) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 3, 7, 13), \"float32\"), data_1: T.Buffer((13, 3, 7, 13), \"float32\"), compute: T.Buffer((13, 3, 7, 13), \"float32\"), compute_1: T.Buffer((13, 3, 7, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((3549,), data=data.data)\n        data_3 = T.Buffer((3549,), data=data_1.data)\n        for i0_i1_fused in T.parallel(39):\n            for i2, i3 in T.grid(7, 13):\n                cse_var_1: T.int32 = i0_i1_fused * 91 + i2 * 13 + i3\n                compute_2 = T.Buffer((3549,), data=compute.data)\n                compute_2[cse_var_1] = T.sqrt(data_2[cse_var_1] + data_3[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(273):\n            for i3 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused_i2_fused * 13 + i3\n                compute_2 = T.Buffer((3549,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(data_2[cse_var_2] + data_3[cse_var_2])", "op_args": [13, 3, 7, 13]}{"op_name": "multi_out_op", "c_code": "void default_function_kernel(float* compute, float* compute_1, float* data, float* data_1) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 1360; ++i0_i1_fused_i2_fused_i3_fused) {\n    compute[i0_i1_fused_i2_fused_i3_fused] = sqrtf((data[i0_i1_fused_i2_fused_i3_fused] + data_1[i0_i1_fused_i2_fused_i3_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 170; ++i0_i1_fused) {\n    for (int32_t i3 = 0; i3 < 8; ++i3) {\n      compute_1[((i0_i1_fused * 8) + i3)] = cosf((data[((i0_i1_fused * 8) + i3)] + data_1[((i0_i1_fused * 8) + i3)]));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  compute[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = __cosf((data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ data, float* __restrict__ data_1) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 4)) < 85) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = sqrtf((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + data_1[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 10, 1, 8), \"float32\"), data_1: T.Buffer((17, 10, 1, 8), \"float32\"), compute: T.Buffer((17, 10, 1, 8), \"float32\"), compute_1: T.Buffer((17, 10, 1, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        data_2 = T.Buffer((1360,), data=data.data)\n        data_3 = T.Buffer((1360,), data=data_1.data)\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(1360):\n            compute_2 = T.Buffer((1360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused_i3_fused] = T.sqrt(data_2[i0_i1_fused_i2_fused_i3_fused] + data_3[i0_i1_fused_i2_fused_i3_fused])\n        for i0_i1_fused in T.parallel(170):\n            for i3 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i3\n                compute_2 = T.Buffer((1360,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(data_2[cse_var_1] + data_3[cse_var_1])", "op_args": [17, 10, 1, 8]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax0_ax1_fused * 7) + ax2)] = a[(((((ax0_ax1_fused / 5) * 240) + ((ax0_ax1_fused % 5) * 20)) + ax2) + 283)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 3)) < 35) {\n    T_strided_slice[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) / 35) * 240) + (((((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 35) / 7) * 20)) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 7)) + 283)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((9, 12, 20), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((2160,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 240 + ax0_ax1_fused % 5 * 20 + ax2 + 283]", "op_args": [7, 9, 12, 20]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 105; ++ax0_ax1_fused_ax2_fused) {\n    T_strided_slice[ax0_ax1_fused_ax2_fused] = a[(((((ax0_ax1_fused_ax2_fused / 35) * 160) + (((ax0_ax1_fused_ax2_fused % 35) / 7) * 20)) + (ax0_ax1_fused_ax2_fused % 7)) + 203)];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(15) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 5)) / 7) * 160) + (((((((int)blockIdx.x) * 15) + ((int)threadIdx.x)) % 35) / 7) * 20)) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7)) + 203)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((13, 8, 20), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(105):\n            T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n            a_1 = T.Buffer((2080,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused_ax2_fused] = a_1[ax0_ax1_fused_ax2_fused // 35 * 160 + ax0_ax1_fused_ax2_fused % 35 // 7 * 20 + ax0_ax1_fused_ax2_fused % 7 + 203]", "op_args": [14, 13, 8, 20]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_strided_slice[(((ax0 * 15) + (ax1 * 3)) + ax2)] = a[((((ax0 * 102) + (ax1 * 6)) + ax2) + 117)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = a[(((((((int)blockIdx.x) / 3) * 102) + (((((((int)blockIdx.x) % 3) * 5) + ((int)threadIdx.x)) / 3) * 6)) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 3)) + 117)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((13, 17, 6), \"float32\"), T_strided_slice: T.Buffer((3, 5, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(5, 3):\n                T_strided_slice_1 = T.Buffer((45,), data=T_strided_slice.data)\n                a_1 = T.Buffer((1326,), data=a.data)\n                T_strided_slice_1[ax0 * 15 + ax1 * 3 + ax2] = a_1[ax0 * 102 + ax1 * 6 + ax2 + 117]", "op_args": [3, 13, 17, 6]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 105; ++ax0_ax1_fused_ax2_fused) {\n    T_strided_slice[ax0_ax1_fused_ax2_fused] = a[(((((ax0_ax1_fused_ax2_fused / 35) * 160) + (((ax0_ax1_fused_ax2_fused % 35) / 7) * 20)) + (ax0_ax1_fused_ax2_fused % 7)) + 203)];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) < 105) {\n    T_strided_slice[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) / 35) * 160) + (((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 35) / 7) * 20)) + ((((int)blockIdx.x) + ((int)threadIdx.x)) % 7)) + 203)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((5, 8, 20), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(105):\n            T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n            a_1 = T.Buffer((800,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused_ax2_fused] = a_1[ax0_ax1_fused_ax2_fused // 35 * 160 + ax0_ax1_fused_ax2_fused % 35 // 7 * 20 + ax0_ax1_fused_ax2_fused % 7 + 203]", "op_args": [8, 5, 8, 20]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax0_ax1_fused * 7) + ax2)] = a[(((((ax0_ax1_fused / 5) * 221) + ((ax0_ax1_fused % 5) * 13)) + ax2) + 250)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) < 105) {\n    T_strided_slice[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) / 35) * 221) + (((((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 35) / 7) * 13)) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 7)) + 250)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((16, 17, 13), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((3536,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 221 + ax0_ax1_fused % 5 * 13 + ax2 + 250]", "op_args": [16, 16, 17, 13]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 10; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_strided_slice[((ax0_ax1_fused * 2) + ax2)] = a[(((((ax0_ax1_fused / 5) * 100) + ((ax0_ax1_fused % 5) * 5)) + ax2) + 113)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((int)threadIdx.x)] = a[(((((((int)threadIdx.x) / 10) * 100) + (((((int)threadIdx.x) % 10) >> 1) * 5)) + (((int)threadIdx.x) & 1)) + 113)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((3, 20, 5), \"float32\"), T_strided_slice: T.Buffer((2, 5, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(10):\n            for ax2 in range(2):\n                T_strided_slice_1 = T.Buffer((20,), data=T_strided_slice.data)\n                a_1 = T.Buffer((300,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 2 + ax2] = a_1[ax0_ax1_fused // 5 * 100 + ax0_ax1_fused % 5 * 5 + ax2 + 113]", "op_args": [1, 3, 20, 5]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_strided_slice[((ax0_ax1_fused * 7) + ax2)] = a[(((((ax0_ax1_fused / 5) * 204) + ((ax0_ax1_fused % 5) * 17)) + ax2) + 241)];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(13) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) < 105) {\n    T_strided_slice[((((int)blockIdx.x) * 13) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) / 35) * 204) + (((((((int)blockIdx.x) * 13) + ((int)threadIdx.x)) % 35) / 7) * 17)) + (((((int)blockIdx.x) * 6) + ((int)threadIdx.x)) % 7)) + 241)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((10, 12, 17), \"float32\"), T_strided_slice: T.Buffer((3, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(15):\n            for ax2 in range(7):\n                T_strided_slice_1 = T.Buffer((105,), data=T_strided_slice.data)\n                a_1 = T.Buffer((2040,), data=a.data)\n                T_strided_slice_1[ax0_ax1_fused * 7 + ax2] = a_1[ax0_ax1_fused // 5 * 204 + ax0_ax1_fused % 5 * 17 + ax2 + 241]", "op_args": [4, 10, 12, 17]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_strided_slice[(((ax0 * 25) + (ax1 * 5)) + ax2)] = a[((((ax0 * 120) + (ax1 * 8)) + ax2) + 139)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  if (((((int)blockIdx.x) * 6) + (((int)threadIdx.x) / 3)) < 25) {\n    T_strided_slice[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = a[(((((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) / 25) * 120) + (((((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) % 25) / 5) * 8)) + (((((int)blockIdx.x) * 3) + ((int)threadIdx.x)) % 5)) + 139)];\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((5, 15, 8), \"float32\"), T_strided_slice: T.Buffer((3, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(5, 5):\n                T_strided_slice_1 = T.Buffer((75,), data=T_strided_slice.data)\n                a_1 = T.Buffer((600,), data=a.data)\n                T_strided_slice_1[ax0 * 25 + ax1 * 5 + ax2] = a_1[ax0 * 120 + ax1 * 8 + ax2 + 139]", "op_args": [8, 5, 15, 8]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 42; ++ax0_ax1_fused_ax2_fused) {\n    T_strided_slice[ax0_ax1_fused_ax2_fused] = a[(((((ax0_ax1_fused_ax2_fused / 14) * 52) + (((ax0_ax1_fused_ax2_fused % 14) / 7) * 13)) + (ax0_ax1_fused_ax2_fused % 7)) + 81)];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(14) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 14) + ((int)threadIdx.x))] = a[((((((int)blockIdx.x) * 52) + ((((int)threadIdx.x) / 7) * 13)) + (((int)threadIdx.x) % 7)) + 81)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((11, 4, 13), \"float32\"), T_strided_slice: T.Buffer((3, 2, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(42):\n            T_strided_slice_1 = T.Buffer((42,), data=T_strided_slice.data)\n            a_1 = T.Buffer((572,), data=a.data)\n            T_strided_slice_1[ax0_ax1_fused_ax2_fused] = a_1[ax0_ax1_fused_ax2_fused // 14 * 52 + ax0_ax1_fused_ax2_fused % 14 // 7 * 13 + ax0_ax1_fused_ax2_fused % 7 + 81]", "op_args": [6, 11, 4, 13]}{"op_name": "strided_slice", "c_code": "void default_function_kernel(float* T_strided_slice, float* a) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_strided_slice[(((ax0 * 30) + (ax1 * 6)) + ax2)] = a[((((ax0 * 72) + (ax1 * 9)) + ax2) + 93)];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_strided_slice, float* __restrict__ a) {\n  T_strided_slice[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = a[((((((int)blockIdx.x) * 72) + ((((int)threadIdx.x) / 6) * 9)) + (((int)threadIdx.x) % 6)) + 93)];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(a: T.Buffer((13, 8, 9), \"float32\"), T_strided_slice: T.Buffer((3, 5, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(5, 6):\n                T_strided_slice_1 = T.Buffer((90,), data=T_strided_slice.data)\n                a_1 = T.Buffer((936,), data=a.data)\n                T_strided_slice_1[ax0 * 30 + ax1 * 6 + ax2] = a_1[ax0 * 72 + ax1 * 9 + ax2 + 93]", "op_args": [13, 13, 8, 9]}{"op_name": "unpack_NCHWc_to_nchw", "c_code": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 192; ++n_c_fused) {\n    for (int32_t h = 0; h < 3; ++h) {\n      for (int32_t w = 0; w < 10; ++w) {\n        output_unpack[(((n_c_fused * 30) + (h * 10)) + w)] = packed_out[(((((n_c_fused >> 1) * 60) + (h * 20)) + (w * 2)) + (n_c_fused & 1))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = packed_out[(((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) / 20)) / 3) * 60) + ((((((int)threadIdx.x) / 10) + ((int)blockIdx.x)) % 3) * 20)) + ((((int)threadIdx.x) % 10) * 2)) + ((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 10)) % 6) / 3))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((8, 12, 3, 10, 2), \"float32\"), output_unpack: T.Buffer((8, 24, 3, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused in T.parallel(192):\n            for h, w in T.grid(3, 10):\n                output_unpack_1 = T.Buffer((5760,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((5760,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 30 + h * 10 + w] = packed_out_1[n_c_fused // 2 * 60 + h * 20 + w * 2 + n_c_fused % 2]", "op_args": [8, 12, 3, 10]}{"op_name": "unpack_NCHWc_to_nchw", "c_code": "void default_function_kernel(float* output_unpack, float* packed_out) {\n  #pragma omp parallel for\n  for (int32_t n_c_fused = 0; n_c_fused < 16; ++n_c_fused) {\n    for (int32_t h = 0; h < 17; ++h) {\n      for (int32_t w = 0; w < 2; ++w) {\n        output_unpack[(((n_c_fused * 34) + (h * 2)) + w)] = packed_out[(((((n_c_fused >> 1) * 68) + (h * 4)) + (w * 2)) + (n_c_fused & 1))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ output_unpack, float* __restrict__ packed_out) {\n  output_unpack[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = packed_out[(((((((int)blockIdx.x) / 17) * 68) + ((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 1)) % 17) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + ((((((int)blockIdx.x) % 17) * 2) + (((int)threadIdx.x) >> 1)) / 17))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(packed_out: T.Buffer((4, 2, 17, 2, 2), \"float32\"), output_unpack: T.Buffer((4, 4, 17, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for n_c_fused in T.parallel(16):\n            for h, w in T.grid(17, 2):\n                output_unpack_1 = T.Buffer((544,), data=output_unpack.data)\n                packed_out_1 = T.Buffer((544,), data=packed_out.data)\n                output_unpack_1[n_c_fused * 34 + h * 2 + w] = packed_out_1[n_c_fused // 2 * 68 + h * 4 + w * 2 + n_c_fused % 2]", "op_args": [4, 2, 17, 2]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 20736; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 192) * 48) + ((((i0_i1_fused_i2_fused_i3_fused % 192) / 48) / 2) * 24)) + ((i0_i1_fused_i2_fused_i3_fused % 48) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 3) * 48) + ((((((((int)blockIdx.x) % 3) * 4) + (((int)threadIdx.x) >> 4)) / 3) / 2) * 24)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 48) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 18, 2, 24), \"float32\"), resize: T.Buffer((6, 18, 4, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(20736):\n            resize_1 = T.Buffer((20736,), data=resize.data)\n            data_1 = T.Buffer((5184,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 192 * 48 + T.Div(i0_i1_fused_i2_fused_i3_fused % 192 // 48, 2) * 24 + T.Div(i0_i1_fused_i2_fused_i3_fused % 48, 2)]", "op_args": [6, 18, 1, 12]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 224640; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 1728) * 432) + ((((i0_i1_fused_i2_fused_i3_fused % 1728) / 24) / 2) * 12)) + ((i0_i1_fused_i2_fused_i3_fused % 24) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(60) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 60) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) / 144) * 432) + ((((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 12)) % 144) >> 1) / 2) * 12)) + ((((((int)blockIdx.x) * 12) + ((int)threadIdx.x)) % 24) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 13, 36, 12), \"float32\"), resize: T.Buffer((10, 13, 72, 24), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(224640):\n            resize_1 = T.Buffer((224640,), data=resize.data)\n            data_1 = T.Buffer((56160,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 1728 * 432 + T.Div(i0_i1_fused_i2_fused_i3_fused % 1728 // 24, 2) * 12 + T.Div(i0_i1_fused_i2_fused_i3_fused % 24, 2)]", "op_args": [10, 13, 18, 6]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 24; ++i2) {\n      for (int32_t i3 = 0; i3 < 76; ++i3) {\n        resize[(((i0_i1_fused * 1824) + (i2 * 76)) + i3)] = data[(((i0_i1_fused * 456) + ((i2 / 2) * 38)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(57) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 57) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) >> 5) * 456) + ((((((((int)blockIdx.x) & 31) * 3) + (((int)threadIdx.x) / 19)) >> 2) / 2) * 38)) + ((((((int)blockIdx.x) * 57) + ((int)threadIdx.x)) % 76) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 8, 12, 38), \"float32\"), resize: T.Buffer((7, 8, 24, 76), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(56):\n            for i2, i3 in T.grid(24, 76):\n                resize_1 = T.Buffer((102144,), data=resize.data)\n                data_1 = T.Buffer((25536,), data=data.data)\n                resize_1[i0_i1_fused * 1824 + i2 * 76 + i3] = data_1[i0_i1_fused * 456 + T.Div(i2, 2) * 38 + T.Div(i3, 2)]", "op_args": [7, 8, 6, 19]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 101376; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 4608) * 1152) + ((((i0_i1_fused_i2_fused_i3_fused % 4608) >> 6) / 2) * 32)) + ((i0_i1_fused_i2_fused_i3_fused & 63) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 96) * 1152) + ((((((((int)blockIdx.x) % 96) * 3) + (((int)threadIdx.x) >> 4)) >> 2) / 2) * 32)) + ((((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) & 63) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((11, 2, 36, 32), \"float32\"), resize: T.Buffer((11, 2, 72, 64), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(101376):\n            resize_1 = T.Buffer((101376,), data=resize.data)\n            data_1 = T.Buffer((25344,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 4608 * 1152 + T.Div(i0_i1_fused_i2_fused_i3_fused % 4608 // 64, 2) * 32 + T.Div(i0_i1_fused_i2_fused_i3_fused % 64, 2)]", "op_args": [11, 2, 18, 16]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 9; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 32; ++i2) {\n      for (int32_t i3 = 0; i3 < 72; ++i3) {\n        resize[(((i0_i1_fused * 2304) + (i2 * 72)) + i3)] = data[(((i0_i1_fused * 576) + ((i2 / 2) * 36)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 36) * 576) + ((((((((int)blockIdx.x) % 36) * 8) + (((int)threadIdx.x) >> 3)) / 9) / 2) * 36)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 72) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 1, 16, 36), \"float32\"), resize: T.Buffer((9, 1, 32, 72), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(9):\n            for i2, i3 in T.grid(32, 72):\n                resize_1 = T.Buffer((20736,), data=resize.data)\n                data_1 = T.Buffer((5184,), data=data.data)\n                resize_1[i0_i1_fused * 2304 + i2 * 72 + i3] = data_1[i0_i1_fused * 576 + T.Div(i2, 2) * 36 + T.Div(i3, 2)]", "op_args": [9, 1, 8, 18]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 64; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused >> 5) * 8) + ((((i0_i1_fused_i2_fused_i3_fused & 31) >> 2) / 2) * 2)) + ((i0_i1_fused_i2_fused_i3_fused & 3) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((int)threadIdx.x)] = data[((((((int)threadIdx.x) >> 5) * 8) + ((((((int)threadIdx.x) & 31) >> 2) / 2) * 2)) + ((((int)threadIdx.x) & 3) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 1, 4, 2), \"float32\"), resize: T.Buffer((2, 1, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(64):\n            resize_1 = T.Buffer((64,), data=resize.data)\n            data_1 = T.Buffer((16,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 32 * 8 + T.Div(i0_i1_fused_i2_fused_i3_fused % 32 // 4, 2) * 2 + T.Div(i0_i1_fused_i2_fused_i3_fused % 4, 2)]", "op_args": [2, 1, 2, 1]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 12288; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 68; ++i3) {\n      resize[((i0_i1_fused_i2_fused * 68) + i3)] = data[((((i0_i1_fused_i2_fused >> 6) * 1088) + (((i0_i1_fused_i2_fused & 63) / 2) * 34)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 68) * 1088) + ((((((((int)blockIdx.x) % 68) * 16) + (((int)threadIdx.x) >> 2)) / 17) / 2) * 34)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 68) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 12, 32, 34), \"float32\"), resize: T.Buffer((16, 12, 64, 68), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(12288):\n            for i3 in range(68):\n                resize_1 = T.Buffer((835584,), data=resize.data)\n                data_1 = T.Buffer((208896,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 68 + i3] = data_1[i0_i1_fused_i2_fused // 64 * 1088 + T.Div(i0_i1_fused_i2_fused % 64, 2) * 34 + T.Div(i3, 2)]", "op_args": [16, 12, 16, 17]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3000; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 48; ++i3) {\n      resize[((i0_i1_fused_i2_fused * 48) + i3)] = data[((((i0_i1_fused_i2_fused / 20) * 240) + (((i0_i1_fused_i2_fused % 20) / 2) * 24)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 15) * 240) + ((((((((int)blockIdx.x) % 15) * 4) + (((int)threadIdx.x) >> 4)) / 3) / 2) * 24)) + ((((((int)blockIdx.x) * 16) + ((int)threadIdx.x)) % 48) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 15, 10, 24), \"float32\"), resize: T.Buffer((10, 15, 20, 48), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3000):\n            for i3 in range(48):\n                resize_1 = T.Buffer((144000,), data=resize.data)\n                data_1 = T.Buffer((36000,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 48 + i3] = data_1[i0_i1_fused_i2_fused // 20 * 240 + T.Div(i0_i1_fused_i2_fused % 20, 2) * 24 + T.Div(i3, 2)]", "op_args": [10, 15, 5, 12]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 6528; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 192) * 48) + ((((i0_i1_fused_i2_fused_i3_fused % 192) >> 3) / 2) * 4)) + ((i0_i1_fused_i2_fused_i3_fused & 7) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) >> 3) * 48) + (((((((int)blockIdx.x) & 7) * 3) + (((int)threadIdx.x) >> 3)) / 2) * 4)) + ((((int)threadIdx.x) & 7) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((2, 17, 12, 4), \"float32\"), resize: T.Buffer((2, 17, 24, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(6528):\n            resize_1 = T.Buffer((6528,), data=resize.data)\n            data_1 = T.Buffer((1632,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 192 * 48 + T.Div(i0_i1_fused_i2_fused_i3_fused % 192 // 8, 2) * 4 + T.Div(i0_i1_fused_i2_fused_i3_fused % 8, 2)]", "op_args": [2, 17, 6, 2]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 4032; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 576) * 144) + ((((i0_i1_fused_i2_fused_i3_fused % 576) >> 3) / 2) * 4)) + ((i0_i1_fused_i2_fused_i3_fused & 7) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) >> 4) * 144) + ((((((((int)blockIdx.x) & 15) * 9) + (((int)threadIdx.x) >> 2)) >> 1) / 2) * 4)) + ((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) & 7) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 1, 36, 4), \"float32\"), resize: T.Buffer((7, 1, 72, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(4032):\n            resize_1 = T.Buffer((4032,), data=resize.data)\n            data_1 = T.Buffer((1008,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 576 * 144 + T.Div(i0_i1_fused_i2_fused_i3_fused % 576 // 8, 2) * 4 + T.Div(i0_i1_fused_i2_fused_i3_fused % 8, 2)]", "op_args": [7, 1, 18, 2]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  for (int32_t i1 = 0; i1 < 16; ++i1) {\n    for (int32_t i2 = 0; i2 < 28; ++i2) {\n      for (int32_t i3 = 0; i3 < 80; ++i3) {\n        resize[(((i1 * 2240) + (i2 * 80)) + i3)] = data[(((i1 * 560) + ((i2 / 2) * 40)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) / 56) * 560) + ((((((int)blockIdx.x) % 56) >> 1) / 2) * 40)) + ((((int)blockIdx.x) & 1) * 20)) + (((int)threadIdx.x) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 16, 14, 40), \"float32\"), resize: T.Buffer((1, 16, 28, 80), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2, i3 in T.grid(16, 28, 80):\n            resize_1 = T.Buffer((35840,), data=resize.data)\n            data_1 = T.Buffer((8960,), data=data.data)\n            resize_1[i1 * 2240 + i2 * 80 + i3] = data_1[i1 * 560 + T.Div(i2, 2) * 40 + T.Div(i3, 2)]", "op_args": [1, 16, 7, 20]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4032; ++i0_i1_fused_i2_fused) {\n    for (int32_t i3 = 0; i3 < 28; ++i3) {\n      resize[((i0_i1_fused_i2_fused * 28) + i3)] = data[((((i0_i1_fused_i2_fused >> 6) * 448) + (((i0_i1_fused_i2_fused & 63) / 2) * 14)) + (i3 / 2))];\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 28) * 448) + ((((((((int)blockIdx.x) % 28) * 16) + (((int)threadIdx.x) >> 2)) / 7) / 2) * 14)) + ((((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 28) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 9, 32, 14), \"float32\"), resize: T.Buffer((7, 9, 64, 28), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(4032):\n            for i3 in range(28):\n                resize_1 = T.Buffer((112896,), data=resize.data)\n                data_1 = T.Buffer((28224,), data=data.data)\n                resize_1[i0_i1_fused_i2_fused * 28 + i3] = data_1[i0_i1_fused_i2_fused // 64 * 448 + T.Div(i0_i1_fused_i2_fused % 64, 2) * 14 + T.Div(i3, 2)]", "op_args": [7, 9, 16, 7]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 40; ++i2) {\n        for (int32_t i3 = 0; i3 < 76; ++i3) {\n          resize[((((i0 * 57760) + (i1 * 3040)) + (i2 * 76)) + i3)] = data[((((i0 * 14440) + (i1 * 760)) + ((i2 / 2) * 38)) + (i3 / 2))];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = data[(((((((int)blockIdx.x) / 80) * 760) + ((((((int)blockIdx.x) % 80) >> 1) / 2) * 38)) + ((((int)blockIdx.x) & 1) * 19)) + (((int)threadIdx.x) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((13, 19, 20, 38), \"float32\"), resize: T.Buffer((13, 19, 40, 76), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(13):\n            for i1, i2, i3 in T.grid(19, 40, 76):\n                resize_1 = T.Buffer((750880,), data=resize.data)\n                data_1 = T.Buffer((187720,), data=data.data)\n                resize_1[i0 * 57760 + i1 * 3040 + i2 * 76 + i3] = data_1[i0 * 14440 + i1 * 760 + T.Div(i2, 2) * 38 + T.Div(i3, 2)]", "op_args": [13, 19, 10, 19]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      for (int32_t i3 = 0; i3 < 80; ++i3) {\n        resize[(((i0_i1_fused * 320) + (i2 * 80)) + i3)] = data[(((i0_i1_fused * 80) + ((i2 / 2) * 40)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) / 40) * 80) + ((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) % 40) / 10) / 2) * 40)) + ((((((int)blockIdx.x) * 56) + ((int)threadIdx.x)) % 80) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 14, 2, 40), \"float32\"), resize: T.Buffer((6, 14, 4, 80), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(84):\n            for i2, i3 in T.grid(4, 80):\n                resize_1 = T.Buffer((26880,), data=resize.data)\n                data_1 = T.Buffer((6720,), data=data.data)\n                resize_1[i0_i1_fused * 320 + i2 * 80 + i3] = data_1[i0_i1_fused * 80 + T.Div(i2, 2) * 40 + T.Div(i3, 2)]", "op_args": [6, 14, 1, 20]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        for (int32_t i3 = 0; i3 < 72; ++i3) {\n          resize[((((i0 * 7200) + (i1 * 1440)) + (i2 * 72)) + i3)] = data[((((i0 * 1800) + (i1 * 360)) + ((i2 / 2) * 36)) + (i3 / 2))];\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = data[((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) / 80) * 360) + ((((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 18)) % 80) >> 2) / 2) * 36)) + ((((((int)blockIdx.x) * 54) + ((int)threadIdx.x)) % 72) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((9, 5, 10, 36), \"float32\"), resize: T.Buffer((9, 5, 20, 72), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(9):\n            for i1, i2, i3 in T.grid(5, 20, 72):\n                resize_1 = T.Buffer((64800,), data=resize.data)\n                data_1 = T.Buffer((16200,), data=data.data)\n                resize_1[i0 * 7200 + i1 * 1440 + i2 * 72 + i3] = data_1[i0 * 1800 + i1 * 360 + T.Div(i2, 2) * 36 + T.Div(i3, 2)]", "op_args": [9, 5, 5, 18]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_i3_fused = 0; i0_i1_fused_i2_fused_i3_fused < 304640; ++i0_i1_fused_i2_fused_i3_fused) {\n    resize[i0_i1_fused_i2_fused_i3_fused] = data[((((i0_i1_fused_i2_fused_i3_fused / 2560) * 640) + ((((i0_i1_fused_i2_fused_i3_fused % 2560) / 40) / 2) * 20)) + ((i0_i1_fused_i2_fused_i3_fused % 40) / 2))];\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) >> 6) * 640) + (((((int)blockIdx.x) & 63) / 2) * 20)) + (((int)threadIdx.x) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 7, 32, 20), \"float32\"), resize: T.Buffer((17, 7, 64, 40), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused_i3_fused in T.parallel(304640):\n            resize_1 = T.Buffer((304640,), data=resize.data)\n            data_1 = T.Buffer((76160,), data=data.data)\n            resize_1[i0_i1_fused_i2_fused_i3_fused] = data_1[i0_i1_fused_i2_fused_i3_fused // 2560 * 640 + T.Div(i0_i1_fused_i2_fused_i3_fused % 2560 // 40, 2) * 20 + T.Div(i0_i1_fused_i2_fused_i3_fused % 40, 2)]", "op_args": [17, 7, 16, 10]}{"op_name": "upsampling", "c_code": "void default_function_kernel(float* data, float* resize) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 48; ++i2) {\n      for (int32_t i3 = 0; i3 < 76; ++i3) {\n        resize[(((i0_i1_fused * 3648) + (i2 * 76)) + i3)] = data[(((i0_i1_fused * 912) + ((i2 / 2) * 38)) + (i3 / 2))];\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ data, float* __restrict__ resize) {\n  resize[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = data[((((((int)blockIdx.x) / 57) * 912) + ((((((((int)blockIdx.x) % 57) * 16) + (((int)threadIdx.x) >> 2)) / 19) / 2) * 38)) + ((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) % 76) / 2))];\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 8, 24, 38), \"float32\"), resize: T.Buffer((10, 8, 48, 76), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(80):\n            for i2, i3 in T.grid(48, 76):\n                resize_1 = T.Buffer((291840,), data=resize.data)\n                data_1 = T.Buffer((72960,), data=data.data)\n                resize_1[i0_i1_fused * 3648 + i2 * 76 + i3] = data_1[i0_i1_fused * 912 + T.Div(i2, 2) * 38 + T.Div(i3, 2)]", "op_args": [10, 8, 12, 19]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    float T_multiply_red[10];\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n          T_multiply_red[ax2_1] = 0.000000e+00f;\n          for (int32_t k1 = 0; k1 < 20; ++k1) {\n            T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (data[((((ax0 * 400) + (k1 * 20)) + (ax2 * 10)) + ax2_1)] * data[((((ax0 * 400) + (k1 * 20)) + (ax2 * 10)) + ax2_1)]));\n          }\n        }\n        for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n          T_cast[((((ax0 * 400) + (ax1 * 20)) + (ax2 * 10)) + ax3)] = ((data[((((ax0 * 400) + (ax1 * 20)) + (ax2 * 10)) + ax3)] * weight[ax1]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3] * 5.000000e-02f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * weight[(((((int)blockIdx.x) % 10) * 2) + (((int)threadIdx.x) / 20))]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) / 10) * 20) + (((int)threadIdx.x) % 20))] * 5.000000e-02f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(19) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 20; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 19) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 19) + ((int)threadIdx.x)) / 20) * 400) + (k1 * 20)) + (((((int)blockIdx.x) * 19) + ((int)threadIdx.x)) % 20))] * data[((((((((int)blockIdx.x) * 19) + ((int)threadIdx.x)) / 20) * 400) + (k1 * 20)) + (((((int)blockIdx.x) * 19) + ((int)threadIdx.x)) % 20))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 20, 2, 10), \"float32\"), weight: T.Buffer((10,), \"float32\"), T_cast: T.Buffer((19, 20, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(19):\n            T_multiply_red = T.allocate([10], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(20, 2):\n                T_multiply_red_1 = T.Buffer((10,), data=T_multiply_red, align=32)\n                data_1 = T.Buffer((7600,), data=data.data)\n                for ax2_1 in range(10):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(20):\n                        cse_var_1: T.int32 = ax0 * 400 + k1 * 20 + ax2 * 10 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(10):\n                    cse_var_2: T.int32 = ax0 * 400 + ax1 * 20 + ax2 * 10 + ax3\n                    T_cast_1 = T.Buffer((7600,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax1] * T.rsqrt(T_multiply_red_1[ax3] * T.float32(0.050000000000000003) + T.float32(1.0000000000000001e-05))", "op_args": [19, 20, 2, 10]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 34; ++ax0_ax1_fused) {\n    float T_multiply_red[10];\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n        T_multiply_red[ax2_1] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 2; ++k1) {\n          T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (data[(((((ax0_ax1_fused >> 1) * 100) + (k1 * 50)) + (ax2 * 10)) + ax2_1)] * data[(((((ax0_ax1_fused >> 1) * 100) + (k1 * 50)) + (ax2 * 10)) + ax2_1)]));\n        }\n      }\n      for (int32_t ax3 = 0; ax3 < 10; ++ax3) {\n        T_cast[(((ax0_ax1_fused * 50) + (ax2 * 10)) + ax3)] = ((data[(((ax0_ax1_fused * 50) + (ax2 * 10)) + ax3)] * weight[(ax0_ax1_fused & 1)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3] * 5.000000e-01f) + 1.000000e-05f))));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 25) + ((int)threadIdx.x))] * weight[((((int)blockIdx.x) & 3) >> 1)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((int)blockIdx.x) >> 2) * 50) + ((((int)blockIdx.x) & 1) * 25)) + ((int)threadIdx.x))] * 5.000000e-01f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) < 425) {\n    T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k1 = 0; k1 < 2; ++k1) {\n    if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) < 425) {\n      T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) / 25) * 100) + (k1 * 50)) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 50))] * data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) / 25) * 100) + (k1 * 50)) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 50))]));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 2, 5, 10), \"float32\"), weight: T.Buffer((10,), \"float32\"), T_cast: T.Buffer((17, 2, 5, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(34):\n            T_multiply_red = T.allocate([10], \"float32\", \"global\")\n            for ax2 in range(5):\n                T_multiply_red_1 = T.Buffer((10,), data=T_multiply_red, align=32)\n                data_1 = T.Buffer((1700,), data=data.data)\n                for ax2_1 in range(10):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(2):\n                        cse_var_1: T.int32 = ax0_ax1_fused // 2 * 100 + k1 * 50 + ax2 * 10 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(10):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 50 + ax2 * 10 + ax3\n                    T_cast_1 = T.Buffer((1700,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused % 2] * T.rsqrt(T_multiply_red_1[ax3] * T.float32(0.5) + T.float32(1.0000000000000001e-05))", "op_args": [17, 2, 5, 10]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 75; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[20];\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_multiply_red[ax2] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 3; ++k1) {\n        T_multiply_red[ax2] = (T_multiply_red[ax2] + (data[(((((ax0_ax1_fused_ax2_fused / 15) * 300) + (k1 * 100)) + ((ax0_ax1_fused_ax2_fused % 5) * 20)) + ax2)] * data[(((((ax0_ax1_fused_ax2_fused / 15) * 300) + (k1 * 100)) + ((ax0_ax1_fused_ax2_fused % 5) * 20)) + ax2)]));\n      }\n    }\n    for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n      T_cast[((ax0_ax1_fused_ax2_fused * 20) + ax3)] = ((data[((ax0_ax1_fused_ax2_fused * 20) + ax3)] * weight[((ax0_ax1_fused_ax2_fused % 15) / 5)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3] * 3.333333e-01f) + 1.000000e-05f))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) < 125) {\n    T_cast[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 12) + (((int)threadIdx.x) >> 2)) % 75) / 25)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) / 12)) / 25) * 100) + (((((int)blockIdx.x) * 48) + ((int)threadIdx.x)) % 100))] * 3.333333e-01f) + 1.000000e-05f))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 125) {\n    T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k1 = 0; k1 < 3; ++k1) {\n    if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) < 125) {\n      T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 25) * 300) + (k1 * 100)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 100))] * data[((((((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 2)) / 25) * 300) + (k1 * 100)) + (((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 100))]));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 3, 5, 20), \"float32\"), weight: T.Buffer((20,), \"float32\"), T_cast: T.Buffer((5, 3, 5, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(75):\n            T_multiply_red = T.allocate([20], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((20,), data=T_multiply_red)\n            data_1 = T.Buffer((1500,), data=data.data)\n            for ax2 in range(20):\n                T_multiply_red_1[ax2] = T.float32(0)\n                for k1 in range(3):\n                    cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused // 15 * 300 + k1 * 100 + ax0_ax1_fused_ax2_fused % 5 * 20 + ax2\n                    T_multiply_red_1[ax2] = T_multiply_red_1[ax2] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax3 in range(20):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 20 + ax3\n                T_cast_1 = T.Buffer((1500,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused_ax2_fused % 15 // 5] * T.rsqrt(T_multiply_red_1[ax3] * T.float32(0.33333333333333331) + T.float32(1.0000000000000001e-05))", "op_args": [5, 3, 5, 20]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    float T_multiply_red[18];\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      T_multiply_red[ax1] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 15; ++k1) {\n        T_multiply_red[ax1] = (T_multiply_red[ax1] + (data[(((ax0 * 270) + (k1 * 18)) + ax1)] * data[(((ax0 * 270) + (k1 * 18)) + ax1)]));\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 15; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_cast[(((ax0 * 270) + (ax1_1 * 18)) + ax2)] = ((data[(((ax0 * 270) + (ax1_1 * 18)) + ax2)] * weight[ax1_1]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax2] * 6.666667e-02f) + 1.000000e-05f))));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  if (((((int)blockIdx.x) * 16) + (((int)threadIdx.x) >> 2)) < 405) {\n    T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) % 135) / 9)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) / 135) * 18) + (((((int)blockIdx.x) * 10) + ((int)threadIdx.x)) % 18))] * 6.666667e-02f) + 1.000000e-05f))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 27) {\n    T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k1 = 0; k1 < 15; ++k1) {\n    if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 2)) < 27) {\n      T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) / 9) * 270) + (k1 * 18)) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 18))] * data[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 1)) / 9) * 270) + (k1 * 18)) + (((((int)blockIdx.x) * 8) + ((int)threadIdx.x)) % 18))]));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 15, 18, 1), \"float32\"), weight: T.Buffer((1,), \"float32\"), T_cast: T.Buffer((6, 15, 18, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            T_multiply_red = T.allocate([18], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((18,), data=T_multiply_red)\n            data_1 = T.Buffer((1620,), data=data.data)\n            for ax1 in range(18):\n                T_multiply_red_1[ax1] = T.float32(0)\n                for k1 in range(15):\n                    cse_var_1: T.int32 = ax0 * 270 + k1 * 18 + ax1\n                    T_multiply_red_1[ax1] = T_multiply_red_1[ax1] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax1, ax2 in T.grid(15, 18):\n                cse_var_2: T.int32 = ax0 * 270 + ax1 * 18 + ax2\n                T_cast_1 = T.Buffer((1620,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax1] * T.rsqrt(T_multiply_red_1[ax2] * T.float32(0.066666666666666666) + T.float32(1.0000000000000001e-05))", "op_args": [6, 15, 18, 1]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    float T_multiply_red[5];\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      T_multiply_red[ax1] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 6; ++k1) {\n        T_multiply_red[ax1] = (T_multiply_red[ax1] + (data[(((ax0 * 30) + (k1 * 5)) + ax1)] * data[(((ax0 * 30) + (k1 * 5)) + ax1)]));\n      }\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 6; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_cast[(((ax0 * 30) + (ax1_1 * 5)) + ax2)] = ((data[(((ax0 * 30) + (ax1_1 * 5)) + ax2)] * weight[ax1_1]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax2] * 1.666667e-01f) + 1.000000e-05f))));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  if (((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) < 225) {\n    T_cast[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 30) / 5)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 32) + (((int)threadIdx.x) >> 1)) / 15) * 5) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5))] * 1.666667e-01f) + 1.000000e-05f))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 75) {\n    T_multiply_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = 0.000000e+00f;\n  }\n  for (int k1 = 0; k1 < 6; ++k1) {\n    if (((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) < 75) {\n      T_multiply_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5) * 30) + (k1 * 5)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5))] * data[((((((((int)blockIdx.x) * 64) + ((int)threadIdx.x)) / 5) * 30) + (k1 * 5)) + (((((int)blockIdx.x) * 4) + ((int)threadIdx.x)) % 5))]));\n    }\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((15, 6, 5, 1), \"float32\"), weight: T.Buffer((1,), \"float32\"), T_cast: T.Buffer((15, 6, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(15):\n            T_multiply_red = T.allocate([5], \"float32\", \"global\")\n            T_multiply_red_1 = T.Buffer((5,), data=T_multiply_red, align=16)\n            data_1 = T.Buffer((450,), data=data.data)\n            for ax1 in range(5):\n                T_multiply_red_1[ax1] = T.float32(0)\n                for k1 in range(6):\n                    cse_var_1: T.int32 = ax0 * 30 + k1 * 5 + ax1\n                    T_multiply_red_1[ax1] = T_multiply_red_1[ax1] + data_1[cse_var_1] * data_1[cse_var_1]\n            for ax1, ax2 in T.grid(6, 5):\n                cse_var_2: T.int32 = ax0 * 30 + ax1 * 5 + ax2\n                T_cast_1 = T.Buffer((450,), data=T_cast.data)\n                T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax1] * T.rsqrt(T_multiply_red_1[ax2] * T.float32(0.16666666666666666) + T.float32(1.0000000000000001e-05))", "op_args": [15, 6, 5, 1]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    float T_multiply_red[1];\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 3; ++ax3) {\n          T_multiply_red[0] = 0.000000e+00f;\n          for (int32_t k1 = 0; k1 < 16; ++k1) {\n            T_multiply_red[0] = (T_multiply_red[0] + (data[((((ax0 * 432) + (k1 * 27)) + (ax2 * 3)) + ax3)] * data[((((ax0 * 432) + (k1 * 27)) + (ax2 * 3)) + ax3)]));\n          }\n          T_cast[((((ax0 * 432) + (ax1 * 27)) + (ax2 * 3)) + ax3)] = ((data[((((ax0 * 432) + (ax1 * 27)) + (ax2 * 3)) + ax3)] * weight[ax1]) * (1.000000e+00f / sqrtf(((T_multiply_red[0] * 6.250000e-02f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 32) + ((int)threadIdx.x)) % 432) / 27)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) / 27) * 27) + (((((((int)blockIdx.x) * 5) + ((int)threadIdx.x)) % 27) / 3) * 3)) + (((((int)blockIdx.x) * 2) + ((int)threadIdx.x)) % 3))] * 6.250000e-02f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(54) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 16; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 54) + ((int)threadIdx.x))] + (data[((((((int)blockIdx.x) * 864) + ((((int)threadIdx.x) / 27) * 432)) + (k1 * 27)) + (((int)threadIdx.x) % 27))] * data[((((((int)blockIdx.x) * 864) + ((((int)threadIdx.x) / 27) * 432)) + (k1 * 27)) + (((int)threadIdx.x) % 27))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((16, 16, 9, 3), \"float32\"), weight: T.Buffer((3,), \"float32\"), T_cast: T.Buffer((16, 16, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(16):\n            T_multiply_red = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2, ax3 in T.grid(16, 9, 3):\n                cse_var_1: T.int32 = ax0 * 432 + ax1 * 27 + ax2 * 3 + ax3\n                T_multiply_red_1 = T.Buffer((1,), data=T_multiply_red, align=4)\n                T_multiply_red_1[0] = T.float32(0)\n                data_1 = T.Buffer((6912,), data=data.data)\n                for k1 in range(16):\n                    cse_var_2: T.int32 = ax0 * 432 + k1 * 27 + ax2 * 3 + ax3\n                    T_multiply_red_1[0] = T_multiply_red_1[0] + data_1[cse_var_2] * data_1[cse_var_2]\n                T_cast_1 = T.Buffer((6912,), data=T_cast.data)\n                T_cast_1[cse_var_1] = data_1[cse_var_1] * weight[ax1] * T.rsqrt(T_multiply_red_1[0] * T.float32(0.0625) + T.float32(1.0000000000000001e-05))", "op_args": [16, 16, 9, 3]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    float T_multiply_red[8];\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n        T_multiply_red[ax2_1] = 0.000000e+00f;\n        for (int32_t k1 = 0; k1 < 7; ++k1) {\n          T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (data[(((((ax0_ax1_fused / 7) * 1064) + (k1 * 152)) + (ax2 * 8)) + ax2_1)] * data[(((((ax0_ax1_fused / 7) * 1064) + (k1 * 152)) + (ax2 * 8)) + ax2_1)]));\n        }\n      }\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        T_cast[(((ax0_ax1_fused * 152) + (ax2 * 8)) + ax3)] = ((data[(((ax0_ax1_fused * 152) + (ax2 * 8)) + ax3)] * weight[(ax0_ax1_fused % 7)]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3] * 1.428571e-01f) + 1.000000e-05f))));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 133) / 19)]) * (1.000000e+00f / sqrtf(((T_multiply_red[((((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) / 133) * 152) + ((((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) % 19) * 8)) + (((int)threadIdx.x) & 7))] * 1.428571e-01f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 7; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) >> 2) * 1064) + (k1 * 152)) + ((((int)blockIdx.x) & 3) * 38)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) >> 2) * 1064) + (k1 * 152)) + ((((int)blockIdx.x) & 3) * 38)) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 7, 19, 8), \"float32\"), weight: T.Buffer((8,), \"float32\"), T_cast: T.Buffer((12, 7, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(84):\n            T_multiply_red = T.allocate([8], \"float32\", \"global\")\n            for ax2 in range(19):\n                T_multiply_red_1 = T.Buffer((8,), data=T_multiply_red, align=32)\n                data_1 = T.Buffer((12768,), data=data.data)\n                for ax2_1 in range(8):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(7):\n                        cse_var_1: T.int32 = ax0_ax1_fused // 7 * 1064 + k1 * 152 + ax2 * 8 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3 in range(8):\n                    cse_var_2: T.int32 = ax0_ax1_fused * 152 + ax2 * 8 + ax3\n                    T_cast_1 = T.Buffer((12768,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax0_ax1_fused % 7] * T.rsqrt(T_multiply_red_1[ax3] * T.float32(0.14285714285714285) + T.float32(1.0000000000000001e-05))", "op_args": [12, 7, 19, 8]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 784; ++ax0_ax1_fused_ax2_fused) {\n    float T_multiply_red[1];\n    for (int32_t ax3 = 0; ax3 < 11; ++ax3) {\n      T_multiply_red[0] = 0.000000e+00f;\n      for (int32_t k1 = 0; k1 < 14; ++k1) {\n        T_multiply_red[0] = (T_multiply_red[0] + (data[(((((ax0_ax1_fused_ax2_fused / 112) * 1232) + (k1 * 88)) + ((ax0_ax1_fused_ax2_fused & 7) * 11)) + ax3)] * data[(((((ax0_ax1_fused_ax2_fused / 112) * 1232) + (k1 * 88)) + ((ax0_ax1_fused_ax2_fused & 7) * 11)) + ax3)]));\n      }\n      T_cast[((ax0_ax1_fused_ax2_fused * 11) + ax3)] = ((data[((ax0_ax1_fused_ax2_fused * 11) + ax3)] * weight[((ax0_ax1_fused_ax2_fused % 112) >> 3)]) * (1.000000e+00f / sqrtf(((T_multiply_red[0] * 7.142857e-02f) + 1.000000e-05f))));\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(28) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 28) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) % 44) * 7) + (((int)threadIdx.x) >> 2)) / 22)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((int)blockIdx.x) / 44) * 88) + (((((int)blockIdx.x) * 28) + ((int)threadIdx.x)) % 88))] * 7.142857e-02f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(56) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 14; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] + (data[((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) / 11) * 1232) + (k1 * 88)) + (((((int)blockIdx.x) * 56) + ((int)threadIdx.x)) % 88))] * data[((((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) / 11) * 1232) + (k1 * 88)) + (((((int)blockIdx.x) * 56) + ((int)threadIdx.x)) % 88))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((7, 14, 8, 11), \"float32\"), weight: T.Buffer((11,), \"float32\"), T_cast: T.Buffer((7, 14, 8, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(784):\n            T_multiply_red = T.allocate([1], \"float32\", \"global\")\n            for ax3 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 11 + ax3\n                T_multiply_red_1 = T.Buffer((1,), data=T_multiply_red, align=4)\n                T_multiply_red_1[0] = T.float32(0)\n                data_1 = T.Buffer((8624,), data=data.data)\n                for k1 in range(14):\n                    cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused // 112 * 1232 + k1 * 88 + ax0_ax1_fused_ax2_fused % 8 * 11 + ax3\n                    T_multiply_red_1[0] = T_multiply_red_1[0] + data_1[cse_var_2] * data_1[cse_var_2]\n                T_cast_1 = T.Buffer((8624,), data=T_cast.data)\n                T_cast_1[cse_var_1] = data_1[cse_var_1] * weight[ax0_ax1_fused_ax2_fused % 112 // 8] * T.rsqrt(T_multiply_red_1[0] * T.float32(0.071428571428571425) + T.float32(1.0000000000000001e-05))", "op_args": [7, 14, 8, 11]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    float T_multiply_red[16];\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax2_1 = 0; ax2_1 < 16; ++ax2_1) {\n          T_multiply_red[ax2_1] = 0.000000e+00f;\n          for (int32_t k1 = 0; k1 < 6; ++k1) {\n            T_multiply_red[ax2_1] = (T_multiply_red[ax2_1] + (data[((((ax0 * 192) + (k1 * 32)) + (ax2 * 16)) + ax2_1)] * data[((((ax0 * 192) + (k1 * 32)) + (ax2 * 16)) + ax2_1)]));\n          }\n        }\n        for (int32_t ax3_s = 0; ax3_s < 16; ++ax3_s) {\n          T_cast[((((ax0 * 192) + (ax1 * 32)) + (ax2 * 16)) + ax3_s)] = ((data[((((ax0 * 192) + (ax1 * 32)) + (ax2 * 16)) + ax3_s)] * weight[ax1]) * (1.000000e+00f / sqrtf(((T_multiply_red[ax3_s] * 1.666667e-01f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 9) + (((int)threadIdx.x) >> 1)) % 96) >> 4)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 3) + (((int)threadIdx.x) / 6)) >> 5) * 32) + (((((int)blockIdx.x) * 18) + ((int)threadIdx.x)) & 31))] * 1.666667e-01f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 6; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) >> 1) * 192) + (k1 * 32)) + ((((int)blockIdx.x) & 1) * 16)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) >> 1) * 192) + (k1 * 32)) + ((((int)blockIdx.x) & 1) * 16)) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((6, 6, 2, 16), \"float32\"), weight: T.Buffer((16,), \"float32\"), T_cast: T.Buffer((6, 6, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(6):\n            T_multiply_red = T.allocate([16], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(6, 2):\n                T_multiply_red_1 = T.Buffer((16,), data=T_multiply_red)\n                data_1 = T.Buffer((1152,), data=data.data)\n                for ax2_1 in range(16):\n                    T_multiply_red_1[ax2_1] = T.float32(0)\n                    for k1 in range(6):\n                        cse_var_1: T.int32 = ax0 * 192 + k1 * 32 + ax2 * 16 + ax2_1\n                        T_multiply_red_1[ax2_1] = T_multiply_red_1[ax2_1] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax3_s in range(16):\n                    cse_var_2: T.int32 = ax0 * 192 + ax1 * 32 + ax2 * 16 + ax3_s\n                    T_cast_1 = T.Buffer((1152,), data=T_cast.data)\n                    T_cast_1[cse_var_2] = data_1[cse_var_2] * weight[ax1] * T.rsqrt(T_multiply_red_1[ax3_s] * T.float32(0.16666666666666666) + T.float32(1.0000000000000001e-05))", "op_args": [6, 6, 2, 16]}{"op_name": "rms_norm", "c_code": "void default_function_kernel(float* T_cast, float* data, float* weight) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    float T_multiply_red[272];\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax1_1 = 0; ax1_1 < 17; ++ax1_1) {\n        for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n          T_multiply_red[((ax1_1 * 16) + ax2)] = 0.000000e+00f;\n          for (int32_t k1 = 0; k1 < 9; ++k1) {\n            T_multiply_red[((ax1_1 * 16) + ax2)] = (T_multiply_red[((ax1_1 * 16) + ax2)] + (data[((((ax0 * 2448) + (k1 * 272)) + (ax1_1 * 16)) + ax2)] * data[((((ax0 * 2448) + (k1 * 272)) + (ax1_1 * 16)) + ax2)]));\n          }\n        }\n      }\n      for (int32_t ax2_1 = 0; ax2_1 < 17; ++ax2_1) {\n        for (int32_t ax3_s = 0; ax3_s < 16; ++ax3_s) {\n          T_cast[((((ax0 * 2448) + (ax1 * 272)) + (ax2_1 * 16)) + ax3_s)] = ((data[((((ax0 * 2448) + (ax1 * 272)) + (ax2_1 * 16)) + ax3_s)] * weight[ax1]) * (1.000000e+00f / sqrtf(((T_multiply_red[((ax2_1 * 16) + ax3_s)] * 1.111111e-01f) + 1.000000e-05f))));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(56) default_function_kernel_1(float* __restrict__ T_cast, float* __restrict__ T_multiply_red, float* __restrict__ data, float* __restrict__ weight) {\n  T_cast[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 56) + ((int)threadIdx.x))] * weight[((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) % 306) / 34)]) * (1.000000e+00f / sqrtf(((T_multiply_red[(((((((int)blockIdx.x) * 7) + (((int)threadIdx.x) >> 3)) / 306) * 272) + (((((int)blockIdx.x) * 56) + ((int)threadIdx.x)) % 272))] * 1.111111e-01f) + 1.000000e-05f))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply_red, float* __restrict__ data) {\n  T_multiply_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = 0.000000e+00f;\n  for (int k1 = 0; k1 < 9; ++k1) {\n    T_multiply_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (T_multiply_red[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (data[(((((((int)blockIdx.x) / 17) * 2448) + (k1 * 272)) + ((((int)blockIdx.x) % 17) * 16)) + ((int)threadIdx.x))] * data[(((((((int)blockIdx.x) / 17) * 2448) + (k1 * 272)) + ((((int)blockIdx.x) % 17) * 16)) + ((int)threadIdx.x))]));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((14, 9, 17, 16), \"float32\"), weight: T.Buffer((16,), \"float32\"), T_cast: T.Buffer((14, 9, 17, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(14):\n            T_multiply_red = T.allocate([272], \"float32\", \"global\")\n            for ax1 in range(9):\n                T_multiply_red_1 = T.Buffer((272,), data=T_multiply_red)\n                data_1 = T.Buffer((34272,), data=data.data)\n                for ax1_1, ax2 in T.grid(17, 16):\n                    T_multiply_red_1[ax1_1 * 16 + ax2] = T.float32(0)\n                    for k1 in range(9):\n                        cse_var_3: T.int32 = ax1_1 * 16\n                        cse_var_2: T.int32 = cse_var_3 + ax2\n                        cse_var_1: T.int32 = ax0 * 2448 + k1 * 272 + cse_var_3 + ax2\n                        T_multiply_red_1[cse_var_2] = T_multiply_red_1[cse_var_2] + data_1[cse_var_1] * data_1[cse_var_1]\n                for ax2, ax3_s in T.grid(17, 16):\n                    cse_var_5: T.int32 = ax2 * 16\n                    cse_var_4: T.int32 = ax0 * 2448 + ax1 * 272 + cse_var_5 + ax3_s\n                    T_cast_1 = T.Buffer((34272,), data=T_cast.data)\n                    T_cast_1[cse_var_4] = data_1[cse_var_4] * weight[ax1] * T.rsqrt(T_multiply_red_1[cse_var_5 + ax3_s] * T.float32(0.1111111111111111) + T.float32(1.0000000000000001e-05))", "op_args": [14, 9, 17, 16]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      for (int32_t ax3 = 0; ax3 < 8; ++ax3) {\n        T_divide[(((ax0 * 32) + (ax2 * 8)) + ax3)] = ((data[(((ax0 * 32) + (ax2 * 8)) + ax3)] - moving_mean[0]) / sqrtf((moving_var[0] + 1.000000e-05f)));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] - moving_mean[0]) / sqrtf((moving_var[0] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((12, 1, 4, 8), \"float32\"), gamma: T.Buffer((1,), \"float32\"), beta: T.Buffer((1,), \"float32\"), moving_mean: T.Buffer((1,), \"float32\"), moving_var: T.Buffer((1,), \"float32\"), T_divide: T.Buffer((12, 1, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(12):\n            for ax2, ax3 in T.grid(4, 8):\n                cse_var_1: T.int32 = ax0 * 32 + ax2 * 8 + ax3\n                T_divide_1 = T.Buffer((384,), data=T_divide.data)\n                data_1 = T.Buffer((384,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - moving_mean[0]) / T.sqrt(moving_var[0] + T.float32(1.0000000000000001e-05))", "op_args": [12, 1, 4, 8]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    float T_reshape[20];\n    float T_reshape_1[20];\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      T_reshape[ax1] = moving_mean[ax1];\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 20; ++ax1_1) {\n      T_reshape_1[ax1_1] = moving_var[ax1_1];\n    }\n    for (int32_t ax1_2 = 0; ax1_2 < 20; ++ax1_2) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 9; ++ax3) {\n          T_divide[((((ax0 * 3240) + (ax1_2 * 162)) + (ax2 * 9)) + ax3)] = ((data[((((ax0 * 3240) + (ax1_2 * 162)) + (ax2 * 9)) + ax3)] - T_reshape[ax1_2]) / sqrtf((T_reshape_1[ax1_2] + 1.000000e-05f)));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 81) * 20) + (((int)threadIdx.x) >> 1)) / 81)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 81) * 20) + (((int)threadIdx.x) >> 1)) / 81)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((4, 20, 18, 9), \"float32\"), gamma: T.Buffer((20,), \"float32\"), beta: T.Buffer((20,), \"float32\"), moving_mean: T.Buffer((20,), \"float32\"), moving_var: T.Buffer((20,), \"float32\"), T_divide: T.Buffer((4, 20, 18, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(4):\n            T_reshape = T.allocate([20], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([20], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((20,), data=T_reshape)\n            for ax1 in range(20):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            T_reshape_3 = T.Buffer((20,), data=T_reshape_1)\n            for ax1 in range(20):\n                T_reshape_3[ax1] = moving_var[ax1]\n            for ax1, ax2, ax3 in T.grid(20, 18, 9):\n                cse_var_1: T.int32 = ax0 * 3240 + ax1 * 162 + ax2 * 9 + ax3\n                T_divide_1 = T.Buffer((12960,), data=T_divide.data)\n                data_1 = T.Buffer((12960,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[ax1]) / T.sqrt(T_reshape_3[ax1] + T.float32(1.0000000000000001e-05))", "op_args": [4, 20, 18, 9]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    float T_reshape[7];\n    float T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      T_reshape[ax1] = moving_mean[ax1];\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 7; ++ax1_1) {\n      T_reshape_1[0] = moving_var[ax1_1];\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 18; ++ax3) {\n          T_divide[((((ax0 * 252) + (ax1_1 * 36)) + (ax2 * 18)) + ax3)] = ((data[((((ax0 * 252) + (ax1_1 * 36)) + (ax2 * 18)) + ax3)] - T_reshape[ax1_1]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(45) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 45) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 9)) % 28) >> 2)]) / sqrtf((moving_var[((((((int)blockIdx.x) * 5) + (((int)threadIdx.x) / 9)) % 28) >> 2)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((10, 7, 2, 18), \"float32\"), gamma: T.Buffer((7,), \"float32\"), beta: T.Buffer((7,), \"float32\"), moving_mean: T.Buffer((7,), \"float32\"), moving_var: T.Buffer((7,), \"float32\"), T_divide: T.Buffer((10, 7, 2, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(10):\n            T_reshape = T.allocate([7], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((7,), data=T_reshape, align=16)\n            for ax1 in range(7):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            for ax1 in range(7):\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax1]\n                for ax2, ax3 in T.grid(2, 18):\n                    cse_var_1: T.int32 = ax0 * 252 + ax1 * 36 + ax2 * 18 + ax3\n                    T_divide_1 = T.Buffer((2520,), data=T_divide.data)\n                    data_1 = T.Buffer((2520,), data=data.data)\n                    T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[ax1]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))", "op_args": [10, 7, 2, 18]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_ax3_fused = 0; ax0_ax1_fused_ax2_fused_ax3_fused < 7220; ++ax0_ax1_fused_ax2_fused_ax3_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused_ax3_fused] = ((data[ax0_ax1_fused_ax2_fused_ax3_fused] - moving_mean[0]) / sqrtf((moving_var[0] + 1.000000e-05f)));\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(38) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 38) + ((int)threadIdx.x))] - moving_mean[0]) / sqrtf((moving_var[0] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((19, 1, 20, 19), \"float32\"), gamma: T.Buffer((1,), \"float32\"), beta: T.Buffer((1,), \"float32\"), moving_mean: T.Buffer((1,), \"float32\"), moving_var: T.Buffer((1,), \"float32\"), T_divide: T.Buffer((19, 1, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused_ax3_fused in T.parallel(7220):\n            T_divide_1 = T.Buffer((7220,), data=T_divide.data)\n            data_1 = T.Buffer((7220,), data=data.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused_ax3_fused] = (data_1[ax0_ax1_fused_ax2_fused_ax3_fused] - moving_mean[0]) / T.sqrt(moving_var[0] + T.float32(1.0000000000000001e-05))", "op_args": [19, 1, 20, 19]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    float T_reshape[12];\n    float T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      T_reshape[ax1] = moving_var[ax1];\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      T_reshape_1[0] = moving_mean[ax1_1];\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 17; ++ax3) {\n          T_divide[((((ax0 * 1020) + (ax1_1 * 85)) + (ax2 * 17)) + ax3)] = ((data[((((ax0 * 1020) + (ax1_1 * 85)) + (ax2 * 17)) + ax3)] - T_reshape_1[0]) / sqrtf((T_reshape[ax1_1] + 1.000000e-05f)));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(34) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 34) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) % 30) * 2) + (((int)threadIdx.x) / 17)) / 5)]) / sqrtf((moving_var[((((((int)blockIdx.x) % 30) * 2) + (((int)threadIdx.x) / 17)) / 5)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 12, 5, 17), \"float32\"), gamma: T.Buffer((12,), \"float32\"), beta: T.Buffer((12,), \"float32\"), moving_mean: T.Buffer((12,), \"float32\"), moving_var: T.Buffer((12,), \"float32\"), T_divide: T.Buffer((17, 12, 5, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(17):\n            T_reshape = T.allocate([12], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((12,), data=T_reshape, align=32)\n            for ax1 in range(12):\n                T_reshape_2[ax1] = moving_var[ax1]\n            for ax1 in range(12):\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_mean[ax1]\n                for ax2, ax3 in T.grid(5, 17):\n                    cse_var_1: T.int32 = ax0 * 1020 + ax1 * 85 + ax2 * 17 + ax3\n                    T_divide_1 = T.Buffer((17340,), data=T_divide.data)\n                    data_1 = T.Buffer((17340,), data=data.data)\n                    T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_3[0]) / T.sqrt(T_reshape_2[ax1] + T.float32(1.0000000000000001e-05))", "op_args": [17, 12, 5, 17]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    float T_reshape[17];\n    float T_reshape_1[1];\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      T_reshape[ax1] = moving_mean[ax1];\n    }\n    for (int32_t ax1_1 = 0; ax1_1 < 17; ++ax1_1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        for (int32_t ax3 = 0; ax3 < 2; ++ax3) {\n          T_reshape_1[0] = moving_var[ax1_1];\n          T_divide[((((ax0 * 272) + (ax1_1 * 16)) + (ax2 * 2)) + ax3)] = ((data[((((ax0 * 272) + (ax1_1 * 16)) + (ax2 * 2)) + ax3)] - T_reshape[ax1_1]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n        }\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) < 289) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - moving_mean[(((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) % 17)]) / sqrtf((moving_var[(((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 4)) % 17)] + 1.000000e-05f)));\n  }\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((17, 17, 8, 2), \"float32\"), gamma: T.Buffer((17,), \"float32\"), beta: T.Buffer((17,), \"float32\"), moving_mean: T.Buffer((17,), \"float32\"), moving_var: T.Buffer((17,), \"float32\"), T_divide: T.Buffer((17, 17, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(17):\n            T_reshape = T.allocate([17], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            T_reshape_2 = T.Buffer((17,), data=T_reshape)\n            for ax1 in range(17):\n                T_reshape_2[ax1] = moving_mean[ax1]\n            for ax1, ax2, ax3 in T.grid(17, 8, 2):\n                cse_var_1: T.int32 = ax0 * 272 + ax1 * 16 + ax2 * 2 + ax3\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax1]\n                T_divide_1 = T.Buffer((4624,), data=T_divide.data)\n                data_1 = T.Buffer((4624,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[ax1]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))", "op_args": [17, 17, 8, 2]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  float T_reshape[1];\n  float T_reshape_1[1];\n  for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_reshape[0] = moving_mean[ax1];\n      for (int32_t ax3 = 0; ax3 < 11; ++ax3) {\n        T_reshape_1[0] = moving_var[ax1];\n        T_divide[(((ax1 * 220) + (ax2 * 11)) + ax3)] = ((data[(((ax1 * 220) + (ax2 * 11)) + ax3)] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(55) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 55) + ((int)threadIdx.x))] - moving_mean[(((int)blockIdx.x) >> 2)]) / sqrtf((moving_var[(((int)blockIdx.x) >> 2)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((1, 14, 20, 11), \"float32\"), gamma: T.Buffer((14,), \"float32\"), beta: T.Buffer((14,), \"float32\"), moving_mean: T.Buffer((14,), \"float32\"), moving_var: T.Buffer((14,), \"float32\"), T_divide: T.Buffer((1, 14, 20, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_reshape = T.allocate([1], \"float32\", \"global\")\n        T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n        for ax1, ax2 in T.grid(14, 20):\n            T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n            T_reshape_2[0] = moving_mean[ax1]\n            for ax3 in range(11):\n                cse_var_1: T.int32 = ax1 * 220 + ax2 * 11 + ax3\n                T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                T_reshape_3[0] = moving_var[ax1]\n                T_divide_1 = T.Buffer((3080,), data=T_divide.data)\n                data_1 = T.Buffer((3080,), data=data.data)\n                T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))", "op_args": [1, 14, 20, 11]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_reshape[0] = moving_mean[(ax0_ax1_fused % 3)];\n      for (int32_t ax3 = 0; ax3 < 20; ++ax3) {\n        T_reshape_1[0] = moving_var[(ax0_ax1_fused % 3)];\n        T_divide[(((ax0_ax1_fused * 200) + (ax2 * 20)) + ax3)] = ((data[(((ax0_ax1_fused * 200) + (ax2 * 20)) + ax3)] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(50) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] - moving_mean[((((int)blockIdx.x) % 12) >> 2)]) / sqrtf((moving_var[((((int)blockIdx.x) % 12) >> 2)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((5, 3, 10, 20), \"float32\"), gamma: T.Buffer((3,), \"float32\"), beta: T.Buffer((3,), \"float32\"), moving_mean: T.Buffer((3,), \"float32\"), moving_var: T.Buffer((3,), \"float32\"), T_divide: T.Buffer((5, 3, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(15):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            for ax2 in range(10):\n                T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n                T_reshape_2[0] = moving_mean[ax0_ax1_fused % 3]\n                for ax3 in range(20):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 200 + ax2 * 20 + ax3\n                    T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                    T_reshape_3[0] = moving_var[ax0_ax1_fused % 3]\n                    T_divide_1 = T.Buffer((3000,), data=T_divide.data)\n                    data_1 = T.Buffer((3000,), data=data.data)\n                    T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))", "op_args": [5, 3, 10, 20]}{"op_name": "batch_norm", "c_code": "void default_function_kernel(float* T_divide, float* data, float* moving_mean, float* moving_var) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 240; ++ax0_ax1_fused) {\n    float T_reshape[1];\n    float T_reshape_1[1];\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_reshape[0] = moving_mean[(ax0_ax1_fused % 12)];\n      for (int32_t ax3 = 0; ax3 < 4; ++ax3) {\n        T_reshape_1[0] = moving_var[(ax0_ax1_fused % 12)];\n        T_divide[(((ax0_ax1_fused * 20) + (ax2 * 4)) + ax3)] = ((data[(((ax0_ax1_fused * 20) + (ax2 * 4)) + ax3)] - T_reshape[0]) / sqrtf((T_reshape_1[0] + 1.000000e-05f)));\n      }\n    }\n  }\n}\n\n", "cuda_code": "extern \"C\" __global__ void __launch_bounds__(30) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ data, float* __restrict__ moving_mean, float* __restrict__ moving_var) {\n  T_divide[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] = ((data[((((int)blockIdx.x) * 30) + ((int)threadIdx.x))] - moving_mean[((((((int)blockIdx.x) & 7) * 3) + (((int)threadIdx.x) / 10)) >> 1)]) / sqrtf((moving_var[((((((int)blockIdx.x) & 7) * 3) + (((int)threadIdx.x) / 10)) >> 1)] + 1.000000e-05f)));\n}\n\n", "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(data: T.Buffer((20, 12, 5, 4), \"float32\"), gamma: T.Buffer((12,), \"float32\"), beta: T.Buffer((12,), \"float32\"), moving_mean: T.Buffer((12,), \"float32\"), moving_var: T.Buffer((12,), \"float32\"), T_divide: T.Buffer((20, 12, 5, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(240):\n            T_reshape = T.allocate([1], \"float32\", \"global\")\n            T_reshape_1 = T.allocate([1], \"float32\", \"global\")\n            for ax2 in range(5):\n                T_reshape_2 = T.Buffer((1,), data=T_reshape, align=4)\n                T_reshape_2[0] = moving_mean[ax0_ax1_fused % 12]\n                for ax3 in range(4):\n                    cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2 * 4 + ax3\n                    T_reshape_3 = T.Buffer((1,), data=T_reshape_1, align=4)\n                    T_reshape_3[0] = moving_var[ax0_ax1_fused % 12]\n                    T_divide_1 = T.Buffer((4800,), data=T_divide.data)\n                    data_1 = T.Buffer((4800,), data=data.data)\n                    T_divide_1[cse_var_1] = (data_1[cse_var_1] - T_reshape_2[0]) / T.sqrt(T_reshape_3[0] + T.float32(1.0000000000000001e-05))", "op_args": [20, 12, 5, 4]}