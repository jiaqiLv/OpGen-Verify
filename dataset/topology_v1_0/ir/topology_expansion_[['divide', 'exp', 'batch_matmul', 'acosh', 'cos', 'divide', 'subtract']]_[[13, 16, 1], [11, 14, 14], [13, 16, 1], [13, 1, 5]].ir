# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(ph_0: T.Buffer((13, 16, 1), "float32"), ph_3: T.Buffer((13, 16, 1), "float32"), ph_6: T.Buffer((13, 1, 5), "float32"), compute: T.Buffer((13, 16, 1), "float32"), compute_1: T.Buffer((13, 16, 5), "float32"), compute_2: T.Buffer((13, 16, 1), "float32"), T_divide: T.Buffer((13, 16, 1), "float32"), T_subtract: T.Buffer((13, 16, 1), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        auto_scheduler_layout_transform = T.allocate([65], "float32", "global")
        ph_0_1 = T.Buffer((208,), data=ph_0.data)
        for i0_i1_fused_i2_fused in T.parallel(208):
            compute_3 = T.Buffer((208,), data=compute.data)
            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])
        auto_scheduler_layout_transform_1 = T.Buffer((65,), data=auto_scheduler_layout_transform)
        for ax0_ax1_fused_ax2_fused in T.parallel(5):
            for ax4 in range(13):
                ph_6_1 = T.Buffer((65,), data=ph_6.data)
                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 13 + ax4] = ph_6_1[ax4 * 5 + ax0_ax1_fused_ax2_fused]
        for i0_outer_i1_outer_fused_i2_outer_fused in T.parallel(10):
            T_batch_matmul_NN = T.allocate([104], "float32", "global")
            T_batch_matmul_NN_1 = T.Buffer((104,), data=T_batch_matmul_NN)
            for b_outer_outer_inner, i_outer_outer_inner in T.grid(13, 2):
                for i_outer_inner_init in range(4):
                    T_batch_matmul_NN_1[b_outer_outer_inner * 8 + i_outer_outer_inner * 4 + i_outer_inner_init] = T.float32(0)
                for i_outer_inner in range(4):
                    cse_var_2: T.int32 = i_outer_outer_inner * 4
                    cse_var_1: T.int32 = b_outer_outer_inner * 8 + cse_var_2 + i_outer_inner
                    T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_outer_inner * 16 + i0_outer_i1_outer_fused_i2_outer_fused // 5 * 8 + cse_var_2 + i_outer_inner] * auto_scheduler_layout_transform_1[i0_outer_i1_outer_fused_i2_outer_fused % 5 * 13 + b_outer_outer_inner]
            for i0_inner, i1_inner in T.grid(13, 8):
                compute_3 = T.Buffer((1040,), data=compute_1.data)
                compute_3[i0_inner * 80 + i0_outer_i1_outer_fused_i2_outer_fused // 5 * 40 + i1_inner * 5 + i0_outer_i1_outer_fused_i2_outer_fused % 5] = T.acosh(T_batch_matmul_NN_1[i0_inner * 8 + i1_inner])
        ph_3_1 = T.Buffer((208,), data=ph_3.data)
        for i0_i1_fused_i2_fused in T.parallel(208):
            compute_3 = T.Buffer((208,), data=compute_2.data)
            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])
        for ax0_ax1_fused_ax2_fused in T.parallel(208):
            T_divide_1 = T.Buffer((208,), data=T_divide.data)
            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] / ph_0_1[ax0_ax1_fused_ax2_fused]
        for ax0_ax1_fused_ax2_fused in T.parallel(208):
            T_subtract_1 = T.Buffer((208,), data=T_subtract.data)
            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]