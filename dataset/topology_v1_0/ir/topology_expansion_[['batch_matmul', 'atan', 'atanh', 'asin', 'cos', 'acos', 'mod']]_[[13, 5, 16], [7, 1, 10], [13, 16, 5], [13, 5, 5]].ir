# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(ph_0: T.Buffer((13, 5, 16), "float32"), ph_3: T.Buffer((13, 16, 5), "float32"), ph_10: T.Buffer((13, 5, 5), "float32"), compute: T.Buffer((13, 5, 16), "float32"), compute_1: T.Buffer((13, 5, 16), "float32"), compute_2: T.Buffer((13, 5, 16), "float32"), compute_3: T.Buffer((13, 5, 5), "float32"), T_mod: T.Buffer((13, 5, 5), "float32")):
        T.func_attr({"from_legacy_te_schedule": T.bool(True), "tir.noalias": T.bool(True)})
        auto_scheduler_layout_transform = T.allocate([1040], "float32", "global")
        T_batch_matmul_NN = T.allocate([325], "float32", "global")
        ph_0_1 = T.Buffer((1040,), data=ph_0.data)
        for i0_i1_fused_i2_fused in T.parallel(1040):
            compute_4 = T.Buffer((1040,), data=compute.data)
            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])
        for i0_i1_fused_i2_fused in T.parallel(1040):
            compute_4 = T.Buffer((1040,), data=compute_1.data)
            compute_4[i0_i1_fused_i2_fused] = T.asin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))
        for i0_i1_fused_i2_fused in T.parallel(1040):
            compute_4 = T.Buffer((1040,), data=compute_2.data)
            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))
        auto_scheduler_layout_transform_1 = T.Buffer((1040,), data=auto_scheduler_layout_transform)
        for ax0_ax1_fused_ax2_fused in T.parallel(5):
            for ax4, ax7, ax8 in T.grid(4, 4, 13):
                ph_3_1 = T.Buffer((1040,), data=ph_3.data)
                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 208 + ax4 * 52 + ax7 * 13 + ax8] = ph_3_1[ax8 * 80 + ax4 * 20 + ax7 * 5 + ax0_ax1_fused_ax2_fused]
        T_batch_matmul_NN_1 = T.Buffer((325,), data=T_batch_matmul_NN)
        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(5):
            for b_inner_init, i_inner_init in T.grid(13, 5):
                T_batch_matmul_NN_1[b_inner_init * 25 + i_inner_init * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)
            for k_outer, k_inner, b_inner, i_inner in T.grid(4, 4, 13, 5):
                cse_var_1: T.int32 = b_inner * 25 + i_inner * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused
                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 80 + i_inner * 16 + k_outer * 4 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 208 + k_outer * 52 + k_inner * 13 + b_inner]
        for i0_i1_fused_i2_fused in T.parallel(325):
            compute_4 = T.Buffer((325,), data=compute_3.data)
            compute_4[i0_i1_fused_i2_fused] = T.acos(T_batch_matmul_NN_1[i0_i1_fused_i2_fused])
        for ax0_ax1_fused_ax2_fused in T.parallel(325):
            T_mod_1 = T.Buffer((325,), data=T_mod.data)
            ph_10_1 = T.Buffer((325,), data=ph_10.data)
            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused], ph_10_1[ax0_ax1_fused_ax2_fused])