[
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 665; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_subtract[(((ax0 * 95) + (ax1 * 19)) + ax2)] = (asinf(ph_0[(((ax0 * 95) + (ax1 * 19)) + ax2)]) - ph_0[(((ax0 * 95) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 665; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "c_code_deepseek": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 665; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_subtract[(((ax0 * 95) + (ax1 * 19)) + ax2)] = (asinf(ph_0[(((ax0 * 95) + (ax1 * 19)) + ax2)]) - ph_0[(((ax0 * 95) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 665; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 5, 19), \"float32\"), compute: T.Buffer((7, 5, 19), \"float32\"), T_subtract: T.Buffer((7, 5, 19), \"float32\"), compute_1: T.Buffer((7, 5, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((665,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(665):\n            compute_2 = T.Buffer((665,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(5, 19):\n                cse_var_1: T.int32 = ax0 * 95 + ax1 * 19 + ax2\n                T_subtract_1 = T.Buffer((665,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(665):\n            compute_2 = T.Buffer((665,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "asin",
                "subtract",
                "acos"
            ]
        ],
        "input_shape": [[7, 5, 19]],
        "output_shape": [[7, 5, 19], [7, 5, 19], [7, 5, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1296; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1296; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1296; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_2[(((i0 * 72) + (i1 * 18)) + i2)] = cosf((ph_0[(((i0 * 72) + (i1 * 18)) + i2)] + ph_3[(((i0 * 72) + (i1 * 18)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 4, 18), \"float32\"), ph_3: T.Buffer((18, 4, 18), \"float32\"), compute: T.Buffer((18, 4, 18), \"float32\"), T_multiply: T.Buffer((18, 4, 18), \"float32\"), compute_1: T.Buffer((18, 4, 18), \"float32\"), compute_2: T.Buffer((18, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1296,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1296):\n            compute_3 = T.Buffer((1296,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1296):\n            T_multiply_1 = T.Buffer((1296,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1296):\n            compute_3 = T.Buffer((1296,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(4, 18):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 18 + i2\n                compute_3 = T.Buffer((1296,), data=compute_2.data)\n                ph_3_1 = T.Buffer((1296,), data=ph_3.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1] + ph_3_1[cse_var_1])",
        "op_args": [
            [
                "add",
                "exp",
                "acos",
                "multiply",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[18, 4, 18], [8, 9, 15], [18, 4, 18]],
        "output_shape": [[8, 9, 15], [18, 4, 18], [18, 4, 18], [18, 4, 18], [18, 4, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3024; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = atanf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 12, 18), \"float32\"), compute: T.Buffer((14, 12, 18), \"float32\"), compute_1: T.Buffer((14, 12, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3024,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3024):\n            compute_2 = T.Buffer((3024,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(168):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((3024,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "exp",
                "atan"
            ]
        ],
        "input_shape": [[14, 12, 18]],
        "output_shape": [[14, 12, 18], [14, 12, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = acoshf((ph_0[((i0_i1_fused * 12) + i2)] / acosf(ph_0[((i0_i1_fused * 12) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] / acosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 2, 12), \"float32\"), compute: T.Buffer((8, 2, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(16):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_1 = T.Buffer((192,), data=compute.data)\n                ph_0_1 = T.Buffer((192,), data=ph_0.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1] / T.acos(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "acos",
                "divide",
                "acosh"
            ]
        ],
        "input_shape": [[8, 2, 12]],
        "output_shape": [[8, 2, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf((ph_0[i0_i1_fused_i2_fused] * ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i0_i1_fused * 2) + i2)] = ceilf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 180; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_mod[((ax0_ax1_fused * 2) + ax2)] = fmodf(fabsf(ph_0[((ax0_ax1_fused * 2) + ax2)]), ph_0[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fmodf(fabsf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 12, 2), \"float32\"), compute: T.Buffer((15, 12, 2), \"float32\"), compute_1: T.Buffer((15, 12, 2), \"float32\"), T_mod: T.Buffer((15, 12, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] * T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((360,), data=compute_1.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(180):\n            for ax2 in range(2):\n                cse_var_2: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_mod_1 = T.Buffer((360,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.fabs(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "ceil",
                "multiply",
                "acos",
                "ceil",
                "abs",
                "mod"
            ]
        ],
        "input_shape": [[15, 12, 2]],
        "output_shape": [[15, 12, 2], [15, 12, 2], [15, 12, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 162) + (i1 * 9)) + i2)] = cosf(ph_0[(((i0 * 162) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 972; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 6; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 18; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_2[(((i0_1 * 162) + (i1_1 * 9)) + i2_1)] = asinhf(ph_0[(((i0_1 * 162) + (i1_1 * 9)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 18, 9), \"float32\"), compute: T.Buffer((6, 18, 9), \"float32\"), compute_1: T.Buffer((6, 18, 9), \"float32\"), compute_2: T.Buffer((6, 18, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((972,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(18, 9):\n                cse_var_1: T.int32 = i0 * 162 + i1 * 9 + i2\n                compute_3 = T.Buffer((972,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(972):\n            compute_3 = T.Buffer((972,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(18, 9):\n                cse_var_2: T.int32 = i0 * 162 + i1 * 9 + i2\n                compute_3 = T.Buffer((972,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "atanh",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[6, 18, 9]],
        "output_shape": [[6, 18, 9], [6, 18, 9], [6, 18, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1980; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1980; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 11, 20), \"float32\"), compute: T.Buffer((9, 11, 20), \"float32\"), compute_1: T.Buffer((9, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1980,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1980):\n            compute_2 = T.Buffer((1980,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1980):\n            compute_2 = T.Buffer((1980,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "asin"
            ]
        ],
        "input_shape": [[9, 11, 20]],
        "output_shape": [[9, 11, 20], [9, 11, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[245];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 196; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 7; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n        auto_scheduler_layout_transform[(((ax4 * 35) + (ax5 * 5)) + ax6)] = ph_3[(((ax5 * 35) + (ax4 * 5)) + ax6)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 7; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 7; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 7; ++b_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n            T_batch_matmul_NN[((((b_outer_inner * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[((((b_outer_inner * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_inner * 5)) + j_outer_inner)] + (ph_0[((((b_outer_inner * 28) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 14)) + (i_inner * 7)) + k_outer)] * auto_scheduler_layout_transform[(((k_outer * 35) + (b_outer_inner * 5)) + j_outer_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 196; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(72) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  float T_batch_matmul_NN[8];\n  __shared__ float compute_shared[576];\n  for (int i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n    for (int i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n      for (int j_inner_init = 0; j_inner_init < 2; ++j_inner_init) {\n        T_batch_matmul_NN[(((i_outer_inner_init * 4) + (i_inner_init * 2)) + j_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 72) + ((int)threadIdx.x))] = __sinf(ph_0[((ax0_ax1_fused_ax2_fused_outer_outer * 72) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  for (int i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n    for (int k_inner = 0; k_inner < 8; ++k_inner) {\n      for (int i_inner = 0; i_inner < 2; ++i_inner) {\n        for (int j_inner = 0; j_inner < 2; ++j_inner) {\n          T_batch_matmul_NN[(((i_outer_inner * 4) + (i_inner * 2)) + j_inner)] = (T_batch_matmul_NN[(((i_outer_inner * 4) + (i_inner * 2)) + j_inner)] + (ph_0[(((((((int)threadIdx.x) >> 2) * 32) + (i_outer_inner * 16)) + (i_inner * 8)) + k_inner)] * compute_shared[(((((((int)threadIdx.x) >> 3) * 64) + (k_inner * 8)) + ((((int)threadIdx.x) & 3) * 2)) + j_inner)]));\n        }\n      }\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 4; ++i1_inner) {\n    for (int i2_inner = 0; i2_inner < 2; ++i2_inner) {\n      compute[(((((((int)threadIdx.x) >> 2) * 32) + (i1_inner * 8)) + ((((int)threadIdx.x) & 3) * 2)) + i2_inner)] = __sinf(T_batch_matmul_NN[((i1_inner * 2) + i2_inner)]);\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 4, 7), \"float32\"), ph_3: T.Buffer((7, 7, 5), \"float32\"), compute: T.Buffer((7, 4, 7), \"float32\"), T_batch_matmul_NN: T.Buffer((7, 4, 5), \"float32\"), compute_1: T.Buffer((7, 4, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([245], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((196,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(196):\n            compute_2 = T.Buffer((196,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((245,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax6 in T.grid(7, 7, 5):\n            ph_3_1 = T.Buffer((245,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax4 * 35 + ax5 * 5 + ax6] = ph_3_1[ax5 * 35 + ax4 * 5 + ax6]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((140,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, j_outer_inner_init, i_inner_init in T.grid(7, 5, 2):\n                T_batch_matmul_NN_1[b_outer_inner_init * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + i_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, j_outer_inner, i_inner in T.grid(7, 7, 5, 2):\n                cse_var_1: T.int32 = b_outer_inner * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + i_inner * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 28 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 14 + i_inner * 7 + k_outer] * auto_scheduler_layout_transform_1[k_outer * 35 + b_outer_inner * 5 + j_outer_inner]\n        for i0_i1_fused_i2_fused in T.parallel(196):\n            compute_2 = T.Buffer((196,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "batch_matmul",
                "sin"
            ]
        ],
        "input_shape": [[7, 4, 7], [7, 7, 5]],
        "output_shape": [[7, 4, 7], [7, 4, 5], [7, 4, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 616; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 616; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 616; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 4), \"float32\"), ph_3: T.Buffer((14, 11, 4), \"float32\"), T_mod: T.Buffer((14, 11, 4), \"float32\"), T_mod_1: T.Buffer((14, 11, 4), \"float32\"), compute: T.Buffer((14, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((616,), data=ph_0.data)\n        ph_3_1 = T.Buffer((616,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(616):\n            T_mod_2 = T.Buffer((616,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(616):\n            T_mod_2 = T.Buffer((616,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(616):\n            compute_1 = T.Buffer((616,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "mod",
                "mod",
                "acos"
            ]
        ],
        "input_shape": [[14, 11, 4], [17, 11, 5], [14, 11, 4]],
        "output_shape": [[14, 11, 4], [17, 11, 5], [14, 11, 4], [14, 11, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4332; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(fmodf(ph_0[i0_i1_fused_i2_fused], cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4332; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(fmodf(ph_0[i0_i1_fused_i2_fused_1], cosf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4332; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf((ph_0[i0_i1_fused_i2_fused_2] + ph_3[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute_3[(((i0 * 228) + (i1 * 19)) + i2)] = fabsf((ph_0[(((i0 * 228) + (i1 * 19)) + i2)] + ph_3[(((i0 * 228) + (i1 * 19)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 12, 19), \"float32\"), ph_3: T.Buffer((19, 12, 19), \"float32\"), compute: T.Buffer((19, 12, 19), \"float32\"), compute_1: T.Buffer((19, 12, 19), \"float32\"), compute_2: T.Buffer((19, 12, 19), \"float32\"), compute_3: T.Buffer((19, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4332,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4332):\n            compute_4 = T.Buffer((4332,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(4332):\n            compute_4 = T.Buffer((4332,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(ph_0_1[i0_i1_fused_i2_fused])))\n        ph_3_1 = T.Buffer((4332,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(4332):\n            compute_4 = T.Buffer((4332,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(12, 19):\n                cse_var_1: T.int32 = i0 * 228 + i1 * 19 + i2\n                compute_4 = T.Buffer((4332,), data=compute_3.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1] + ph_3_1[cse_var_1])",
        "op_args": [
            [
                "add",
                "cos",
                "mod",
                "exp",
                "exp",
                "ceil",
                "abs"
            ]
        ],
        "input_shape": [[19, 12, 19], [19, 19, 11], [19, 12, 19]],
        "output_shape": [[19, 19, 11], [19, 12, 19], [19, 12, 19], [19, 12, 19], [19, 12, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_divide, float* T_multiply, float* ph_0, float* ph_3, float* ph_8) {\n  float compute[400];\n  float auto_scheduler_layout_transform[80];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 400; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 400; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] / compute[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n    for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n        auto_scheduler_layout_transform[(((ax4 * 10) + (ax7 * 5)) + ax8)] = ph_8[(((ax8 * 16) + (ax4 * 2)) + ax7)];\n      }\n    }\n  }\n  for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n    for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n      T_batch_matmul_NN[((b_inner_init * 5) + i_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 8; ++k_outer) {\n    for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n          T_batch_matmul_NN[((b_inner * 5) + i_inner)] = (T_batch_matmul_NN[((b_inner * 5) + i_inner)] + ((ph_0[((((b_inner * 80) + (i_inner * 16)) + (k_outer * 2)) + k_inner)] / compute[((((b_inner * 80) + (i_inner * 16)) + (k_outer * 2)) + k_inner)]) * auto_scheduler_layout_transform[(((k_outer * 10) + (k_inner * 5)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[16];\n  __shared__ float ph_8_shared[40];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 8; ++i_c_outer_inner_init) {\n    T_batch_matmul_NN_local[i_c_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(i_c_outer_inner_init + 8)] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_8_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))] = ph_8[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 8; ++i_c_outer_inner) {\n      T_batch_matmul_NN_local[i_c_outer_inner] = (T_batch_matmul_NN_local[i_c_outer_inner] + ((ph_0[(((((int)threadIdx.x) * 40) + (i_c_outer_inner * 5)) + k_outer_inner)] / __expf(ph_0[(((((int)threadIdx.x) * 40) + (i_c_outer_inner * 5)) + k_outer_inner)])) * ph_8_shared[((((int)threadIdx.x) * 5) + k_outer_inner)]));\n      T_batch_matmul_NN_local[(i_c_outer_inner + 8)] = (T_batch_matmul_NN_local[(i_c_outer_inner + 8)] + ((ph_0[((((((int)threadIdx.x) * 40) + (i_c_outer_inner * 5)) + k_outer_inner) + 160)] / __expf(ph_0[((((((int)threadIdx.x) * 40) + (i_c_outer_inner * 5)) + k_outer_inner) + 160)])) * ph_8_shared[(((((int)threadIdx.x) * 5) + k_outer_inner) + 20)]));\n    }\n  }\n  for (int i_inner = 0; i_inner < 8; ++i_inner) {\n    T_batch_matmul_NN[((((int)threadIdx.x) * 8) + i_inner)] = T_batch_matmul_NN_local[i_inner];\n    T_batch_matmul_NN[(((((int)threadIdx.x) * 8) + i_inner) + 32)] = T_batch_matmul_NN_local[(i_inner + 8)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 5, 16), \"float32\"), ph_3: T.Buffer((5, 5, 16), \"float32\"), ph_8: T.Buffer((5, 16, 1), \"float32\"), T_divide: T.Buffer((5, 5, 16), \"float32\"), T_multiply: T.Buffer((5, 5, 16), \"float32\"), T_batch_matmul_NN: T.Buffer((5, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([400], \"float32\", \"global\")\n        auto_scheduler_layout_transform = T.allocate([80], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((400,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(400):\n            T_divide_1 = T.Buffer((400,), data=T_divide.data)\n            ph_3_1 = T.Buffer((400,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        compute_1 = T.Buffer((400,), data=compute)\n        for i0_i1_fused_i2_fused in T.parallel(400):\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(400):\n            T_multiply_1 = T.Buffer((400,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / compute_1[ax0_ax1_fused_ax2_fused] * ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((80,), data=auto_scheduler_layout_transform)\n        for ax4, ax7, ax8 in T.grid(8, 2, 5):\n            ph_8_1 = T.Buffer((80,), data=ph_8.data)\n            auto_scheduler_layout_transform_1[ax4 * 10 + ax7 * 5 + ax8] = ph_8_1[ax8 * 16 + ax4 * 2 + ax7]\n        T_batch_matmul_NN_1 = T.Buffer((25,), data=T_batch_matmul_NN.data)\n        for b_inner_init, i_inner_init in T.grid(5, 5):\n            T_batch_matmul_NN_1[b_inner_init * 5 + i_inner_init] = T.float32(0)\n        for k_outer, k_inner, b_inner, i_inner in T.grid(8, 2, 5, 5):\n            cse_var_2: T.int32 = b_inner * 5 + i_inner\n            cse_var_1: T.int32 = b_inner * 80 + i_inner * 16 + k_outer * 2 + k_inner\n            T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_1] / compute_1[cse_var_1] * auto_scheduler_layout_transform_1[k_outer * 10 + k_inner * 5 + b_inner]",
        "op_args": [
            [
                "divide",
                "exp",
                "divide",
                "multiply",
                "batch_matmul"
            ]
        ],
        "input_shape": [[5, 5, 16], [17, 10, 15], [5, 5, 16], [5, 16, 1]],
        "output_shape": [[5, 5, 16], [17, 10, 15], [5, 5, 16], [5, 5, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 78; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 78; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 26; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = asinf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 13, 3), \"float32\"), ph_3: T.Buffer((2, 13, 3), \"float32\"), T_subtract: T.Buffer((2, 13, 3), \"float32\"), T_mod: T.Buffer((2, 13, 3), \"float32\"), compute: T.Buffer((2, 13, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((78,), data=ph_0.data)\n        ph_3_1 = T.Buffer((78,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(78):\n            T_subtract_1 = T.Buffer((78,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(78):\n            T_mod_1 = T.Buffer((78,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(26):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_1 = T.Buffer((78,), data=compute.data)\n                compute_1[cse_var_1] = T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "subtract",
                "mod",
                "asin"
            ]
        ],
        "input_shape": [[2, 13, 3], [8, 17, 11], [2, 13, 3]],
        "output_shape": [[2, 13, 3], [8, 17, 11], [2, 13, 3], [2, 13, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_mod[(((ax0 * 152) + (ax1 * 19)) + ax2)] = fmodf(ph_0[(((ax0 * 152) + (ax1 * 19)) + ax2)], ph_3[(((ax0 * 152) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1216; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_1[((i0_i1_fused * 19) + i2)] = fabsf(asinhf(ph_0[((i0_i1_fused * 19) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = fabsf(asinhf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 8, 19), \"float32\"), ph_3: T.Buffer((8, 8, 19), \"float32\"), T_mod: T.Buffer((8, 8, 19), \"float32\"), compute: T.Buffer((8, 8, 19), \"float32\"), compute_1: T.Buffer((8, 8, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1216,), data=ph_0.data)\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(8, 19):\n                cse_var_1: T.int32 = ax0 * 152 + ax1 * 19 + ax2\n                T_mod_1 = T.Buffer((1216,), data=T_mod.data)\n                ph_3_1 = T.Buffer((1216,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1216):\n            compute_2 = T.Buffer((1216,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(64):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((1216,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "asin",
                "asinh",
                "abs"
            ]
        ],
        "input_shape": [[8, 8, 19], [4, 18, 9], [8, 8, 19]],
        "output_shape": [[8, 8, 19], [4, 18, 9], [8, 8, 19], [8, 8, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3420; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf((ph_0[i0_i1_fused_i2_fused] + atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3420; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_divide[(((ax0 * 228) + (ax1 * 19)) + ax2)] = (asinhf(ph_0[(((ax0 * 228) + (ax1 * 19)) + ax2)]) / ph_0[(((ax0 * 228) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 12, 19), \"float32\"), compute: T.Buffer((15, 12, 19), \"float32\"), compute_1: T.Buffer((15, 12, 19), \"float32\"), T_divide: T.Buffer((15, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3420,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3420):\n            compute_2 = T.Buffer((3420,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] + T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(3420):\n            compute_2 = T.Buffer((3420,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(12, 19):\n                cse_var_1: T.int32 = ax0 * 228 + ax1 * 19 + ax2\n                T_divide_1 = T.Buffer((3420,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]",
        "op_args": [
            [
                "atan",
                "add",
                "acos",
                "asin",
                "asinh",
                "divide"
            ]
        ],
        "input_shape": [[15, 12, 19]],
        "output_shape": [[15, 12, 19], [15, 12, 19], [15, 12, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_10, float* ph_3) {\n  float auto_scheduler_layout_transform[480];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1248; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf((ph_0[i0_i1_fused_i2_fused] - (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1248; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1248; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused_1], ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 10; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 3; ++ax5) {\n        for (int32_t ax7 = 0; ax7 < 4; ++ax7) {\n          auto_scheduler_layout_transform[((((ax0_ax1_fused_ax2_fused_1 * 48) + (ax4 * 12)) + (ax5 * 4)) + ax7)] = ph_10[((((((ax0_ax1_fused_ax2_fused_1 & 1) * 240) + (ax5 * 80)) + (ax4 * 20)) + (ax7 * 5)) + (ax0_ax1_fused_ax2_fused_1 >> 1))];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 130; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 3; ++b_outer_inner_init) {\n      T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 195) + (b_outer_inner_init * 65)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1))] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 4; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 3; ++b_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 4; ++k_inner) {\n          T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 195) + (b_outer_inner * 65)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1))] = (T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 195) + (b_outer_inner * 65)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1))] + (fmodf(ph_0[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 624) + (b_outer_inner * 208)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 10) * 16)) + (k_outer * 4)) + k_inner)], ph_3[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 624) + (b_outer_inner * 208)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 10) * 16)) + (k_outer * 4)) + k_inner)]) * auto_scheduler_layout_transform[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 10) * 48) + (k_outer * 12)) + (b_outer_inner * 4)) + k_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(50) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_10, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[1];\n  __shared__ float ph_10_shared[10];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 4; ++k_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 10) {\n      ph_10_shared[((int)threadIdx.x)] = ph_10[((((((int)blockIdx.x) * 40) + ((((int)threadIdx.x) / 5) * 20)) + (k_outer_outer * 5)) + (((int)threadIdx.x) % 5))];\n    }\n    __syncthreads();\n    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (fmodf(ph_0[(((((int)blockIdx.x) * 40) + ((((int)threadIdx.x) / 5) * 4)) + k_outer_outer)], ph_3[(((((int)blockIdx.x) * 40) + ((((int)threadIdx.x) / 5) * 4)) + k_outer_outer)]) * ph_10_shared[(((((int)threadIdx.x) / 25) * 5) + (((int)threadIdx.x) % 5))]));\n  }\n  T_batch_matmul_NN[((((int)blockIdx.x) * 50) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 13, 16), \"float32\"), ph_3: T.Buffer((6, 13, 16), \"float32\"), ph_10: T.Buffer((6, 16, 5), \"float32\"), compute: T.Buffer((6, 13, 16), \"float32\"), T_add: T.Buffer((6, 13, 16), \"float32\"), compute_1: T.Buffer((6, 13, 16), \"float32\"), T_batch_matmul_NN: T.Buffer((6, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([480], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1248,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1248,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(1248):\n            compute_2 = T.Buffer((1248,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] - ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1248):\n            T_add_1 = T.Buffer((1248,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1248):\n            compute_2 = T.Buffer((1248,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((480,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(10):\n            for ax4, ax5, ax7 in T.grid(4, 3, 4):\n                ph_10_1 = T.Buffer((480,), data=ph_10.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 48 + ax4 * 12 + ax5 * 4 + ax7] = ph_10_1[ax0_ax1_fused_ax2_fused % 2 * 240 + ax5 * 80 + ax4 * 20 + ax7 * 5 + ax0_ax1_fused_ax2_fused // 2]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(130):\n            T_batch_matmul_NN_1 = T.Buffer((390,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init in range(3):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 2 * 195 + b_outer_inner_init * 65 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2] = T.float32(0)\n            for k_outer, b_outer_inner, k_inner in T.grid(4, 3, 4):\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 195 + b_outer_inner * 65 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2\n                cse_var_1: T.int32 = cse_var_3 * 624 + b_outer_inner * 208 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 10 * 16 + k_outer * 4 + k_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]) * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 10 * 48 + k_outer * 12 + b_outer_inner * 4 + k_inner]",
        "op_args": [
            [
                "mod",
                "divide",
                "subtract",
                "asinh",
                "add",
                "atanh",
                "batch_matmul"
            ]
        ],
        "input_shape": [[6, 13, 16], [5, 1, 13], [6, 13, 16], [6, 16, 5]],
        "output_shape": [[5, 1, 13], [6, 13, 16], [6, 13, 16], [6, 13, 16], [6, 13, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 720; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_2[(((i0 * 36) + (i1 * 18)) + i2)] = fabsf(ph_0[(((i0 * 36) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 2, 18), \"float32\"), compute: T.Buffer((20, 2, 18), \"float32\"), compute_1: T.Buffer((20, 2, 18), \"float32\"), compute_2: T.Buffer((20, 2, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_3 = T.Buffer((720,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_3 = T.Buffer((720,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(2, 18):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 18 + i2\n                compute_3 = T.Buffer((720,), data=compute_2.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acosh",
                "asinh",
                "asinh",
                "abs"
            ]
        ],
        "input_shape": [[20, 2, 18]],
        "output_shape": [[20, 2, 18], [20, 2, 18], [20, 2, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 252; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = asinhf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / acoshf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 14, 2), \"float32\"), compute: T.Buffer((18, 14, 2), \"float32\"), compute_1: T.Buffer((18, 14, 2), \"float32\"), T_divide: T.Buffer((18, 14, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((504,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(252):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((504,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_2 = T.Buffer((504,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(504):\n            T_divide_1 = T.Buffer((504,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "asinh",
                "atanh",
                "asin",
                "acosh",
                "divide"
            ]
        ],
        "input_shape": [[18, 14, 2]],
        "output_shape": [[18, 14, 2], [18, 14, 2], [18, 14, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4680; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] / sinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4680; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf((ph_0[i0_i1_fused_i2_fused_1] / sinf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 360; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 13) + ax2)] = ((ph_0[((ax0_ax1_fused * 13) + ax2)] * ph_3[((ax0_ax1_fused * 13) + ax2)]) * ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 20, 13), \"float32\"), ph_3: T.Buffer((18, 20, 13), \"float32\"), compute: T.Buffer((18, 20, 13), \"float32\"), compute_1: T.Buffer((18, 20, 13), \"float32\"), T_multiply: T.Buffer((18, 20, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4680,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4680):\n            compute_2 = T.Buffer((4680,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] / T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4680):\n            compute_2 = T.Buffer((4680,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] / T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(360):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_multiply_1 = T.Buffer((4680,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((4680,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1] * ph_0_1[cse_var_1]",
        "op_args": [
            [
                "multiply",
                "sin",
                "divide",
                "ceil",
                "cos",
                "multiply"
            ]
        ],
        "input_shape": [[18, 20, 13], [9, 14, 12], [18, 20, 13]],
        "output_shape": [[9, 14, 12], [18, 20, 13], [18, 20, 13], [18, 20, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 54; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 54; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 54; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = cosf((ph_0[((i0_i1_fused * 18) + i2)] * ph_3[((i0_i1_fused * 18) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(__sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 3, 18), \"float32\"), ph_3: T.Buffer((1, 3, 18), \"float32\"), T_add: T.Buffer((1, 3, 18), \"float32\"), T_subtract: T.Buffer((1, 3, 18), \"float32\"), compute: T.Buffer((1, 3, 18), \"float32\"), compute_1: T.Buffer((1, 3, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((54,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(54):\n            T_subtract_1 = T.Buffer((54,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(54):\n            compute_2 = T.Buffer((54,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((54,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(54):\n            T_add_1 = T.Buffer((54,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(3):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((54,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1] * ph_3_1[cse_var_1])",
        "op_args": [
            [
                "multiply",
                "add",
                "sin",
                "subtract",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[1, 3, 18], [1, 4, 3], [1, 3, 18]],
        "output_shape": [[1, 4, 3], [1, 3, 18], [1, 3, 18], [1, 3, 18], [1, 3, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 150; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 150; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 150; ++ax0_ax1_fused_ax2_fused_2) {\n    T_multiply[ax0_ax1_fused_ax2_fused_2] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused_2]) * ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 10, 5), \"float32\"), ph_3: T.Buffer((3, 10, 5), \"float32\"), T_mod: T.Buffer((3, 10, 5), \"float32\"), T_add: T.Buffer((3, 10, 5), \"float32\"), T_multiply: T.Buffer((3, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        ph_3_1 = T.Buffer((150,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_mod_1 = T.Buffer((150,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_add_1 = T.Buffer((150,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_multiply_1 = T.Buffer((150,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "add",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[3, 10, 5], [6, 17, 2], [3, 10, 5]],
        "output_shape": [[3, 10, 5], [6, 17, 2], [3, 10, 5], [3, 10, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 63; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinhf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 315; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 63; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_mod[((ax0_ax1_fused * 5) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 5) + ax2)], acosf(ph_0[((ax0_ax1_fused * 5) + ax2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 9, 5), \"float32\"), compute: T.Buffer((7, 9, 5), \"float32\"), compute_1: T.Buffer((7, 9, 5), \"float32\"), T_mod: T.Buffer((7, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((315,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(63):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((315,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(315):\n            compute_2 = T.Buffer((315,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(63):\n            for ax2 in range(5):\n                cse_var_2: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_mod_1 = T.Buffer((315,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], T.acos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asinh",
                "acosh",
                "asin",
                "acos",
                "mod"
            ]
        ],
        "input_shape": [[7, 9, 5]],
        "output_shape": [[7, 9, 5], [7, 9, 5], [7, 9, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 136; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = atanf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_multiply[(((ax0 * 170) + (ax1 * 10)) + ax2)] = (asinhf(ph_0[(((ax0 * 170) + (ax1 * 10)) + ax2)]) * ph_0[(((ax0 * 170) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 17, 10), \"float32\"), compute: T.Buffer((8, 17, 10), \"float32\"), T_multiply: T.Buffer((8, 17, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1360,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(136):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_1 = T.Buffer((1360,), data=compute.data)\n                compute_1[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(17, 10):\n                cse_var_2: T.int32 = ax0 * 170 + ax1 * 10 + ax2\n                T_multiply_1 = T.Buffer((1360,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]",
        "op_args": [
            [
                "atan",
                "asinh",
                "multiply"
            ]
        ],
        "input_shape": [[8, 17, 10]],
        "output_shape": [[8, 17, 10], [8, 17, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 85; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_divide[((ax0_ax1_fused * 7) + ax2)] = (ph_0[((ax0_ax1_fused * 7) + ax2)] / ph_3[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 595; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] * fabsf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 595; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf((ph_0[i0_i1_fused_i2_fused_1] * fabsf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 17, 7), \"float32\"), ph_3: T.Buffer((5, 17, 7), \"float32\"), T_divide: T.Buffer((5, 17, 7), \"float32\"), compute: T.Buffer((5, 17, 7), \"float32\"), compute_1: T.Buffer((5, 17, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((595,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(85):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_divide_1 = T.Buffer((595,), data=T_divide.data)\n                ph_3_1 = T.Buffer((595,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(595):\n            compute_2 = T.Buffer((595,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] * T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(595):\n            compute_2 = T.Buffer((595,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] * T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "abs",
                "multiply",
                "atanh",
                "acos"
            ]
        ],
        "input_shape": [[5, 17, 7], [17, 11, 5], [5, 17, 7]],
        "output_shape": [[5, 17, 7], [17, 11, 5], [5, 17, 7], [5, 17, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 45) + (i1 * 5)) + i2)] = atanhf(ph_0[(((i0 * 45) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_1[((i0_i1_fused * 5) + i2_1)] = asinf(acoshf(ph_0[((i0_i1_fused * 5) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] - atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - atanf(ph_0[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 9, 5), \"float32\"), compute: T.Buffer((8, 9, 5), \"float32\"), compute_1: T.Buffer((8, 9, 5), \"float32\"), compute_2: T.Buffer((8, 9, 5), \"float32\"), T_add: T.Buffer((8, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(9, 5):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 5 + i2\n                compute_3 = T.Buffer((360,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(72):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((360,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asin(T.acosh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_1 = T.Buffer((360,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "acosh",
                "asin",
                "atan",
                "subtract",
                "exp",
                "add"
            ]
        ],
        "input_shape": [[8, 9, 5]],
        "output_shape": [[8, 9, 5], [8, 9, 5], [8, 9, 5], [8, 9, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 720; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_multiply[(((ax0 * 48) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 48) + (ax1 * 4)) + ax2)] * ph_3[(((ax0 * 48) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 15; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n        T_add[(((ax0_1 * 48) + (ax1_1 * 4)) + ax2_1)] = (cosf(ph_0[(((ax0_1 * 48) + (ax1_1 * 4)) + ax2_1)]) + ph_0[(((ax0_1 * 48) + (ax1_1 * 4)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 12, 4), \"float32\"), ph_3: T.Buffer((15, 12, 4), \"float32\"), T_mod: T.Buffer((15, 12, 4), \"float32\"), T_multiply: T.Buffer((15, 12, 4), \"float32\"), T_add: T.Buffer((15, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        ph_3_1 = T.Buffer((720,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(720):\n            T_mod_1 = T.Buffer((720,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(12, 4):\n                cse_var_1: T.int32 = ax0 * 48 + ax1 * 4 + ax2\n                T_multiply_1 = T.Buffer((720,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(12, 4):\n                cse_var_2: T.int32 = ax0 * 48 + ax1 * 4 + ax2\n                T_add_1 = T.Buffer((720,), data=T_add.data)\n                T_add_1[cse_var_2] = T.cos(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "mod",
                "multiply",
                "cos",
                "add"
            ]
        ],
        "input_shape": [[15, 12, 4], [14, 16, 16], [15, 12, 4]],
        "output_shape": [[15, 12, 4], [14, 16, 16], [15, 12, 4], [15, 12, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 20; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_mod[((ax0_ax1_fused * 19) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 19) + ax2)], ph_3[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 380; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 190) + (i1 * 19)) + i2)] = sinf(atanhf(ph_0[(((i0 * 190) + (i1 * 19)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 10, 19), \"float32\"), ph_3: T.Buffer((2, 10, 19), \"float32\"), T_mod: T.Buffer((2, 10, 19), \"float32\"), T_subtract: T.Buffer((2, 10, 19), \"float32\"), compute: T.Buffer((2, 10, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((380,), data=ph_0.data)\n        ph_3_1 = T.Buffer((380,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(20):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_mod_1 = T.Buffer((380,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(380):\n            T_subtract_1 = T.Buffer((380,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(10, 19):\n                cse_var_2: T.int32 = i0 * 190 + i1 * 19 + i2\n                compute_1 = T.Buffer((380,), data=compute.data)\n                compute_1[cse_var_2] = T.sin(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "subtract",
                "atanh",
                "sin"
            ]
        ],
        "input_shape": [[2, 10, 19], [4, 8, 4], [2, 10, 19]],
        "output_shape": [[2, 10, 19], [4, 8, 4], [2, 10, 19], [2, 10, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_mod[(((ax0 * 24) + (ax1 * 3)) + ax2)] = fmodf(ph_0[(((ax0 * 24) + (ax1 * 3)) + ax2)], ph_3[(((ax0 * 24) + (ax1 * 3)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] - asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_1[(((i0 * 24) + (i1 * 3)) + i2)] = asinf((ph_0[(((i0 * 24) + (i1 * 3)) + i2)] - asinhf(ph_0[(((i0 * 24) + (i1 * 3)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 8, 3), \"float32\"), ph_3: T.Buffer((15, 8, 3), \"float32\"), T_mod: T.Buffer((15, 8, 3), \"float32\"), compute: T.Buffer((15, 8, 3), \"float32\"), compute_1: T.Buffer((15, 8, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(8, 3):\n                cse_var_1: T.int32 = ax0 * 24 + ax1 * 3 + ax2\n                T_mod_1 = T.Buffer((360,), data=T_mod.data)\n                ph_3_1 = T.Buffer((360,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_2 = T.Buffer((360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(8, 3):\n                cse_var_2: T.int32 = i0 * 24 + i1 * 3 + i2\n                compute_2 = T.Buffer((360,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2] - T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "asinh",
                "subtract",
                "ceil",
                "asin"
            ]
        ],
        "input_shape": [[15, 8, 3], [1, 11, 9], [15, 8, 3]],
        "output_shape": [[15, 8, 3], [1, 11, 9], [15, 8, 3], [15, 8, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 352; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 22; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = atanhf(cosf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 352; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 11, 16), \"float32\"), compute: T.Buffer((2, 11, 16), \"float32\"), compute_1: T.Buffer((2, 11, 16), \"float32\"), compute_2: T.Buffer((2, 11, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((352,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(352):\n            compute_3 = T.Buffer((352,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(22):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((352,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(352):\n            compute_3 = T.Buffer((352,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "cos",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[2, 11, 16]],
        "output_shape": [[2, 11, 16], [2, 11, 16], [2, 11, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_5) {\n  float auto_scheduler_layout_transform[1045];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2299; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2299; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t ax4 = 0; ax4 < 19; ++ax4) {\n    for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n      for (int32_t ax8 = 0; ax8 < 11; ++ax8) {\n        auto_scheduler_layout_transform[(((ax4 * 55) + (ax6 * 11)) + ax8)] = ph_5[(((ax8 * 95) + (ax4 * 5)) + ax6)];\n      }\n    }\n  }\n  for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 11; ++i_outer_inner_init) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 11; ++b_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 55) + (i_outer_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 19; ++k_outer) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 11; ++i_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 11; ++b_inner) {\n          T_batch_matmul_NN[(((b_inner * 55) + (i_outer_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_inner * 55) + (i_outer_inner * 5)) + j_outer_inner)] + (ph_0[(((b_inner * 209) + (i_outer_inner * 19)) + k_outer)] * auto_scheduler_layout_transform[(((k_outer * 55) + (j_outer_inner * 11)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_5_shared[80];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  T_batch_matmul_NN_local[1] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_5_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x))] = ph_5[((((((int)blockIdx.x) / 5) * 80) + (ax0_ax1_fused_ax2_fused_outer_outer * 10)) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 4; ++k_outer_inner) {\n    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (ph_0[(((((((int)blockIdx.x) / 5) * 80) + ((((int)threadIdx.x) / 5) * 20)) + ((((int)blockIdx.x) % 5) * 4)) + k_outer_inner)] * ph_5_shared[((((((int)threadIdx.x) / 5) * 20) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    T_batch_matmul_NN_local[1] = (T_batch_matmul_NN_local[1] + (ph_0[((((((((int)blockIdx.x) / 5) * 80) + ((((int)threadIdx.x) / 5) * 20)) + ((((int)blockIdx.x) % 5) * 4)) + k_outer_inner) + 40)] * ph_5_shared[(((((((int)threadIdx.x) / 5) * 20) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5)) + 40)]));\n  }\n  T_batch_matmul_NN[(((((((int)blockIdx.x) / 5) * 100) + ((((int)threadIdx.x) / 5) * 25)) + ((((int)blockIdx.x) % 5) * 5)) + (((int)threadIdx.x) % 5))] = T_batch_matmul_NN_local[0];\n  T_batch_matmul_NN[((((((((int)blockIdx.x) / 5) * 100) + ((((int)threadIdx.x) / 5) * 25)) + ((((int)blockIdx.x) % 5) * 5)) + (((int)threadIdx.x) % 5)) + 50)] = T_batch_matmul_NN_local[1];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 11, 19), \"float32\"), ph_5: T.Buffer((11, 19, 5), \"float32\"), compute: T.Buffer((11, 11, 19), \"float32\"), compute_1: T.Buffer((11, 11, 19), \"float32\"), T_batch_matmul_NN: T.Buffer((11, 11, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1045], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((2299,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2299):\n            compute_2 = T.Buffer((2299,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2299):\n            compute_2 = T.Buffer((2299,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((1045,), data=auto_scheduler_layout_transform)\n        for ax4, ax6, ax8 in T.grid(19, 5, 11):\n            ph_5_1 = T.Buffer((1045,), data=ph_5.data)\n            auto_scheduler_layout_transform_1[ax4 * 55 + ax6 * 11 + ax8] = ph_5_1[ax8 * 95 + ax4 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((605,), data=T_batch_matmul_NN.data)\n        for i_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(11, 5, 11):\n            T_batch_matmul_NN_1[b_inner_init * 55 + i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n        for k_outer, i_outer_inner, j_outer_inner, b_inner in T.grid(19, 11, 5, 11):\n            cse_var_1: T.int32 = b_inner * 55 + i_outer_inner * 5 + j_outer_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 209 + i_outer_inner * 19 + k_outer] * auto_scheduler_layout_transform_1[k_outer * 55 + j_outer_inner * 11 + b_inner]",
        "op_args": [
            [
                "ceil",
                "acos",
                "sin",
                "batch_matmul"
            ]
        ],
        "input_shape": [[11, 11, 19], [11, 19, 5]],
        "output_shape": [[11, 11, 19], [11, 11, 19], [11, 11, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = atanf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n        compute_1[(((i0 * 300) + (i1 * 15)) + i2_1)] = atanf(atanhf(ph_0[(((i0 * 300) + (i1 * 15)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 900; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 900; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = asinhf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 20, 15), \"float32\"), compute: T.Buffer((3, 20, 15), \"float32\"), compute_1: T.Buffer((3, 20, 15), \"float32\"), compute_2: T.Buffer((3, 20, 15), \"float32\"), compute_3: T.Buffer((3, 20, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_4 = T.Buffer((900,), data=compute.data)\n                compute_4[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(20, 15):\n                cse_var_2: T.int32 = i0 * 300 + i1 * 15 + i2\n                compute_4 = T.Buffer((900,), data=compute_1.data)\n                compute_4[cse_var_2] = T.atan(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_4 = T.Buffer((900,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_4 = T.Buffer((900,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atan",
                "atanh",
                "atan",
                "asin",
                "asinh",
                "asinh"
            ]
        ],
        "input_shape": [[3, 20, 15]],
        "output_shape": [[3, 20, 15], [3, 20, 15], [3, 20, 15], [3, 20, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 55; ++ax0_ax1_fused) {\n    T_add[ax0_ax1_fused] = (ph_0[ax0_ax1_fused] + ph_3[ax0_ax1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 55; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 55; ++ax0_ax1_fused_1) {\n    T_multiply[ax0_ax1_fused_1] = (atanhf(ph_0[ax0_ax1_fused_1]) * ph_0[ax0_ax1_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 5, 1), \"float32\"), ph_3: T.Buffer((11, 5, 1), \"float32\"), T_add: T.Buffer((11, 5, 1), \"float32\"), compute: T.Buffer((11, 5, 1), \"float32\"), T_multiply: T.Buffer((11, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((55,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(55):\n            T_add_1 = T.Buffer((55,), data=T_add.data)\n            ph_3_1 = T.Buffer((55,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused] = ph_0_1[ax0_ax1_fused] + ph_3_1[ax0_ax1_fused]\n        for i0_i1_fused_i2_fused in T.parallel(55):\n            compute_1 = T.Buffer((55,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(55):\n            T_multiply_1 = T.Buffer((55,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused] = T.atanh(ph_0_1[ax0_ax1_fused]) * ph_0_1[ax0_ax1_fused]",
        "op_args": [
            [
                "add",
                "exp",
                "atanh",
                "multiply"
            ]
        ],
        "input_shape": [[11, 5, 1], [11, 10, 16], [11, 5, 1]],
        "output_shape": [[11, 5, 1], [11, 10, 16], [11, 5, 1], [11, 5, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  float compute_1[216];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 216; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 216; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_add[((ax0_ax1_fused * 18) + ax2)] = (ph_0[((ax0_ax1_fused * 18) + ax2)] + compute_1[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 216; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + __expf(asinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 6, 18), \"float32\"), compute: T.Buffer((2, 6, 18), \"float32\"), T_add: T.Buffer((2, 6, 18), \"float32\"), T_subtract: T.Buffer((2, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([216], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((216,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_2 = T.Buffer((216,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_2 = T.Buffer((216,), data=compute_1)\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_2[i0_i1_fused_i2_fused] = T.exp(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_add_1 = T.Buffer((216,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + compute_2[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(216):\n            T_subtract_1 = T.Buffer((216,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "sin",
                "asin",
                "exp",
                "add",
                "acosh",
                "subtract"
            ]
        ],
        "input_shape": [[2, 6, 18]],
        "output_shape": [[2, 6, 18], [2, 6, 18], [2, 6, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      compute[((i0 * 15) + i1)] = cosf(ph_0[((i0 * 15) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 300; ++i0_i1_fused) {\n    float compute_4[1];\n    compute_4[0] = expf(ph_0[i0_i1_fused]);\n    compute_1[i0_i1_fused] = atanhf(compute_4[0]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 300; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 300; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 15, 1), \"float32\"), compute: T.Buffer((20, 15, 1), \"float32\"), compute_1: T.Buffer((20, 15, 1), \"float32\"), compute_2: T.Buffer((20, 15, 1), \"float32\"), compute_3: T.Buffer((20, 15, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((300,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1 in range(15):\n                cse_var_1: T.int32 = i0 * 15 + i1\n                compute_4 = T.Buffer((300,), data=compute.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(300):\n            compute_4 = T.allocate([1], \"float32\", \"global\")\n            compute_5 = T.Buffer((1,), data=compute_4, align=4)\n            compute_5[0] = T.exp(ph_0_1[i0_i1_fused])\n            compute_6 = T.Buffer((300,), data=compute_1.data)\n            compute_6[i0_i1_fused] = T.atanh(compute_5[0])\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            compute_4 = T.Buffer((300,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            compute_4 = T.Buffer((300,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "exp",
                "atanh",
                "asin",
                "asin"
            ]
        ],
        "input_shape": [[20, 15, 1]],
        "output_shape": [[20, 15, 1], [20, 15, 1], [20, 15, 1], [20, 15, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 150; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 150; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 150; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = fabsf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_3[((i0_i1_fused * 15) + i2)] = asinf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 5, 15), \"float32\"), compute: T.Buffer((2, 5, 15), \"float32\"), compute_1: T.Buffer((2, 5, 15), \"float32\"), compute_2: T.Buffer((2, 5, 15), \"float32\"), compute_3: T.Buffer((2, 5, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_4 = T.Buffer((150,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_4 = T.Buffer((150,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_4 = T.Buffer((150,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(10):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_4 = T.Buffer((150,), data=compute_3.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atanh",
                "asin",
                "atanh",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[2, 5, 15]],
        "output_shape": [[2, 5, 15], [2, 5, 15], [2, 5, 15], [2, 5, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 270; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinhf(cosf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1350; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_divide[(((ax0 * 90) + (ax1 * 5)) + ax2)] = (ph_0[(((ax0 * 90) + (ax1 * 5)) + ax2)] / ph_3[(((ax0 * 90) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1350; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 18, 5), \"float32\"), ph_3: T.Buffer((15, 18, 5), \"float32\"), T_divide: T.Buffer((15, 18, 5), \"float32\"), compute: T.Buffer((15, 18, 5), \"float32\"), compute_1: T.Buffer((15, 18, 5), \"float32\"), compute_2: T.Buffer((15, 18, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1350,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(270):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((1350,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1350):\n            compute_3 = T.Buffer((1350,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((1350,), data=ph_3.data)\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(18, 5):\n                cse_var_2: T.int32 = ax0 * 90 + ax1 * 5 + ax2\n                T_divide_1 = T.Buffer((1350,), data=T_divide.data)\n                T_divide_1[cse_var_2] = ph_0_1[cse_var_2] / ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(1350):\n            compute_3 = T.Buffer((1350,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "divide",
                "cos",
                "asinh",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[15, 18, 5], [8, 9, 2], [15, 18, 5]],
        "output_shape": [[8, 9, 2], [15, 18, 5], [15, 18, 5], [15, 18, 5], [15, 18, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 816; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 816; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 17, 8), \"float32\"), compute: T.Buffer((6, 17, 8), \"float32\"), compute_1: T.Buffer((6, 17, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((816,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(816):\n            compute_2 = T.Buffer((816,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(816):\n            compute_2 = T.Buffer((816,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "atanh"
            ]
        ],
        "input_shape": [[6, 17, 8]],
        "output_shape": [[6, 17, 8], [6, 17, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_mod[(((ax0 * 48) + (ax1 * 6)) + ax2)] = fmodf(ph_0[(((ax0 * 48) + (ax1 * 6)) + ax2)], ph_3[(((ax0 * 48) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 5; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 8; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 6; ++ax2_1) {\n        T_add[(((ax0_1 * 48) + (ax1_1 * 6)) + ax2_1)] = ((ph_0[(((ax0_1 * 48) + (ax1_1 * 6)) + ax2_1)] / fabsf(ph_0[(((ax0_1 * 48) + (ax1_1 * 6)) + ax2_1)])) + ph_0[(((ax0_1 * 48) + (ax1_1 * 6)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 8, 6), \"float32\"), ph_3: T.Buffer((5, 8, 6), \"float32\"), T_mod: T.Buffer((5, 8, 6), \"float32\"), T_add: T.Buffer((5, 8, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(8, 6):\n                cse_var_1: T.int32 = ax0 * 48 + ax1 * 6 + ax2\n                T_mod_1 = T.Buffer((240,), data=T_mod.data)\n                ph_3_1 = T.Buffer((240,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(8, 6):\n                cse_var_2: T.int32 = ax0 * 48 + ax1 * 6 + ax2\n                T_add_1 = T.Buffer((240,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] / T.fabs(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "mod",
                "abs",
                "divide",
                "add"
            ]
        ],
        "input_shape": [[5, 8, 6], [9, 6, 20], [5, 8, 6]],
        "output_shape": [[5, 8, 6], [9, 6, 20], [5, 8, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4608; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4608; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4608; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_2[((i0_i1_fused * 16) + i2)] = cosf((ph_0[((i0_i1_fused * 16) + i2)] / ph_3[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 288; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_3[((i0_i1_fused_1 * 16) + i2_1)] = sinf((ph_0[((i0_i1_fused_1 * 16) + i2_1)] / ph_3[((i0_i1_fused_1 * 16) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 16, 16), \"float32\"), ph_3: T.Buffer((18, 16, 16), \"float32\"), T_add: T.Buffer((18, 16, 16), \"float32\"), compute: T.Buffer((18, 16, 16), \"float32\"), compute_1: T.Buffer((18, 16, 16), \"float32\"), compute_2: T.Buffer((18, 16, 16), \"float32\"), compute_3: T.Buffer((18, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4608,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4608):\n            compute_4 = T.Buffer((4608,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4608):\n            compute_4 = T.Buffer((4608,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((4608,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4608):\n            T_add_1 = T.Buffer((4608,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((4608,), data=compute_2.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1] / ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((4608,), data=compute_3.data)\n                compute_4[cse_var_2] = T.sin(ph_0_1[cse_var_2] / ph_3_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "add",
                "asin",
                "sin",
                "acosh",
                "cos",
                "sin"
            ]
        ],
        "input_shape": [[18, 16, 16], [16, 10, 8], [18, 16, 16]],
        "output_shape": [[16, 10, 8], [18, 16, 16], [18, 16, 16], [18, 16, 16], [18, 16, 16], [18, 16, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_divide[(((ax0 * 190) + (ax1 * 10)) + ax2)] = (ph_0[(((ax0 * 190) + (ax1 * 10)) + ax2)] / ph_3[(((ax0 * 190) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 380; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n      T_mod[((ax0_ax1_fused * 10) + ax2_1)] = fmodf((ph_0[((ax0_ax1_fused * 10) + ax2_1)] / fmodf(ph_0[((ax0_ax1_fused * 10) + ax2_1)], ph_3[((ax0_ax1_fused * 10) + ax2_1)])), ph_0[((ax0_ax1_fused * 10) + ax2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 19, 10), \"float32\"), ph_3: T.Buffer((20, 19, 10), \"float32\"), T_divide: T.Buffer((20, 19, 10), \"float32\"), T_mod: T.Buffer((20, 19, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3800,), data=ph_0.data)\n        ph_3_1 = T.Buffer((3800,), data=ph_3.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(19, 10):\n                cse_var_1: T.int32 = ax0 * 190 + ax1 * 10 + ax2\n                T_divide_1 = T.Buffer((3800,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(380):\n            for ax2 in range(10):\n                cse_var_2: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_mod_1 = T.Buffer((3800,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] / T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "mod",
                "divide",
                "mod"
            ]
        ],
        "input_shape": [[20, 19, 10], [3, 13, 14], [20, 19, 10]],
        "output_shape": [[20, 19, 10], [3, 13, 14], [20, 19, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1820; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 140) + (i1 * 14)) + i2)] = atanhf(ph_0[(((i0 * 140) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 130; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 14) + ax2)] = (fabsf(ph_0[((ax0_ax1_fused * 14) + ax2)]) * ph_0[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1820; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add_1[ax0_ax1_fused_ax2_fused_1] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 10, 14), \"float32\"), ph_3: T.Buffer((13, 10, 14), \"float32\"), T_add: T.Buffer((13, 10, 14), \"float32\"), compute: T.Buffer((13, 10, 14), \"float32\"), T_multiply: T.Buffer((13, 10, 14), \"float32\"), T_add_1: T.Buffer((13, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1820,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1820):\n            T_add_2 = T.Buffer((1820,), data=T_add.data)\n            ph_3_1 = T.Buffer((1820,), data=ph_3.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(10, 14):\n                cse_var_1: T.int32 = i0 * 140 + i1 * 14 + i2\n                compute_1 = T.Buffer((1820,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(130):\n            for ax2 in range(14):\n                cse_var_2: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_multiply_1 = T.Buffer((1820,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1820):\n            T_add_2 = T.Buffer((1820,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "atanh",
                "abs",
                "multiply",
                "add"
            ]
        ],
        "input_shape": [[13, 10, 14], [15, 17, 12], [13, 10, 14]],
        "output_shape": [[13, 10, 14], [15, 17, 12], [13, 10, 14], [13, 10, 14], [13, 10, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4320; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 240) + (i1 * 20)) + i2)] = fabsf(ph_0[(((i0 * 240) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4320; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 12, 20), \"float32\"), ph_3: T.Buffer((18, 12, 20), \"float32\"), T_mod: T.Buffer((18, 12, 20), \"float32\"), compute: T.Buffer((18, 12, 20), \"float32\"), compute_1: T.Buffer((18, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4320,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4320):\n            T_mod_1 = T.Buffer((4320,), data=T_mod.data)\n            ph_3_1 = T.Buffer((4320,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(12, 20):\n                cse_var_1: T.int32 = i0 * 240 + i1 * 20 + i2\n                compute_2 = T.Buffer((4320,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(4320):\n            compute_2 = T.Buffer((4320,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "mod",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[18, 12, 20], [17, 9, 9], [18, 12, 20]],
        "output_shape": [[18, 12, 20], [17, 9, 9], [18, 12, 20], [18, 12, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 114; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = acoshf((ph_0[((i0_i1_fused * 9) + i2)] / acoshf(ph_0[((i0_i1_fused * 9) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1026; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_2[(((i0 * 171) + (i1 * 9)) + i2_1)] = fabsf(ceilf(ph_0[(((i0 * 171) + (i1 * 9)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1026; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fabsf(ceilf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 19, 9), \"float32\"), compute: T.Buffer((6, 19, 9), \"float32\"), compute_1: T.Buffer((6, 19, 9), \"float32\"), compute_2: T.Buffer((6, 19, 9), \"float32\"), compute_3: T.Buffer((6, 19, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1026,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(114):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_4 = T.Buffer((1026,), data=compute.data)\n                compute_4[cse_var_1] = T.acosh(ph_0_1[cse_var_1] / T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1026):\n            compute_4 = T.Buffer((1026,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(19, 9):\n                cse_var_2: T.int32 = i0 * 171 + i1 * 9 + i2\n                compute_4 = T.Buffer((1026,), data=compute_2.data)\n                compute_4[cse_var_2] = T.fabs(T.ceil(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1026):\n            compute_4 = T.Buffer((1026,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "divide",
                "acosh",
                "sin",
                "ceil",
                "abs",
                "acos"
            ]
        ],
        "input_shape": [[6, 19, 9]],
        "output_shape": [[6, 19, 9], [6, 19, 9], [6, 19, 9], [6, 19, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 64; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], asinhf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], asinhf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 1, 4), \"float32\"), T_mod: T.Buffer((16, 1, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(64):\n            T_mod_1 = T.Buffer((64,), data=T_mod.data)\n            ph_0_1 = T.Buffer((64,), data=ph_0.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]))",
        "op_args": [
            [
                "asinh",
                "mod"
            ]
        ],
        "input_shape": [[16, 1, 4]],
        "output_shape": [[16, 1, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 252; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 252; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_2[((i0_i1_fused * 9) + i2)] = expf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 252; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = acoshf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 2, 9), \"float32\"), compute: T.Buffer((14, 2, 9), \"float32\"), compute_1: T.Buffer((14, 2, 9), \"float32\"), compute_2: T.Buffer((14, 2, 9), \"float32\"), compute_3: T.Buffer((14, 2, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((252,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(252):\n            compute_4 = T.Buffer((252,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(252):\n            compute_4 = T.Buffer((252,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(28):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_4 = T.Buffer((252,), data=compute_2.data)\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(252):\n            compute_4 = T.Buffer((252,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "atan",
                "atan",
                "exp",
                "acosh"
            ]
        ],
        "input_shape": [[14, 2, 9]],
        "output_shape": [[14, 2, 9], [14, 2, 9], [14, 2, 9], [14, 2, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 143; ++i0_i1_fused) {\n    compute[i0_i1_fused] = fabsf((ph_0[i0_i1_fused] - cosf(ph_0[i0_i1_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      T_add[((ax0 * 13) + ax1)] = ((ph_0[((ax0 * 13) + ax1)] * acoshf(ph_0[((ax0 * 13) + ax1)])) + ph_0[((ax0 * 13) + ax1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 13, 1), \"float32\"), compute: T.Buffer((11, 13, 1), \"float32\"), T_add: T.Buffer((11, 13, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((143,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(143):\n            compute_1 = T.Buffer((143,), data=compute.data)\n            compute_1[i0_i1_fused] = T.fabs(ph_0_1[i0_i1_fused] - T.cos(ph_0_1[i0_i1_fused]))\n        for ax0 in T.parallel(11):\n            for ax1 in range(13):\n                cse_var_1: T.int32 = ax0 * 13 + ax1\n                T_add_1 = T.Buffer((143,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] * T.acosh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]",
        "op_args": [
            [
                "cos",
                "subtract",
                "abs",
                "acosh",
                "multiply",
                "add"
            ]
        ],
        "input_shape": [[11, 13, 1]],
        "output_shape": [[11, 13, 1], [11, 13, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1728; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 96) + (i1 * 16)) + i2)] = asinhf(ph_0[(((i0 * 96) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 6, 16), \"float32\"), compute: T.Buffer((18, 6, 16), \"float32\"), compute_1: T.Buffer((18, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1728,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1728):\n            compute_2 = T.Buffer((1728,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(6, 16):\n                cse_var_1: T.int32 = i0 * 96 + i1 * 16 + i2\n                compute_2 = T.Buffer((1728,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atanh",
                "asinh"
            ]
        ],
        "input_shape": [[18, 6, 16]],
        "output_shape": [[18, 6, 16], [18, 6, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1995; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_mod[(((ax0 * 105) + (ax1 * 15)) + ax2)] = fmodf(ph_0[(((ax0 * 105) + (ax1 * 15)) + ax2)], ph_3[(((ax0 * 105) + (ax1 * 15)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 19; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 7; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 15; ++ax2_1) {\n        T_subtract[(((ax0_1 * 105) + (ax1_1 * 15)) + ax2_1)] = (ceilf(ph_0[(((ax0_1 * 105) + (ax1_1 * 15)) + ax2_1)]) - ph_0[(((ax0_1 * 105) + (ax1_1 * 15)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 7, 15), \"float32\"), ph_3: T.Buffer((19, 7, 15), \"float32\"), T_add: T.Buffer((19, 7, 15), \"float32\"), T_mod: T.Buffer((19, 7, 15), \"float32\"), T_subtract: T.Buffer((19, 7, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1995,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1995,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1995):\n            T_add_1 = T.Buffer((1995,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(7, 15):\n                cse_var_1: T.int32 = ax0 * 105 + ax1 * 15 + ax2\n                T_mod_1 = T.Buffer((1995,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(7, 15):\n                cse_var_2: T.int32 = ax0 * 105 + ax1 * 15 + ax2\n                T_subtract_1 = T.Buffer((1995,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.ceil(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "add",
                "mod",
                "ceil",
                "subtract"
            ]
        ],
        "input_shape": [[19, 7, 15], [11, 5, 9], [19, 7, 15]],
        "output_shape": [[19, 7, 15], [11, 5, 9], [19, 7, 15], [19, 7, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 234; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_mod[((ax0_ax1_fused * 17) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 17) + ax2)], ph_3[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 306) + (i1 * 17)) + i2)] = ceilf(ph_0[(((i0 * 306) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3978; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 18, 17), \"float32\"), ph_3: T.Buffer((13, 18, 17), \"float32\"), T_mod: T.Buffer((13, 18, 17), \"float32\"), compute: T.Buffer((13, 18, 17), \"float32\"), compute_1: T.Buffer((13, 18, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3978,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(234):\n            for ax2 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_mod_1 = T.Buffer((3978,), data=T_mod.data)\n                ph_3_1 = T.Buffer((3978,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(18, 17):\n                cse_var_2: T.int32 = i0 * 306 + i1 * 17 + i2\n                compute_2 = T.Buffer((3978,), data=compute.data)\n                compute_2[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(3978):\n            compute_2 = T.Buffer((3978,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "mod",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[13, 18, 17], [15, 19, 7], [13, 18, 17]],
        "output_shape": [[13, 18, 17], [15, 19, 7], [13, 18, 17], [13, 18, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_add[(((ax0 * 120) + (ax1 * 8)) + ax2)] = (ph_0[(((ax0 * 120) + (ax1 * 8)) + ax2)] + ph_3[(((ax0 * 120) + (ax1 * 8)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 2; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 15; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n        T_multiply[(((ax0_1 * 120) + (ax1_1 * 8)) + ax2_1)] = (fabsf(ph_0[(((ax0_1 * 120) + (ax1_1 * 8)) + ax2_1)]) * ph_0[(((ax0_1 * 120) + (ax1_1 * 8)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 240; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 15, 8), \"float32\"), ph_3: T.Buffer((2, 15, 8), \"float32\"), T_mod: T.Buffer((2, 15, 8), \"float32\"), T_add: T.Buffer((2, 15, 8), \"float32\"), T_multiply: T.Buffer((2, 15, 8), \"float32\"), T_subtract: T.Buffer((2, 15, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        ph_3_1 = T.Buffer((240,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_mod_1 = T.Buffer((240,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(15, 8):\n                cse_var_1: T.int32 = ax0 * 120 + ax1 * 8 + ax2\n                T_add_1 = T.Buffer((240,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(15, 8):\n                cse_var_2: T.int32 = ax0 * 120 + ax1 * 8 + ax2\n                T_multiply_1 = T.Buffer((240,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_subtract_1 = T.Buffer((240,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "add",
                "abs",
                "multiply",
                "subtract"
            ]
        ],
        "input_shape": [[2, 15, 8], [5, 8, 13], [2, 15, 8]],
        "output_shape": [[2, 15, 8], [5, 8, 13], [2, 15, 8], [2, 15, 8], [2, 15, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 3; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_add[((ax0_ax1_fused * 16) + ax2)] = (ph_0[((ax0_ax1_fused * 16) + ax2)] + sinf(acoshf(ph_0[((ax0_ax1_fused * 16) + ax2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax2_1 = 0; ax2_1 < 16; ++ax2_1) {\n      T_multiply[((ax0 * 16) + ax2_1)] = (sinf(ph_0[((ax0 * 16) + ax2_1)]) * ph_0[((ax0 * 16) + ax2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __sinf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 1, 16), \"float32\"), compute: T.Buffer((3, 1, 16), \"float32\"), T_add: T.Buffer((3, 1, 16), \"float32\"), T_multiply: T.Buffer((3, 1, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_1 = T.Buffer((48,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_add_1 = T.Buffer((48,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.sin(T.acosh(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(3):\n            for ax2 in range(16):\n                cse_var_2: T.int32 = ax0 * 16 + ax2\n                T_multiply_1 = T.Buffer((48,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.sin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]",
        "op_args": [
            [
                "sin",
                "acosh",
                "sin",
                "add",
                "sin",
                "multiply"
            ]
        ],
        "input_shape": [[3, 1, 16]],
        "output_shape": [[3, 1, 16], [3, 1, 16], [3, 1, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_mod[(((ax0 * 160) + (ax1 * 10)) + ax2)] = fmodf(ph_0[(((ax0 * 160) + (ax1 * 10)) + ax2)], ph_3[(((ax0 * 160) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = atanf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 16, 10), \"float32\"), ph_3: T.Buffer((6, 16, 10), \"float32\"), T_multiply: T.Buffer((6, 16, 10), \"float32\"), T_mod: T.Buffer((6, 16, 10), \"float32\"), compute: T.Buffer((6, 16, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        ph_3_1 = T.Buffer((960,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            T_multiply_1 = T.Buffer((960,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(16, 10):\n                cse_var_1: T.int32 = ax0 * 160 + ax1 * 10 + ax2\n                T_mod_1 = T.Buffer((960,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_1 = T.Buffer((960,), data=compute.data)\n                compute_1[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "mod",
                "atan"
            ]
        ],
        "input_shape": [[6, 16, 10], [10, 12, 2], [6, 16, 10]],
        "output_shape": [[6, 16, 10], [10, 12, 2], [6, 16, 10], [6, 16, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 165; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = asinf(acoshf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2640; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2640; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinf(ceilf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2640; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 15, 16), \"float32\"), compute: T.Buffer((11, 15, 16), \"float32\"), compute_1: T.Buffer((11, 15, 16), \"float32\"), compute_2: T.Buffer((11, 15, 16), \"float32\"), compute_3: T.Buffer((11, 15, 16), \"float32\"), T_subtract: T.Buffer((11, 15, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2640,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_4 = T.Buffer((2640,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(165):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((2640,), data=compute_1.data)\n                compute_4[cse_var_1] = T.asin(T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_4 = T.Buffer((2640,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_4 = T.Buffer((2640,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(2640):\n            T_subtract_1 = T.Buffer((2640,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "sin",
                "acosh",
                "asin",
                "asinh",
                "ceil",
                "asin",
                "subtract"
            ]
        ],
        "input_shape": [[11, 15, 16]],
        "output_shape": [[11, 15, 16], [11, 15, 16], [11, 15, 16], [11, 15, 16], [11, 15, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 238; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = ceilf(fmodf(ph_0[((i0_i1_fused * 4) + i2)], ceilf(ph_0[((i0_i1_fused * 4) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 952; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 952; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 17, 4), \"float32\"), compute: T.Buffer((14, 17, 4), \"float32\"), compute_1: T.Buffer((14, 17, 4), \"float32\"), compute_2: T.Buffer((14, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((952,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(238):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_3 = T.Buffer((952,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(T.truncmod(ph_0_1[cse_var_1], T.ceil(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(952):\n            compute_3 = T.Buffer((952,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(952):\n            compute_3 = T.Buffer((952,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "mod",
                "ceil",
                "acosh",
                "cos",
                "cos"
            ]
        ],
        "input_shape": [[14, 17, 4]],
        "output_shape": [[14, 17, 4], [14, 17, 4], [14, 17, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  float compute_1[468];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 468; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_1[(((i0 * 78) + (i1 * 6)) + i2)] = expf(ph_0[(((i0 * 78) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 468; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - acoshf(compute_1[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - acoshf(__expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 13, 6), \"float32\"), compute: T.Buffer((6, 13, 6), \"float32\"), T_subtract: T.Buffer((6, 13, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([468], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_2 = T.Buffer((468,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        compute_2 = T.Buffer((468,), data=compute_1)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(13, 6):\n                cse_var_1: T.int32 = i0 * 78 + i1 * 6 + i2\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(468):\n            T_subtract_1 = T.Buffer((468,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.acosh(compute_2[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "ceil",
                "exp",
                "acosh",
                "subtract"
            ]
        ],
        "input_shape": [[6, 13, 6]],
        "output_shape": [[6, 13, 6], [6, 13, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 221) + (i1 * 17)) + i2)] = fabsf(ph_0[(((i0 * 221) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 52; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n      compute_1[((i0_i1_fused * 17) + i2_1)] = asinhf(cosf(ph_0[((i0_i1_fused * 17) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 13, 17), \"float32\"), compute: T.Buffer((4, 13, 17), \"float32\"), compute_1: T.Buffer((4, 13, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((884,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(13, 17):\n                cse_var_1: T.int32 = i0 * 221 + i1 * 17 + i2\n                compute_2 = T.Buffer((884,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(52):\n            for i2 in range(17):\n                cse_var_2: T.int32 = i0_i1_fused * 17 + i2\n                compute_2 = T.Buffer((884,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[4, 13, 17]],
        "output_shape": [[4, 13, 17], [4, 13, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 612; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] / asinhf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 612; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_1] = fmodf((ph_0[ax0_ax1_fused_ax2_fused_1] / asinhf(ph_0[ax0_ax1_fused_ax2_fused_1])), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 36; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = cosf((ph_0[((i0_i1_fused * 17) + i2)] * ph_3[((i0_i1_fused * 17) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n        compute_1[(((i0 * 68) + (i1 * 17)) + i2_1)] = fabsf((ph_0[(((i0 * 68) + (i1 * 17)) + i2_1)] * ph_3[(((i0 * 68) + (i1 * 17)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 4, 17), \"float32\"), ph_3: T.Buffer((9, 4, 17), \"float32\"), T_mod: T.Buffer((9, 4, 17), \"float32\"), T_mod_1: T.Buffer((9, 4, 17), \"float32\"), compute: T.Buffer((9, 4, 17), \"float32\"), compute_1: T.Buffer((9, 4, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((612,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(612):\n            T_mod_2 = T.Buffer((612,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] / T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(612):\n            T_mod_2 = T.Buffer((612,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] / T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        ph_3_1 = T.Buffer((612,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(36):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_2 = T.Buffer((612,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1] * ph_3_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(4, 17):\n                cse_var_2: T.int32 = i0 * 68 + i1 * 17 + i2\n                compute_2 = T.Buffer((612,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2] * ph_3_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "asinh",
                "divide",
                "mod",
                "mod",
                "cos",
                "abs"
            ]
        ],
        "input_shape": [[9, 4, 17], [18, 5, 19], [9, 4, 17]],
        "output_shape": [[18, 5, 19], [9, 4, 17], [9, 4, 17], [9, 4, 17], [9, 4, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2964; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 247) + (i1 * 13)) + i2)] = asinf(acoshf(ph_0[(((i0 * 247) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 228; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_2[((i0_i1_fused * 13) + i2_1)] = asinf(ph_0[((i0_i1_fused * 13) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 19, 13), \"float32\"), compute: T.Buffer((12, 19, 13), \"float32\"), compute_1: T.Buffer((12, 19, 13), \"float32\"), compute_2: T.Buffer((12, 19, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2964,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2964):\n            compute_3 = T.Buffer((2964,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(19, 13):\n                cse_var_1: T.int32 = i0 * 247 + i1 * 13 + i2\n                compute_3 = T.Buffer((2964,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asin(T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(228):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_3 = T.Buffer((2964,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "acosh",
                "asin",
                "asin"
            ]
        ],
        "input_shape": [[12, 19, 13]],
        "output_shape": [[12, 19, 13], [12, 19, 13], [12, 19, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = asinhf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2100; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2100; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(__sinf(ph_0[((int)blockIdx.x)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 10, 15), \"float32\"), compute: T.Buffer((14, 10, 15), \"float32\"), compute_1: T.Buffer((14, 10, 15), \"float32\"), compute_2: T.Buffer((14, 10, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2100,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(140):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_3 = T.Buffer((2100,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2100):\n            compute_3 = T.Buffer((2100,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2100):\n            compute_3 = T.Buffer((2100,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "sin",
                "exp",
                "ceil"
            ]
        ],
        "input_shape": [[14, 10, 15]],
        "output_shape": [[14, 10, 15], [14, 10, 15], [14, 10, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  float compute_4[1008];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1008; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1008; ++i0_i1_fused_i2_fused_1) {\n    compute_4[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1008; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = sinf(compute_4[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_2[((i0_i1_fused * 14) + i2)] = fabsf(compute_4[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 1008; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = asinf((ph_0[i0_i1_fused_i2_fused_3] - ph_3[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 8, 14), \"float32\"), ph_3: T.Buffer((9, 8, 14), \"float32\"), compute: T.Buffer((9, 8, 14), \"float32\"), compute_1: T.Buffer((9, 8, 14), \"float32\"), compute_2: T.Buffer((9, 8, 14), \"float32\"), compute_3: T.Buffer((9, 8, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_4 = T.allocate([1008], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1008,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_5 = T.Buffer((1008,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        compute_5 = T.Buffer((1008,), data=compute_4)\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_6 = T.Buffer((1008,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.sin(compute_5[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(72):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_6 = T.Buffer((1008,), data=compute_2.data)\n                compute_6[cse_var_1] = T.fabs(compute_5[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_6 = T.Buffer((1008,), data=compute_3.data)\n            ph_3_1 = T.Buffer((1008,), data=ph_3.data)\n            compute_6[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "asinh",
                "exp",
                "sin",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[9, 8, 14], [9, 3, 17], [9, 8, 14]],
        "output_shape": [[9, 3, 17], [9, 8, 14], [9, 8, 14], [9, 8, 14], [9, 8, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 480; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_2[(((i0 * 48) + (i1 * 8)) + i2)] = fabsf(ph_0[(((i0 * 48) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 6, 8), \"float32\"), compute: T.Buffer((10, 6, 8), \"float32\"), compute_1: T.Buffer((10, 6, 8), \"float32\"), compute_2: T.Buffer((10, 6, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_3 = T.Buffer((480,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_3 = T.Buffer((480,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(6, 8):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 8 + i2\n                compute_3 = T.Buffer((480,), data=compute_2.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "acos",
                "abs",
                "abs"
            ]
        ],
        "input_shape": [[10, 6, 8]],
        "output_shape": [[10, 6, 8], [10, 6, 8], [10, 6, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 15) + (i1 * 3)) + i2)] = acoshf((ph_0[(((i0 * 15) + (i1 * 3)) + i2)] + asinhf(ph_0[(((i0 * 15) + (i1 * 3)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 300; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 300; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 5, 3), \"float32\"), compute: T.Buffer((20, 5, 3), \"float32\"), compute_1: T.Buffer((20, 5, 3), \"float32\"), compute_2: T.Buffer((20, 5, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((300,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(5, 3):\n                cse_var_1: T.int32 = i0 * 15 + i1 * 3 + i2\n                compute_3 = T.Buffer((300,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            compute_3 = T.Buffer((300,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            compute_3 = T.Buffer((300,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "add",
                "acosh",
                "ceil",
                "acosh",
                "sin"
            ]
        ],
        "input_shape": [[20, 5, 3]],
        "output_shape": [[20, 5, 3], [20, 5, 3], [20, 5, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 60) + (i1 * 12)) + i2)] = acosf(ph_0[(((i0 * 60) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 600; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 600; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ceilf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((int)blockIdx.x)] = (atanf(ph_0[((int)blockIdx.x)]) * ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 5, 12), \"float32\"), compute: T.Buffer((10, 5, 12), \"float32\"), T_multiply: T.Buffer((10, 5, 12), \"float32\"), T_mod: T.Buffer((10, 5, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((600,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(5, 12):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 12 + i2\n                compute_1 = T.Buffer((600,), data=compute.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(600):\n            T_multiply_1 = T.Buffer((600,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(600):\n            T_mod_1 = T.Buffer((600,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]))",
        "op_args": [
            [
                "acos",
                "atan",
                "multiply",
                "ceil",
                "mod"
            ]
        ],
        "input_shape": [[10, 5, 12]],
        "output_shape": [[10, 5, 12], [10, 5, 12], [10, 5, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3610; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf((ph_0[i0_i1_fused_i2_fused] * atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3610; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 190; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_2[((i0_i1_fused * 19) + i2)] = expf(fabsf(ph_0[((i0_i1_fused * 19) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3610; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = expf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 19, 19), \"float32\"), compute: T.Buffer((10, 19, 19), \"float32\"), compute_1: T.Buffer((10, 19, 19), \"float32\"), compute_2: T.Buffer((10, 19, 19), \"float32\"), compute_3: T.Buffer((10, 19, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3610,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3610):\n            compute_4 = T.Buffer((3610,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] * T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(3610):\n            compute_4 = T.Buffer((3610,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(190):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_4 = T.Buffer((3610,), data=compute_2.data)\n                compute_4[cse_var_1] = T.exp(T.fabs(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(3610):\n            compute_4 = T.Buffer((3610,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atan",
                "multiply",
                "acosh",
                "cos",
                "abs",
                "exp",
                "exp"
            ]
        ],
        "input_shape": [[10, 19, 19]],
        "output_shape": [[10, 19, 19], [10, 19, 19], [10, 19, 19], [10, 19, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_7) {\n  float auto_scheduler_layout_transform[34];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 680; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 680; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n    for (int32_t ax7 = 0; ax7 < 17; ++ax7) {\n      auto_scheduler_layout_transform[((ax5 * 17) + ax7)] = ph_7[((ax5 * 17) + ax7)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_outer_inner_init * 5)) + i_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 17; ++k_inner) {\n          for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n            T_batch_matmul_NN[((((b_outer_inner * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_outer_inner * 5)) + i_inner)] = (T_batch_matmul_NN[((((b_outer_inner * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_outer_inner * 5)) + i_inner)] + ((ph_0[(((((b_outer_inner * 340) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 170)) + (i_outer_inner * 85)) + (i_inner * 17)) + k_inner)] * atanhf(ph_0[(((((b_outer_inner * 340) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 170)) + (i_outer_inner * 85)) + (i_inner * 17)) + k_inner)])) * auto_scheduler_layout_transform[((b_outer_inner * 17) + k_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_7) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float T_multiply_shared[80];\n  __shared__ float ph_7_shared[10];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n      for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n        T_batch_matmul_NN_local[(((b_c_outer_inner_init * 4) + (i_c_outer_inner_init * 2)) + i_c_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    for (int ax0_ax1_fused_ax2_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_inner_s < 4; ++ax0_ax1_fused_ax2_fused_inner_s) {\n      T_multiply_shared[(((ax0_ax1_fused_ax2_fused_outer_outer * 8) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_inner_s)] = (ph_0[((((((int)blockIdx.x) * 80) + (ax0_ax1_fused_ax2_fused_outer_outer * 8)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_inner_s)] * atanhf(ph_0[((((((int)blockIdx.x) * 80) + (ax0_ax1_fused_ax2_fused_outer_outer * 8)) + (((int)threadIdx.x) * 4)) + ax0_ax1_fused_ax2_fused_inner_s)]));\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 5; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_7_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + ((int)threadIdx.x))] = ph_7[(((((int)blockIdx.x) * 10) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 2)) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n      for (int k_inner = 0; k_inner < 5; ++k_inner) {\n        for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n          T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + (i_c_outer_inner * 2)) + i_c_inner)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + (i_c_outer_inner * 2)) + i_c_inner)] + (T_multiply_shared[(((((b_c_outer_inner * 40) + (((int)threadIdx.x) * 20)) + (i_c_outer_inner * 10)) + (i_c_inner * 5)) + k_inner)] * ph_7_shared[((b_c_outer_inner * 5) + k_inner)]));\n        }\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int i_inner = 0; i_inner < 4; ++i_inner) {\n      T_batch_matmul_NN[((((((int)blockIdx.x) * 16) + (b_inner * 8)) + (((int)threadIdx.x) * 4)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 4) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 20, 17), \"float32\"), ph_7: T.Buffer((2, 17, 1), \"float32\"), compute: T.Buffer((2, 20, 17), \"float32\"), compute_1: T.Buffer((2, 20, 17), \"float32\"), T_batch_matmul_NN: T.Buffer((2, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([34], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((680,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(680):\n            compute_2 = T.Buffer((680,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(680):\n            compute_2 = T.Buffer((680,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((34,), data=auto_scheduler_layout_transform)\n        for ax5, ax7 in T.grid(2, 17):\n            cse_var_1: T.int32 = ax5 * 17 + ax7\n            ph_7_1 = T.Buffer((34,), data=ph_7.data)\n            auto_scheduler_layout_transform_1[cse_var_1] = ph_7_1[cse_var_1]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((40,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_outer_inner_init, i_inner_init in T.grid(2, 2, 5):\n                T_batch_matmul_NN_1[b_outer_inner_init * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + i_outer_inner_init * 5 + i_inner_init] = T.float32(0)\n            for b_outer_inner, i_outer_inner, k_inner, i_inner in T.grid(2, 2, 17, 5):\n                cse_var_3: T.int32 = b_outer_inner * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + i_outer_inner * 5 + i_inner\n                cse_var_2: T.int32 = b_outer_inner * 340 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 170 + i_outer_inner * 85 + i_inner * 17 + k_inner\n                T_batch_matmul_NN_1[cse_var_3] = T_batch_matmul_NN_1[cse_var_3] + ph_0_1[cse_var_2] * T.atanh(ph_0_1[cse_var_2]) * auto_scheduler_layout_transform_1[b_outer_inner * 17 + k_inner]",
        "op_args": [
            [
                "asin",
                "acosh",
                "asin",
                "atanh",
                "multiply",
                "batch_matmul"
            ]
        ],
        "input_shape": [[2, 20, 17], [2, 17, 1]],
        "output_shape": [[2, 20, 17], [2, 20, 17], [2, 20, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1170; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 90) + (i1 * 5)) + i2)] = atanf(cosf(ph_0[(((i0 * 90) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 234; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_2[((i0_i1_fused * 5) + i2_1)] = atanf(ph_0[((i0_i1_fused * 5) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 18, 5), \"float32\"), compute: T.Buffer((13, 18, 5), \"float32\"), compute_1: T.Buffer((13, 18, 5), \"float32\"), compute_2: T.Buffer((13, 18, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_3 = T.Buffer((1170,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(18, 5):\n                cse_var_1: T.int32 = i0 * 90 + i1 * 5 + i2\n                compute_3 = T.Buffer((1170,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(234):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((1170,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "cos",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[13, 18, 5]],
        "output_shape": [[13, 18, 5], [13, 18, 5], [13, 18, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 60; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 11) + ax2)] = (ph_0[((ax0_ax1_fused * 11) + ax2)] - ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 10, 11), \"float32\"), ph_3: T.Buffer((6, 10, 11), \"float32\"), T_subtract: T.Buffer((6, 10, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(60):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_subtract_1 = T.Buffer((660,), data=T_subtract.data)\n                ph_0_1 = T.Buffer((660,), data=ph_0.data)\n                ph_3_1 = T.Buffer((660,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]",
        "op_args": [
            [
                "subtract"
            ]
        ],
        "input_shape": [[6, 10, 11], [6, 20, 3], [6, 10, 11]],
        "output_shape": [[6, 10, 11], [6, 20, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 540; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 36) + (i1 * 18)) + i2)] = acosf(ph_0[(((i0 * 36) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 30; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 18) + ax2)] = (atanhf(ph_0[((ax0_ax1_fused * 18) + ax2)]) - ph_0[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 2, 18), \"float32\"), ph_3: T.Buffer((15, 2, 18), \"float32\"), T_add: T.Buffer((15, 2, 18), \"float32\"), compute: T.Buffer((15, 2, 18), \"float32\"), T_subtract: T.Buffer((15, 2, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(540):\n            T_add_1 = T.Buffer((540,), data=T_add.data)\n            ph_3_1 = T.Buffer((540,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(2, 18):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 18 + i2\n                compute_1 = T.Buffer((540,), data=compute.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(30):\n            for ax2 in range(18):\n                cse_var_2: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_subtract_1 = T.Buffer((540,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.atanh(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "add",
                "acos",
                "atanh",
                "subtract"
            ]
        ],
        "input_shape": [[15, 2, 18], [11, 10, 19], [15, 2, 18]],
        "output_shape": [[15, 2, 18], [11, 10, 19], [15, 2, 18], [15, 2, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* T_multiply_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 105; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] * ph_3[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 420; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 420; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 7, 4), \"float32\"), ph_3: T.Buffer((15, 7, 4), \"float32\"), T_multiply: T.Buffer((15, 7, 4), \"float32\"), T_add: T.Buffer((15, 7, 4), \"float32\"), T_multiply_1: T.Buffer((15, 7, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((420,), data=ph_0.data)\n        ph_3_1 = T.Buffer((420,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(105):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_multiply_2 = T.Buffer((420,), data=T_multiply.data)\n                T_multiply_2[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(420):\n            T_add_1 = T.Buffer((420,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(420):\n            T_multiply_2 = T.Buffer((420,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "multiply",
                "add",
                "cos",
                "multiply"
            ]
        ],
        "input_shape": [[15, 7, 4], [5, 15, 8], [15, 7, 4]],
        "output_shape": [[15, 7, 4], [5, 15, 8], [15, 7, 4], [15, 7, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[4];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 20; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n      auto_scheduler_layout_transform[((ax0_ax1_fused_ax2_fused * 2) + ax4)] = ph_4[((ax0_ax1_fused_ax2_fused * 2) + ax4)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 5; ++i_outer_inner_init) {\n      T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5) + i_outer_inner_init)] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 5; ++i_outer_inner) {\n        T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5) + i_outer_inner)] = (T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5) + i_outer_inner)] + (asinhf(ph_0[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10) + (i_outer_inner * 2)) + k_outer)]) * auto_scheduler_layout_transform[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 2) + k_outer)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float compute_shared[80];\n  __shared__ float ph_4_shared[40];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_inner_init * 2) + i_c_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 40; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x))] = asinhf(ph_0[(((((ax0_ax1_fused_ax2_fused_outer_outer / 5) * 40) + (((int)blockIdx.x) * 10)) + ((ax0_ax1_fused_ax2_fused_outer_outer % 5) * 2)) + ((int)threadIdx.x))]);\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 20; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_4_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + ((int)threadIdx.x))] = ph_4[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n    for (int k_inner = 0; k_inner < 5; ++k_inner) {\n      for (int b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n        T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_outer_inner)] = (T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_outer_inner)] + (compute_shared[((((((int)threadIdx.x) * 40) + (b_c_inner * 10)) + (i_c_outer_inner * 5)) + k_inner)] * ph_4_shared[(((((int)threadIdx.x) * 20) + (b_c_inner * 5)) + k_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    for (int i_inner = 0; i_inner < 2; ++i_inner) {\n      T_batch_matmul_NN[((((((int)threadIdx.x) * 32) + (b_inner * 8)) + (((int)blockIdx.x) * 2)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 2) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 5, 2), \"float32\"), ph_4: T.Buffer((2, 2, 1), \"float32\"), compute: T.Buffer((2, 5, 2), \"float32\"), T_batch_matmul_NN: T.Buffer((2, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([4], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((20,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(20):\n            compute_1 = T.Buffer((20,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((4,), data=auto_scheduler_layout_transform, align=16)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2):\n            for ax4 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 2 + ax4\n                ph_4_1 = T.Buffer((4,), data=ph_4.data)\n                auto_scheduler_layout_transform_1[cse_var_1] = ph_4_1[cse_var_1]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((10,), data=T_batch_matmul_NN.data)\n            for i_outer_inner_init in range(5):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + i_outer_inner_init] = T.float32(0)\n            for k_outer, i_outer_inner in T.grid(2, 5):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + i_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.asinh(ph_0_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + i_outer_inner * 2 + k_outer]) * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 2 + k_outer]",
        "op_args": [
            [
                "asin",
                "asinh",
                "batch_matmul"
            ]
        ],
        "input_shape": [[2, 5, 2], [2, 2, 1]],
        "output_shape": [[2, 5, 2], [2, 5, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[91];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 91; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 91; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 91; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(compute_3[ax0_ax1_fused_ax2_fused], ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 91; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = asinhf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 91; ++i0_i1_fused) {\n    compute_2[i0_i1_fused] = atanhf((ph_0[i0_i1_fused] + ph_3[i0_i1_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(__expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 13, 1), \"float32\"), ph_3: T.Buffer((7, 13, 1), \"float32\"), compute: T.Buffer((7, 13, 1), \"float32\"), T_mod: T.Buffer((7, 13, 1), \"float32\"), compute_1: T.Buffer((7, 13, 1), \"float32\"), compute_2: T.Buffer((7, 13, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([91], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((91,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(91):\n            compute_4 = T.Buffer((91,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((91,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(91):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(91):\n            T_mod_1 = T.Buffer((91,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(compute_4[ax0_ax1_fused_ax2_fused], ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(91):\n            compute_5 = T.Buffer((91,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(91):\n            compute_5 = T.Buffer((91,), data=compute_2.data)\n            ph_3_1 = T.Buffer((91,), data=ph_3.data)\n            compute_5[i0_i1_fused] = T.atanh(ph_0_1[i0_i1_fused] + ph_3_1[i0_i1_fused])",
        "op_args": [
            [
                "add",
                "sin",
                "exp",
                "mod",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[7, 13, 1], [2, 10, 7], [7, 13, 1]],
        "output_shape": [[2, 10, 7], [7, 13, 1], [7, 13, 1], [7, 13, 1], [7, 13, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[550];\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    float compute_3[110];\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2_s = 0; i2_s < 10; ++i2_s) {\n        compute_3[((i1 * 10) + i2_s)] = expf((ph_0[(((ax0 * 110) + (i1 * 10)) + i2_s)] * atanf(ph_0[(((ax0 * 110) + (i1 * 10)) + i2_s)])));\n      }\n    }\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_subtract[(((ax0 * 110) + (ax1 * 10)) + ax2)] = (ph_0[(((ax0 * 110) + (ax1 * 10)) + ax2)] - compute_3[((ax1 * 10) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 550; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 550; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = asinf(compute_2[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 55; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute_1[((i0_i1_fused * 10) + i2)] = asinhf(compute_2[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] - __expf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 11, 10), \"float32\"), T_subtract: T.Buffer((5, 11, 10), \"float32\"), compute: T.Buffer((5, 11, 10), \"float32\"), compute_1: T.Buffer((5, 11, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([550], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((550,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            compute_3 = T.allocate([110], \"float32\", \"global\")\n            compute_4 = T.Buffer((110,), data=compute_3)\n            for i1, i2_s in T.grid(11, 10):\n                cse_var_2: T.int32 = i1 * 10\n                cse_var_1: T.int32 = ax0 * 110 + cse_var_2 + i2_s\n                compute_4[cse_var_2 + i2_s] = T.exp(ph_0_1[cse_var_1] * T.atan(ph_0_1[cse_var_1]))\n            for ax1, ax2 in T.grid(11, 10):\n                cse_var_4: T.int32 = ax1 * 10\n                cse_var_3: T.int32 = ax0 * 110 + cse_var_4 + ax2\n                T_subtract_1 = T.Buffer((550,), data=T_subtract.data)\n                T_subtract_1[cse_var_3] = ph_0_1[cse_var_3] - compute_4[cse_var_4 + ax2]\n        compute_3 = T.Buffer((550,), data=compute_2)\n        for i0_i1_fused_i2_fused in T.parallel(550):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(550):\n            compute_4 = T.Buffer((550,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(compute_3[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(55):\n            for i2 in range(10):\n                cse_var_5: T.int32 = i0_i1_fused * 10 + i2\n                compute_4 = T.Buffer((550,), data=compute_1.data)\n                compute_4[cse_var_5] = T.asinh(compute_3[cse_var_5])",
        "op_args": [
            [
                "atan",
                "multiply",
                "exp",
                "subtract",
                "exp",
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[5, 11, 10]],
        "output_shape": [[5, 11, 10], [5, 11, 10], [5, 11, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 198) + (i1 * 18)) + i2)] = sinf(ph_0[(((i0 * 198) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1386; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 77; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_2[((i0_i1_fused * 18) + i2_1)] = expf(ph_0[((i0_i1_fused * 18) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 18), \"float32\"), compute: T.Buffer((7, 11, 18), \"float32\"), compute_1: T.Buffer((7, 11, 18), \"float32\"), compute_2: T.Buffer((7, 11, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1386,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(11, 18):\n                cse_var_1: T.int32 = i0 * 198 + i1 * 18 + i2\n                compute_3 = T.Buffer((1386,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1386):\n            compute_3 = T.Buffer((1386,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(77):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((1386,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "sin",
                "asin",
                "exp",
                "exp"
            ]
        ],
        "input_shape": [[7, 11, 18]],
        "output_shape": [[7, 11, 18], [7, 11, 18], [7, 11, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 10) + (i1 * 5)) + i2)] = cosf(ph_0[(((i0 * 10) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 70; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + fabsf(cosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 14; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_1[((i0_i1_fused * 5) + i2_1)] = acoshf(ph_0[((i0_i1_fused * 5) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + fabsf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 2, 5), \"float32\"), compute: T.Buffer((7, 2, 5), \"float32\"), T_add: T.Buffer((7, 2, 5), \"float32\"), compute_1: T.Buffer((7, 2, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((70,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(2, 5):\n                cse_var_1: T.int32 = i0 * 10 + i1 * 5 + i2\n                compute_2 = T.Buffer((70,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(70):\n            T_add_1 = T.Buffer((70,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.fabs(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused in T.parallel(14):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((70,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "cos",
                "abs",
                "add",
                "acosh"
            ]
        ],
        "input_shape": [[7, 2, 5]],
        "output_shape": [[7, 2, 5], [7, 2, 5], [7, 2, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 80) + (i1 * 20)) + i2)] = fabsf(ph_0[(((i0 * 80) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1120; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1120; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_3[((i0_i1_fused * 20) + i2_1)] = atanhf((ph_0[((i0_i1_fused * 20) + i2_1)] + ph_3[((i0_i1_fused * 20) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 4, 20), \"float32\"), ph_3: T.Buffer((14, 4, 20), \"float32\"), compute: T.Buffer((14, 4, 20), \"float32\"), compute_1: T.Buffer((14, 4, 20), \"float32\"), compute_2: T.Buffer((14, 4, 20), \"float32\"), compute_3: T.Buffer((14, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1120,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(4, 20):\n                cse_var_1: T.int32 = i0 * 80 + i1 * 20 + i2\n                compute_4 = T.Buffer((1120,), data=compute.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1120):\n            compute_4 = T.Buffer((1120,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1120):\n            compute_4 = T.Buffer((1120,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(56):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_4 = T.Buffer((1120,), data=compute_3.data)\n                ph_3_1 = T.Buffer((1120,), data=ph_3.data)\n                compute_4[cse_var_2] = T.atanh(ph_0_1[cse_var_2] + ph_3_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "abs",
                "abs",
                "ceil",
                "atan",
                "atanh"
            ]
        ],
        "input_shape": [[14, 4, 20], [12, 8, 8], [14, 4, 20]],
        "output_shape": [[12, 8, 8], [14, 4, 20], [14, 4, 20], [14, 4, 20], [14, 4, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 20; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = atanhf(ph_0[((i0 * 2) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 10; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n      compute_1[((i0_1 * 2) + i1_1)] = ceilf(ph_0[((i0_1 * 2) + i1_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 2, 1), \"float32\"), ph_3: T.Buffer((10, 2, 1), \"float32\"), T_multiply: T.Buffer((10, 2, 1), \"float32\"), compute: T.Buffer((10, 2, 1), \"float32\"), compute_1: T.Buffer((10, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((20,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(20):\n            T_multiply_1 = T.Buffer((20,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((20,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(10):\n            for i1 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i1\n                compute_2 = T.Buffer((20,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(10):\n            for i1 in range(2):\n                cse_var_2: T.int32 = i0 * 2 + i1\n                compute_2 = T.Buffer((20,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[10, 2, 1], [4, 2, 7], [10, 2, 1]],
        "output_shape": [[10, 2, 1], [4, 2, 7], [10, 2, 1], [10, 2, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 119; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 119; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 119; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acoshf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_3[((i0_i1_fused * 17) + i2)] = ceilf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(asinhf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 1, 17), \"float32\"), compute: T.Buffer((7, 1, 17), \"float32\"), compute_1: T.Buffer((7, 1, 17), \"float32\"), compute_2: T.Buffer((7, 1, 17), \"float32\"), compute_3: T.Buffer((7, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((119,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(119):\n            compute_4 = T.Buffer((119,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(119):\n            compute_4 = T.Buffer((119,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(119):\n            compute_4 = T.Buffer((119,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(7):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_4 = T.Buffer((119,), data=compute_3.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "asinh",
                "acos",
                "acosh",
                "ceil"
            ]
        ],
        "input_shape": [[7, 1, 17]],
        "output_shape": [[7, 1, 17], [7, 1, 17], [7, 1, 17], [7, 1, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 12) + (i1 * 6)) + i2)] = ceilf(ph_0[(((i0 * 12) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 36; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(cosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 36; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 36; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_2[((i0_i1_fused * 6) + i2_1)] = acoshf(ceilf(ph_0[((i0_i1_fused * 6) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 6), \"float32\"), compute: T.Buffer((3, 2, 6), \"float32\"), T_mod: T.Buffer((3, 2, 6), \"float32\"), compute_1: T.Buffer((3, 2, 6), \"float32\"), T_subtract: T.Buffer((3, 2, 6), \"float32\"), compute_2: T.Buffer((3, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((36,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(2, 6):\n                cse_var_1: T.int32 = i0 * 12 + i1 * 6 + i2\n                compute_3 = T.Buffer((36,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(36):\n            T_mod_1 = T.Buffer((36,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_3 = T.Buffer((36,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(36):\n            T_subtract_1 = T.Buffer((36,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((36,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acosh(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "ceil",
                "cos",
                "mod",
                "asin",
                "ceil",
                "subtract",
                "acosh"
            ]
        ],
        "input_shape": [[3, 2, 6]],
        "output_shape": [[3, 2, 6], [3, 2, 6], [3, 2, 6], [3, 2, 6], [3, 2, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_mod[(((ax0 * 60) + (ax1 * 4)) + ax2)] = fmodf(ph_0[(((ax0 * 60) + (ax1 * 4)) + ax2)], ph_3[(((ax0 * 60) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = acosf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 4), \"float32\"), ph_3: T.Buffer((16, 15, 4), \"float32\"), T_mod: T.Buffer((16, 15, 4), \"float32\"), T_add: T.Buffer((16, 15, 4), \"float32\"), compute: T.Buffer((16, 15, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        ph_3_1 = T.Buffer((960,), data=ph_3.data)\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(15, 4):\n                cse_var_1: T.int32 = ax0 * 60 + ax1 * 4 + ax2\n                T_mod_1 = T.Buffer((960,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            T_add_1 = T.Buffer((960,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_1 = T.Buffer((960,), data=compute.data)\n                compute_1[cse_var_2] = T.acos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "mod",
                "add",
                "acos"
            ]
        ],
        "input_shape": [[16, 15, 4], [16, 6, 1], [16, 15, 4]],
        "output_shape": [[16, 15, 4], [16, 6, 1], [16, 15, 4], [16, 15, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 221; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = asinf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 884; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (atanf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_1[(((i0 * 52) + (i1 * 4)) + i2_1)] = cosf(sinf(ph_0[(((i0 * 52) + (i1 * 4)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + (atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 13, 4), \"float32\"), compute: T.Buffer((17, 13, 4), \"float32\"), T_add: T.Buffer((17, 13, 4), \"float32\"), compute_1: T.Buffer((17, 13, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((884,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(221):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_2 = T.Buffer((884,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(884):\n            T_add_1 = T.Buffer((884,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + (T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(13, 4):\n                cse_var_2: T.int32 = i0 * 52 + i1 * 4 + i2\n                compute_2 = T.Buffer((884,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asin",
                "atan",
                "subtract",
                "add",
                "sin",
                "cos"
            ]
        ],
        "input_shape": [[17, 13, 4]],
        "output_shape": [[17, 13, 4], [17, 13, 4], [17, 13, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 285; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (atanhf(ph_0[((ax0_ax1_fused * 8) + ax2)]) / ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2280; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2280; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 19, 8), \"float32\"), compute: T.Buffer((15, 19, 8), \"float32\"), T_divide: T.Buffer((15, 19, 8), \"float32\"), compute_1: T.Buffer((15, 19, 8), \"float32\"), compute_2: T.Buffer((15, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2280):\n            compute_3 = T.Buffer((2280,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(285):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((2280,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2280):\n            compute_3 = T.Buffer((2280,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2280):\n            compute_3 = T.Buffer((2280,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "divide",
                "ceil",
                "atan"
            ]
        ],
        "input_shape": [[15, 19, 8]],
        "output_shape": [[15, 19, 8], [15, 19, 8], [15, 19, 8], [15, 19, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 288) + (i1 * 18)) + i2)] = asinf(ph_0[(((i0 * 288) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 16; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_1[(((i0_1 * 288) + (i1_1 * 18)) + i2_1)] = sinf(ph_0[(((i0_1 * 288) + (i1_1 * 18)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 16, 18), \"float32\"), compute: T.Buffer((4, 16, 18), \"float32\"), compute_1: T.Buffer((4, 16, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1152,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(16, 18):\n                cse_var_1: T.int32 = i0 * 288 + i1 * 18 + i2\n                compute_2 = T.Buffer((1152,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(16, 18):\n                cse_var_2: T.int32 = i0 * 288 + i1 * 18 + i2\n                compute_2 = T.Buffer((1152,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asin",
                "sin"
            ]
        ],
        "input_shape": [[4, 16, 18]],
        "output_shape": [[4, 16, 18], [4, 16, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_4) {\n  float T_batch_matmul_NN_rf[266];\n  #pragma omp parallel for\n  for (int32_t b_i_fused_j_fused = 0; b_i_fused_j_fused < 14; ++b_i_fused_j_fused) {\n    for (int32_t k_inner_init = 0; k_inner_init < 19; ++k_inner_init) {\n      T_batch_matmul_NN_rf[((b_i_fused_j_fused * 19) + k_inner_init)] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 19; ++k_inner) {\n      T_batch_matmul_NN_rf[((b_i_fused_j_fused * 19) + k_inner)] = (T_batch_matmul_NN_rf[((b_i_fused_j_fused * 19) + k_inner)] + (ceilf(ph_0[((b_i_fused_j_fused * 19) + k_inner)]) * ph_4[k_inner]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 14; ++ax0_ax1_fused_ax2_fused) {\n    T_batch_matmul_NN[ax0_ax1_fused_ax2_fused] = 0.000000e+00f;\n    for (int32_t k_inner_v = 0; k_inner_v < 19; ++k_inner_v) {\n      T_batch_matmul_NN[ax0_ax1_fused_ax2_fused] = (T_batch_matmul_NN[ax0_ax1_fused_ax2_fused] + T_batch_matmul_NN_rf[((ax0_ax1_fused_ax2_fused * 19) + k_inner_v)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 266; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 266; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 14; ++i0_i1_fused) {\n    float compute_3[19];\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_3[i2] = expf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_2[((i0_i1_fused * 19) + i2_1)] = acosf(compute_3[i2_1]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float compute_shared[48];\n  for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n    T_batch_matmul_NN_local[j_c_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 4; ++k_outer_outer) {\n    __syncthreads();\n    compute_shared[((int)threadIdx.x)] = ceilf(ph_0[(((((((int)blockIdx.x) >> 1) * 192) + ((((int)threadIdx.x) >> 1) * 8)) + (k_outer_outer * 2)) + (((int)threadIdx.x) & 1))]);\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n      for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n        T_batch_matmul_NN_local[j_c_inner] = (T_batch_matmul_NN_local[j_c_inner] + (compute_shared[(((((int)threadIdx.x) >> 1) * 2) + k_outer_inner)] * ph_0[((((((((((int)blockIdx.x) >> 1) * 192) + ((((int)threadIdx.x) >> 4) * 64)) + (k_outer_outer * 16)) + (k_outer_inner * 8)) + ((((int)blockIdx.x) & 1) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_c_inner)]));\n      }\n    }\n  }\n  for (int j_inner = 0; j_inner < 2; ++j_inner) {\n    T_batch_matmul_NN[((((((((int)blockIdx.x) >> 1) * 192) + ((((int)threadIdx.x) >> 1) * 8)) + ((((int)blockIdx.x) & 1) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + j_inner)] = T_batch_matmul_NN_local[j_inner];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 14, 19), \"float32\"), ph_4: T.Buffer((1, 19, 1), \"float32\"), T_batch_matmul_NN: T.Buffer((1, 14, 1), \"float32\"), compute: T.Buffer((1, 14, 19), \"float32\"), compute_1: T.Buffer((1, 14, 19), \"float32\"), compute_2: T.Buffer((1, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        T_batch_matmul_NN_rf = T.allocate([266], \"float32\", \"global\")\n        T_batch_matmul_NN_rf_1 = T.Buffer((266,), data=T_batch_matmul_NN_rf)\n        ph_0_1 = T.Buffer((266,), data=ph_0.data)\n        for b_i_fused_j_fused in T.parallel(14):\n            for k_inner_init in range(19):\n                T_batch_matmul_NN_rf_1[b_i_fused_j_fused * 19 + k_inner_init] = T.float32(0)\n            for k_inner in range(19):\n                cse_var_1: T.int32 = b_i_fused_j_fused * 19 + k_inner\n                ph_4_1 = T.Buffer((19,), data=ph_4.data)\n                T_batch_matmul_NN_rf_1[cse_var_1] = T_batch_matmul_NN_rf_1[cse_var_1] + T.ceil(ph_0_1[cse_var_1]) * ph_4_1[k_inner]\n        for ax0_ax1_fused_ax2_fused in T.parallel(14):\n            T_batch_matmul_NN_1 = T.Buffer((14,), data=T_batch_matmul_NN.data)\n            T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused] = T.float32(0)\n            for k_inner_v in range(19):\n                T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused] = T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused] + T_batch_matmul_NN_rf_1[ax0_ax1_fused_ax2_fused * 19 + k_inner_v]\n        for i0_i1_fused_i2_fused in T.parallel(266):\n            compute_3 = T.Buffer((266,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(266):\n            compute_3 = T.Buffer((266,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(14):\n            compute_3 = T.allocate([19], \"float32\", \"global\")\n            compute_4 = T.Buffer((19,), data=compute_3)\n            for i2 in range(19):\n                compute_4[i2] = T.exp(ph_0_1[i0_i1_fused * 19 + i2])\n            for i2 in range(19):\n                compute_5 = T.Buffer((266,), data=compute_2.data)\n                compute_5[i0_i1_fused * 19 + i2] = T.acos(compute_4[i2])",
        "op_args": [
            [
                "exp",
                "ceil",
                "batch_matmul",
                "acosh",
                "atanh",
                "acos",
                "acos"
            ]
        ],
        "input_shape": [[1, 14, 19], [1, 19, 1]],
        "output_shape": [[1, 14, 1], [1, 14, 19], [1, 14, 19], [1, 14, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 323; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_add[((ax0_ax1_fused * 7) + ax2)] = (ph_0[((ax0_ax1_fused * 7) + ax2)] + ph_3[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2261; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2261; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 17, 7), \"float32\"), ph_3: T.Buffer((19, 17, 7), \"float32\"), T_add: T.Buffer((19, 17, 7), \"float32\"), compute: T.Buffer((19, 17, 7), \"float32\"), compute_1: T.Buffer((19, 17, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2261,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(323):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_add_1 = T.Buffer((2261,), data=T_add.data)\n                ph_3_1 = T.Buffer((2261,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2261):\n            compute_2 = T.Buffer((2261,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2261):\n            compute_2 = T.Buffer((2261,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "acos",
                "asin"
            ]
        ],
        "input_shape": [[19, 17, 7], [7, 18, 11], [19, 17, 7]],
        "output_shape": [[19, 17, 7], [7, 18, 11], [19, 17, 7], [19, 17, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3808; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_add[(((ax0 * 272) + (ax1 * 17)) + ax2)] = (ph_0[(((ax0 * 272) + (ax1 * 17)) + ax2)] + ph_3[(((ax0 * 272) + (ax1 * 17)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3808; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 3808; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add_1[ax0_ax1_fused_ax2_fused_1] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] + ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 16, 17), \"float32\"), ph_3: T.Buffer((14, 16, 17), \"float32\"), T_multiply: T.Buffer((14, 16, 17), \"float32\"), T_add: T.Buffer((14, 16, 17), \"float32\"), compute: T.Buffer((14, 16, 17), \"float32\"), T_add_1: T.Buffer((14, 16, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3808,), data=ph_0.data)\n        ph_3_1 = T.Buffer((3808,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3808):\n            T_multiply_1 = T.Buffer((3808,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(16, 17):\n                cse_var_1: T.int32 = ax0 * 272 + ax1 * 17 + ax2\n                T_add_2 = T.Buffer((3808,), data=T_add.data)\n                T_add_2[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(3808):\n            compute_1 = T.Buffer((3808,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(3808):\n            T_add_2 = T.Buffer((3808,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "multiply",
                "add",
                "acosh",
                "sin",
                "add"
            ]
        ],
        "input_shape": [[14, 16, 17], [16, 7, 8], [14, 16, 17]],
        "output_shape": [[14, 16, 17], [16, 7, 8], [14, 16, 17], [14, 16, 17], [14, 16, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1331; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1331; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1331; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(asinhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_3[(((i0 * 121) + (i1 * 11)) + i2)] = cosf((ph_0[(((i0 * 121) + (i1 * 11)) + i2)] * ph_3[(((i0 * 121) + (i1 * 11)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 11; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n        compute_4[(((i0_1 * 121) + (i1_1 * 11)) + i2_1)] = asinhf((ph_0[(((i0_1 * 121) + (i1_1 * 11)) + i2_1)] * ph_3[(((i0_1 * 121) + (i1_1 * 11)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 11, 11), \"float32\"), ph_3: T.Buffer((11, 11, 11), \"float32\"), compute: T.Buffer((11, 11, 11), \"float32\"), compute_1: T.Buffer((11, 11, 11), \"float32\"), compute_2: T.Buffer((11, 11, 11), \"float32\"), compute_3: T.Buffer((11, 11, 11), \"float32\"), compute_4: T.Buffer((11, 11, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1331,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1331):\n            compute_5 = T.Buffer((1331,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1331):\n            compute_5 = T.Buffer((1331,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1331):\n            compute_5 = T.Buffer((1331,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.cos(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((1331,), data=ph_3.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(11, 11):\n                cse_var_1: T.int32 = i0 * 121 + i1 * 11 + i2\n                compute_5 = T.Buffer((1331,), data=compute_3.data)\n                compute_5[cse_var_1] = T.cos(ph_0_1[cse_var_1] * ph_3_1[cse_var_1])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(11, 11):\n                cse_var_2: T.int32 = i0 * 121 + i1 * 11 + i2\n                compute_5 = T.Buffer((1331,), data=compute_4.data)\n                compute_5[cse_var_2] = T.asinh(ph_0_1[cse_var_2] * ph_3_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "atan",
                "asinh",
                "exp",
                "cos",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[11, 11, 11], [1, 12, 17], [11, 11, 11]],
        "output_shape": [[1, 12, 17], [11, 11, 11], [11, 11, 11], [11, 11, 11], [11, 11, 11], [11, 11, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1083; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1083; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 57; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_1[((i0_i1_fused * 19) + i2)] = cosf(sinf(ph_0[((i0_i1_fused * 19) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 19, 19), \"float32\"), ph_3: T.Buffer((3, 19, 19), \"float32\"), T_subtract: T.Buffer((3, 19, 19), \"float32\"), compute: T.Buffer((3, 19, 19), \"float32\"), compute_1: T.Buffer((3, 19, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1083,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1083):\n            T_subtract_1 = T.Buffer((1083,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((1083,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1083):\n            compute_2 = T.Buffer((1083,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(57):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((1083,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(T.sin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "subtract",
                "atan",
                "sin",
                "cos"
            ]
        ],
        "input_shape": [[3, 19, 19], [6, 2, 17], [3, 19, 19]],
        "output_shape": [[3, 19, 19], [6, 2, 17], [3, 19, 19], [3, 19, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3990; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_mod[(((ax0 * 210) + (ax1 * 15)) + ax2)] = fmodf(ph_0[(((ax0 * 210) + (ax1 * 15)) + ax2)], (atanhf(ph_0[(((ax0 * 210) + (ax1 * 15)) + ax2)]) + ph_0[(((ax0 * 210) + (ax1 * 15)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3990; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 14, 15), \"float32\"), compute: T.Buffer((19, 14, 15), \"float32\"), T_mod: T.Buffer((19, 14, 15), \"float32\"), compute_1: T.Buffer((19, 14, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3990,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3990):\n            compute_2 = T.Buffer((3990,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(14, 15):\n                cse_var_1: T.int32 = ax0 * 210 + ax1 * 15 + ax2\n                T_mod_1 = T.Buffer((3990,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3990):\n            compute_2 = T.Buffer((3990,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "atanh",
                "add",
                "mod",
                "cos"
            ]
        ],
        "input_shape": [[19, 14, 15]],
        "output_shape": [[19, 14, 15], [19, 14, 15], [19, 14, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 969; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 323; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_1[((i0_i1_fused * 3) + i2)] = atanf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 19, 3), \"float32\"), compute: T.Buffer((17, 19, 3), \"float32\"), compute_1: T.Buffer((17, 19, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((969,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(969):\n            compute_2 = T.Buffer((969,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(323):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((969,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[17, 19, 3]],
        "output_shape": [[17, 19, 3], [17, 19, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 20; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = asinhf((ph_0[((i0_i1_fused * 15) + i2)] - asinf(ph_0[((i0_i1_fused * 15) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 20, 15), \"float32\"), compute: T.Buffer((1, 20, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(20):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_1 = T.Buffer((300,), data=compute.data)\n                ph_0_1 = T.Buffer((300,), data=ph_0.data)\n                compute_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1] - T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asin",
                "subtract",
                "asinh"
            ]
        ],
        "input_shape": [[1, 20, 15]],
        "output_shape": [[1, 20, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i0_i1_fused * 20) + i2)] = cosf(fmodf(ph_0[((i0_i1_fused * 20) + i2)], atanhf(ph_0[((i0_i1_fused * 20) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4800; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4800; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = acosf(asinf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 20), \"float32\"), compute: T.Buffer((12, 20, 20), \"float32\"), compute_1: T.Buffer((12, 20, 20), \"float32\"), compute_2: T.Buffer((12, 20, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4800,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((4800,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "mod",
                "cos",
                "abs",
                "asin",
                "acos"
            ]
        ],
        "input_shape": [[12, 20, 20]],
        "output_shape": [[12, 20, 20], [12, 20, 20], [12, 20, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 154; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = atanf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 462; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 462; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 14, 3), \"float32\"), compute: T.Buffer((11, 14, 3), \"float32\"), compute_1: T.Buffer((11, 14, 3), \"float32\"), compute_2: T.Buffer((11, 14, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((462,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(154):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((462,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_3 = T.Buffer((462,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_3 = T.Buffer((462,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "atanh",
                "atan",
                "asin"
            ]
        ],
        "input_shape": [[11, 14, 3]],
        "output_shape": [[11, 14, 3], [11, 14, 3], [11, 14, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4845; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4845; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4845; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_3[(((i0 * 255) + (i1 * 15)) + i2)] = atanf(ph_0[(((i0 * 255) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 17, 15), \"float32\"), compute: T.Buffer((19, 17, 15), \"float32\"), compute_1: T.Buffer((19, 17, 15), \"float32\"), compute_2: T.Buffer((19, 17, 15), \"float32\"), compute_3: T.Buffer((19, 17, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4845,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4845):\n            compute_4 = T.Buffer((4845,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4845):\n            compute_4 = T.Buffer((4845,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4845):\n            compute_4 = T.Buffer((4845,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(17, 15):\n                cse_var_1: T.int32 = i0 * 255 + i1 * 15 + i2\n                compute_4 = T.Buffer((4845,), data=compute_3.data)\n                compute_4[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acos",
                "cos",
                "asinh",
                "sin",
                "atan"
            ]
        ],
        "input_shape": [[19, 17, 15]],
        "output_shape": [[19, 17, 15], [19, 17, 15], [19, 17, 15], [19, 17, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  float compute_1[2640];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2640; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], sinf((ph_0[ax0_ax1_fused_ax2_fused] * fabsf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2640; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = acosf(compute_1[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acosf(__expf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], __sinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 11), \"float32\"), T_mod: T.Buffer((16, 15, 11), \"float32\"), compute: T.Buffer((16, 15, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([2640], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((2640,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2640):\n            T_mod_1 = T.Buffer((2640,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.sin(ph_0_1[ax0_ax1_fused_ax2_fused] * T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        compute_2 = T.Buffer((2640,), data=compute_1)\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_3 = T.Buffer((2640,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(compute_2[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "multiply",
                "sin",
                "mod",
                "exp",
                "acos"
            ]
        ],
        "input_shape": [[16, 15, 11]],
        "output_shape": [[16, 15, 11], [16, 15, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 156; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 156; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * cosf(asinhf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 39; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute_1[((i0_i1_fused * 4) + i2)] = cosf(asinf(ph_0[((i0_i1_fused * 4) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_2[(((i0 * 12) + (i1 * 4)) + i2_1)] = fabsf(asinf(ph_0[(((i0 * 12) + (i1 * 4)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * __cosf(asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = fabsf(asinf(ph_0[((int)blockIdx.x)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 3, 4), \"float32\"), compute: T.Buffer((13, 3, 4), \"float32\"), T_multiply: T.Buffer((13, 3, 4), \"float32\"), compute_1: T.Buffer((13, 3, 4), \"float32\"), compute_2: T.Buffer((13, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((156,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(156):\n            compute_3 = T.Buffer((156,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(156):\n            T_multiply_1 = T.Buffer((156,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.cos(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused in T.parallel(39):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_3 = T.Buffer((156,), data=compute_1.data)\n                compute_3[cse_var_1] = T.cos(T.asin(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(3, 4):\n                cse_var_2: T.int32 = i0 * 12 + i1 * 4 + i2\n                compute_3 = T.Buffer((156,), data=compute_2.data)\n                compute_3[cse_var_2] = T.fabs(T.asin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "ceil",
                "asinh",
                "cos",
                "multiply",
                "asin",
                "cos",
                "abs"
            ]
        ],
        "input_shape": [[13, 3, 4]],
        "output_shape": [[13, 3, 4], [13, 3, 4], [13, 3, 4], [13, 3, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_mod[((ax0 * 2) + ax1)] = fmodf(ph_0[((ax0 * 2) + ax1)], ph_3[((ax0 * 2) + ax1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 28; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 28; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 2, 1), \"float32\"), ph_3: T.Buffer((14, 2, 1), \"float32\"), T_mod: T.Buffer((14, 2, 1), \"float32\"), compute: T.Buffer((14, 2, 1), \"float32\"), compute_1: T.Buffer((14, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((28,), data=ph_0.data)\n        for ax0 in T.parallel(14):\n            for ax1 in range(2):\n                cse_var_1: T.int32 = ax0 * 2 + ax1\n                T_mod_1 = T.Buffer((28,), data=T_mod.data)\n                ph_3_1 = T.Buffer((28,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(28):\n            compute_2 = T.Buffer((28,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(28):\n            compute_2 = T.Buffer((28,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "sin",
                "ceil",
                "asinh"
            ]
        ],
        "input_shape": [[14, 2, 1], [20, 9, 19], [14, 2, 1]],
        "output_shape": [[14, 2, 1], [20, 9, 19], [14, 2, 1], [14, 2, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_multiply[(((ax0 * 14) + (ax1 * 2)) + ax2)] = (ph_0[(((ax0 * 14) + (ax1 * 2)) + ax2)] * ceilf(ph_0[(((ax0 * 14) + (ax1 * 2)) + ax2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] * ceilf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 7, 2), \"float32\"), T_multiply: T.Buffer((17, 7, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(17):\n            for ax1, ax2 in T.grid(7, 2):\n                cse_var_1: T.int32 = ax0 * 14 + ax1 * 2 + ax2\n                T_multiply_1 = T.Buffer((238,), data=T_multiply.data)\n                ph_0_1 = T.Buffer((238,), data=ph_0.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * T.ceil(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "ceil",
                "multiply"
            ]
        ],
        "input_shape": [[17, 7, 2]],
        "output_shape": [[17, 7, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_multiply[(((ax0 * 10) + (ax1 * 5)) + ax2)] = (fmodf(ph_0[(((ax0 * 10) + (ax1 * 5)) + ax2)], acoshf(ph_0[(((ax0 * 10) + (ax1 * 5)) + ax2)])) * ph_0[(((ax0 * 10) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 15; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 5; ++ax2_1) {\n        T_subtract[(((ax0_1 * 10) + (ax1_1 * 5)) + ax2_1)] = (ph_0[(((ax0_1 * 10) + (ax1_1 * 5)) + ax2_1)] - fabsf(ph_0[(((ax0_1 * 10) + (ax1_1 * 5)) + ax2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], acoshf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 2, 5), \"float32\"), T_multiply: T.Buffer((15, 2, 5), \"float32\"), T_subtract: T.Buffer((15, 2, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(2, 5):\n                cse_var_1: T.int32 = ax0 * 10 + ax1 * 5 + ax2\n                T_multiply_1 = T.Buffer((150,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.acosh(ph_0_1[cse_var_1])) * ph_0_1[cse_var_1]\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(2, 5):\n                cse_var_2: T.int32 = ax0 * 10 + ax1 * 5 + ax2\n                T_subtract_1 = T.Buffer((150,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] - T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acosh",
                "mod",
                "multiply",
                "abs",
                "subtract"
            ]
        ],
        "input_shape": [[15, 2, 5]],
        "output_shape": [[15, 2, 5], [15, 2, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 7; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 15) + ax2)] = (ph_0[((ax0_ax1_fused * 15) + ax2)] - ph_3[((ax0_ax1_fused * 15) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 105; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf((ph_0[i0_i1_fused_i2_fused] / (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 105; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf((ph_0[i0_i1_fused_i2_fused_1] / (ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 15), \"float32\"), ph_3: T.Buffer((1, 7, 15), \"float32\"), T_subtract: T.Buffer((1, 7, 15), \"float32\"), compute: T.Buffer((1, 7, 15), \"float32\"), compute_1: T.Buffer((1, 7, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((105,), data=ph_0.data)\n        ph_3_1 = T.Buffer((105,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(7):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_subtract_1 = T.Buffer((105,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(105):\n            compute_2 = T.Buffer((105,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] / (ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(105):\n            compute_2 = T.Buffer((105,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] / (ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "divide",
                "divide",
                "acos",
                "asin"
            ]
        ],
        "input_shape": [[1, 7, 15], [13, 4, 18], [1, 7, 15]],
        "output_shape": [[1, 7, 15], [13, 4, 18], [1, 7, 15], [1, 7, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 20; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i1 * 3) + i2)] = acosf(fmodf(ph_0[((i1 * 3) + i2)], cosf(ph_0[((i1 * 3) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acosf(fmodf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 20, 3), \"float32\"), compute: T.Buffer((1, 20, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i1, i2 in T.grid(20, 3):\n            cse_var_1: T.int32 = i1 * 3 + i2\n            compute_1 = T.Buffer((60,), data=compute.data)\n            ph_0_1 = T.Buffer((60,), data=ph_0.data)\n            compute_1[cse_var_1] = T.acos(T.truncmod(ph_0_1[cse_var_1], T.cos(ph_0_1[cse_var_1])))",
        "op_args": [
            [
                "cos",
                "mod",
                "acos"
            ]
        ],
        "input_shape": [[1, 20, 3]],
        "output_shape": [[1, 20, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = fabsf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 15, 4), \"float32\"), compute: T.Buffer((4, 15, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_1 = T.Buffer((240,), data=compute.data)\n                ph_0_1 = T.Buffer((240,), data=ph_0.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "abs"
            ]
        ],
        "input_shape": [[4, 15, 4]],
        "output_shape": [[4, 15, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = acosf((ph_0[((i0_i1_fused * 7) + i2)] + acoshf(ph_0[((i0_i1_fused * 7) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 180; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 7) + i2_1)] = cosf((ph_0[((i0_i1_fused_1 * 7) + i2_1)] * acoshf(ph_0[((i0_i1_fused_1 * 7) + i2_1)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] + acoshf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 20, 7), \"float32\"), compute: T.Buffer((9, 20, 7), \"float32\"), compute_1: T.Buffer((9, 20, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1260,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1] + T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1260,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2] * T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acosh",
                "add",
                "acos",
                "acosh",
                "multiply",
                "cos"
            ]
        ],
        "input_shape": [[9, 20, 7]],
        "output_shape": [[9, 20, 7], [9, 20, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_mod, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_mod[(((ax0 * 76) + (ax1 * 19)) + ax2)] = fmodf(ph_0[(((ax0 * 76) + (ax1 * 19)) + ax2)], ph_3[(((ax0 * 76) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 5; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 4; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 19; ++ax2_1) {\n        T_divide[(((ax0_1 * 76) + (ax1_1 * 19)) + ax2_1)] = (fmodf(ph_0[(((ax0_1 * 76) + (ax1_1 * 19)) + ax2_1)], atanf(ph_0[(((ax0_1 * 76) + (ax1_1 * 19)) + ax2_1)])) / ph_0[(((ax0_1 * 76) + (ax1_1 * 19)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 380; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], atanf(ph_0[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 4, 19), \"float32\"), ph_3: T.Buffer((5, 4, 19), \"float32\"), T_mod: T.Buffer((5, 4, 19), \"float32\"), T_divide: T.Buffer((5, 4, 19), \"float32\"), T_add: T.Buffer((5, 4, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((380,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(4, 19):\n                cse_var_1: T.int32 = ax0 * 76 + ax1 * 19 + ax2\n                T_mod_1 = T.Buffer((380,), data=T_mod.data)\n                ph_3_1 = T.Buffer((380,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(4, 19):\n                cse_var_2: T.int32 = ax0 * 76 + ax1 * 19 + ax2\n                T_divide_1 = T.Buffer((380,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], T.atan(ph_0_1[cse_var_2])) / ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(380):\n            T_add_1 = T.Buffer((380,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.atan(ph_0_1[ax0_ax1_fused_ax2_fused])) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "atan",
                "mod",
                "divide",
                "add"
            ]
        ],
        "input_shape": [[5, 4, 19], [4, 10, 7], [5, 4, 19]],
        "output_shape": [[5, 4, 19], [4, 10, 7], [5, 4, 19], [5, 4, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 360) + (i1 * 18)) + i2)] = atanhf(ph_0[(((i0 * 360) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[((i0_i1_fused * 18) + i2_1)] = cosf(asinf(ph_0[((i0_i1_fused * 18) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1800; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 20, 18), \"float32\"), compute: T.Buffer((5, 20, 18), \"float32\"), compute_1: T.Buffer((5, 20, 18), \"float32\"), compute_2: T.Buffer((5, 20, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1800,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(20, 18):\n                cse_var_1: T.int32 = i0 * 360 + i1 * 18 + i2\n                compute_3 = T.Buffer((1800,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(100):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((1800,), data=compute_1.data)\n                compute_3[cse_var_2] = T.cos(T.asin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1800):\n            compute_3 = T.Buffer((1800,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "asin",
                "cos",
                "ceil"
            ]
        ],
        "input_shape": [[5, 20, 18]],
        "output_shape": [[5, 20, 18], [5, 20, 18], [5, 20, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 44; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 11) + ax2)] = (ph_0[((ax0_ax1_fused * 11) + ax2)] * ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 484; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 484; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 4, 11), \"float32\"), ph_3: T.Buffer((11, 4, 11), \"float32\"), T_multiply: T.Buffer((11, 4, 11), \"float32\"), compute: T.Buffer((11, 4, 11), \"float32\"), compute_1: T.Buffer((11, 4, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((484,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(44):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_multiply_1 = T.Buffer((484,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((484,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(484):\n            compute_2 = T.Buffer((484,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(484):\n            compute_2 = T.Buffer((484,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "sin",
                "atanh"
            ]
        ],
        "input_shape": [[11, 4, 11], [13, 5, 11], [11, 4, 11]],
        "output_shape": [[11, 4, 11], [13, 5, 11], [11, 4, 11], [11, 4, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 135; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 135; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 135; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    compute_2[i0_i1_fused] = expf(asinf(ph_0[i0_i1_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 15, 1), \"float32\"), compute: T.Buffer((9, 15, 1), \"float32\"), T_divide: T.Buffer((9, 15, 1), \"float32\"), compute_1: T.Buffer((9, 15, 1), \"float32\"), compute_2: T.Buffer((9, 15, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((135,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(135):\n            compute_3 = T.Buffer((135,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(135):\n            T_divide_1 = T.Buffer((135,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(135):\n            compute_3 = T.Buffer((135,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(135):\n            compute_3 = T.Buffer((135,), data=compute_2.data)\n            compute_3[i0_i1_fused] = T.exp(T.asin(ph_0_1[i0_i1_fused]))",
        "op_args": [
            [
                "ceil",
                "atan",
                "divide",
                "acosh",
                "asin",
                "exp"
            ]
        ],
        "input_shape": [[9, 15, 1]],
        "output_shape": [[9, 15, 1], [9, 15, 1], [9, 15, 1], [9, 15, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 340; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] / ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 136) + (i1 * 8)) + i2)] = asinf(ph_0[(((i0 * 136) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2720; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 17, 8), \"float32\"), ph_3: T.Buffer((20, 17, 8), \"float32\"), T_divide: T.Buffer((20, 17, 8), \"float32\"), compute: T.Buffer((20, 17, 8), \"float32\"), compute_1: T.Buffer((20, 17, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2720,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(340):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((2720,), data=T_divide.data)\n                ph_3_1 = T.Buffer((2720,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(17, 8):\n                cse_var_2: T.int32 = i0 * 136 + i1 * 8 + i2\n                compute_2 = T.Buffer((2720,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2720):\n            compute_2 = T.Buffer((2720,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "asin",
                "asin"
            ]
        ],
        "input_shape": [[20, 17, 8], [19, 11, 19], [20, 17, 8]],
        "output_shape": [[20, 17, 8], [19, 11, 19], [20, 17, 8], [20, 17, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0, float* ph_3) {\n  float compute_5[112];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 112; ++i0_i1_fused_i2_fused_1) {\n    compute_5[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 112; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = acoshf(compute_5[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 112; ++i0_i1_fused_i2_fused_3) {\n    compute_2[i0_i1_fused_i2_fused_3] = asinf(compute_5[i0_i1_fused_i2_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_3[(((i0 * 14) + (i1 * 7)) + i2)] = fabsf((ph_0[(((i0 * 14) + (i1 * 7)) + i2)] - ph_3[(((i0 * 14) + (i1 * 7)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 112; ++i0_i1_fused_i2_fused_4) {\n    compute_4[i0_i1_fused_i2_fused_4] = acosf((ph_0[i0_i1_fused_i2_fused_4] - ph_3[i0_i1_fused_i2_fused_4]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(__expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 2, 7), \"float32\"), ph_3: T.Buffer((8, 2, 7), \"float32\"), compute: T.Buffer((8, 2, 7), \"float32\"), compute_1: T.Buffer((8, 2, 7), \"float32\"), compute_2: T.Buffer((8, 2, 7), \"float32\"), compute_3: T.Buffer((8, 2, 7), \"float32\"), compute_4: T.Buffer((8, 2, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_5 = T.allocate([112], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((112,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_6 = T.Buffer((112,), data=compute.data)\n            compute_6[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        compute_6 = T.Buffer((112,), data=compute_5)\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_6[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_7 = T.Buffer((112,), data=compute_1.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acosh(compute_6[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_7 = T.Buffer((112,), data=compute_2.data)\n            compute_7[i0_i1_fused_i2_fused] = T.asin(compute_6[i0_i1_fused_i2_fused])\n        ph_3_1 = T.Buffer((112,), data=ph_3.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(2, 7):\n                cse_var_1: T.int32 = i0 * 14 + i1 * 7 + i2\n                compute_7 = T.Buffer((112,), data=compute_3.data)\n                compute_7[cse_var_1] = T.fabs(ph_0_1[cse_var_1] - ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_7 = T.Buffer((112,), data=compute_4.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "exp",
                "exp",
                "acosh",
                "asin",
                "abs",
                "acos"
            ]
        ],
        "input_shape": [[8, 2, 7], [1, 1, 16], [8, 2, 7]],
        "output_shape": [[1, 1, 16], [8, 2, 7], [8, 2, 7], [8, 2, 7], [8, 2, 7], [8, 2, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 630; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / fabsf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 6, 15), \"float32\"), T_divide: T.Buffer((7, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(630):\n            T_divide_1 = T.Buffer((630,), data=T_divide.data)\n            ph_0_1 = T.Buffer((630,), data=ph_0.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "abs",
                "divide"
            ]
        ],
        "input_shape": [[7, 6, 15]],
        "output_shape": [[7, 6, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        T_add[(((ax0 * 221) + (ax1 * 13)) + ax2)] = ((ph_0[(((ax0 * 221) + (ax1 * 13)) + ax2)] / atanf(ph_0[(((ax0 * 221) + (ax1 * 13)) + ax2)])) + ph_0[(((ax0 * 221) + (ax1 * 13)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 442; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 34; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute_1[((i0_i1_fused * 13) + i2)] = asinhf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 17, 13), \"float32\"), T_add: T.Buffer((2, 17, 13), \"float32\"), compute: T.Buffer((2, 17, 13), \"float32\"), compute_1: T.Buffer((2, 17, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((442,), data=ph_0.data)\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(17, 13):\n                cse_var_1: T.int32 = ax0 * 221 + ax1 * 13 + ax2\n                T_add_1 = T.Buffer((442,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] / T.atan(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(442):\n            compute_2 = T.Buffer((442,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(34):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((442,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atan",
                "divide",
                "add",
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[2, 17, 13]],
        "output_shape": [[2, 17, 13], [2, 17, 13], [2, 17, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3120; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3120; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 240) + (i1 * 16)) + i2)] = fabsf(ph_0[(((i0 * 240) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 15, 16), \"float32\"), compute: T.Buffer((13, 15, 16), \"float32\"), T_multiply: T.Buffer((13, 15, 16), \"float32\"), compute_1: T.Buffer((13, 15, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3120,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3120):\n            compute_2 = T.Buffer((3120,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3120):\n            T_multiply_1 = T.Buffer((3120,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(15, 16):\n                cse_var_1: T.int32 = i0 * 240 + i1 * 16 + i2\n                compute_2 = T.Buffer((3120,), data=compute_1.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "asinh",
                "multiply",
                "abs"
            ]
        ],
        "input_shape": [[13, 15, 16]],
        "output_shape": [[13, 15, 16], [13, 15, 16], [13, 15, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[480];\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 96) + (i1 * 12)) + i2)] = acoshf(ph_0[(((i0 * 96) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (compute_2[ax0_ax1_fused_ax2_fused] - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 480; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf((ph_0[i0_i1_fused_i2_fused_1] - acoshf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 8, 12), \"float32\"), compute: T.Buffer((5, 8, 12), \"float32\"), T_subtract: T.Buffer((5, 8, 12), \"float32\"), compute_1: T.Buffer((5, 8, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([480], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(8, 12):\n                cse_var_1: T.int32 = i0 * 96 + i1 * 12 + i2\n                compute_3 = T.Buffer((480,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        compute_3 = T.Buffer((480,), data=compute_2)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_subtract_1 = T.Buffer((480,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = compute_3[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_4 = T.Buffer((480,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "exp",
                "subtract",
                "acosh",
                "subtract",
                "exp"
            ]
        ],
        "input_shape": [[5, 8, 12]],
        "output_shape": [[5, 8, 12], [5, 8, 12], [5, 8, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 180; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 180; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_3[(((i0 * 60) + (i1 * 20)) + i2)] = fabsf(atanhf(ph_0[(((i0 * 60) + (i1 * 20)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 3, 20), \"float32\"), compute: T.Buffer((3, 3, 20), \"float32\"), compute_1: T.Buffer((3, 3, 20), \"float32\"), compute_2: T.Buffer((3, 3, 20), \"float32\"), compute_3: T.Buffer((3, 3, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_4 = T.Buffer((180,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_4 = T.Buffer((180,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_4 = T.Buffer((180,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(3, 20):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 20 + i2\n                compute_4 = T.Buffer((180,), data=compute_3.data)\n                compute_4[cse_var_1] = T.fabs(T.atanh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "ceil",
                "acos",
                "ceil",
                "asinh",
                "atanh",
                "abs"
            ]
        ],
        "input_shape": [[3, 3, 20]],
        "output_shape": [[3, 3, 20], [3, 3, 20], [3, 3, 20], [3, 3, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 30; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_add[((ax0_ax1_fused * 13) + ax2)] = (ph_0[((ax0_ax1_fused * 13) + ax2)] + ph_3[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = asinhf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 390; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 6, 13), \"float32\"), ph_3: T.Buffer((5, 6, 13), \"float32\"), T_add: T.Buffer((5, 6, 13), \"float32\"), compute: T.Buffer((5, 6, 13), \"float32\"), compute_1: T.Buffer((5, 6, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((390,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(30):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_add_1 = T.Buffer((390,), data=T_add.data)\n                ph_3_1 = T.Buffer((390,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((390,), data=compute.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(390):\n            compute_2 = T.Buffer((390,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "asinh",
                "ceil"
            ]
        ],
        "input_shape": [[5, 6, 13], [10, 4, 9], [5, 6, 13]],
        "output_shape": [[5, 6, 13], [10, 4, 9], [5, 6, 13], [5, 6, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2145; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2145; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_2[(((i0 * 165) + (i1 * 11)) + i2)] = fabsf(ph_0[(((i0 * 165) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2145; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 15, 11), \"float32\"), compute: T.Buffer((13, 15, 11), \"float32\"), compute_1: T.Buffer((13, 15, 11), \"float32\"), compute_2: T.Buffer((13, 15, 11), \"float32\"), compute_3: T.Buffer((13, 15, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2145,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2145):\n            compute_4 = T.Buffer((2145,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2145):\n            compute_4 = T.Buffer((2145,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(15, 11):\n                cse_var_1: T.int32 = i0 * 165 + i1 * 11 + i2\n                compute_4 = T.Buffer((2145,), data=compute_2.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2145):\n            compute_4 = T.Buffer((2145,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "abs",
                "acosh",
                "abs",
                "exp"
            ]
        ],
        "input_shape": [[13, 15, 11]],
        "output_shape": [[13, 15, 11], [13, 15, 11], [13, 15, 11], [13, 15, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 192) + (i1 * 16)) + i2)] = ceilf((ph_0[(((i0 * 192) + (i1 * 16)) + i2)] + cosf(ph_0[(((i0 * 192) + (i1 * 16)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 7; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n        compute_1[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)] = sinf(ph_0[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2_2 = 0; i2_2 < 16; ++i2_2) {\n      compute_2[((i0_i1_fused * 16) + i2_2)] = asinhf(ph_0[((i0_i1_fused * 16) + i2_2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 12, 16), \"float32\"), compute: T.Buffer((7, 12, 16), \"float32\"), compute_1: T.Buffer((7, 12, 16), \"float32\"), compute_2: T.Buffer((7, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1344,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_1: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_3 = T.Buffer((1344,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1] + T.cos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_2: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_3 = T.Buffer((1344,), data=compute_1.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(16):\n                cse_var_3: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((1344,), data=compute_2.data)\n                compute_3[cse_var_3] = T.asinh(ph_0_1[cse_var_3])",
        "op_args": [
            [
                "cos",
                "add",
                "ceil",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[7, 12, 16]],
        "output_shape": [[7, 12, 16], [7, 12, 16], [7, 12, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 12) + ax2)] = (ph_0[((ax0_ax1_fused * 12) + ax2)] * fabsf(ph_0[((ax0_ax1_fused * 12) + ax2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 14, 12), \"float32\"), T_multiply: T.Buffer((6, 14, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(84):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_multiply_1 = T.Buffer((1008,), data=T_multiply.data)\n                ph_0_1 = T.Buffer((1008,), data=ph_0.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "abs",
                "multiply"
            ]
        ],
        "input_shape": [[6, 14, 12]],
        "output_shape": [[6, 14, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 351; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 351; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ceilf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 117) + (i1 * 13)) + i2)] = atanhf(ceilf(ph_0[(((i0 * 117) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 351; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 27; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_add[((ax0_ax1_fused * 13) + ax2)] = ((ph_0[((ax0_ax1_fused * 13) + ax2)] / ph_3[((ax0_ax1_fused * 13) + ax2)]) + ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(ceilf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 9, 13), \"float32\"), ph_3: T.Buffer((3, 9, 13), \"float32\"), compute: T.Buffer((3, 9, 13), \"float32\"), T_mod: T.Buffer((3, 9, 13), \"float32\"), compute_1: T.Buffer((3, 9, 13), \"float32\"), compute_2: T.Buffer((3, 9, 13), \"float32\"), T_add: T.Buffer((3, 9, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((351,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(351):\n            compute_3 = T.Buffer((351,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(351):\n            T_mod_1 = T.Buffer((351,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(9, 13):\n                cse_var_1: T.int32 = i0 * 117 + i1 * 13 + i2\n                compute_3 = T.Buffer((351,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(T.ceil(ph_0_1[cse_var_1]))\n        ph_3_1 = T.Buffer((351,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(351):\n            compute_3 = T.Buffer((351,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(27):\n            for ax2 in range(13):\n                cse_var_2: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_add_1 = T.Buffer((351,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] / ph_3_1[cse_var_2] + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "divide",
                "abs",
                "ceil",
                "mod",
                "atanh",
                "ceil",
                "add"
            ]
        ],
        "input_shape": [[3, 9, 13], [18, 2, 17], [3, 9, 13]],
        "output_shape": [[18, 2, 17], [3, 9, 13], [3, 9, 13], [3, 9, 13], [3, 9, 13], [3, 9, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 80) + (i1 * 20)) + i2)] = asinf(ph_0[(((i0 * 80) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_1[((i0_i1_fused * 20) + i2_1)] = ceilf(asinhf(ph_0[((i0_i1_fused * 20) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 4, 20), \"float32\"), compute: T.Buffer((12, 4, 20), \"float32\"), compute_1: T.Buffer((12, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(4, 20):\n                cse_var_1: T.int32 = i0 * 80 + i1 * 20 + i2\n                compute_2 = T.Buffer((960,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_2 = T.Buffer((960,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asin",
                "asinh",
                "ceil"
            ]
        ],
        "input_shape": [[12, 4, 20]],
        "output_shape": [[12, 4, 20], [12, 4, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 280; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] - ceilf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 280; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf((ph_0[((int)blockIdx.x)] - ceilf(ph_0[((int)blockIdx.x)])), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 10, 2), \"float32\"), T_mod: T.Buffer((14, 10, 2), \"float32\"), compute: T.Buffer((14, 10, 2), \"float32\"), compute_1: T.Buffer((14, 10, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((280,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(280):\n            T_mod_1 = T.Buffer((280,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] - T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_2 = T.Buffer((280,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_2 = T.Buffer((280,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "subtract",
                "mod",
                "atan",
                "atanh"
            ]
        ],
        "input_shape": [[14, 10, 2]],
        "output_shape": [[14, 10, 2], [14, 10, 2], [14, 10, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2888; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_mod[(((ax0 * 152) + (ax1 * 8)) + ax2)] = fmodf(ph_0[(((ax0 * 152) + (ax1 * 8)) + ax2)], atanhf(atanhf(ph_0[(((ax0 * 152) + (ax1 * 8)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2888; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], atanhf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 19, 8), \"float32\"), compute: T.Buffer((19, 19, 8), \"float32\"), T_mod: T.Buffer((19, 19, 8), \"float32\"), compute_1: T.Buffer((19, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2888,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2888):\n            compute_2 = T.Buffer((2888,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(19, 8):\n                cse_var_1: T.int32 = ax0 * 152 + ax1 * 8 + ax2\n                T_mod_1 = T.Buffer((2888,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.atanh(T.atanh(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(2888):\n            compute_2 = T.Buffer((2888,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "atanh",
                "atanh",
                "mod",
                "abs"
            ]
        ],
        "input_shape": [[19, 19, 8]],
        "output_shape": [[19, 19, 8], [19, 19, 8], [19, 19, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_multiply[(((ax0 * 80) + (ax1 * 16)) + ax2)] = (ph_0[(((ax0 * 80) + (ax1 * 16)) + ax2)] * ph_3[(((ax0 * 80) + (ax1 * 16)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 25; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = asinf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 400; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 5, 16), \"float32\"), ph_3: T.Buffer((5, 5, 16), \"float32\"), T_multiply: T.Buffer((5, 5, 16), \"float32\"), compute: T.Buffer((5, 5, 16), \"float32\"), compute_1: T.Buffer((5, 5, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((400,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(5, 16):\n                cse_var_1: T.int32 = ax0 * 80 + ax1 * 16 + ax2\n                T_multiply_1 = T.Buffer((400,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((400,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(25):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((400,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(400):\n            compute_2 = T.Buffer((400,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "asin",
                "acos"
            ]
        ],
        "input_shape": [[5, 5, 16], [1, 10, 2], [5, 5, 16]],
        "output_shape": [[5, 5, 16], [1, 10, 2], [5, 5, 16], [5, 5, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 342; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = asinhf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_multiply[(((ax0 * 266) + (ax1 * 14)) + ax2)] = ((ph_0[(((ax0 * 266) + (ax1 * 14)) + ax2)] / (ph_0[(((ax0 * 266) + (ax1 * 14)) + ax2)] / acoshf(asinhf(ph_0[(((ax0 * 266) + (ax1 * 14)) + ax2)])))) * ph_0[(((ax0 * 266) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4788; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / (ph_0[ax0_ax1_fused_ax2_fused] / acoshf(asinhf(ph_0[ax0_ax1_fused_ax2_fused])))) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / acoshf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])))) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / acoshf(asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])))) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 19, 14), \"float32\"), compute: T.Buffer((18, 19, 14), \"float32\"), T_multiply: T.Buffer((18, 19, 14), \"float32\"), T_add: T.Buffer((18, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4788,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(342):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_1 = T.Buffer((4788,), data=compute.data)\n                compute_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(19, 14):\n                cse_var_2: T.int32 = ax0 * 266 + ax1 * 14 + ax2\n                T_multiply_1 = T.Buffer((4788,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] / (ph_0_1[cse_var_2] / T.acosh(T.asinh(ph_0_1[cse_var_2]))) * ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(4788):\n            T_add_1 = T.Buffer((4788,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / (ph_0_1[ax0_ax1_fused_ax2_fused] / T.acosh(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]))) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asinh",
                "asinh",
                "acosh",
                "divide",
                "divide",
                "multiply",
                "add"
            ]
        ],
        "input_shape": [[18, 19, 14]],
        "output_shape": [[18, 19, 14], [18, 19, 14], [18, 19, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1170; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused], asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0_i1_fused * 9) + i2)] = expf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 130; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 9) + i2_1)] = asinhf(fabsf(ph_0[((i0_i1_fused_1 * 9) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(fabsf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 10, 9), \"float32\"), compute: T.Buffer((13, 10, 9), \"float32\"), compute_1: T.Buffer((13, 10, 9), \"float32\"), compute_2: T.Buffer((13, 10, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_3 = T.Buffer((1170,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.asinh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((1170,), data=compute_1.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((1170,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(T.fabs(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asinh",
                "mod",
                "acosh",
                "exp",
                "abs",
                "asinh"
            ]
        ],
        "input_shape": [[13, 10, 9]],
        "output_shape": [[13, 10, 9], [13, 10, 9], [13, 10, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 210; ++i0_i1_fused_i2_fused) {\n    float compute_3[1];\n    compute_3[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] * compute_3[0]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 210; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_2[(((i0 * 30) + (i1 * 2)) + i2)] = cosf(ph_0[(((i0 * 30) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 15, 2), \"float32\"), compute: T.Buffer((7, 15, 2), \"float32\"), compute_1: T.Buffer((7, 15, 2), \"float32\"), compute_2: T.Buffer((7, 15, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((210,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(210):\n            compute_3 = T.allocate([1], \"float32\", \"global\")\n            compute_4 = T.Buffer((1,), data=compute_3, align=4)\n            compute_4[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_5 = T.Buffer((210,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] * compute_4[0])\n        for i0_i1_fused_i2_fused in T.parallel(210):\n            compute_3 = T.Buffer((210,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(15, 2):\n                cse_var_1: T.int32 = i0 * 30 + i1 * 2 + i2\n                compute_3 = T.Buffer((210,), data=compute_2.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "exp",
                "multiply",
                "exp",
                "acosh",
                "cos"
            ]
        ],
        "input_shape": [[7, 15, 2]],
        "output_shape": [[7, 15, 2], [7, 15, 2], [7, 15, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1444; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 76; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = atanf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1444; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 19, 19), \"float32\"), ph_3: T.Buffer((4, 19, 19), \"float32\"), T_divide: T.Buffer((4, 19, 19), \"float32\"), compute: T.Buffer((4, 19, 19), \"float32\"), compute_1: T.Buffer((4, 19, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1444,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1444):\n            T_divide_1 = T.Buffer((1444,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1444,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(76):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((1444,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1444):\n            compute_2 = T.Buffer((1444,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "atan",
                "ceil",
                "acosh"
            ]
        ],
        "input_shape": [[4, 19, 19], [2, 19, 17], [4, 19, 19]],
        "output_shape": [[4, 19, 19], [2, 19, 17], [4, 19, 19], [4, 19, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 272; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 14) + ax2)] = ((ph_0[((ax0_ax1_fused * 14) + ax2)] - sinf(ph_0[((ax0_ax1_fused * 14) + ax2)])) - ph_0[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 17, 14), \"float32\"), T_subtract: T.Buffer((16, 17, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(272):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_subtract_1 = T.Buffer((3808,), data=T_subtract.data)\n                ph_0_1 = T.Buffer((3808,), data=ph_0.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.sin(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]",
        "op_args": [
            [
                "sin",
                "subtract",
                "subtract"
            ]
        ],
        "input_shape": [[16, 17, 14]],
        "output_shape": [[16, 17, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = fabsf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_divide[((ax0_ax1_fused * 19) + ax2)] = (ph_0[((ax0_ax1_fused * 19) + ax2)] / ceilf(acosf(ph_0[((ax0_ax1_fused * 19) + ax2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ceilf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 9, 19), \"float32\"), compute: T.Buffer((2, 9, 19), \"float32\"), T_divide: T.Buffer((2, 9, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((342,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_1 = T.Buffer((342,), data=compute.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(19):\n                cse_var_2: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_divide_1 = T.Buffer((342,), data=T_divide.data)\n                T_divide_1[cse_var_2] = ph_0_1[cse_var_2] / T.ceil(T.acos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "acos",
                "ceil",
                "divide"
            ]
        ],
        "input_shape": [[2, 9, 19]],
        "output_shape": [[2, 9, 19], [2, 9, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 64) + (i1 * 8)) + i2)] = fabsf(ph_0[(((i0 * 64) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 20; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        compute_1[(((i0_1 * 64) + (i1_1 * 8)) + i2_1)] = cosf(ph_0[(((i0_1 * 64) + (i1_1 * 8)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 8, 8), \"float32\"), compute: T.Buffer((20, 8, 8), \"float32\"), T_mod: T.Buffer((20, 8, 8), \"float32\"), compute_1: T.Buffer((20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1280,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(8, 8):\n                cse_var_1: T.int32 = i0 * 64 + i1 * 8 + i2\n                compute_2 = T.Buffer((1280,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1280):\n            T_mod_1 = T.Buffer((1280,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(8, 8):\n                cse_var_2: T.int32 = i0 * 64 + i1 * 8 + i2\n                compute_2 = T.Buffer((1280,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "acos",
                "mod",
                "cos"
            ]
        ],
        "input_shape": [[20, 8, 8]],
        "output_shape": [[20, 8, 8], [20, 8, 8], [20, 8, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2912; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 182) + (i1 * 13)) + i2)] = expf(fabsf(ph_0[(((i0 * 182) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2912; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 224; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_mod[((ax0_ax1_fused * 13) + ax2)] = fmodf((ph_0[((ax0_ax1_fused * 13) + ax2)] - ph_3[((ax0_ax1_fused * 13) + ax2)]), ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2912; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_1] = fmodf((ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 14, 13), \"float32\"), ph_3: T.Buffer((16, 14, 13), \"float32\"), T_multiply: T.Buffer((16, 14, 13), \"float32\"), compute: T.Buffer((16, 14, 13), \"float32\"), compute_1: T.Buffer((16, 14, 13), \"float32\"), T_mod: T.Buffer((16, 14, 13), \"float32\"), T_mod_1: T.Buffer((16, 14, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2912,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2912,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2912):\n            T_multiply_1 = T.Buffer((2912,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(14, 13):\n                cse_var_1: T.int32 = i0 * 182 + i1 * 13 + i2\n                compute_2 = T.Buffer((2912,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(T.fabs(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2912):\n            compute_2 = T.Buffer((2912,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(224):\n            for ax2 in range(13):\n                cse_var_2: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_mod_2 = T.Buffer((2912,), data=T_mod.data)\n                T_mod_2[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] - ph_3_1[cse_var_2], ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2912):\n            T_mod_2 = T.Buffer((2912,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused], ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "subtract",
                "multiply",
                "abs",
                "exp",
                "asin",
                "mod",
                "mod"
            ]
        ],
        "input_shape": [[16, 14, 13], [14, 16, 8], [16, 14, 13]],
        "output_shape": [[14, 16, 8], [16, 14, 13], [16, 14, 13], [16, 14, 13], [16, 14, 13], [16, 14, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 168) + (i1 * 14)) + i2)] = expf(ph_0[(((i0 * 168) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 840; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      compute_2[((i0_i1_fused * 14) + i2_1)] = cosf((ph_0[((i0_i1_fused * 14) + i2_1)] + cosf(ph_0[((i0_i1_fused * 14) + i2_1)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 12, 14), \"float32\"), compute: T.Buffer((5, 12, 14), \"float32\"), compute_1: T.Buffer((5, 12, 14), \"float32\"), compute_2: T.Buffer((5, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((840,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(12, 14):\n                cse_var_1: T.int32 = i0 * 168 + i1 * 14 + i2\n                compute_3 = T.Buffer((840,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            compute_3 = T.Buffer((840,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(14):\n                cse_var_2: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((840,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2] + T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "exp",
                "ceil",
                "atanh",
                "cos",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[5, 12, 14]],
        "output_shape": [[5, 12, 14], [5, 12, 14], [5, 12, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = asinhf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2128; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2128; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 7, 19), \"float32\"), compute: T.Buffer((16, 7, 19), \"float32\"), compute_1: T.Buffer((16, 7, 19), \"float32\"), compute_2: T.Buffer((16, 7, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2128,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(112):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((2128,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2128):\n            compute_3 = T.Buffer((2128,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2128):\n            compute_3 = T.Buffer((2128,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "asin",
                "acosh",
                "ceil"
            ]
        ],
        "input_shape": [[16, 7, 19]],
        "output_shape": [[16, 7, 19], [16, 7, 19], [16, 7, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  float compute_4[384];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 384; ++i0_i1_fused_i2_fused_1) {\n    compute_4[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_1[(((i0 * 48) + (i1 * 3)) + i2)] = fabsf(compute_4[(((i0 * 48) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 384; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 384; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = acoshf(ph_0[i0_i1_fused_i2_fused_3]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(__expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 16, 3), \"float32\"), compute: T.Buffer((8, 16, 3), \"float32\"), compute_1: T.Buffer((8, 16, 3), \"float32\"), compute_2: T.Buffer((8, 16, 3), \"float32\"), compute_3: T.Buffer((8, 16, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_4 = T.allocate([384], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((384,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_5 = T.Buffer((384,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        compute_5 = T.Buffer((384,), data=compute_4)\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(16, 3):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 3 + i2\n                compute_6 = T.Buffer((384,), data=compute_1.data)\n                compute_6[cse_var_1] = T.fabs(compute_5[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_6 = T.Buffer((384,), data=compute_2.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_6 = T.Buffer((384,), data=compute_3.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "exp",
                "abs",
                "acos",
                "acosh"
            ]
        ],
        "input_shape": [[8, 16, 3]],
        "output_shape": [[8, 16, 3], [8, 16, 3], [8, 16, 3], [8, 16, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 612; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 612; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] / (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 68; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0_i1_fused * 9) + i2)] = fabsf((ph_0[((i0_i1_fused * 9) + i2)] / (ph_0[((i0_i1_fused * 9) + i2)] / ph_3[((i0_i1_fused * 9) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 17, 9), \"float32\"), ph_3: T.Buffer((4, 17, 9), \"float32\"), T_multiply: T.Buffer((4, 17, 9), \"float32\"), compute: T.Buffer((4, 17, 9), \"float32\"), compute_1: T.Buffer((4, 17, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((612,), data=ph_0.data)\n        ph_3_1 = T.Buffer((612,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(612):\n            T_multiply_1 = T.Buffer((612,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(612):\n            compute_2 = T.Buffer((612,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] / (ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(68):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((612,), data=compute_1.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1] / (ph_0_1[cse_var_1] / ph_3_1[cse_var_1]))",
        "op_args": [
            [
                "multiply",
                "divide",
                "divide",
                "abs",
                "abs"
            ]
        ],
        "input_shape": [[4, 17, 9], [20, 16, 5], [4, 17, 9]],
        "output_shape": [[4, 17, 9], [20, 16, 5], [4, 17, 9], [4, 17, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1001; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_1[(((i0 * 91) + (i1 * 7)) + i2)] = fabsf(asinf(ph_0[(((i0 * 91) + (i1 * 7)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 143; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_2[((i0_i1_fused * 7) + i2_1)] = asinf(ph_0[((i0_i1_fused * 7) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1001; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = acoshf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 13, 7), \"float32\"), compute: T.Buffer((11, 13, 7), \"float32\"), compute_1: T.Buffer((11, 13, 7), \"float32\"), compute_2: T.Buffer((11, 13, 7), \"float32\"), compute_3: T.Buffer((11, 13, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1001,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1001):\n            compute_4 = T.Buffer((1001,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(13, 7):\n                cse_var_1: T.int32 = i0 * 91 + i1 * 7 + i2\n                compute_4 = T.Buffer((1001,), data=compute_1.data)\n                compute_4[cse_var_1] = T.fabs(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(143):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_4 = T.Buffer((1001,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1001):\n            compute_4 = T.Buffer((1001,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "asin",
                "abs",
                "asin",
                "ceil",
                "acosh"
            ]
        ],
        "input_shape": [[11, 13, 7]],
        "output_shape": [[11, 13, 7], [11, 13, 7], [11, 13, 7], [11, 13, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0) {\n  float compute_1[80];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 80; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ceilf(compute_1[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ceilf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 16, 5), \"float32\"), compute: T.Buffer((1, 16, 5), \"float32\"), T_divide: T.Buffer((1, 16, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([80], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((80,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(16):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((80,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        compute_2 = T.Buffer((80,), data=compute_1)\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(80):\n            T_divide_1 = T.Buffer((80,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.ceil(compute_2[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "asin",
                "exp",
                "ceil",
                "divide"
            ]
        ],
        "input_shape": [[1, 16, 5]],
        "output_shape": [[1, 16, 5], [1, 16, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 32; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 8; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 4) + ax2)] = (fmodf(ph_0[((ax0_ax1_fused * 4) + ax2)], atanhf(ph_0[((ax0_ax1_fused * 4) + ax2)])) - ph_0[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 8) + (i1 * 4)) + i2)] = acoshf(fmodf(ph_0[(((i0 * 8) + (i1 * 4)) + i2)], atanhf(ph_0[(((i0 * 8) + (i1 * 4)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 2, 4), \"float32\"), ph_3: T.Buffer((4, 2, 4), \"float32\"), T_multiply: T.Buffer((4, 2, 4), \"float32\"), T_subtract: T.Buffer((4, 2, 4), \"float32\"), compute: T.Buffer((4, 2, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(32):\n            T_multiply_1 = T.Buffer((32,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((32,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_subtract_1 = T.Buffer((32,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1])) - ph_0_1[cse_var_1]\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(2, 4):\n                cse_var_2: T.int32 = i0 * 8 + i1 * 4 + i2\n                compute_1 = T.Buffer((32,), data=compute.data)\n                compute_1[cse_var_2] = T.acosh(T.truncmod(ph_0_1[cse_var_2], T.atanh(ph_0_1[cse_var_2])))",
        "op_args": [
            [
                "multiply",
                "atanh",
                "mod",
                "subtract",
                "acosh"
            ]
        ],
        "input_shape": [[4, 2, 4], [19, 11, 13], [4, 2, 4]],
        "output_shape": [[4, 2, 4], [19, 11, 13], [4, 2, 4], [4, 2, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 40) + (i1 * 4)) + i2)] = fabsf(asinf(ph_0[(((i0 * 40) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 720; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 720; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanhf((ph_0[i0_i1_fused_i2_fused_2] * ph_3[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n      compute_4[((i0_i1_fused * 4) + i2_1)] = sinf((ph_0[((i0_i1_fused * 4) + i2_1)] * ph_3[((i0_i1_fused * 4) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 10, 4), \"float32\"), ph_3: T.Buffer((18, 10, 4), \"float32\"), compute: T.Buffer((18, 10, 4), \"float32\"), compute_1: T.Buffer((18, 10, 4), \"float32\"), compute_2: T.Buffer((18, 10, 4), \"float32\"), compute_3: T.Buffer((18, 10, 4), \"float32\"), compute_4: T.Buffer((18, 10, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_5 = T.Buffer((720,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(10, 4):\n                cse_var_1: T.int32 = i0 * 40 + i1 * 4 + i2\n                compute_5 = T.Buffer((720,), data=compute_1.data)\n                compute_5[cse_var_1] = T.fabs(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_5 = T.Buffer((720,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((720,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_5 = T.Buffer((720,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_5 = T.Buffer((720,), data=compute_4.data)\n                compute_5[cse_var_2] = T.sin(ph_0_1[cse_var_2] * ph_3_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "sin",
                "asin",
                "abs",
                "atanh",
                "atanh",
                "sin"
            ]
        ],
        "input_shape": [[18, 10, 4], [13, 19, 4], [18, 10, 4]],
        "output_shape": [[13, 19, 4], [18, 10, 4], [18, 10, 4], [18, 10, 4], [18, 10, 4], [18, 10, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 150; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 150; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 150; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 150; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 10, 3), \"float32\"), compute: T.Buffer((5, 10, 3), \"float32\"), compute_1: T.Buffer((5, 10, 3), \"float32\"), compute_2: T.Buffer((5, 10, 3), \"float32\"), T_mod: T.Buffer((5, 10, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_3 = T.Buffer((150,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_3 = T.Buffer((150,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_3 = T.Buffer((150,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_mod_1 = T.Buffer((150,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "asin",
                "asin",
                "cos",
                "atan",
                "acos",
                "mod"
            ]
        ],
        "input_shape": [[5, 10, 3]],
        "output_shape": [[5, 10, 3], [5, 10, 3], [5, 10, 3], [5, 10, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 972; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 972; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_1[(((i0 * 54) + (i1 * 18)) + i2)] = asinhf(ph_0[(((i0 * 54) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 3, 18), \"float32\"), ph_3: T.Buffer((18, 3, 18), \"float32\"), T_mod: T.Buffer((18, 3, 18), \"float32\"), compute: T.Buffer((18, 3, 18), \"float32\"), compute_1: T.Buffer((18, 3, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((972,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(972):\n            T_mod_1 = T.Buffer((972,), data=T_mod.data)\n            ph_3_1 = T.Buffer((972,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(972):\n            compute_2 = T.Buffer((972,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(3, 18):\n                cse_var_1: T.int32 = i0 * 54 + i1 * 18 + i2\n                compute_2 = T.Buffer((972,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "mod",
                "ceil",
                "asinh"
            ]
        ],
        "input_shape": [[18, 3, 18], [20, 1, 20], [18, 3, 18]],
        "output_shape": [[18, 3, 18], [20, 1, 20], [18, 3, 18], [18, 3, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3458; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 247) + (i1 * 13)) + i2)] = sinf(ph_0[(((i0 * 247) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 266; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_1[((i0_i1_fused * 13) + i2_1)] = cosf(ceilf(ph_0[((i0_i1_fused * 13) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 19, 13), \"float32\"), ph_3: T.Buffer((14, 19, 13), \"float32\"), T_subtract: T.Buffer((14, 19, 13), \"float32\"), compute: T.Buffer((14, 19, 13), \"float32\"), compute_1: T.Buffer((14, 19, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3458,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3458):\n            T_subtract_1 = T.Buffer((3458,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((3458,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(19, 13):\n                cse_var_1: T.int32 = i0 * 247 + i1 * 13 + i2\n                compute_2 = T.Buffer((3458,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(266):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((3458,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "subtract",
                "sin",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[14, 19, 13], [10, 11, 19], [14, 19, 13]],
        "output_shape": [[14, 19, 13], [10, 11, 19], [14, 19, 13], [14, 19, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 119) + (i1 * 7)) + i2)] = asinhf(ph_0[(((i0 * 119) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 10; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 119) + (i1_1 * 7)) + i2_1)] = asinhf(atanf(ph_0[(((i0_1 * 119) + (i1_1 * 7)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 170; ++i0_i1_fused) {\n    for (int32_t i2_2 = 0; i2_2 < 7; ++i2_2) {\n      compute_2[((i0_i1_fused * 7) + i2_2)] = ceilf((ph_0[((i0_i1_fused * 7) + i2_2)] + sinf(ph_0[((i0_i1_fused * 7) + i2_2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 17, 7), \"float32\"), compute: T.Buffer((10, 17, 7), \"float32\"), compute_1: T.Buffer((10, 17, 7), \"float32\"), compute_2: T.Buffer((10, 17, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1190,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(17, 7):\n                cse_var_1: T.int32 = i0 * 119 + i1 * 7 + i2\n                compute_3 = T.Buffer((1190,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(17, 7):\n                cse_var_2: T.int32 = i0 * 119 + i1 * 7 + i2\n                compute_3 = T.Buffer((1190,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asinh(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused in T.parallel(170):\n            for i2 in range(7):\n                cse_var_3: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((1190,), data=compute_2.data)\n                compute_3[cse_var_3] = T.ceil(ph_0_1[cse_var_3] + T.sin(ph_0_1[cse_var_3]))",
        "op_args": [
            [
                "asinh",
                "atan",
                "asinh",
                "sin",
                "add",
                "ceil"
            ]
        ],
        "input_shape": [[10, 17, 7]],
        "output_shape": [[10, 17, 7], [10, 17, 7], [10, 17, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3150; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], ceilf(ph_0[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 210) + (i1 * 14)) + i2)] = expf(ph_0[(((i0 * 210) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3150; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3150; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 15, 14), \"float32\"), T_subtract: T.Buffer((15, 15, 14), \"float32\"), compute: T.Buffer((15, 15, 14), \"float32\"), compute_1: T.Buffer((15, 15, 14), \"float32\"), compute_2: T.Buffer((15, 15, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3150,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3150):\n            T_subtract_1 = T.Buffer((3150,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused])) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(15, 14):\n                cse_var_1: T.int32 = i0 * 210 + i1 * 14 + i2\n                compute_3 = T.Buffer((3150,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3150):\n            compute_3 = T.Buffer((3150,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(3150):\n            compute_3 = T.Buffer((3150,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "mod",
                "subtract",
                "exp",
                "acosh",
                "asin",
                "abs"
            ]
        ],
        "input_shape": [[15, 15, 14]],
        "output_shape": [[15, 15, 14], [15, 15, 14], [15, 15, 14], [15, 15, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 2, 1), \"float32\"), compute: T.Buffer((2, 2, 1), \"float32\"), compute_1: T.Buffer((2, 2, 1), \"float32\"), compute_2: T.Buffer((2, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4):\n            compute_3 = T.Buffer((4,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4):\n            compute_3 = T.Buffer((4,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4):\n            compute_3 = T.Buffer((4,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "cos",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[2, 2, 1]],
        "output_shape": [[2, 2, 1], [2, 2, 1], [2, 2, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 225; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      compute[((i0 * 15) + i1)] = atanhf(ph_0[((i0 * 15) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 15; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 15; ++i1_1) {\n      compute_1[((i0_1 * 15) + i1_1)] = cosf(ph_0[((i0_1 * 15) + i1_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 15, 1), \"float32\"), ph_3: T.Buffer((15, 15, 1), \"float32\"), T_mod: T.Buffer((15, 15, 1), \"float32\"), compute: T.Buffer((15, 15, 1), \"float32\"), compute_1: T.Buffer((15, 15, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((225,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(225):\n            T_mod_1 = T.Buffer((225,), data=T_mod.data)\n            ph_3_1 = T.Buffer((225,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(15):\n            for i1 in range(15):\n                cse_var_1: T.int32 = i0 * 15 + i1\n                compute_2 = T.Buffer((225,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(15):\n            for i1 in range(15):\n                cse_var_2: T.int32 = i0 * 15 + i1\n                compute_2 = T.Buffer((225,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "mod",
                "atanh",
                "cos"
            ]
        ],
        "input_shape": [[15, 15, 1], [13, 20, 15], [15, 15, 1]],
        "output_shape": [[15, 15, 1], [13, 20, 15], [15, 15, 1], [15, 15, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* T_mod, float* T_subtract, float* ph_0, float* ph_3, float* ph_7) {\n  float auto_scheduler_layout_transform[64];\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_mod[((ax0 * 4) + ax2)] = fmodf(ph_0[((ax0 * 4) + ax2)], ph_3[((ax0 * 4) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 16; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n      auto_scheduler_layout_transform[((ax0_ax1_fused * 4) + ax2_1)] = ph_7[((ax0_ax1_fused * 4) + ax2_1)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_i_fused_j_fused = 0; b_i_fused_j_fused < 16; ++b_i_fused_j_fused) {\n    T_batch_matmul_NN[b_i_fused_j_fused] = 0.000000e+00f;\n    for (int32_t k = 0; k < 4; ++k) {\n      T_batch_matmul_NN[b_i_fused_j_fused] = (T_batch_matmul_NN[b_i_fused_j_fused] + (cosf(ph_0[((b_i_fused_j_fused * 4) + k)]) * auto_scheduler_layout_transform[((b_i_fused_j_fused * 4) + k)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 64; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 64; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] * ph_3[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_7) {\n  float T_batch_matmul_NN_local[32];\n  __shared__ float compute_shared[64];\n  __shared__ float ph_7_shared[8];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 4; ++b_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_outer_inner_init * 2) + b_c_inner_init)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_outer_inner_init * 2) + b_c_inner_init) + 8)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_outer_inner_init * 2) + b_c_inner_init) + 16)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_outer_inner_init * 2) + b_c_inner_init) + 24)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 32; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x))] = __cosf(ph_0[(((ax0_ax1_fused_ax2_fused_outer_outer * 10) + (((int)threadIdx.x) * 5)) + k_outer_outer)]);\n    }\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 4; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      ph_7_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 2) + ((int)threadIdx.x))] = ph_7[(((ax0_ax1_fused_ax2_fused_outer_outer_1 * 10) + (((int)threadIdx.x) * 5)) + k_outer_outer)];\n    }\n    __syncthreads();\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 4; ++b_c_outer_inner) {\n      for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n        T_batch_matmul_NN_local[((b_c_outer_inner * 2) + b_c_inner)] = (T_batch_matmul_NN_local[((b_c_outer_inner * 2) + b_c_inner)] + (compute_shared[(((b_c_outer_inner * 16) + (b_c_inner * 8)) + ((int)threadIdx.x))] * ph_7_shared[((b_c_outer_inner * 2) + b_c_inner)]));\n        T_batch_matmul_NN_local[(((b_c_outer_inner * 2) + b_c_inner) + 8)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 2) + b_c_inner) + 8)] + (compute_shared[((((b_c_outer_inner * 16) + (b_c_inner * 8)) + ((int)threadIdx.x)) + 2)] * ph_7_shared[((b_c_outer_inner * 2) + b_c_inner)]));\n        T_batch_matmul_NN_local[(((b_c_outer_inner * 2) + b_c_inner) + 16)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 2) + b_c_inner) + 16)] + (compute_shared[((((b_c_outer_inner * 16) + (b_c_inner * 8)) + ((int)threadIdx.x)) + 4)] * ph_7_shared[((b_c_outer_inner * 2) + b_c_inner)]));\n        T_batch_matmul_NN_local[(((b_c_outer_inner * 2) + b_c_inner) + 24)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 2) + b_c_inner) + 24)] + (compute_shared[((((b_c_outer_inner * 16) + (b_c_inner * 8)) + ((int)threadIdx.x)) + 6)] * ph_7_shared[((b_c_outer_inner * 2) + b_c_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 8; ++b_inner) {\n    T_batch_matmul_NN[((b_inner * 8) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[(((b_inner * 8) + ((int)threadIdx.x)) + 2)] = T_batch_matmul_NN_local[(b_inner + 8)];\n    T_batch_matmul_NN[(((b_inner * 8) + ((int)threadIdx.x)) + 4)] = T_batch_matmul_NN_local[(b_inner + 16)];\n    T_batch_matmul_NN[(((b_inner * 8) + ((int)threadIdx.x)) + 6)] = T_batch_matmul_NN_local[(b_inner + 24)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 1, 4), \"float32\"), ph_3: T.Buffer((16, 1, 4), \"float32\"), ph_7: T.Buffer((16, 4, 1), \"float32\"), T_mod: T.Buffer((16, 1, 4), \"float32\"), T_batch_matmul_NN: T.Buffer((16, 1, 1), \"float32\"), T_subtract: T.Buffer((16, 1, 4), \"float32\"), T_add: T.Buffer((16, 1, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([64], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((64,), data=ph_0.data)\n        ph_3_1 = T.Buffer((64,), data=ph_3.data)\n        for ax0 in T.parallel(16):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0 * 4 + ax2\n                T_mod_1 = T.Buffer((64,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        auto_scheduler_layout_transform_1 = T.Buffer((64,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused in T.parallel(16):\n            for ax2 in range(4):\n                cse_var_2: T.int32 = ax0_ax1_fused * 4 + ax2\n                ph_7_1 = T.Buffer((64,), data=ph_7.data)\n                auto_scheduler_layout_transform_1[cse_var_2] = ph_7_1[cse_var_2]\n        for b_i_fused_j_fused in T.parallel(16):\n            T_batch_matmul_NN_1 = T.Buffer((16,), data=T_batch_matmul_NN.data)\n            T_batch_matmul_NN_1[b_i_fused_j_fused] = T.float32(0)\n            for k in range(4):\n                cse_var_3: T.int32 = b_i_fused_j_fused * 4 + k\n                T_batch_matmul_NN_1[b_i_fused_j_fused] = T_batch_matmul_NN_1[b_i_fused_j_fused] + T.cos(ph_0_1[cse_var_3]) * auto_scheduler_layout_transform_1[cse_var_3]\n        for ax0_ax1_fused_ax2_fused in T.parallel(64):\n            T_subtract_1 = T.Buffer((64,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(64):\n            T_add_1 = T.Buffer((64,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "multiply",
                "mod",
                "cos",
                "batch_matmul",
                "subtract",
                "add"
            ]
        ],
        "input_shape": [[16, 1, 4], [13, 6, 2], [16, 1, 4], [16, 4, 1]],
        "output_shape": [[13, 6, 2], [16, 1, 4], [16, 1, 1], [16, 1, 4], [16, 1, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 360; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 15) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 15) + ax2)]) * ph_0[((ax0_ax1_fused * 15) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5400; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 5400; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acoshf(asinhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 20, 15), \"float32\"), compute: T.Buffer((18, 20, 15), \"float32\"), T_multiply: T.Buffer((18, 20, 15), \"float32\"), compute_1: T.Buffer((18, 20, 15), \"float32\"), compute_2: T.Buffer((18, 20, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5400,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(5400):\n            compute_3 = T.Buffer((5400,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(360):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_multiply_1 = T.Buffer((5400,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(5400):\n            compute_3 = T.Buffer((5400,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(5400):\n            compute_3 = T.Buffer((5400,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acos",
                "asin",
                "multiply",
                "acos",
                "asinh",
                "acosh"
            ]
        ],
        "input_shape": [[18, 20, 15]],
        "output_shape": [[18, 20, 15], [18, 20, 15], [18, 20, 15], [18, 20, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 108) + (i1 * 6)) + i2)] = expf((ph_0[(((i0 * 108) + (i1 * 6)) + i2)] * acosf(ph_0[(((i0 * 108) + (i1 * 6)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 18, 6), \"float32\"), compute: T.Buffer((20, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(18, 6):\n                cse_var_1: T.int32 = i0 * 108 + i1 * 6 + i2\n                compute_1 = T.Buffer((2160,), data=compute.data)\n                ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n                compute_1[cse_var_1] = T.exp(ph_0_1[cse_var_1] * T.acos(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "acos",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[20, 18, 6]],
        "output_shape": [[20, 18, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 680; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 680; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 680; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 680; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = expf(ph_0[i0_i1_fused_i2_fused_3]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 4, 17), \"float32\"), compute: T.Buffer((10, 4, 17), \"float32\"), compute_1: T.Buffer((10, 4, 17), \"float32\"), compute_2: T.Buffer((10, 4, 17), \"float32\"), compute_3: T.Buffer((10, 4, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((680,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(680):\n            compute_4 = T.Buffer((680,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(680):\n            compute_4 = T.Buffer((680,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(680):\n            compute_4 = T.Buffer((680,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(680):\n            compute_4 = T.Buffer((680,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "acosh",
                "sin",
                "exp",
                "exp"
            ]
        ],
        "input_shape": [[10, 4, 17]],
        "output_shape": [[10, 4, 17], [10, 4, 17], [10, 4, 17], [10, 4, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[1350];\n  float T_batch_matmul_NN[630];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1890; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1890; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1890; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 6; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n        for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n          for (int32_t ax8 = 0; ax8 < 3; ++ax8) {\n            auto_scheduler_layout_transform[(((((ax4 * 270) + (ax5 * 45)) + (ax6 * 9)) + (ax7 * 3)) + ax8)] = ph_3[(((((ax5 * 225) + (ax8 * 75)) + (ax4 * 15)) + (ax7 * 5)) + ax6)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 7; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 6; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        for (int32_t b_inner_init = 0; b_inner_init < 3; ++b_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 105) + (b_inner_init * 35)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 5; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 6; ++b_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t k_inner = 0; k_inner < 3; ++k_inner) {\n            for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n              T_batch_matmul_NN[((((b_outer_inner * 105) + (b_inner * 35)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner)] = (T_batch_matmul_NN[((((b_outer_inner * 105) + (b_inner * 35)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner)] + (ph_0[(((((b_outer_inner * 315) + (b_inner * 105)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 15)) + (k_outer * 3)) + k_inner)] * auto_scheduler_layout_transform[(((((k_outer * 270) + (b_outer_inner * 45)) + (j_outer_inner * 9)) + (k_inner * 3)) + b_inner)]));\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 630; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(T_batch_matmul_NN[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[10];\n  __shared__ float ph_3_shared[180];\n  T_batch_matmul_NN[0] = 0.000000e+00f;\n  T_batch_matmul_NN[1] = 0.000000e+00f;\n  T_batch_matmul_NN[2] = 0.000000e+00f;\n  T_batch_matmul_NN[3] = 0.000000e+00f;\n  T_batch_matmul_NN[4] = 0.000000e+00f;\n  T_batch_matmul_NN[5] = 0.000000e+00f;\n  T_batch_matmul_NN[6] = 0.000000e+00f;\n  T_batch_matmul_NN[7] = 0.000000e+00f;\n  T_batch_matmul_NN[8] = 0.000000e+00f;\n  T_batch_matmul_NN[9] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      *(float2*)(ph_3_shared + ((ax0_ax1_fused_ax2_fused_outer_outer * 36) + (((int)threadIdx.x) * 2))) = *(float2*)(ph_3 + ((((((ax0_ax1_fused_ax2_fused_outer_outer * 36) + (((int)threadIdx.x) * 2)) / 20) * 40) + (k_outer_outer * 20)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 16) + (((int)threadIdx.x) * 2)) % 20)));\n    }\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_batch_matmul_NN[0] = (T_batch_matmul_NN[0] + (ph_0[(((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5))]));\n        T_batch_matmul_NN[1] = (T_batch_matmul_NN[1] + (ph_0[(((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 1)]));\n        T_batch_matmul_NN[2] = (T_batch_matmul_NN[2] + (ph_0[(((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 2)]));\n        T_batch_matmul_NN[3] = (T_batch_matmul_NN[3] + (ph_0[(((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 3)]));\n        T_batch_matmul_NN[4] = (T_batch_matmul_NN[4] + (ph_0[(((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 4)]));\n        T_batch_matmul_NN[5] = (T_batch_matmul_NN[5] + (ph_0[((((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner) + 16)] * ph_3_shared[((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5))]));\n        T_batch_matmul_NN[6] = (T_batch_matmul_NN[6] + (ph_0[((((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner) + 16)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 1)]));\n        T_batch_matmul_NN[7] = (T_batch_matmul_NN[7] + (ph_0[((((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner) + 16)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 2)]));\n        T_batch_matmul_NN[8] = (T_batch_matmul_NN[8] + (ph_0[((((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner) + 16)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 3)]));\n        T_batch_matmul_NN[9] = (T_batch_matmul_NN[9] + (ph_0[((((((((((int)threadIdx.x) >> 1) * 64) + (((int)blockIdx.x) * 32)) + ((((int)threadIdx.x) & 1) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner) + 16)] * ph_3_shared[(((((((int)threadIdx.x) >> 1) * 20) + (k_outer_inner * 10)) + (k_inner * 5)) + 4)]));\n      }\n    }\n  }\n  compute[((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5))] = __sinf(T_batch_matmul_NN[0]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 1)] = __sinf(T_batch_matmul_NN[1]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 2)] = __sinf(T_batch_matmul_NN[2]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 3)] = __sinf(T_batch_matmul_NN[3]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 4)] = __sinf(T_batch_matmul_NN[4]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 10)] = __sinf(T_batch_matmul_NN[5]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 11)] = __sinf(T_batch_matmul_NN[6]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 12)] = __sinf(T_batch_matmul_NN[7]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 13)] = __sinf(T_batch_matmul_NN[8]);\n  compute[(((((((int)threadIdx.x) >> 1) * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 5)) + 14)] = __sinf(T_batch_matmul_NN[9]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 7, 15), \"float32\"), ph_3: T.Buffer((18, 15, 5), \"float32\"), compute: T.Buffer((18, 7, 15), \"float32\"), T_divide: T.Buffer((18, 7, 15), \"float32\"), compute_1: T.Buffer((18, 7, 15), \"float32\"), compute_2: T.Buffer((18, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1350], \"float32\", \"global\")\n        T_batch_matmul_NN = T.allocate([630], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1890,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1890):\n            compute_3 = T.Buffer((1890,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1890):\n            T_divide_1 = T.Buffer((1890,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1890):\n            compute_3 = T.Buffer((1890,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((1350,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax6, ax7, ax8 in T.grid(5, 6, 5, 3, 3):\n            ph_3_1 = T.Buffer((1350,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax4 * 270 + ax5 * 45 + ax6 * 9 + ax7 * 3 + ax8] = ph_3_1[ax5 * 225 + ax8 * 75 + ax4 * 15 + ax7 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((630,), data=T_batch_matmul_NN)\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(7):\n            for b_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(6, 5, 3):\n                T_batch_matmul_NN_1[b_outer_inner_init * 105 + b_inner_init * 35 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, j_outer_inner, k_inner, b_inner in T.grid(5, 6, 5, 3, 3):\n                cse_var_1: T.int32 = b_outer_inner * 105 + b_inner * 35 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 315 + b_inner * 105 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 15 + k_outer * 3 + k_inner] * auto_scheduler_layout_transform_1[k_outer * 270 + b_outer_inner * 45 + j_outer_inner * 9 + k_inner * 3 + b_inner]\n        for i0_i1_fused_i2_fused in T.parallel(630):\n            compute_3 = T.Buffer((630,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T_batch_matmul_NN_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "batch_matmul",
                "asinh",
                "abs",
                "divide",
                "ceil",
                "sin"
            ]
        ],
        "input_shape": [[18, 7, 15], [4, 9, 16], [18, 15, 5]],
        "output_shape": [[4, 9, 16], [18, 7, 15], [18, 7, 15], [18, 7, 15], [18, 7, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* T_multiply_1, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = fabsf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1080; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1080; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1080; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 1080; ++ax0_ax1_fused_ax2_fused_2) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused_2] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused_2]) * ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_4(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 6, 18), \"float32\"), compute: T.Buffer((10, 6, 18), \"float32\"), T_add: T.Buffer((10, 6, 18), \"float32\"), compute_1: T.Buffer((10, 6, 18), \"float32\"), T_multiply: T.Buffer((10, 6, 18), \"float32\"), T_multiply_1: T.Buffer((10, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1080,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((1080,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1080):\n            T_add_1 = T.Buffer((1080,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_2 = T.Buffer((1080,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1080):\n            T_multiply_2 = T.Buffer((1080,), data=T_multiply.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1080):\n            T_multiply_2 = T.Buffer((1080,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "abs",
                "atanh",
                "add",
                "sin",
                "abs",
                "multiply",
                "multiply"
            ]
        ],
        "input_shape": [[10, 6, 18]],
        "output_shape": [[10, 6, 18], [10, 6, 18], [10, 6, 18], [10, 6, 18], [10, 6, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_mod[((ax0_ax1_fused * 5) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 5) + ax2)], ph_3[((ax0_ax1_fused * 5) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = acoshf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 10, 5), \"float32\"), ph_3: T.Buffer((8, 10, 5), \"float32\"), T_mod: T.Buffer((8, 10, 5), \"float32\"), compute: T.Buffer((8, 10, 5), \"float32\"), compute_1: T.Buffer((8, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((400,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(80):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_mod_1 = T.Buffer((400,), data=T_mod.data)\n                ph_3_1 = T.Buffer((400,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(400):\n            compute_2 = T.Buffer((400,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(80):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((400,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "mod",
                "abs",
                "acosh"
            ]
        ],
        "input_shape": [[8, 10, 5], [8, 11, 19], [8, 10, 5]],
        "output_shape": [[8, 10, 5], [8, 11, 19], [8, 10, 5], [8, 10, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_add[(((ax0 * 187) + (ax1 * 17)) + ax2)] = (ph_0[(((ax0 * 187) + (ax1 * 17)) + ax2)] + ph_3[(((ax0 * 187) + (ax1 * 17)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1122; ++ax0_ax1_fused_ax2_fused) {\n    T_add_1[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1122; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 11, 17), \"float32\"), ph_3: T.Buffer((6, 11, 17), \"float32\"), T_add: T.Buffer((6, 11, 17), \"float32\"), T_add_1: T.Buffer((6, 11, 17), \"float32\"), T_multiply: T.Buffer((6, 11, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1122,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1122,), data=ph_3.data)\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(11, 17):\n                cse_var_1: T.int32 = ax0 * 187 + ax1 * 17 + ax2\n                T_add_2 = T.Buffer((1122,), data=T_add.data)\n                T_add_2[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1122):\n            T_add_2 = T.Buffer((1122,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1122):\n            T_multiply_1 = T.Buffer((1122,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "add",
                "acos",
                "multiply"
            ]
        ],
        "input_shape": [[6, 11, 17], [17, 13, 4], [6, 11, 17]],
        "output_shape": [[6, 11, 17], [17, 13, 4], [6, 11, 17], [6, 11, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 960; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 960; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_divide[(((ax0 * 60) + (ax1 * 5)) + ax2)] = (atanf(ph_0[(((ax0 * 60) + (ax1 * 5)) + ax2)]) / ph_0[(((ax0 * 60) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 12, 5), \"float32\"), compute: T.Buffer((16, 12, 5), \"float32\"), compute_1: T.Buffer((16, 12, 5), \"float32\"), compute_2: T.Buffer((16, 12, 5), \"float32\"), T_divide: T.Buffer((16, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_3 = T.Buffer((960,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_3 = T.Buffer((960,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_3 = T.Buffer((960,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(12, 5):\n                cse_var_1: T.int32 = ax0 * 60 + ax1 * 5 + ax2\n                T_divide_1 = T.Buffer((960,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]",
        "op_args": [
            [
                "abs",
                "acos",
                "acosh",
                "asinh",
                "atan",
                "divide"
            ]
        ],
        "input_shape": [[16, 12, 5]],
        "output_shape": [[16, 12, 5], [16, 12, 5], [16, 12, 5], [16, 12, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 198) + (i1 * 11)) + i2)] = asinf(ph_0[(((i0 * 198) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 162; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n      compute_1[((i0_i1_fused * 11) + i2_1)] = asinhf(ph_0[((i0_i1_fused * 11) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 18, 11), \"float32\"), compute: T.Buffer((9, 18, 11), \"float32\"), compute_1: T.Buffer((9, 18, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1782,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(18, 11):\n                cse_var_1: T.int32 = i0 * 198 + i1 * 11 + i2\n                compute_2 = T.Buffer((1782,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(162):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_2 = T.Buffer((1782,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[9, 18, 11]],
        "output_shape": [[9, 18, 11], [9, 18, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 42; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0_i1_fused * 6) + i2)] = cosf(atanhf(ph_0[((i0_i1_fused * 6) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 42; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 6), \"float32\"), compute: T.Buffer((1, 7, 6), \"float32\"), compute_1: T.Buffer((1, 7, 6), \"float32\"), compute_2: T.Buffer((1, 7, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((42,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(42):\n            compute_3 = T.Buffer((42,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(7):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((42,), data=compute_1.data)\n                compute_3[cse_var_1] = T.cos(T.atanh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(42):\n            compute_3 = T.Buffer((42,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[1, 7, 6]],
        "output_shape": [[1, 7, 6], [1, 7, 6], [1, 7, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 168; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] * acoshf((ph_0[((ax0_ax1_fused * 10) + ax2)] / fabsf(ph_0[((ax0_ax1_fused * 10) + ax2)]))));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1680; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1680; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 10), \"float32\"), T_multiply: T.Buffer((12, 14, 10), \"float32\"), compute: T.Buffer((12, 14, 10), \"float32\"), compute_1: T.Buffer((12, 14, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1680,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(168):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_multiply_1 = T.Buffer((1680,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * T.acosh(ph_0_1[cse_var_1] / T.fabs(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_2 = T.Buffer((1680,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_2 = T.Buffer((1680,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "divide",
                "acosh",
                "multiply",
                "asin",
                "acosh",
                "atan"
            ]
        ],
        "input_shape": [[12, 14, 10]],
        "output_shape": [[12, 14, 10], [12, 14, 10], [12, 14, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 462; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_add[(((ax0 * 154) + (ax1 * 14)) + ax2)] = (ph_0[(((ax0 * 154) + (ax1 * 14)) + ax2)] + ph_3[(((ax0 * 154) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 462; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_1[(((i0 * 154) + (i1 * 14)) + i2)] = atanf(atanhf(ph_0[(((i0 * 154) + (i1 * 14)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 11, 14), \"float32\"), ph_3: T.Buffer((3, 11, 14), \"float32\"), T_divide: T.Buffer((3, 11, 14), \"float32\"), T_add: T.Buffer((3, 11, 14), \"float32\"), compute: T.Buffer((3, 11, 14), \"float32\"), compute_1: T.Buffer((3, 11, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((462,), data=ph_0.data)\n        ph_3_1 = T.Buffer((462,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(462):\n            T_divide_1 = T.Buffer((462,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(11, 14):\n                cse_var_1: T.int32 = ax0 * 154 + ax1 * 14 + ax2\n                T_add_1 = T.Buffer((462,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_2 = T.Buffer((462,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(11, 14):\n                cse_var_2: T.int32 = i0 * 154 + i1 * 14 + i2\n                compute_2 = T.Buffer((462,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "add",
                "atanh",
                "acos",
                "atan"
            ]
        ],
        "input_shape": [[3, 11, 14], [17, 9, 19], [3, 11, 14]],
        "output_shape": [[3, 11, 14], [17, 9, 19], [3, 11, 14], [3, 11, 14], [3, 11, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 90; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 90; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 5; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_divide[((ax0_ax1_fused * 18) + ax2)] = (ph_0[((ax0_ax1_fused * 18) + ax2)] / ph_3[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 90; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 1, 18), \"float32\"), ph_3: T.Buffer((5, 1, 18), \"float32\"), T_divide: T.Buffer((5, 1, 18), \"float32\"), compute: T.Buffer((5, 1, 18), \"float32\"), compute_1: T.Buffer((5, 1, 18), \"float32\"), compute_2: T.Buffer((5, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((90,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(90):\n            compute_3 = T.Buffer((90,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(90):\n            compute_3 = T.Buffer((90,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((90,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(5):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_divide_1 = T.Buffer((90,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(90):\n            compute_3 = T.Buffer((90,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "divide",
                "asin",
                "atan",
                "atanh",
                "cos"
            ]
        ],
        "input_shape": [[5, 1, 18], [10, 12, 8], [5, 1, 18]],
        "output_shape": [[10, 12, 8], [5, 1, 18], [5, 1, 18], [5, 1, 18], [5, 1, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1056; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_mod[(((ax0 * 176) + (ax1 * 11)) + ax2)] = fmodf(ph_0[(((ax0 * 176) + (ax1 * 11)) + ax2)], ph_3[(((ax0 * 176) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = acoshf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 16, 11), \"float32\"), ph_3: T.Buffer((6, 16, 11), \"float32\"), T_divide: T.Buffer((6, 16, 11), \"float32\"), T_mod: T.Buffer((6, 16, 11), \"float32\"), compute: T.Buffer((6, 16, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1056,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1056,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1056):\n            T_divide_1 = T.Buffer((1056,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(16, 11):\n                cse_var_1: T.int32 = ax0 * 176 + ax1 * 11 + ax2\n                T_mod_1 = T.Buffer((1056,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_1 = T.Buffer((1056,), data=compute.data)\n                compute_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "mod",
                "acosh"
            ]
        ],
        "input_shape": [[6, 16, 11], [5, 13, 2], [6, 16, 11]],
        "output_shape": [[6, 16, 11], [5, 13, 2], [6, 16, 11], [6, 16, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = atanhf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_multiply[(((ax0 * 256) + (ax1 * 16)) + ax2)] = (ph_0[(((ax0 * 256) + (ax1 * 16)) + ax2)] * acoshf(atanf(ph_0[(((ax0 * 256) + (ax1 * 16)) + ax2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] * acoshf(atanf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 16, 16), \"float32\"), compute: T.Buffer((18, 16, 16), \"float32\"), T_multiply: T.Buffer((18, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4608,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_1 = T.Buffer((4608,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(16, 16):\n                cse_var_2: T.int32 = ax0 * 256 + ax1 * 16 + ax2\n                T_multiply_1 = T.Buffer((4608,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] * T.acosh(T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "atanh",
                "atan",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[18, 16, 16]],
        "output_shape": [[18, 16, 16], [18, 16, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 192) + (i1 * 16)) + i2)] = atanhf(ph_0[(((i0 * 192) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n        compute_1[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)] = cosf(atanf(ph_0[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1728; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 12, 16), \"float32\"), compute: T.Buffer((9, 12, 16), \"float32\"), compute_1: T.Buffer((9, 12, 16), \"float32\"), compute_2: T.Buffer((9, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1728,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_1: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_3 = T.Buffer((1728,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_2: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_3 = T.Buffer((1728,), data=compute_1.data)\n                compute_3[cse_var_2] = T.cos(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1728):\n            compute_3 = T.Buffer((1728,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "atan",
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[9, 12, 16]],
        "output_shape": [[9, 12, 16], [9, 12, 16], [9, 12, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 156) + (i1 * 13)) + i2)] = acosf(ph_0[(((i0 * 156) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 3; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n        compute_1[(((i0_1 * 156) + (i1_1 * 13)) + i2_1)] = fabsf(ph_0[(((i0_1 * 156) + (i1_1 * 13)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 12, 13), \"float32\"), compute: T.Buffer((3, 12, 13), \"float32\"), compute_1: T.Buffer((3, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(12, 13):\n                cse_var_1: T.int32 = i0 * 156 + i1 * 13 + i2\n                compute_2 = T.Buffer((468,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(12, 13):\n                cse_var_2: T.int32 = i0 * 156 + i1 * 13 + i2\n                compute_2 = T.Buffer((468,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acos",
                "abs"
            ]
        ],
        "input_shape": [[3, 12, 13]],
        "output_shape": [[3, 12, 13], [3, 12, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_subtract[(((ax0 * 100) + (ax1 * 20)) + ax2)] = (ph_0[(((ax0 * 100) + (ax1 * 20)) + ax2)] - ph_3[(((ax0 * 100) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 100) + (i1 * 20)) + i2)] = acoshf((ph_0[(((i0 * 100) + (i1 * 20)) + i2)] + atanhf(ph_0[(((i0 * 100) + (i1 * 20)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1600; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused] = atanhf(ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 5, 20), \"float32\"), ph_3: T.Buffer((16, 5, 20), \"float32\"), T_subtract: T.Buffer((16, 5, 20), \"float32\"), compute: T.Buffer((16, 5, 20), \"float32\"), T_subtract_1: T.Buffer((16, 5, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1600,), data=ph_0.data)\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(5, 20):\n                cse_var_1: T.int32 = ax0 * 100 + ax1 * 20 + ax2\n                T_subtract_2 = T.Buffer((1600,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((1600,), data=ph_3.data)\n                T_subtract_2[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(5, 20):\n                cse_var_2: T.int32 = i0 * 100 + i1 * 20 + i2\n                compute_1 = T.Buffer((1600,), data=compute.data)\n                compute_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2] + T.atanh(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1600):\n            T_subtract_2 = T.Buffer((1600,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "subtract",
                "atanh",
                "add",
                "acosh",
                "subtract"
            ]
        ],
        "input_shape": [[16, 5, 20], [17, 1, 13], [16, 5, 20]],
        "output_shape": [[16, 5, 20], [17, 1, 13], [16, 5, 20], [16, 5, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 11; ++i1) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i1 * 3) + i2)] = acosf(ph_0[((i1 * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 11; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_add[((ax0_ax1_fused * 3) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 3) + ax2)]) + ph_0[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 33; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 33; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 3), \"float32\"), compute: T.Buffer((1, 11, 3), \"float32\"), T_add: T.Buffer((1, 11, 3), \"float32\"), compute_1: T.Buffer((1, 11, 3), \"float32\"), compute_2: T.Buffer((1, 11, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((33,), data=ph_0.data)\n        for i1, i2 in T.grid(11, 3):\n            cse_var_1: T.int32 = i1 * 3 + i2\n            compute_3 = T.Buffer((33,), data=compute.data)\n            compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(11):\n            for ax2 in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_add_1 = T.Buffer((33,), data=T_add.data)\n                T_add_1[cse_var_2] = T.asin(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(33):\n            compute_3 = T.Buffer((33,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(33):\n            compute_3 = T.Buffer((33,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acos",
                "asin",
                "add",
                "acos",
                "atanh",
                "exp"
            ]
        ],
        "input_shape": [[1, 11, 3]],
        "output_shape": [[1, 11, 3], [1, 11, 3], [1, 11, 3], [1, 11, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_subtract[(((ax0 * 216) + (ax1 * 18)) + ax2)] = (ph_0[(((ax0 * 216) + (ax1 * 18)) + ax2)] - ph_3[(((ax0 * 216) + (ax1 * 18)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 36; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = atanhf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 12, 18), \"float32\"), ph_3: T.Buffer((3, 12, 18), \"float32\"), T_subtract: T.Buffer((3, 12, 18), \"float32\"), compute: T.Buffer((3, 12, 18), \"float32\"), compute_1: T.Buffer((3, 12, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((648,), data=ph_0.data)\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(12, 18):\n                cse_var_1: T.int32 = ax0 * 216 + ax1 * 18 + ax2\n                T_subtract_1 = T.Buffer((648,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((648,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_2 = T.Buffer((648,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(36):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((648,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "subtract",
                "acosh",
                "atanh"
            ]
        ],
        "input_shape": [[3, 12, 18], [5, 19, 10], [3, 12, 18]],
        "output_shape": [[3, 12, 18], [5, 19, 10], [3, 12, 18], [3, 12, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  float compute_3[850];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 850; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_3[(((i0 * 85) + (i1 * 5)) + i2)] = expf(ph_0[(((i0 * 85) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 850; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 850; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 17, 5), \"float32\"), compute: T.Buffer((10, 17, 5), \"float32\"), compute_1: T.Buffer((10, 17, 5), \"float32\"), compute_2: T.Buffer((10, 17, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([850], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((850,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(850):\n            compute_4 = T.Buffer((850,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((850,), data=compute_3)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(17, 5):\n                cse_var_1: T.int32 = i0 * 85 + i1 * 5 + i2\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(850):\n            compute_5 = T.Buffer((850,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acos(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(850):\n            compute_5 = T.Buffer((850,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "exp",
                "acos",
                "sin"
            ]
        ],
        "input_shape": [[10, 17, 5]],
        "output_shape": [[10, 17, 5], [10, 17, 5], [10, 17, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      compute[((i0 * 16) + i1)] = sinf(ph_0[((i0 * 16) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 48; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    compute_2[i0_i1_fused] = fabsf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 48; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 16, 1), \"float32\"), compute: T.Buffer((3, 16, 1), \"float32\"), compute_1: T.Buffer((3, 16, 1), \"float32\"), compute_2: T.Buffer((3, 16, 1), \"float32\"), compute_3: T.Buffer((3, 16, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1 in range(16):\n                cse_var_1: T.int32 = i0 * 16 + i1\n                compute_4 = T.Buffer((48,), data=compute.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_4 = T.Buffer((48,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(48):\n            compute_4 = T.Buffer((48,), data=compute_2.data)\n            compute_4[i0_i1_fused] = T.fabs(ph_0_1[i0_i1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(48):\n            compute_4 = T.Buffer((48,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "acos",
                "asin",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[3, 16, 1]],
        "output_shape": [[3, 16, 1], [3, 16, 1], [3, 16, 1], [3, 16, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 462; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 42) + (i1 * 14)) + i2)] = ceilf(ph_0[(((i0 * 42) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 33; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      compute_1[((i0_i1_fused * 14) + i2_1)] = atanhf(ph_0[((i0_i1_fused * 14) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 3, 14), \"float32\"), ph_3: T.Buffer((11, 3, 14), \"float32\"), T_multiply: T.Buffer((11, 3, 14), \"float32\"), compute: T.Buffer((11, 3, 14), \"float32\"), compute_1: T.Buffer((11, 3, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((462,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(462):\n            T_multiply_1 = T.Buffer((462,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((462,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(3, 14):\n                cse_var_1: T.int32 = i0 * 42 + i1 * 14 + i2\n                compute_2 = T.Buffer((462,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(33):\n            for i2 in range(14):\n                cse_var_2: T.int32 = i0_i1_fused * 14 + i2\n                compute_2 = T.Buffer((462,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "ceil",
                "atanh"
            ]
        ],
        "input_shape": [[11, 3, 14], [15, 20, 3], [11, 3, 14]],
        "output_shape": [[11, 3, 14], [15, 20, 3], [11, 3, 14], [11, 3, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 324; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = atanhf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 648; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - (atanf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_1[(((i0 * 36) + (i1 * 2)) + i2_1)] = atanhf(asinf(ph_0[(((i0 * 36) + (i1 * 2)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 648; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (asinf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 18, 2), \"float32\"), compute: T.Buffer((18, 18, 2), \"float32\"), T_subtract: T.Buffer((18, 18, 2), \"float32\"), compute_1: T.Buffer((18, 18, 2), \"float32\"), T_add: T.Buffer((18, 18, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((648,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(324):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((648,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_subtract_1 = T.Buffer((648,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(18, 2):\n                cse_var_2: T.int32 = i0 * 36 + i1 * 2 + i2\n                compute_2 = T.Buffer((648,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(T.asin(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_add_1 = T.Buffer((648,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "atan",
                "divide",
                "subtract",
                "asin",
                "atanh",
                "add"
            ]
        ],
        "input_shape": [[18, 18, 2]],
        "output_shape": [[18, 18, 2], [18, 18, 2], [18, 18, 2], [18, 18, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_multiply[((ax1 * 14) + ax2)] = (ph_0[((ax1 * 14) + ax2)] * ph_3[((ax1 * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 98; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 98; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 14), \"float32\"), ph_3: T.Buffer((1, 7, 14), \"float32\"), T_multiply: T.Buffer((1, 7, 14), \"float32\"), T_divide: T.Buffer((1, 7, 14), \"float32\"), compute: T.Buffer((1, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((98,), data=ph_0.data)\n        ph_3_1 = T.Buffer((98,), data=ph_3.data)\n        for ax1, ax2 in T.grid(7, 14):\n            cse_var_1: T.int32 = ax1 * 14 + ax2\n            T_multiply_1 = T.Buffer((98,), data=T_multiply.data)\n            T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(98):\n            T_divide_1 = T.Buffer((98,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(98):\n            compute_1 = T.Buffer((98,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "divide",
                "atanh"
            ]
        ],
        "input_shape": [[1, 7, 14], [13, 8, 13], [1, 7, 14]],
        "output_shape": [[1, 7, 14], [13, 8, 13], [1, 7, 14], [1, 7, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 720; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 720; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = fabsf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 720; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = cosf(ceilf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 15), \"float32\"), compute: T.Buffer((8, 6, 15), \"float32\"), compute_1: T.Buffer((8, 6, 15), \"float32\"), compute_2: T.Buffer((8, 6, 15), \"float32\"), compute_3: T.Buffer((8, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "asin",
                "ceil",
                "abs",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[8, 6, 15]],
        "output_shape": [[8, 6, 15], [8, 6, 15], [8, 6, 15], [8, 6, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 14; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = acosf((ph_0[((i0_i1_fused * 8) + i2)] * acoshf(ph_0[((i0_i1_fused * 8) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 14; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 8) + i2_1)] = ceilf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 7, 8), \"float32\"), compute: T.Buffer((2, 7, 8), \"float32\"), compute_1: T.Buffer((2, 7, 8), \"float32\"), compute_2: T.Buffer((2, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((112,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(14):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((112,), data=compute.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1] * T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_3 = T.Buffer((112,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(14):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((112,), data=compute_2.data)\n                compute_3[cse_var_2] = T.ceil(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acosh",
                "multiply",
                "acos",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[2, 7, 8]],
        "output_shape": [[2, 7, 8], [2, 7, 8], [2, 7, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 10; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i1 * 4) + i2)] = asinhf(ph_0[((i1 * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 40; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n      compute_2[((i0_i1_fused * 4) + i2_1)] = expf(ph_0[((i0_i1_fused * 4) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 10, 4), \"float32\"), compute: T.Buffer((1, 10, 4), \"float32\"), compute_1: T.Buffer((1, 10, 4), \"float32\"), compute_2: T.Buffer((1, 10, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((40,), data=ph_0.data)\n        for i1, i2 in T.grid(10, 4):\n            cse_var_1: T.int32 = i1 * 4 + i2\n            compute_3 = T.Buffer((40,), data=compute.data)\n            compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(40):\n            compute_3 = T.Buffer((40,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(10):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_3 = T.Buffer((40,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "sin",
                "atanh",
                "exp"
            ]
        ],
        "input_shape": [[1, 10, 4]],
        "output_shape": [[1, 10, 4], [1, 10, 4], [1, 10, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1560; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1560; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_2[(((i0 * 195) + (i1 * 13)) + i2)] = acoshf(ph_0[(((i0 * 195) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_3[((i0_i1_fused * 13) + i2_1)] = atanf(ph_0[((i0_i1_fused * 13) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 15, 13), \"float32\"), compute: T.Buffer((8, 15, 13), \"float32\"), compute_1: T.Buffer((8, 15, 13), \"float32\"), compute_2: T.Buffer((8, 15, 13), \"float32\"), compute_3: T.Buffer((8, 15, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1560,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_4 = T.Buffer((1560,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_4 = T.Buffer((1560,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(15, 13):\n                cse_var_1: T.int32 = i0 * 195 + i1 * 13 + i2\n                compute_4 = T.Buffer((1560,), data=compute_2.data)\n                compute_4[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(120):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_4 = T.Buffer((1560,), data=compute_3.data)\n                compute_4[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "ceil",
                "acos",
                "acosh",
                "atan"
            ]
        ],
        "input_shape": [[8, 15, 13]],
        "output_shape": [[8, 15, 13], [8, 15, 13], [8, 15, 13], [8, 15, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 20; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = acoshf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 160; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 160; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(fmodf(ph_0[i0_i1_fused_i2_fused_1], atanf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 20, 8), \"float32\"), compute: T.Buffer((1, 20, 8), \"float32\"), compute_1: T.Buffer((1, 20, 8), \"float32\"), compute_2: T.Buffer((1, 20, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((160,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(20):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((160,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            compute_3 = T.Buffer((160,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            compute_3 = T.Buffer((160,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.atan(ph_0_1[i0_i1_fused_i2_fused])))",
        "op_args": [
            [
                "acosh",
                "acos",
                "atan",
                "atan",
                "mod",
                "atan"
            ]
        ],
        "input_shape": [[1, 20, 8]],
        "output_shape": [[1, 20, 8], [1, 20, 8], [1, 20, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 126; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 126; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 7; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_mod[((ax0_ax1_fused * 18) + ax2)] = fmodf(atanf(ph_0[((ax0_ax1_fused * 18) + ax2)]), ph_0[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 18), \"float32\"), ph_3: T.Buffer((1, 7, 18), \"float32\"), T_multiply: T.Buffer((1, 7, 18), \"float32\"), T_add: T.Buffer((1, 7, 18), \"float32\"), T_mod: T.Buffer((1, 7, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((126,), data=ph_0.data)\n        ph_3_1 = T.Buffer((126,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(126):\n            T_multiply_1 = T.Buffer((126,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(126):\n            T_add_1 = T.Buffer((126,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(7):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_mod_1 = T.Buffer((126,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.atan(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])",
        "op_args": [
            [
                "multiply",
                "add",
                "atan",
                "mod"
            ]
        ],
        "input_shape": [[1, 7, 18], [4, 8, 10], [1, 7, 18]],
        "output_shape": [[1, 7, 18], [4, 8, 10], [1, 7, 18], [1, 7, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 162; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 18) + ax2)] = (ph_0[((ax0_ax1_fused * 18) + ax2)] * ph_3[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 324) + (i1 * 18)) + i2)] = cosf(ph_0[(((i0 * 324) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2916; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2916; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 18, 18), \"float32\"), ph_3: T.Buffer((9, 18, 18), \"float32\"), T_multiply: T.Buffer((9, 18, 18), \"float32\"), compute: T.Buffer((9, 18, 18), \"float32\"), compute_1: T.Buffer((9, 18, 18), \"float32\"), compute_2: T.Buffer((9, 18, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2916,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(162):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_multiply_1 = T.Buffer((2916,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((2916,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(18, 18):\n                cse_var_2: T.int32 = i0 * 324 + i1 * 18 + i2\n                compute_3 = T.Buffer((2916,), data=compute.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2916):\n            compute_3 = T.Buffer((2916,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2916):\n            compute_3 = T.Buffer((2916,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "cos",
                "abs",
                "sin",
                "asin"
            ]
        ],
        "input_shape": [[9, 18, 18], [18, 13, 14], [9, 18, 18]],
        "output_shape": [[9, 18, 18], [18, 13, 14], [9, 18, 18], [9, 18, 18], [9, 18, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1344; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_subtract[(((ax0 * 96) + (ax1 * 12)) + ax2)] = (ph_0[(((ax0 * 96) + (ax1 * 12)) + ax2)] - fabsf(sinf(ph_0[(((ax0 * 96) + (ax1 * 12)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1344; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_2[((i0_i1_fused * 12) + i2)] = atanf(asinhf(ph_0[((i0_i1_fused * 12) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - fabsf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanf(asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 8, 12), \"float32\"), compute: T.Buffer((14, 8, 12), \"float32\"), T_subtract: T.Buffer((14, 8, 12), \"float32\"), compute_1: T.Buffer((14, 8, 12), \"float32\"), compute_2: T.Buffer((14, 8, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1344,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1344):\n            compute_3 = T.Buffer((1344,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(8, 12):\n                cse_var_1: T.int32 = ax0 * 96 + ax1 * 12 + ax2\n                T_subtract_1 = T.Buffer((1344,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.fabs(T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1344):\n            compute_3 = T.Buffer((1344,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(112):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((1344,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "cos",
                "sin",
                "abs",
                "subtract",
                "asinh",
                "exp",
                "atan"
            ]
        ],
        "input_shape": [[14, 8, 12]],
        "output_shape": [[14, 8, 12], [14, 8, 12], [14, 8, 12], [14, 8, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 304; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_add[((ax0_ax1_fused * 20) + ax2)] = (ph_0[((ax0_ax1_fused * 20) + ax2)] + ph_3[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 6080; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6080; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_1[(((i0 * 320) + (i1 * 20)) + i2)] = cosf(acoshf(ph_0[(((i0 * 320) + (i1 * 20)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 16, 20), \"float32\"), ph_3: T.Buffer((19, 16, 20), \"float32\"), T_add: T.Buffer((19, 16, 20), \"float32\"), T_multiply: T.Buffer((19, 16, 20), \"float32\"), compute: T.Buffer((19, 16, 20), \"float32\"), compute_1: T.Buffer((19, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((6080,), data=ph_0.data)\n        ph_3_1 = T.Buffer((6080,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_add_1 = T.Buffer((6080,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(6080):\n            T_multiply_1 = T.Buffer((6080,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(6080):\n            compute_2 = T.Buffer((6080,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(16, 20):\n                cse_var_2: T.int32 = i0 * 320 + i1 * 20 + i2\n                compute_2 = T.Buffer((6080,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "add",
                "multiply",
                "acosh",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[19, 16, 20], [1, 9, 10], [19, 16, 20]],
        "output_shape": [[19, 16, 20], [1, 9, 10], [19, 16, 20], [19, 16, 20], [19, 16, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 405; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 405; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 405; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 9, 9), \"float32\"), ph_3: T.Buffer((5, 9, 9), \"float32\"), T_multiply: T.Buffer((5, 9, 9), \"float32\"), compute: T.Buffer((5, 9, 9), \"float32\"), compute_1: T.Buffer((5, 9, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((405,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(405):\n            T_multiply_1 = T.Buffer((405,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((405,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(405):\n            compute_2 = T.Buffer((405,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(405):\n            compute_2 = T.Buffer((405,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "cos",
                "asin"
            ]
        ],
        "input_shape": [[5, 9, 9], [18, 5, 5], [5, 9, 9]],
        "output_shape": [[5, 9, 9], [18, 5, 5], [5, 9, 9], [5, 9, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = acoshf(fmodf(ph_0[((i0_i1_fused * 19) + i2)], sinf(ph_0[((i0_i1_fused * 19) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 76; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 76; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 2, 19), \"float32\"), compute: T.Buffer((2, 2, 19), \"float32\"), compute_1: T.Buffer((2, 2, 19), \"float32\"), compute_2: T.Buffer((2, 2, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((76,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((76,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(T.truncmod(ph_0_1[cse_var_1], T.sin(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(76):\n            compute_3 = T.Buffer((76,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(76):\n            compute_3 = T.Buffer((76,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "mod",
                "acosh",
                "atan",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[2, 2, 19]],
        "output_shape": [[2, 2, 19], [2, 2, 19], [2, 2, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1144; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1144; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 88; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_add[((ax0_ax1_fused * 13) + ax2)] = (acosf(ph_0[((ax0_ax1_fused * 13) + ax2)]) + ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 8, 13), \"float32\"), ph_3: T.Buffer((11, 8, 13), \"float32\"), T_multiply: T.Buffer((11, 8, 13), \"float32\"), compute: T.Buffer((11, 8, 13), \"float32\"), T_add: T.Buffer((11, 8, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1144,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1144):\n            T_multiply_1 = T.Buffer((1144,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((1144,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1144):\n            compute_1 = T.Buffer((1144,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(88):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_add_1 = T.Buffer((1144,), data=T_add.data)\n                T_add_1[cse_var_1] = T.acos(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]",
        "op_args": [
            [
                "multiply",
                "asin",
                "acos",
                "add"
            ]
        ],
        "input_shape": [[11, 8, 13], [14, 6, 14], [11, 8, 13]],
        "output_shape": [[11, 8, 13], [14, 6, 14], [11, 8, 13], [11, 8, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4560; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - acoshf(ph_0[ax0_ax1_fused_ax2_fused])) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 240; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 19) + ax2)] = ((ph_0[((ax0_ax1_fused * 19) + ax2)] / acoshf(ph_0[((ax0_ax1_fused * 19) + ax2)])) - ph_0[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = expf((ph_0[((i0_i1_fused * 19) + i2)] / acoshf(ph_0[((i0_i1_fused * 19) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 19), \"float32\"), T_divide: T.Buffer((16, 15, 19), \"float32\"), T_subtract: T.Buffer((16, 15, 19), \"float32\"), compute: T.Buffer((16, 15, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4560,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4560):\n            T_divide_1 = T.Buffer((4560,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] - T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused])) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(240):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_subtract_1 = T.Buffer((4560,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] / T.acosh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_1 = T.Buffer((4560,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(ph_0_1[cse_var_2] / T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acosh",
                "subtract",
                "divide",
                "acosh",
                "divide",
                "subtract",
                "exp"
            ]
        ],
        "input_shape": [[16, 15, 19]],
        "output_shape": [[16, 15, 19], [16, 15, 19], [16, 15, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    float compute[1];\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        compute[0] = expf(ph_0[(((ax0 * 195) + (ax1 * 13)) + ax2)]);\n        T_subtract[(((ax0 * 195) + (ax1 * 13)) + ax2)] = (ph_0[(((ax0 * 195) + (ax1 * 13)) + ax2)] - compute[0]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 15, 13), \"float32\"), T_subtract: T.Buffer((18, 15, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(18):\n            compute = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(15, 13):\n                cse_var_1: T.int32 = ax0 * 195 + ax1 * 13 + ax2\n                compute_1 = T.Buffer((1,), data=compute, align=4)\n                ph_0_1 = T.Buffer((3510,), data=ph_0.data)\n                compute_1[0] = T.exp(ph_0_1[cse_var_1])\n                T_subtract_1 = T.Buffer((3510,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - compute_1[0]",
        "op_args": [
            [
                "exp",
                "subtract"
            ]
        ],
        "input_shape": [[18, 15, 13]],
        "output_shape": [[18, 15, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5508; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5508; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], asinhf(sinf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], asinhf(__sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 17, 18), \"float32\"), compute: T.Buffer((18, 17, 18), \"float32\"), T_mod: T.Buffer((18, 17, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5508,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(5508):\n            compute_1 = T.Buffer((5508,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(5508):\n            T_mod_1 = T.Buffer((5508,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.asinh(T.sin(ph_0_1[ax0_ax1_fused_ax2_fused])))",
        "op_args": [
            [
                "sin",
                "sin",
                "asinh",
                "mod"
            ]
        ],
        "input_shape": [[18, 17, 18]],
        "output_shape": [[18, 17, 18], [18, 17, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 126; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = fabsf((ph_0[((i0_i1_fused * 7) + i2)] - asinf(ph_0[((i0_i1_fused * 7) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 7, 7), \"float32\"), compute: T.Buffer((18, 7, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(126):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_1 = T.Buffer((882,), data=compute.data)\n                ph_0_1 = T.Buffer((882,), data=ph_0.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1] - T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asin",
                "subtract",
                "abs"
            ]
        ],
        "input_shape": [[18, 7, 7]],
        "output_shape": [[18, 7, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    float compute_1[20];\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_1[i2] = expf(ph_0[((i0_i1_fused * 20) + i2)]);\n    }\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute[((i0_i1_fused * 20) + i2_1)] = fabsf(fmodf(ph_0[((i0_i1_fused * 20) + i2_1)], compute_1[i2_1]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 30; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 20) + ax2)] = ((ph_0[((ax0_ax1_fused * 20) + ax2)] + asinf(ph_0[((ax0_ax1_fused * 20) + ax2)])) * ph_0[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 600; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = asinf(ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 10, 20), \"float32\"), compute: T.Buffer((3, 10, 20), \"float32\"), T_multiply: T.Buffer((3, 10, 20), \"float32\"), T_subtract: T.Buffer((3, 10, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((600,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(30):\n            compute_1 = T.allocate([20], \"float32\", \"global\")\n            compute_2 = T.Buffer((20,), data=compute_1)\n            for i2 in range(20):\n                compute_2[i2] = T.exp(ph_0_1[i0_i1_fused * 20 + i2])\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((600,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(T.truncmod(ph_0_1[cse_var_1], compute_2[i2]))\n        for ax0_ax1_fused in T.parallel(30):\n            for ax2 in range(20):\n                cse_var_2: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_multiply_1 = T.Buffer((600,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = (ph_0_1[cse_var_2] + T.asin(ph_0_1[cse_var_2])) * ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(600):\n            T_subtract_1 = T.Buffer((600,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "exp",
                "mod",
                "abs",
                "asin",
                "add",
                "multiply",
                "subtract"
            ]
        ],
        "input_shape": [[3, 10, 20]],
        "output_shape": [[3, 10, 20], [3, 10, 20], [3, 10, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 176) + (i1 * 16)) + i2)] = acosf(atanhf(ph_0[(((i0 * 176) + (i1 * 16)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 15; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n        compute_2[(((i0_1 * 176) + (i1_1 * 16)) + i2_1)] = acosf(ph_0[(((i0_1 * 176) + (i1_1 * 16)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2640; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 11, 16), \"float32\"), compute: T.Buffer((15, 11, 16), \"float32\"), compute_1: T.Buffer((15, 11, 16), \"float32\"), compute_2: T.Buffer((15, 11, 16), \"float32\"), compute_3: T.Buffer((15, 11, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2640,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_4 = T.Buffer((2640,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(11, 16):\n                cse_var_1: T.int32 = i0 * 176 + i1 * 16 + i2\n                compute_4 = T.Buffer((2640,), data=compute_1.data)\n                compute_4[cse_var_1] = T.acos(T.atanh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(11, 16):\n                cse_var_2: T.int32 = i0 * 176 + i1 * 16 + i2\n                compute_4 = T.Buffer((2640,), data=compute_2.data)\n                compute_4[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_4 = T.Buffer((2640,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "atanh",
                "acos",
                "acos",
                "abs"
            ]
        ],
        "input_shape": [[15, 11, 16]],
        "output_shape": [[15, 11, 16], [15, 11, 16], [15, 11, 16], [15, 11, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_mod[(((ax0 * 32) + (ax1 * 16)) + ax2)] = fmodf(ph_0[(((ax0 * 32) + (ax1 * 16)) + ax2)], ceilf(ph_0[(((ax0 * 32) + (ax1 * 16)) + ax2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 2, 16), \"float32\"), T_mod: T.Buffer((8, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(2, 16):\n                cse_var_1: T.int32 = ax0 * 32 + ax1 * 16 + ax2\n                T_mod_1 = T.Buffer((256,), data=T_mod.data)\n                ph_0_1 = T.Buffer((256,), data=ph_0.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.ceil(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "ceil",
                "mod"
            ]
        ],
        "input_shape": [[8, 2, 16]],
        "output_shape": [[8, 2, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 266) + (i1 * 19)) + i2)] = expf(ph_0[(((i0 * 266) + (i1 * 19)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3724; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 14; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 14; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_1[(((i0_1 * 266) + (i1_1 * 19)) + i2_1)] = atanf(ph_0[(((i0_1 * 266) + (i1_1 * 19)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3724; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 14, 19), \"float32\"), compute: T.Buffer((14, 14, 19), \"float32\"), T_add: T.Buffer((14, 14, 19), \"float32\"), compute_1: T.Buffer((14, 14, 19), \"float32\"), compute_2: T.Buffer((14, 14, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3724,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(14, 19):\n                cse_var_1: T.int32 = i0 * 266 + i1 * 19 + i2\n                compute_3 = T.Buffer((3724,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3724):\n            T_add_1 = T.Buffer((3724,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(14, 19):\n                cse_var_2: T.int32 = i0 * 266 + i1 * 19 + i2\n                compute_3 = T.Buffer((3724,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(3724):\n            compute_3 = T.Buffer((3724,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "abs",
                "add",
                "atan",
                "abs"
            ]
        ],
        "input_shape": [[14, 14, 19]],
        "output_shape": [[14, 14, 19], [14, 14, 19], [14, 14, 19], [14, 14, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1188; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 108; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_divide[((ax0_ax1_fused * 11) + ax2)] = (atanf(ph_0[((ax0_ax1_fused * 11) + ax2)]) / ph_0[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1188; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((int)blockIdx.x)] = (atanf(ph_0[((int)blockIdx.x)]) / ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 18, 11), \"float32\"), compute: T.Buffer((6, 18, 11), \"float32\"), T_divide: T.Buffer((6, 18, 11), \"float32\"), compute_1: T.Buffer((6, 18, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1188,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            compute_2 = T.Buffer((1188,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(108):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_divide_1 = T.Buffer((1188,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            compute_2 = T.Buffer((1188,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "atan",
                "divide",
                "asin"
            ]
        ],
        "input_shape": [[6, 18, 11]],
        "output_shape": [[6, 18, 11], [6, 18, 11], [6, 18, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = expf(acosf(ph_0[((i0_i1_fused * 18) + i2)]));\n    }\n  }\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_2[((i1 * 18) + i2_1)] = acoshf(ph_0[((i1 * 18) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 144; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 8, 18), \"float32\"), compute: T.Buffer((1, 8, 18), \"float32\"), compute_1: T.Buffer((1, 8, 18), \"float32\"), compute_2: T.Buffer((1, 8, 18), \"float32\"), compute_3: T.Buffer((1, 8, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((144,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_4 = T.Buffer((144,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(8):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_4 = T.Buffer((144,), data=compute_1.data)\n                compute_4[cse_var_1] = T.exp(T.acos(ph_0_1[cse_var_1]))\n        for i1, i2 in T.grid(8, 18):\n            cse_var_2: T.int32 = i1 * 18 + i2\n            compute_4 = T.Buffer((144,), data=compute_2.data)\n            compute_4[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_4 = T.Buffer((144,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "acos",
                "exp",
                "acosh",
                "ceil"
            ]
        ],
        "input_shape": [[1, 8, 18]],
        "output_shape": [[1, 8, 18], [1, 8, 18], [1, 8, 18], [1, 8, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1520; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - sinf(ph_0[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 380; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = ceilf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1520; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 20, 4), \"float32\"), T_add: T.Buffer((19, 20, 4), \"float32\"), compute: T.Buffer((19, 20, 4), \"float32\"), compute_1: T.Buffer((19, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1520,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1520):\n            T_add_1 = T.Buffer((1520,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(380):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_2 = T.Buffer((1520,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1520):\n            compute_2 = T.Buffer((1520,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "subtract",
                "add",
                "ceil",
                "abs"
            ]
        ],
        "input_shape": [[19, 20, 4]],
        "output_shape": [[19, 20, 4], [19, 20, 4], [19, 20, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 120; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 120; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_add_1[(((ax0 * 6) + (ax1 * 2)) + ax2)] = ((ph_0[(((ax0 * 6) + (ax1 * 2)) + ax2)] * (ph_0[(((ax0 * 6) + (ax1 * 2)) + ax2)] * ph_3[(((ax0 * 6) + (ax1 * 2)) + ax2)])) + ph_0[(((ax0 * 6) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 3, 2), \"float32\"), ph_3: T.Buffer((20, 3, 2), \"float32\"), T_add: T.Buffer((20, 3, 2), \"float32\"), compute: T.Buffer((20, 3, 2), \"float32\"), T_add_1: T.Buffer((20, 3, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((120,), data=ph_0.data)\n        ph_3_1 = T.Buffer((120,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(120):\n            T_add_2 = T.Buffer((120,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(120):\n            compute_1 = T.Buffer((120,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(3, 2):\n                cse_var_1: T.int32 = ax0 * 6 + ax1 * 2 + ax2\n                T_add_2 = T.Buffer((120,), data=T_add_1.data)\n                T_add_2[cse_var_1] = ph_0_1[cse_var_1] * (ph_0_1[cse_var_1] * ph_3_1[cse_var_1]) + ph_0_1[cse_var_1]",
        "op_args": [
            [
                "add",
                "multiply",
                "multiply",
                "cos",
                "add"
            ]
        ],
        "input_shape": [[20, 3, 2], [1, 11, 2], [20, 3, 2]],
        "output_shape": [[20, 3, 2], [1, 11, 2], [20, 3, 2], [20, 3, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 143; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 2) + ax2)] = (ph_0[((ax0_ax1_fused * 2) + ax2)] * ph_3[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 143; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 2; ++ax2_1) {\n      T_multiply_1[((ax0_ax1_fused_1 * 2) + ax2_1)] = (ph_0[((ax0_ax1_fused_1 * 2) + ax2_1)] * ph_3[((ax0_ax1_fused_1 * 2) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 286; ++i0_i1_fused_i2_fused) {\n    float compute_1[1];\n    compute_1[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute[i0_i1_fused_i2_fused] = sinf(compute_1[0]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 13, 2), \"float32\"), ph_3: T.Buffer((11, 13, 2), \"float32\"), T_multiply: T.Buffer((11, 13, 2), \"float32\"), T_multiply_1: T.Buffer((11, 13, 2), \"float32\"), compute: T.Buffer((11, 13, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((286,), data=ph_0.data)\n        ph_3_1 = T.Buffer((286,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(143):\n            for ax2 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_multiply_2 = T.Buffer((286,), data=T_multiply.data)\n                T_multiply_2[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(143):\n            for ax2 in range(2):\n                cse_var_2: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_multiply_2 = T.Buffer((286,), data=T_multiply_1.data)\n                T_multiply_2[cse_var_2] = ph_0_1[cse_var_2] * ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(286):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_3 = T.Buffer((286,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(compute_2[0])",
        "op_args": [
            [
                "multiply",
                "multiply",
                "exp",
                "sin"
            ]
        ],
        "input_shape": [[11, 13, 2], [9, 1, 2], [11, 13, 2]],
        "output_shape": [[11, 13, 2], [9, 1, 2], [11, 13, 2], [11, 13, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[1100];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3520; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3520; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3520; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 10; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax9 = 0; ax9 < 11; ++ax9) {\n      for (int32_t ax10 = 0; ax10 < 10; ++ax10) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused_1 * 110) + (ax9 * 10)) + ax10)] = ph_3[(((((ax0_ax1_fused_ax2_fused_1 / 5) * 550) + (ax10 * 55)) + (ax9 * 5)) + (ax0_ax1_fused_ax2_fused_1 % 5))];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_i1_outer_fused_i2_outer_fused = 0; i0_outer_i1_outer_fused_i2_outer_fused < 160; ++i0_outer_i1_outer_fused_i2_outer_fused) {\n    float T_batch_matmul_NN[10];\n    for (int32_t b_inner_init = 0; b_inner_init < 10; ++b_inner_init) {\n      T_batch_matmul_NN[b_inner_init] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 11; ++k_inner) {\n      for (int32_t b_inner = 0; b_inner < 10; ++b_inner) {\n        T_batch_matmul_NN[b_inner] = (T_batch_matmul_NN[b_inner] + (ph_0[(((((i0_outer_i1_outer_fused_i2_outer_fused / 80) * 1760) + (b_inner * 176)) + (((i0_outer_i1_outer_fused_i2_outer_fused % 80) / 5) * 11)) + k_inner)] * auto_scheduler_layout_transform[(((((i0_outer_i1_outer_fused_i2_outer_fused / 80) * 550) + ((i0_outer_i1_outer_fused_i2_outer_fused % 5) * 110)) + (k_inner * 10)) + b_inner)]));\n      }\n    }\n    for (int32_t i0_inner = 0; i0_inner < 10; ++i0_inner) {\n      compute_2[((((i0_outer_i1_outer_fused_i2_outer_fused / 80) * 800) + (i0_inner * 80)) + (i0_outer_i1_outer_fused_i2_outer_fused % 80))] = acosf(T_batch_matmul_NN[i0_inner]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(180) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[1];\n  __shared__ float ph_3_shared[180];\n  T_batch_matmul_NN[0] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    ph_3_shared[((int)threadIdx.x)] = ph_3[((((((int)threadIdx.x) / 20) * 40) + (k_outer_outer * 20)) + (((int)threadIdx.x) % 20))];\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 4; ++k_outer_inner) {\n      T_batch_matmul_NN[0] = (T_batch_matmul_NN[0] + (ph_0[((((((((int)threadIdx.x) / 20) * 64) + (((int)blockIdx.x) * 32)) + (((((int)threadIdx.x) % 20) / 5) * 8)) + (k_outer_outer * 4)) + k_outer_inner)] * ph_3_shared[((((((int)threadIdx.x) / 20) * 20) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  compute[((((((int)threadIdx.x) / 20) * 40) + (((int)blockIdx.x) * 20)) + (((int)threadIdx.x) % 20))] = acosf(T_batch_matmul_NN[0]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 16, 11), \"float32\"), ph_3: T.Buffer((20, 11, 5), \"float32\"), compute: T.Buffer((20, 16, 11), \"float32\"), T_add: T.Buffer((20, 16, 11), \"float32\"), compute_1: T.Buffer((20, 16, 11), \"float32\"), compute_2: T.Buffer((20, 16, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1100], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((3520,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3520):\n            compute_3 = T.Buffer((3520,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3520):\n            T_add_1 = T.Buffer((3520,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3520):\n            compute_3 = T.Buffer((3520,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((1100,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(10):\n            for ax9, ax10 in T.grid(11, 10):\n                ph_3_1 = T.Buffer((1100,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 110 + ax9 * 10 + ax10] = ph_3_1[ax0_ax1_fused_ax2_fused // 5 * 550 + ax10 * 55 + ax9 * 5 + ax0_ax1_fused_ax2_fused % 5]\n        for i0_outer_i1_outer_fused_i2_outer_fused in T.parallel(160):\n            T_batch_matmul_NN = T.allocate([10], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((10,), data=T_batch_matmul_NN, align=32)\n            for b_inner_init in range(10):\n                T_batch_matmul_NN_1[b_inner_init] = T.float32(0)\n            for k_inner, b_inner in T.grid(11, 10):\n                cse_var_1: T.int32 = i0_outer_i1_outer_fused_i2_outer_fused // 80\n                T_batch_matmul_NN_1[b_inner] = T_batch_matmul_NN_1[b_inner] + ph_0_1[cse_var_1 * 1760 + b_inner * 176 + i0_outer_i1_outer_fused_i2_outer_fused % 80 // 5 * 11 + k_inner] * auto_scheduler_layout_transform_1[cse_var_1 * 550 + i0_outer_i1_outer_fused_i2_outer_fused % 5 * 110 + k_inner * 10 + b_inner]\n            for i0_inner in range(10):\n                compute_3 = T.Buffer((1600,), data=compute_2.data)\n                compute_3[i0_outer_i1_outer_fused_i2_outer_fused // 80 * 800 + i0_inner * 80 + i0_outer_i1_outer_fused_i2_outer_fused % 80] = T.acos(T_batch_matmul_NN_1[i0_inner])",
        "op_args": [
            [
                "batch_matmul",
                "asin",
                "asinh",
                "add",
                "asinh",
                "acos"
            ]
        ],
        "input_shape": [[20, 16, 11], [20, 7, 2], [20, 11, 5]],
        "output_shape": [[20, 7, 2], [20, 16, 11], [20, 16, 11], [20, 16, 11], [20, 16, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1088; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_add[(((ax0 * 68) + (ax1 * 4)) + ax2)] = (cosf(ph_0[(((ax0 * 68) + (ax1 * 4)) + ax2)]) + ph_0[(((ax0 * 68) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1088; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 17, 4), \"float32\"), compute: T.Buffer((16, 17, 4), \"float32\"), T_add: T.Buffer((16, 17, 4), \"float32\"), compute_1: T.Buffer((16, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1088,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_2 = T.Buffer((1088,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(17, 4):\n                cse_var_1: T.int32 = ax0 * 68 + ax1 * 4 + ax2\n                T_add_1 = T.Buffer((1088,), data=T_add.data)\n                T_add_1[cse_var_1] = T.cos(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_2 = T.Buffer((1088,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "cos",
                "add",
                "exp"
            ]
        ],
        "input_shape": [[16, 17, 4]],
        "output_shape": [[16, 17, 4], [16, 17, 4], [16, 17, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_divide[(((ax0 * 132) + (ax1 * 12)) + ax2)] = (ph_0[(((ax0 * 132) + (ax1 * 12)) + ax2)] / ph_3[(((ax0 * 132) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1056; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] - atanhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 132) + (i1 * 12)) + i2)] = asinhf((ph_0[(((i0 * 132) + (i1 * 12)) + i2)] - atanhf(ph_0[(((i0 * 132) + (i1 * 12)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 11, 12), \"float32\"), ph_3: T.Buffer((8, 11, 12), \"float32\"), T_divide: T.Buffer((8, 11, 12), \"float32\"), compute: T.Buffer((8, 11, 12), \"float32\"), compute_1: T.Buffer((8, 11, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1056,), data=ph_0.data)\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(11, 12):\n                cse_var_1: T.int32 = ax0 * 132 + ax1 * 12 + ax2\n                T_divide_1 = T.Buffer((1056,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1056,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1056):\n            compute_2 = T.Buffer((1056,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(11, 12):\n                cse_var_2: T.int32 = i0 * 132 + i1 * 12 + i2\n                compute_2 = T.Buffer((1056,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2] - T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "atanh",
                "subtract",
                "exp",
                "asinh"
            ]
        ],
        "input_shape": [[8, 11, 12], [8, 18, 3], [8, 11, 12]],
        "output_shape": [[8, 11, 12], [8, 18, 3], [8, 11, 12], [8, 11, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[10];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 170; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n    for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n      auto_scheduler_layout_transform[((ax4 * 5) + ax8)] = ph_4[((ax8 * 2) + ax4)];\n    }\n  }\n  for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n    for (int32_t i_inner_init = 0; i_inner_init < 17; ++i_inner_init) {\n      T_batch_matmul_NN[((b_inner_init * 17) + i_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n    for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n      for (int32_t i_inner = 0; i_inner < 17; ++i_inner) {\n        T_batch_matmul_NN[((b_inner * 17) + i_inner)] = (T_batch_matmul_NN[((b_inner * 17) + i_inner)] + (fabsf(ph_0[(((b_inner * 34) + (i_inner * 2)) + k_outer)]) * auto_scheduler_layout_transform[((k_outer * 5) + b_inner)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[16];\n  __shared__ float compute_shared[80];\n  __shared__ float ph_4_shared[10];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 4; ++i_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n      for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n        T_batch_matmul_NN_local[(((b_c_inner_init * 8) + (i_c_outer_inner_init * 2)) + i_c_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 80; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[ax0_ax1_fused_ax2_fused_outer_outer] = fabsf(ph_0[((((int)blockIdx.x) * 80) + ax0_ax1_fused_ax2_fused_outer_outer)]);\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 10; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_4_shared[ax0_ax1_fused_ax2_fused_outer_outer_1] = ph_4[((((int)blockIdx.x) * 10) + ax0_ax1_fused_ax2_fused_outer_outer_1)];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 4; ++i_c_outer_inner) {\n      for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n        for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n          T_batch_matmul_NN_local[(((b_c_inner * 8) + (i_c_outer_inner * 2)) + i_c_inner)] = (T_batch_matmul_NN_local[(((b_c_inner * 8) + (i_c_outer_inner * 2)) + i_c_inner)] + (compute_shared[((((b_c_inner * 40) + (i_c_outer_inner * 10)) + (i_c_inner * 5)) + k_outer_inner)] * ph_4_shared[((b_c_inner * 5) + k_outer_inner)]));\n        }\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int i_inner = 0; i_inner < 8; ++i_inner) {\n      T_batch_matmul_NN[(((((int)blockIdx.x) * 16) + (b_inner * 8)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 8) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 17, 2), \"float32\"), ph_4: T.Buffer((5, 2, 1), \"float32\"), compute: T.Buffer((5, 17, 2), \"float32\"), T_batch_matmul_NN: T.Buffer((5, 17, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([10], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((170,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(170):\n            compute_1 = T.Buffer((170,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((10,), data=auto_scheduler_layout_transform, align=32)\n        for ax4, ax8 in T.grid(2, 5):\n            ph_4_1 = T.Buffer((10,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[ax4 * 5 + ax8] = ph_4_1[ax8 * 2 + ax4]\n        T_batch_matmul_NN_1 = T.Buffer((85,), data=T_batch_matmul_NN.data)\n        for b_inner_init, i_inner_init in T.grid(5, 17):\n            T_batch_matmul_NN_1[b_inner_init * 17 + i_inner_init] = T.float32(0)\n        for k_outer, b_inner, i_inner in T.grid(2, 5, 17):\n            cse_var_1: T.int32 = b_inner * 17 + i_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + T.fabs(ph_0_1[b_inner * 34 + i_inner * 2 + k_outer]) * auto_scheduler_layout_transform_1[k_outer * 5 + b_inner]",
        "op_args": [
            [
                "exp",
                "abs",
                "batch_matmul"
            ]
        ],
        "input_shape": [[5, 17, 2], [5, 2, 1]],
        "output_shape": [[5, 17, 2], [5, 17, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1960; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1960; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1960; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_divide[(((ax0 * 140) + (ax1 * 14)) + ax2)] = ((ph_0[(((ax0 * 140) + (ax1 * 14)) + ax2)] / ph_3[(((ax0 * 140) + (ax1 * 14)) + ax2)]) / ph_0[(((ax0 * 140) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 10, 14), \"float32\"), ph_3: T.Buffer((14, 10, 14), \"float32\"), T_multiply: T.Buffer((14, 10, 14), \"float32\"), T_add: T.Buffer((14, 10, 14), \"float32\"), compute: T.Buffer((14, 10, 14), \"float32\"), compute_1: T.Buffer((14, 10, 14), \"float32\"), T_divide: T.Buffer((14, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1960,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1960,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1960):\n            T_multiply_1 = T.Buffer((1960,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1960):\n            T_add_1 = T.Buffer((1960,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1960):\n            compute_2 = T.Buffer((1960,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1960):\n            compute_2 = T.Buffer((1960,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(10, 14):\n                cse_var_1: T.int32 = ax0 * 140 + ax1 * 14 + ax2\n                T_divide_1 = T.Buffer((1960,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1] / ph_0_1[cse_var_1]",
        "op_args": [
            [
                "divide",
                "multiply",
                "acosh",
                "add",
                "acosh",
                "acos",
                "divide"
            ]
        ],
        "input_shape": [[14, 10, 14], [11, 6, 9], [14, 10, 14]],
        "output_shape": [[11, 6, 9], [14, 10, 14], [14, 10, 14], [14, 10, 14], [14, 10, 14], [14, 10, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 133) + (i1 * 7)) + i2)] = cosf(ph_0[(((i0 * 133) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 14; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 19; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 133) + (i1_1 * 7)) + i2_1)] = sinf(acosf(ph_0[(((i0_1 * 133) + (i1_1 * 7)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1862; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 19, 7), \"float32\"), compute: T.Buffer((14, 19, 7), \"float32\"), compute_1: T.Buffer((14, 19, 7), \"float32\"), compute_2: T.Buffer((14, 19, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1862,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(19, 7):\n                cse_var_1: T.int32 = i0 * 133 + i1 * 7 + i2\n                compute_3 = T.Buffer((1862,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(19, 7):\n                cse_var_2: T.int32 = i0 * 133 + i1 * 7 + i2\n                compute_3 = T.Buffer((1862,), data=compute_1.data)\n                compute_3[cse_var_2] = T.sin(T.acos(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1862):\n            compute_3 = T.Buffer((1862,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "acos",
                "sin",
                "atan"
            ]
        ],
        "input_shape": [[14, 19, 7]],
        "output_shape": [[14, 19, 7], [14, 19, 7], [14, 19, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = atanf(sinf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n        compute_1[(((i0 * 40) + (i1 * 5)) + i2_1)] = ceilf(sinf(ph_0[(((i0 * 40) + (i1 * 5)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = sinf((ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(__sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 8, 5), \"float32\"), ph_3: T.Buffer((12, 8, 5), \"float32\"), T_mod: T.Buffer((12, 8, 5), \"float32\"), compute: T.Buffer((12, 8, 5), \"float32\"), compute_1: T.Buffer((12, 8, 5), \"float32\"), compute_2: T.Buffer((12, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((480,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(T.sin(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(8, 5):\n                cse_var_2: T.int32 = i0 * 40 + i1 * 5 + i2\n                compute_3 = T.Buffer((480,), data=compute_1.data)\n                compute_3[cse_var_2] = T.ceil(T.sin(ph_0_1[cse_var_2]))\n        ph_3_1 = T.Buffer((480,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_mod_1 = T.Buffer((480,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_3 = T.Buffer((480,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "mod",
                "sin",
                "atan",
                "ceil",
                "sin"
            ]
        ],
        "input_shape": [[12, 8, 5], [14, 10, 1], [12, 8, 5]],
        "output_shape": [[14, 10, 1], [12, 8, 5], [12, 8, 5], [12, 8, 5], [12, 8, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1482; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1482; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1482; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1482; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 6, 19), \"float32\"), ph_3: T.Buffer((13, 6, 19), \"float32\"), T_divide: T.Buffer((13, 6, 19), \"float32\"), compute: T.Buffer((13, 6, 19), \"float32\"), T_multiply: T.Buffer((13, 6, 19), \"float32\"), compute_1: T.Buffer((13, 6, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1482,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1482):\n            T_divide_1 = T.Buffer((1482,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1482,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1482):\n            compute_2 = T.Buffer((1482,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1482):\n            T_multiply_1 = T.Buffer((1482,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1482):\n            compute_2 = T.Buffer((1482,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "atanh",
                "acosh",
                "multiply",
                "ceil"
            ]
        ],
        "input_shape": [[13, 6, 19], [7, 10, 12], [13, 6, 19]],
        "output_shape": [[13, 6, 19], [7, 10, 12], [13, 6, 19], [13, 6, 19], [13, 6, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0 * 2) + i2)] = atanhf(ph_0[((i0 * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_1[((i0_i1_fused * 2) + i2_1)] = fabsf(atanf(ph_0[((i0_i1_fused * 2) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 22; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 1, 2), \"float32\"), compute: T.Buffer((11, 1, 2), \"float32\"), compute_1: T.Buffer((11, 1, 2), \"float32\"), compute_2: T.Buffer((11, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((22,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i2\n                compute_3 = T.Buffer((22,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(11):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((22,), data=compute_1.data)\n                compute_3[cse_var_2] = T.fabs(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(22):\n            compute_3 = T.Buffer((22,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "atan",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[11, 1, 2]],
        "output_shape": [[11, 1, 2], [11, 1, 2], [11, 1, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  float compute_5[3200];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3200; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3200; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_2[((i0_i1_fused * 20) + i2)] = cosf(ph_0[((i0_i1_fused * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3200; ++i0_i1_fused_i2_fused_2) {\n    compute_5[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 3200; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = expf(compute_5[i0_i1_fused_i2_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 3200; ++i0_i1_fused_i2_fused_4) {\n    compute_4[i0_i1_fused_i2_fused_4] = atanf(compute_5[i0_i1_fused_i2_fused_4]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(__expf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 20, 20), \"float32\"), compute: T.Buffer((8, 20, 20), \"float32\"), compute_1: T.Buffer((8, 20, 20), \"float32\"), compute_2: T.Buffer((8, 20, 20), \"float32\"), compute_3: T.Buffer((8, 20, 20), \"float32\"), compute_4: T.Buffer((8, 20, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_5 = T.allocate([3200], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((3200,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3200):\n            compute_6 = T.Buffer((3200,), data=compute.data)\n            compute_6[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3200):\n            compute_6 = T.Buffer((3200,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acosh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2\n                compute_6 = T.Buffer((3200,), data=compute_2.data)\n                compute_6[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        compute_6 = T.Buffer((3200,), data=compute_5)\n        for i0_i1_fused_i2_fused in T.parallel(3200):\n            compute_6[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3200):\n            compute_7 = T.Buffer((3200,), data=compute_3.data)\n            compute_7[i0_i1_fused_i2_fused] = T.exp(compute_6[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3200):\n            compute_7 = T.Buffer((3200,), data=compute_4.data)\n            compute_7[i0_i1_fused_i2_fused] = T.atan(compute_6[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "acos",
                "acosh",
                "cos",
                "exp",
                "exp",
                "atan"
            ]
        ],
        "input_shape": [[8, 20, 20]],
        "output_shape": [[8, 20, 20], [8, 20, 20], [8, 20, 20], [8, 20, 20], [8, 20, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 288; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 288; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 288; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_2[(((i0 * 36) + (i1 * 12)) + i2)] = expf(asinhf(ph_0[(((i0 * 36) + (i1 * 12)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 3, 12), \"float32\"), ph_3: T.Buffer((8, 3, 12), \"float32\"), T_mod: T.Buffer((8, 3, 12), \"float32\"), compute: T.Buffer((8, 3, 12), \"float32\"), compute_1: T.Buffer((8, 3, 12), \"float32\"), compute_2: T.Buffer((8, 3, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((288,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(288):\n            T_mod_1 = T.Buffer((288,), data=T_mod.data)\n            ph_3_1 = T.Buffer((288,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(288):\n            compute_3 = T.Buffer((288,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(288):\n            compute_3 = T.Buffer((288,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(3, 12):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 12 + i2\n                compute_3 = T.Buffer((288,), data=compute_2.data)\n                compute_3[cse_var_1] = T.exp(T.asinh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "mod",
                "acos",
                "asinh",
                "sin",
                "exp"
            ]
        ],
        "input_shape": [[8, 3, 12], [16, 13, 7], [8, 3, 12]],
        "output_shape": [[8, 3, 12], [16, 13, 7], [8, 3, 12], [8, 3, 12], [8, 3, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = expf((ph_0[((i0_i1_fused * 12) + i2)] - fabsf(ph_0[((i0_i1_fused * 12) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1680; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n        compute_2[(((i0 * 240) + (i1 * 12)) + i2_1)] = cosf(ceilf(ph_0[(((i0 * 240) + (i1 * 12)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 20, 12), \"float32\"), compute: T.Buffer((7, 20, 12), \"float32\"), compute_1: T.Buffer((7, 20, 12), \"float32\"), compute_2: T.Buffer((7, 20, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1680,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(140):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((1680,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1] - T.fabs(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_3 = T.Buffer((1680,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(20, 12):\n                cse_var_2: T.int32 = i0 * 240 + i1 * 12 + i2\n                compute_3 = T.Buffer((1680,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "subtract",
                "exp",
                "atan",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[7, 20, 12]],
        "output_shape": [[7, 20, 12], [7, 20, 12], [7, 20, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_10, float* ph_3) {\n  float auto_scheduler_layout_transform[1040];\n  float T_batch_matmul_NN[325];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1040; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1040; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1040; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(atanhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n      for (int32_t ax7 = 0; ax7 < 4; ++ax7) {\n        for (int32_t ax8 = 0; ax8 < 13; ++ax8) {\n          auto_scheduler_layout_transform[((((ax0_ax1_fused_ax2_fused * 208) + (ax4 * 52)) + (ax7 * 13)) + ax8)] = ph_3[((((ax8 * 80) + (ax4 * 20)) + (ax7 * 5)) + ax0_ax1_fused_ax2_fused)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 5; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_inner_init = 0; b_inner_init < 13; ++b_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 25) + (i_inner_init * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 4; ++k_outer) {\n      for (int32_t k_inner = 0; k_inner < 4; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 13; ++b_inner) {\n          for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n            T_batch_matmul_NN[(((b_inner * 25) + (i_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[(((b_inner * 25) + (i_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (ph_0[((((b_inner * 80) + (i_inner * 16)) + (k_outer * 4)) + k_inner)] * auto_scheduler_layout_transform[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 208) + (k_outer * 52)) + (k_inner * 13)) + b_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 325; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = acosf(T_batch_matmul_NN[i0_i1_fused_i2_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 325; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(T_batch_matmul_NN[ax0_ax1_fused_ax2_fused_1], ph_10[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float ph_3_shared[40];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 4; ++i_c_outer_inner_init) {\n      T_batch_matmul_NN_local[((b_c_outer_inner_init * 4) + i_c_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    ph_3_shared[((int)threadIdx.x)] = ph_3[((((((int)threadIdx.x) / 5) * 25) + (k_outer_outer * 5)) + (((int)threadIdx.x) % 5))];\n    __syncthreads();\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      for (int i_c_outer_inner = 0; i_c_outer_inner < 4; ++i_c_outer_inner) {\n        T_batch_matmul_NN_local[((b_c_outer_inner * 4) + i_c_outer_inner)] = (T_batch_matmul_NN_local[((b_c_outer_inner * 4) + i_c_outer_inner)] + (ph_0[((((((((int)threadIdx.x) / 10) * 80) + (b_c_outer_inner * 40)) + (((((int)threadIdx.x) % 10) / 5) * 20)) + (i_c_outer_inner * 5)) + k_outer_outer)] * ph_3_shared[((((((int)threadIdx.x) / 10) * 10) + (b_c_outer_inner * 5)) + (((int)threadIdx.x) % 5))]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int i_inner = 0; i_inner < 4; ++i_inner) {\n      T_batch_matmul_NN[((((((((int)threadIdx.x) / 10) * 80) + (b_inner * 40)) + (((((int)threadIdx.x) % 10) / 5) * 20)) + (i_inner * 5)) + (((int)threadIdx.x) % 5))] = T_batch_matmul_NN_local[((b_inner * 4) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_5(float* __restrict__ T_batch_matmul_NN, float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(T_batch_matmul_NN[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_batch_matmul_NN, float* __restrict__ compute) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(T_batch_matmul_NN[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 5, 16), \"float32\"), ph_3: T.Buffer((13, 16, 5), \"float32\"), ph_10: T.Buffer((13, 5, 5), \"float32\"), compute: T.Buffer((13, 5, 16), \"float32\"), compute_1: T.Buffer((13, 5, 16), \"float32\"), compute_2: T.Buffer((13, 5, 16), \"float32\"), compute_3: T.Buffer((13, 5, 5), \"float32\"), T_mod: T.Buffer((13, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1040], \"float32\", \"global\")\n        T_batch_matmul_NN = T.allocate([325], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1040,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1040):\n            compute_4 = T.Buffer((1040,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1040):\n            compute_4 = T.Buffer((1040,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1040):\n            compute_4 = T.Buffer((1040,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((1040,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax4, ax7, ax8 in T.grid(4, 4, 13):\n                ph_3_1 = T.Buffer((1040,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 208 + ax4 * 52 + ax7 * 13 + ax8] = ph_3_1[ax8 * 80 + ax4 * 20 + ax7 * 5 + ax0_ax1_fused_ax2_fused]\n        T_batch_matmul_NN_1 = T.Buffer((325,), data=T_batch_matmul_NN)\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(5):\n            for b_inner_init, i_inner_init in T.grid(13, 5):\n                T_batch_matmul_NN_1[b_inner_init * 25 + i_inner_init * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for k_outer, k_inner, b_inner, i_inner in T.grid(4, 4, 13, 5):\n                cse_var_1: T.int32 = b_inner * 25 + i_inner * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 80 + i_inner * 16 + k_outer * 4 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 208 + k_outer * 52 + k_inner * 13 + b_inner]\n        for i0_i1_fused_i2_fused in T.parallel(325):\n            compute_4 = T.Buffer((325,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T_batch_matmul_NN_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(325):\n            T_mod_1 = T.Buffer((325,), data=T_mod.data)\n            ph_10_1 = T.Buffer((325,), data=ph_10.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused], ph_10_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "batch_matmul",
                "atan",
                "atanh",
                "asin",
                "cos",
                "acos",
                "mod"
            ]
        ],
        "input_shape": [[13, 5, 16], [7, 1, 10], [13, 16, 5], [13, 5, 5]],
        "output_shape": [[7, 1, 10], [13, 5, 16], [13, 5, 16], [13, 5, 16], [13, 5, 5], [13, 5, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 285) + (i1 * 19)) + i2)] = sinf((ph_0[(((i0 * 285) + (i1 * 19)) + i2)] / ceilf(ph_0[(((i0 * 285) + (i1 * 19)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 15; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_2[(((i0_1 * 285) + (i1_1 * 19)) + i2_1)] = acoshf(ph_0[(((i0_1 * 285) + (i1_1 * 19)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 19), \"float32\"), compute: T.Buffer((16, 15, 19), \"float32\"), compute_1: T.Buffer((16, 15, 19), \"float32\"), compute_2: T.Buffer((16, 15, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4560,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(15, 19):\n                cse_var_1: T.int32 = i0 * 285 + i1 * 19 + i2\n                compute_3 = T.Buffer((4560,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1] / T.ceil(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4560):\n            compute_3 = T.Buffer((4560,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(15, 19):\n                cse_var_2: T.int32 = i0 * 285 + i1 * 19 + i2\n                compute_3 = T.Buffer((4560,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "ceil",
                "divide",
                "sin",
                "acos",
                "acosh"
            ]
        ],
        "input_shape": [[16, 15, 19]],
        "output_shape": [[16, 15, 19], [16, 15, 19], [16, 15, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 525; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 525; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 525; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 15, 5), \"float32\"), compute: T.Buffer((7, 15, 5), \"float32\"), compute_1: T.Buffer((7, 15, 5), \"float32\"), compute_2: T.Buffer((7, 15, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((525,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(525):\n            compute_3 = T.Buffer((525,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(525):\n            compute_3 = T.Buffer((525,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(525):\n            compute_3 = T.Buffer((525,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "cos",
                "acosh",
                "exp"
            ]
        ],
        "input_shape": [[7, 15, 5]],
        "output_shape": [[7, 15, 5], [7, 15, 5], [7, 15, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[135];\n  for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n    for (int32_t ax5 = 0; ax5 < 9; ++ax5) {\n      for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n        auto_scheduler_layout_transform[(((ax3 * 27) + (ax5 * 3)) + ax7)] = ph_3[(((ax5 * 15) + (ax7 * 5)) + ax3)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 5; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 9; ++b_outer_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 7; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 35) + (i_inner_init * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 9; ++b_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 3; ++k_inner) {\n        for (int32_t i_inner = 0; i_inner < 7; ++i_inner) {\n          T_batch_matmul_NN[(((b_outer_inner * 35) + (i_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[(((b_outer_inner * 35) + (i_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (ph_0[(((b_outer_inner * 21) + (i_inner * 3)) + k_inner)] * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 27) + (b_outer_inner * 3)) + k_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_0_shared[40];\n  __shared__ float ph_3_shared[8];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  T_batch_matmul_NN_local[1] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      ph_0_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 10) + ((int)threadIdx.x))] = ph_0[((((((((int)blockIdx.x) / 5) * 80) + (ax0_ax1_fused_ax2_fused_outer_outer * 20)) + ((((int)threadIdx.x) >> 1) * 4)) + (k_outer_outer * 2)) + (((int)threadIdx.x) & 1))];\n    }\n    if (((int)threadIdx.x) < 8) {\n      ph_3_shared[((int)threadIdx.x)] = ph_3[((((((((int)blockIdx.x) / 5) * 80) + ((((int)threadIdx.x) >> 1) * 20)) + (k_outer_outer * 10)) + ((((int)threadIdx.x) & 1) * 5)) + (((int)blockIdx.x) % 5))];\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 2; ++k_inner) {\n      T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (ph_0_shared[((((int)threadIdx.x) * 2) + k_inner)] * ph_3_shared[(((((int)threadIdx.x) / 5) * 2) + k_inner)]));\n      T_batch_matmul_NN_local[1] = (T_batch_matmul_NN_local[1] + (ph_0_shared[(((((int)threadIdx.x) * 2) + k_inner) + 20)] * ph_3_shared[((((((int)threadIdx.x) / 5) * 2) + k_inner) + 4)]));\n    }\n  }\n  T_batch_matmul_NN[((((((int)blockIdx.x) / 5) * 100) + (((int)threadIdx.x) * 5)) + (((int)blockIdx.x) % 5))] = T_batch_matmul_NN_local[0];\n  T_batch_matmul_NN[(((((((int)blockIdx.x) / 5) * 100) + (((int)threadIdx.x) * 5)) + (((int)blockIdx.x) % 5)) + 50)] = T_batch_matmul_NN_local[1];\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 7, 3), \"float32\"), ph_3: T.Buffer((9, 3, 5), \"float32\"), T_batch_matmul_NN: T.Buffer((9, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([135], \"float32\", \"global\")\n        auto_scheduler_layout_transform_1 = T.Buffer((135,), data=auto_scheduler_layout_transform)\n        for ax3, ax5, ax7 in T.grid(5, 9, 3):\n            ph_3_1 = T.Buffer((135,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax3 * 27 + ax5 * 3 + ax7] = ph_3_1[ax5 * 15 + ax7 * 5 + ax3]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(5):\n            T_batch_matmul_NN_1 = T.Buffer((315,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_inner_init in T.grid(9, 7):\n                T_batch_matmul_NN_1[b_outer_inner_init * 35 + i_inner_init * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for b_outer_inner, k_inner, i_inner in T.grid(9, 3, 7):\n                cse_var_1: T.int32 = b_outer_inner * 35 + i_inner * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                ph_0_1 = T.Buffer((189,), data=ph_0.data)\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 21 + i_inner * 3 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 27 + b_outer_inner * 3 + k_inner]",
        "op_args": [
            [
                "batch_matmul"
            ]
        ],
        "input_shape": [[9, 7, 3], [5, 14, 8], [9, 3, 5]],
        "output_shape": [[9, 7, 5], [5, 14, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 51; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = atanhf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 714; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(atanf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 714; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 714; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 17, 14), \"float32\"), compute: T.Buffer((3, 17, 14), \"float32\"), T_mod: T.Buffer((3, 17, 14), \"float32\"), compute_1: T.Buffer((3, 17, 14), \"float32\"), compute_2: T.Buffer((3, 17, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((714,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(51):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((714,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(714):\n            T_mod_1 = T.Buffer((714,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(714):\n            compute_3 = T.Buffer((714,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(714):\n            compute_3 = T.Buffer((714,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "atan",
                "mod",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[3, 17, 14]],
        "output_shape": [[3, 17, 14], [3, 17, 14], [3, 17, 14], [3, 17, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 216; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_1[(((i0 * 18) + (i1 * 3)) + i2)] = cosf(atanf(ph_0[(((i0 * 18) + (i1 * 3)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 216; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n      compute_3[((i0_i1_fused * 3) + i2_1)] = asinf(asinhf(ph_0[((i0_i1_fused * 3) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 6, 3), \"float32\"), compute: T.Buffer((12, 6, 3), \"float32\"), compute_1: T.Buffer((12, 6, 3), \"float32\"), compute_2: T.Buffer((12, 6, 3), \"float32\"), compute_3: T.Buffer((12, 6, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((216,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_4 = T.Buffer((216,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(6, 3):\n                cse_var_1: T.int32 = i0 * 18 + i1 * 3 + i2\n                compute_4 = T.Buffer((216,), data=compute_1.data)\n                compute_4[cse_var_1] = T.cos(T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_4 = T.Buffer((216,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(72):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0_i1_fused * 3 + i2\n                compute_4 = T.Buffer((216,), data=compute_3.data)\n                compute_4[cse_var_2] = T.asin(T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asin",
                "atan",
                "cos",
                "cos",
                "asinh",
                "asin"
            ]
        ],
        "input_shape": [[12, 6, 3]],
        "output_shape": [[12, 6, 3], [12, 6, 3], [12, 6, 3], [12, 6, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_mod[(((ax0 * 144) + (ax1 * 16)) + ax2)] = fmodf(ph_0[(((ax0 * 144) + (ax1 * 16)) + ax2)], ph_3[(((ax0 * 144) + (ax1 * 16)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2592; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 162; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = acoshf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 9, 16), \"float32\"), ph_3: T.Buffer((18, 9, 16), \"float32\"), T_mod: T.Buffer((18, 9, 16), \"float32\"), compute: T.Buffer((18, 9, 16), \"float32\"), compute_1: T.Buffer((18, 9, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2592,), data=ph_0.data)\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(9, 16):\n                cse_var_1: T.int32 = ax0 * 144 + ax1 * 16 + ax2\n                T_mod_1 = T.Buffer((2592,), data=T_mod.data)\n                ph_3_1 = T.Buffer((2592,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2592):\n            compute_2 = T.Buffer((2592,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(162):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((2592,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "mod",
                "acos",
                "acosh"
            ]
        ],
        "input_shape": [[18, 9, 16], [19, 6, 17], [18, 9, 16]],
        "output_shape": [[18, 9, 16], [19, 6, 17], [18, 9, 16], [18, 9, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[3344];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3344; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3344; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = asinf(fmodf(ph_0[i0_i1_fused_i2_fused_1], compute_2[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3344; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] / fabsf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 176; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_1[((i0_i1_fused * 19) + i2)] = sinf((ph_0[((i0_i1_fused * 19) + i2)] / fabsf(ph_0[((i0_i1_fused * 19) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / fabsf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 11, 19), \"float32\"), compute: T.Buffer((16, 11, 19), \"float32\"), T_mod: T.Buffer((16, 11, 19), \"float32\"), compute_1: T.Buffer((16, 11, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([3344], \"float32\", \"global\")\n        compute_3 = T.Buffer((3344,), data=compute_2)\n        ph_0_1 = T.Buffer((3344,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3344):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3344):\n            compute_4 = T.Buffer((3344,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], compute_3[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(3344):\n            T_mod_1 = T.Buffer((3344,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] / T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(176):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_4 = T.Buffer((3344,), data=compute_1.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1] / T.fabs(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "exp",
                "mod",
                "asin",
                "abs",
                "divide",
                "mod",
                "sin"
            ]
        ],
        "input_shape": [[16, 11, 19]],
        "output_shape": [[16, 11, 19], [16, 11, 19], [16, 11, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 300) + (i1 * 15)) + i2)] = cosf(ph_0[(((i0 * 300) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n      compute_1[((i0_i1_fused * 15) + i2_1)] = acosf(ph_0[((i0_i1_fused * 15) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 20, 15), \"float32\"), compute: T.Buffer((13, 20, 15), \"float32\"), compute_1: T.Buffer((13, 20, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3900,), data=ph_0.data)\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(20, 15):\n                cse_var_1: T.int32 = i0 * 300 + i1 * 15 + i2\n                compute_2 = T.Buffer((3900,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(260):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((3900,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "acos"
            ]
        ],
        "input_shape": [[13, 20, 15]],
        "output_shape": [[13, 20, 15], [13, 20, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = asinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 245; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 245; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 5, 7), \"float32\"), compute: T.Buffer((7, 5, 7), \"float32\"), T_add: T.Buffer((7, 5, 7), \"float32\"), compute_1: T.Buffer((7, 5, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((245,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(35):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((245,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(245):\n            T_add_1 = T.Buffer((245,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(245):\n            compute_2 = T.Buffer((245,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "asinh",
                "multiply",
                "add",
                "ceil"
            ]
        ],
        "input_shape": [[7, 5, 7]],
        "output_shape": [[7, 5, 7], [7, 5, 7], [7, 5, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 140; ++ax0_ax1_fused_ax2_fused) {\n    float compute[1];\n    compute[0] = expf(ph_0[ax0_ax1_fused_ax2_fused]);\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / compute[0]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 4), \"float32\"), T_divide: T.Buffer((5, 7, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(140):\n            compute = T.allocate([1], \"float32\", \"global\")\n            compute_1 = T.Buffer((1,), data=compute, align=4)\n            ph_0_1 = T.Buffer((140,), data=ph_0.data)\n            compute_1[0] = T.exp(ph_0_1[ax0_ax1_fused_ax2_fused])\n            T_divide_1 = T.Buffer((140,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / compute_1[0]",
        "op_args": [
            [
                "exp",
                "divide"
            ]
        ],
        "input_shape": [[5, 7, 4]],
        "output_shape": [[5, 7, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6460; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 340; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 19) + ax2)] = (acoshf(ph_0[((ax0_ax1_fused * 19) + ax2)]) - ph_0[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 6460; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 6460; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ceilf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 19), \"float32\"), compute: T.Buffer((17, 20, 19), \"float32\"), T_subtract: T.Buffer((17, 20, 19), \"float32\"), compute_1: T.Buffer((17, 20, 19), \"float32\"), compute_2: T.Buffer((17, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((6460,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(6460):\n            compute_3 = T.Buffer((6460,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(340):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_subtract_1 = T.Buffer((6460,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(6460):\n            compute_3 = T.Buffer((6460,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(6460):\n            compute_3 = T.Buffer((6460,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atan",
                "acosh",
                "subtract",
                "asin",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[17, 20, 19]],
        "output_shape": [[17, 20, 19], [17, 20, 19], [17, 20, 19], [17, 20, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 180; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 9; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 20) + ax2)] = ((ph_0[((ax0_ax1_fused * 20) + ax2)] - fabsf(ph_0[((ax0_ax1_fused * 20) + ax2)])) * ph_0[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 180; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf((ph_0[i0_i1_fused_i2_fused_2] - fabsf(ph_0[i0_i1_fused_i2_fused_2])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 1, 20), \"float32\"), compute: T.Buffer((9, 1, 20), \"float32\"), compute_1: T.Buffer((9, 1, 20), \"float32\"), T_multiply: T.Buffer((9, 1, 20), \"float32\"), compute_2: T.Buffer((9, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(9):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_multiply_1 = T.Buffer((180,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = (ph_0_1[cse_var_1] - T.fabs(ph_0_1[cse_var_1])) * ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] - T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "atanh",
                "atanh",
                "abs",
                "subtract",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[9, 1, 20]],
        "output_shape": [[9, 1, 20], [9, 1, 20], [9, 1, 20], [9, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3520; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 220) + (i1 * 20)) + i2)] = sinf(ph_0[(((i0 * 220) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n        compute_1[(((i0_1 * 220) + (i1_1 * 20)) + i2_1)] = fabsf(ph_0[(((i0_1 * 220) + (i1_1 * 20)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 11, 20), \"float32\"), ph_3: T.Buffer((16, 11, 20), \"float32\"), T_multiply: T.Buffer((16, 11, 20), \"float32\"), compute: T.Buffer((16, 11, 20), \"float32\"), compute_1: T.Buffer((16, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3520,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3520):\n            T_multiply_1 = T.Buffer((3520,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((3520,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(11, 20):\n                cse_var_1: T.int32 = i0 * 220 + i1 * 20 + i2\n                compute_2 = T.Buffer((3520,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(11, 20):\n                cse_var_2: T.int32 = i0 * 220 + i1 * 20 + i2\n                compute_2 = T.Buffer((3520,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "sin",
                "abs"
            ]
        ],
        "input_shape": [[16, 11, 20], [8, 5, 17], [16, 11, 20]],
        "output_shape": [[16, 11, 20], [8, 5, 17], [16, 11, 20], [16, 11, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_subtract[(((ax0 * 56) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 56) + (ax1 * 4)) + ax2)] - ph_3[(((ax0 * 56) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1120; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1120; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ceilf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 56) + (i1 * 4)) + i2)] = ceilf(ceilf(ph_0[(((i0 * 56) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 14, 4), \"float32\"), ph_3: T.Buffer((20, 14, 4), \"float32\"), T_subtract: T.Buffer((20, 14, 4), \"float32\"), compute: T.Buffer((20, 14, 4), \"float32\"), T_mod: T.Buffer((20, 14, 4), \"float32\"), compute_1: T.Buffer((20, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1120,), data=ph_0.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(14, 4):\n                cse_var_1: T.int32 = ax0 * 56 + ax1 * 4 + ax2\n                T_subtract_1 = T.Buffer((1120,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((1120,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1120):\n            compute_2 = T.Buffer((1120,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1120):\n            T_mod_1 = T.Buffer((1120,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(14, 4):\n                cse_var_2: T.int32 = i0 * 56 + i1 * 4 + i2\n                compute_2 = T.Buffer((1120,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "subtract",
                "acos",
                "ceil",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[20, 14, 4], [12, 10, 16], [20, 14, 4]],
        "output_shape": [[20, 14, 4], [12, 10, 16], [20, 14, 4], [20, 14, 4], [20, 14, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2250; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2250; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 150; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_1[((i0_i1_fused * 15) + i2)] = acosf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 10, 15), \"float32\"), compute: T.Buffer((15, 10, 15), \"float32\"), T_subtract: T.Buffer((15, 10, 15), \"float32\"), compute_1: T.Buffer((15, 10, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2250,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2250):\n            compute_2 = T.Buffer((2250,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2250):\n            T_subtract_1 = T.Buffer((2250,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(150):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((2250,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acosh",
                "atan",
                "subtract",
                "acos"
            ]
        ],
        "input_shape": [[15, 10, 15]],
        "output_shape": [[15, 10, 15], [15, 10, 15], [15, 10, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_add[((ax0 * 4) + ax2)] = (ph_0[((ax0 * 4) + ax2)] + ph_3[((ax0 * 4) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 12; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 12; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused_1] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 3; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = asinf(ceilf(ph_0[((i0_i1_fused * 4) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 1, 4), \"float32\"), ph_3: T.Buffer((3, 1, 4), \"float32\"), T_add: T.Buffer((3, 1, 4), \"float32\"), T_multiply: T.Buffer((3, 1, 4), \"float32\"), T_multiply_1: T.Buffer((3, 1, 4), \"float32\"), compute: T.Buffer((3, 1, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((12,), data=ph_0.data)\n        ph_3_1 = T.Buffer((12,), data=ph_3.data)\n        for ax0 in T.parallel(3):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0 * 4 + ax2\n                T_add_1 = T.Buffer((12,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(12):\n            T_multiply_2 = T.Buffer((12,), data=T_multiply.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(12):\n            T_multiply_2 = T.Buffer((12,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(3):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_1 = T.Buffer((12,), data=compute.data)\n                compute_1[cse_var_2] = T.asin(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "add",
                "multiply",
                "ceil",
                "multiply",
                "asin"
            ]
        ],
        "input_shape": [[3, 1, 4], [11, 12, 5], [3, 1, 4]],
        "output_shape": [[3, 1, 4], [11, 12, 5], [3, 1, 4], [3, 1, 4], [3, 1, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_add[((ax0_ax1_fused * 11) + ax2)] = (ph_0[((ax0_ax1_fused * 11) + ax2)] + ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 198; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 11; ++ax2_1) {\n        T_multiply[(((ax0 * 22) + (ax1 * 11)) + ax2_1)] = (ceilf(ph_0[(((ax0 * 22) + (ax1 * 11)) + ax2_1)]) * ph_0[(((ax0 * 22) + (ax1 * 11)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 2, 11), \"float32\"), ph_3: T.Buffer((9, 2, 11), \"float32\"), T_add: T.Buffer((9, 2, 11), \"float32\"), compute: T.Buffer((9, 2, 11), \"float32\"), T_multiply: T.Buffer((9, 2, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((198,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_add_1 = T.Buffer((198,), data=T_add.data)\n                ph_3_1 = T.Buffer((198,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(198):\n            compute_1 = T.Buffer((198,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(2, 11):\n                cse_var_2: T.int32 = ax0 * 22 + ax1 * 11 + ax2\n                T_multiply_1 = T.Buffer((198,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.ceil(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]",
        "op_args": [
            [
                "add",
                "abs",
                "ceil",
                "multiply"
            ]
        ],
        "input_shape": [[9, 2, 11], [15, 15, 17], [9, 2, 11]],
        "output_shape": [[9, 2, 11], [15, 15, 17], [9, 2, 11], [9, 2, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_multiply[(((ax0 * 200) + (ax1 * 10)) + ax2)] = (ph_0[(((ax0 * 200) + (ax1 * 10)) + ax2)] * ph_3[(((ax0 * 200) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2000; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = acosf(ceilf(ph_0[((i0_i1_fused * 10) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2000; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(ceilf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 20, 10), \"float32\"), ph_3: T.Buffer((10, 20, 10), \"float32\"), T_multiply: T.Buffer((10, 20, 10), \"float32\"), T_multiply_1: T.Buffer((10, 20, 10), \"float32\"), compute: T.Buffer((10, 20, 10), \"float32\"), compute_1: T.Buffer((10, 20, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2000,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2000,), data=ph_3.data)\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(20, 10):\n                cse_var_1: T.int32 = ax0 * 200 + ax1 * 10 + ax2\n                T_multiply_2 = T.Buffer((2000,), data=T_multiply.data)\n                T_multiply_2[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2000):\n            T_multiply_2 = T.Buffer((2000,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(200):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_2 = T.Buffer((2000,), data=compute.data)\n                compute_2[cse_var_2] = T.acos(T.ceil(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(2000):\n            compute_2 = T.Buffer((2000,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "multiply",
                "ceil",
                "acos",
                "exp"
            ]
        ],
        "input_shape": [[10, 20, 10], [14, 6, 10], [10, 20, 10]],
        "output_shape": [[10, 20, 10], [14, 6, 10], [10, 20, 10], [10, 20, 10], [10, 20, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4864; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 304; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = acoshf(atanf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4864; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 304; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_mod[((ax0_ax1_fused * 16) + ax2)] = fmodf(cosf(ph_0[((ax0_ax1_fused * 16) + ax2)]), ph_0[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(__cosf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 16, 16), \"float32\"), compute: T.Buffer((19, 16, 16), \"float32\"), compute_1: T.Buffer((19, 16, 16), \"float32\"), compute_2: T.Buffer((19, 16, 16), \"float32\"), T_mod: T.Buffer((19, 16, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4864,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4864):\n            compute_3 = T.Buffer((4864,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(304):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((4864,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acosh(T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4864):\n            compute_3 = T.Buffer((4864,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(16):\n                cse_var_2: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_mod_1 = T.Buffer((4864,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.cos(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "sin",
                "atan",
                "acosh",
                "sin",
                "cos",
                "mod"
            ]
        ],
        "input_shape": [[19, 16, 16]],
        "output_shape": [[19, 16, 16], [19, 16, 16], [19, 16, 16], [19, 16, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1200; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] + ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_1[((i0_i1_fused * 12) + i2)] = fabsf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1200; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 100; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 12) + i2_1)] = asinhf(fabsf(ph_0[((i0_i1_fused_1 * 12) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 5, 12), \"float32\"), compute: T.Buffer((20, 5, 12), \"float32\"), compute_1: T.Buffer((20, 5, 12), \"float32\"), T_divide: T.Buffer((20, 5, 12), \"float32\"), compute_2: T.Buffer((20, 5, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1200,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1200):\n            compute_3 = T.Buffer((1200,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] + T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(100):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((1200,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1200):\n            T_divide_1 = T.Buffer((1200,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(100):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((1200,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(T.fabs(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "ceil",
                "add",
                "atanh",
                "abs",
                "abs",
                "divide",
                "asinh"
            ]
        ],
        "input_shape": [[20, 5, 12]],
        "output_shape": [[20, 5, 12], [20, 5, 12], [20, 5, 12], [20, 5, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 224; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 224; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t i1 = 0; i1 < 14; ++i1) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_2[((i1 * 16) + i2)] = atanf(ph_0[((i1 * 16) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 14, 16), \"float32\"), compute: T.Buffer((1, 14, 16), \"float32\"), compute_1: T.Buffer((1, 14, 16), \"float32\"), compute_2: T.Buffer((1, 14, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((224,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(224):\n            compute_3 = T.Buffer((224,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(224):\n            compute_3 = T.Buffer((224,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i1, i2 in T.grid(14, 16):\n            cse_var_1: T.int32 = i1 * 16 + i2\n            compute_3 = T.Buffer((224,), data=compute_2.data)\n            compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "acosh",
                "acosh",
                "atan"
            ]
        ],
        "input_shape": [[1, 14, 16]],
        "output_shape": [[1, 14, 16], [1, 14, 16], [1, 14, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 56; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_mod[((ax0_ax1_fused * 13) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 13) + ax2)], fabsf(ph_0[((ax0_ax1_fused * 13) + ax2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 7, 13), \"float32\"), T_mod: T.Buffer((8, 7, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(56):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_mod_1 = T.Buffer((728,), data=T_mod.data)\n                ph_0_1 = T.Buffer((728,), data=ph_0.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.fabs(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "abs",
                "mod"
            ]
        ],
        "input_shape": [[8, 7, 13]],
        "output_shape": [[8, 7, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3744; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3744; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute_2[((i0_i1_fused * 13) + i2)] = asinf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3744; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 16, 13), \"float32\"), compute: T.Buffer((18, 16, 13), \"float32\"), compute_1: T.Buffer((18, 16, 13), \"float32\"), compute_2: T.Buffer((18, 16, 13), \"float32\"), compute_3: T.Buffer((18, 16, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3744,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3744):\n            compute_4 = T.Buffer((3744,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3744):\n            compute_4 = T.Buffer((3744,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_4 = T.Buffer((3744,), data=compute_2.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3744):\n            compute_4 = T.Buffer((3744,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "asinh",
                "acos",
                "asin",
                "ceil"
            ]
        ],
        "input_shape": [[18, 16, 13]],
        "output_shape": [[18, 16, 13], [18, 16, 13], [18, 16, 13], [18, 16, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2520; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2520; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_2[(((i0 * 140) + (i1 * 7)) + i2)] = atanhf(cosf(ph_0[(((i0 * 140) + (i1 * 7)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 18; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 20; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_3[(((i0_1 * 140) + (i1_1 * 7)) + i2_1)] = sinf((ph_0[(((i0_1 * 140) + (i1_1 * 7)) + i2_1)] - ph_3[(((i0_1 * 140) + (i1_1 * 7)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2520; ++i0_i1_fused_i2_fused_2) {\n    compute_4[i0_i1_fused_i2_fused_2] = atanf((ph_0[i0_i1_fused_i2_fused_2] - ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 20, 7), \"float32\"), ph_3: T.Buffer((18, 20, 7), \"float32\"), compute: T.Buffer((18, 20, 7), \"float32\"), compute_1: T.Buffer((18, 20, 7), \"float32\"), compute_2: T.Buffer((18, 20, 7), \"float32\"), compute_3: T.Buffer((18, 20, 7), \"float32\"), compute_4: T.Buffer((18, 20, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2520,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2520):\n            compute_5 = T.Buffer((2520,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2520):\n            compute_5 = T.Buffer((2520,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.cos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(20, 7):\n                cse_var_1: T.int32 = i0 * 140 + i1 * 7 + i2\n                compute_5 = T.Buffer((2520,), data=compute_2.data)\n                compute_5[cse_var_1] = T.atanh(T.cos(ph_0_1[cse_var_1]))\n        ph_3_1 = T.Buffer((2520,), data=ph_3.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(20, 7):\n                cse_var_2: T.int32 = i0 * 140 + i1 * 7 + i2\n                compute_5 = T.Buffer((2520,), data=compute_3.data)\n                compute_5[cse_var_2] = T.sin(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2520):\n            compute_5 = T.Buffer((2520,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "acosh",
                "cos",
                "cos",
                "atanh",
                "sin",
                "atan"
            ]
        ],
        "input_shape": [[18, 20, 7], [14, 20, 18], [18, 20, 7]],
        "output_shape": [[14, 20, 18], [18, 20, 7], [18, 20, 7], [18, 20, 7], [18, 20, 7], [18, 20, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 546; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] + sinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 182; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_1[((i0_i1_fused * 3) + i2)] = asinhf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n        compute_2[(((i0 * 39) + (i1 * 3)) + i2_1)] = cosf(ph_0[(((i0 * 39) + (i1 * 3)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 13, 3), \"float32\"), compute: T.Buffer((14, 13, 3), \"float32\"), compute_1: T.Buffer((14, 13, 3), \"float32\"), compute_2: T.Buffer((14, 13, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((546,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(546):\n            compute_3 = T.Buffer((546,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] + T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(182):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((546,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(13, 3):\n                cse_var_2: T.int32 = i0 * 39 + i1 * 3 + i2\n                compute_3 = T.Buffer((546,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "sin",
                "add",
                "cos",
                "asinh",
                "cos"
            ]
        ],
        "input_shape": [[14, 13, 3]],
        "output_shape": [[14, 13, 3], [14, 13, 3], [14, 13, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3000; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3000; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_1[((i0_i1_fused * 15) + i2)] = sinf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 20, 15), \"float32\"), compute: T.Buffer((10, 20, 15), \"float32\"), T_subtract: T.Buffer((10, 20, 15), \"float32\"), compute_1: T.Buffer((10, 20, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3000,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3000):\n            compute_2 = T.Buffer((3000,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3000):\n            T_subtract_1 = T.Buffer((3000,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(200):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((3000,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "exp",
                "acosh",
                "subtract",
                "sin"
            ]
        ],
        "input_shape": [[10, 20, 15]],
        "output_shape": [[10, 20, 15], [10, 20, 15], [10, 20, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2380; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf((ph_0[i0_i1_fused_i2_fused] - ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2380; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 170; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_2[((i0_i1_fused * 14) + i2)] = asinhf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 10, 14), \"float32\"), compute: T.Buffer((17, 10, 14), \"float32\"), compute_1: T.Buffer((17, 10, 14), \"float32\"), compute_2: T.Buffer((17, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2380,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2380):\n            compute_3 = T.Buffer((2380,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] - T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2380):\n            compute_3 = T.Buffer((2380,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(170):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((2380,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "ceil",
                "subtract",
                "sin",
                "exp",
                "asinh"
            ]
        ],
        "input_shape": [[17, 10, 14]],
        "output_shape": [[17, 10, 14], [17, 10, 14], [17, 10, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = atanf((ph_0[((i0_i1_fused * 9) + i2)] / asinf(ph_0[((i0_i1_fused * 9) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 80; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 9) + i2_1)] = expf(ph_0[((i0_i1_fused_1 * 9) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 4, 9), \"float32\"), compute: T.Buffer((20, 4, 9), \"float32\"), compute_1: T.Buffer((20, 4, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(80):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((720,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1] / T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(80):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((720,), data=compute_1.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asin",
                "divide",
                "atan",
                "exp"
            ]
        ],
        "input_shape": [[20, 4, 9]],
        "output_shape": [[20, 4, 9], [20, 4, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 40) + (i1 * 5)) + i2)] = sinf(ph_0[(((i0 * 40) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 8, 5), \"float32\"), compute: T.Buffer((18, 8, 5), \"float32\"), compute_1: T.Buffer((18, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(8, 5):\n                cse_var_1: T.int32 = i0 * 40 + i1 * 5 + i2\n                compute_2 = T.Buffer((720,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_2 = T.Buffer((720,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "acosh",
                "atan"
            ]
        ],
        "input_shape": [[18, 8, 5]],
        "output_shape": [[18, 8, 5], [18, 8, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1080; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_divide[(((ax0 * 90) + (ax1 * 9)) + ax2)] = (cosf(ph_0[(((ax0 * 90) + (ax1 * 9)) + ax2)]) / ph_0[(((ax0 * 90) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute_1[(((i0 * 90) + (i1 * 9)) + i2)] = fabsf(cosf(ph_0[(((i0 * 90) + (i1 * 9)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1080; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf((ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 10, 9), \"float32\"), ph_3: T.Buffer((12, 10, 9), \"float32\"), compute: T.Buffer((12, 10, 9), \"float32\"), T_divide: T.Buffer((12, 10, 9), \"float32\"), compute_1: T.Buffer((12, 10, 9), \"float32\"), compute_2: T.Buffer((12, 10, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1080,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_3 = T.Buffer((1080,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(10, 9):\n                cse_var_1: T.int32 = ax0 * 90 + ax1 * 9 + ax2\n                T_divide_1 = T.Buffer((1080,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.cos(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(10, 9):\n                cse_var_2: T.int32 = i0 * 90 + i1 * 9 + i2\n                compute_3 = T.Buffer((1080,), data=compute_1.data)\n                compute_3[cse_var_2] = T.fabs(T.cos(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_3 = T.Buffer((1080,), data=compute_2.data)\n            ph_3_1 = T.Buffer((1080,), data=ph_3.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "ceil",
                "cos",
                "divide",
                "abs",
                "acosh"
            ]
        ],
        "input_shape": [[12, 10, 9], [11, 17, 2], [12, 10, 9]],
        "output_shape": [[11, 17, 2], [12, 10, 9], [12, 10, 9], [12, 10, 9], [12, 10, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 238) + (i1 * 14)) + i2)] = cosf((ph_0[(((i0 * 238) + (i1 * 14)) + i2)] + acoshf(ph_0[(((i0 * 238) + (i1 * 14)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 340; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      compute_1[((i0_i1_fused * 14) + i2_1)] = cosf(ph_0[((i0_i1_fused * 14) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4760; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(atanf(ph_0[((int)blockIdx.x)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 17, 14), \"float32\"), compute: T.Buffer((20, 17, 14), \"float32\"), compute_1: T.Buffer((20, 17, 14), \"float32\"), compute_2: T.Buffer((20, 17, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4760,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(17, 14):\n                cse_var_1: T.int32 = i0 * 238 + i1 * 14 + i2\n                compute_3 = T.Buffer((4760,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1] + T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(340):\n            for i2 in range(14):\n                cse_var_2: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((4760,), data=compute_1.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_3 = T.Buffer((4760,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "add",
                "cos",
                "cos",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[20, 17, 14]],
        "output_shape": [[20, 17, 14], [20, 17, 14], [20, 17, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_mod[((ax0_ax1_fused * 9) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 9) + ax2)], ph_3[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0_i1_fused * 9) + i2)] = expf(acoshf(ph_0[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_2[(((i0 * 72) + (i1 * 9)) + i2_1)] = asinf(acoshf(ph_0[(((i0 * 72) + (i1 * 9)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 8, 9), \"float32\"), ph_3: T.Buffer((8, 8, 9), \"float32\"), T_mod: T.Buffer((8, 8, 9), \"float32\"), compute: T.Buffer((8, 8, 9), \"float32\"), compute_1: T.Buffer((8, 8, 9), \"float32\"), compute_2: T.Buffer((8, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((576,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_mod_1 = T.Buffer((576,), data=T_mod.data)\n                ph_3_1 = T.Buffer((576,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_3 = T.Buffer((576,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(64):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((576,), data=compute_1.data)\n                compute_3[cse_var_2] = T.exp(T.acosh(ph_0_1[cse_var_2]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(8, 9):\n                cse_var_3: T.int32 = i0 * 72 + i1 * 9 + i2\n                compute_3 = T.Buffer((576,), data=compute_2.data)\n                compute_3[cse_var_3] = T.asin(T.acosh(ph_0_1[cse_var_3]))",
        "op_args": [
            [
                "mod",
                "asinh",
                "acosh",
                "exp",
                "asin"
            ]
        ],
        "input_shape": [[8, 8, 9], [8, 13, 5], [8, 8, 9]],
        "output_shape": [[8, 8, 9], [8, 13, 5], [8, 8, 9], [8, 8, 9], [8, 8, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_6) {\n  float auto_scheduler_layout_transform[1105];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1989; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1989; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1989; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 17; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax5 = 0; ax5 < 5; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 13; ++ax6) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused * 65) + (ax5 * 13)) + ax6)] = ph_6[(((ax0_ax1_fused_ax2_fused * 65) + (ax6 * 5)) + ax5)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_i1_outer_fused_i2_outer_fused = 0; i0_outer_i1_outer_fused_i2_outer_fused < 17; ++i0_outer_i1_outer_fused_i2_outer_fused) {\n    float T_batch_matmul_NN[45];\n    for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 9; ++i_outer_outer_inner) {\n      for (int32_t j_outer_outer_inner = 0; j_outer_outer_inner < 5; ++j_outer_outer_inner) {\n        T_batch_matmul_NN[((i_outer_outer_inner * 5) + j_outer_outer_inner)] = 0.000000e+00f;\n        for (int32_t k_outer = 0; k_outer < 13; ++k_outer) {\n          T_batch_matmul_NN[((i_outer_outer_inner * 5) + j_outer_outer_inner)] = (T_batch_matmul_NN[((i_outer_outer_inner * 5) + j_outer_outer_inner)] + (ph_0[(((i0_outer_i1_outer_fused_i2_outer_fused * 117) + (i_outer_outer_inner * 13)) + k_outer)] * auto_scheduler_layout_transform[(((i0_outer_i1_outer_fused_i2_outer_fused * 65) + (j_outer_outer_inner * 13)) + k_outer)]));\n        }\n      }\n    }\n    for (int32_t i1_inner = 0; i1_inner < 9; ++i1_inner) {\n      for (int32_t i2_inner_s = 0; i2_inner_s < 5; ++i2_inner_s) {\n        compute_3[(((i0_outer_i1_outer_fused_i2_outer_fused * 45) + (i1_inner * 5)) + i2_inner_s)] = atanhf(T_batch_matmul_NN[((i1_inner * 5) + i2_inner_s)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN[9];\n  __shared__ float ph_6_shared[1];\n  for (int i_outer_inner_init = 0; i_outer_inner_init < 3; ++i_outer_inner_init) {\n    T_batch_matmul_NN[i_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN[(i_outer_inner_init + 3)] = 0.000000e+00f;\n    T_batch_matmul_NN[(i_outer_inner_init + 6)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 8; ++k_outer_outer) {\n    __syncthreads();\n    ph_6_shared[0] = ph_6[((k_outer_outer * 5) + ((int)blockIdx.x))];\n    __syncthreads();\n    for (int i_outer_inner = 0; i_outer_inner < 3; ++i_outer_inner) {\n      T_batch_matmul_NN[i_outer_inner] = (T_batch_matmul_NN[i_outer_inner] + (ph_0[((i_outer_inner * 8) + k_outer_outer)] * ph_6_shared[0]));\n      T_batch_matmul_NN[(i_outer_inner + 3)] = (T_batch_matmul_NN[(i_outer_inner + 3)] + (ph_0[(((i_outer_inner * 8) + k_outer_outer) + 24)] * ph_6_shared[0]));\n      T_batch_matmul_NN[(i_outer_inner + 6)] = (T_batch_matmul_NN[(i_outer_inner + 6)] + (ph_0[(((i_outer_inner * 8) + k_outer_outer) + 48)] * ph_6_shared[0]));\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 3; ++i1_inner) {\n    compute[((i1_inner * 5) + ((int)blockIdx.x))] = atanhf(T_batch_matmul_NN[i1_inner]);\n    compute[(((i1_inner * 5) + ((int)blockIdx.x)) + 15)] = atanhf(T_batch_matmul_NN[(i1_inner + 3)]);\n    compute[(((i1_inner * 5) + ((int)blockIdx.x)) + 30)] = atanhf(T_batch_matmul_NN[(i1_inner + 6)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 9, 13), \"float32\"), ph_6: T.Buffer((17, 13, 5), \"float32\"), compute: T.Buffer((17, 9, 13), \"float32\"), compute_1: T.Buffer((17, 9, 13), \"float32\"), compute_2: T.Buffer((17, 9, 13), \"float32\"), compute_3: T.Buffer((17, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1105], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1989,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1989):\n            compute_4 = T.Buffer((1989,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1989):\n            compute_4 = T.Buffer((1989,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1989):\n            compute_4 = T.Buffer((1989,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((1105,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(17):\n            for ax5, ax6 in T.grid(5, 13):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 65\n                ph_6_1 = T.Buffer((1105,), data=ph_6.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax5 * 13 + ax6] = ph_6_1[cse_var_1 + ax6 * 5 + ax5]\n        for i0_outer_i1_outer_fused_i2_outer_fused in T.parallel(17):\n            T_batch_matmul_NN = T.allocate([45], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((45,), data=T_batch_matmul_NN)\n            for i_outer_outer_inner, j_outer_outer_inner in T.grid(9, 5):\n                T_batch_matmul_NN_1[i_outer_outer_inner * 5 + j_outer_outer_inner] = T.float32(0)\n                for k_outer in range(13):\n                    cse_var_2: T.int32 = i_outer_outer_inner * 5 + j_outer_outer_inner\n                    T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[i0_outer_i1_outer_fused_i2_outer_fused * 117 + i_outer_outer_inner * 13 + k_outer] * auto_scheduler_layout_transform_1[i0_outer_i1_outer_fused_i2_outer_fused * 65 + j_outer_outer_inner * 13 + k_outer]\n            for i1_inner, i2_inner_s in T.grid(9, 5):\n                cse_var_3: T.int32 = i1_inner * 5\n                compute_4 = T.Buffer((765,), data=compute_3.data)\n                compute_4[i0_outer_i1_outer_fused_i2_outer_fused * 45 + cse_var_3 + i2_inner_s] = T.atanh(T_batch_matmul_NN_1[cse_var_3 + i2_inner_s])",
        "op_args": [
            [
                "cos",
                "atan",
                "cos",
                "acos",
                "batch_matmul",
                "atanh"
            ]
        ],
        "input_shape": [[17, 9, 13], [17, 13, 5]],
        "output_shape": [[17, 9, 13], [17, 9, 13], [17, 9, 13], [17, 9, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 390; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 390; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 390; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 2, 15), \"float32\"), ph_3: T.Buffer((13, 2, 15), \"float32\"), T_multiply: T.Buffer((13, 2, 15), \"float32\"), compute: T.Buffer((13, 2, 15), \"float32\"), compute_1: T.Buffer((13, 2, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((390,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(390):\n            T_multiply_1 = T.Buffer((390,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((390,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(390):\n            compute_2 = T.Buffer((390,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(390):\n            compute_2 = T.Buffer((390,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "asinh",
                "asin"
            ]
        ],
        "input_shape": [[13, 2, 15], [8, 7, 14], [13, 2, 15]],
        "output_shape": [[13, 2, 15], [8, 7, 14], [13, 2, 15], [13, 2, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 6; ++i1) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i1 * 16) + i2)] = cosf(ph_0[((i1 * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_1[((i0_i1_fused * 16) + i2_1)] = atanhf(ph_0[((i0_i1_fused * 16) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 6, 16), \"float32\"), compute: T.Buffer((1, 6, 16), \"float32\"), compute_1: T.Buffer((1, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((96,), data=ph_0.data)\n        for i1, i2 in T.grid(6, 16):\n            cse_var_1: T.int32 = i1 * 16 + i2\n            compute_2 = T.Buffer((96,), data=compute.data)\n            compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((96,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "atanh"
            ]
        ],
        "input_shape": [[1, 6, 16]],
        "output_shape": [[1, 6, 16], [1, 6, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = expf((ph_0[((i0_i1_fused * 10) + i2)] / (ph_0[((i0_i1_fused * 10) + i2)] + ph_3[((i0_i1_fused * 10) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_mod[(((ax0 * 130) + (ax1 * 10)) + ax2)] = fmodf((ph_0[(((ax0 * 130) + (ax1 * 10)) + ax2)] / (ph_0[(((ax0 * 130) + (ax1 * 10)) + ax2)] + ph_3[(((ax0 * 130) + (ax1 * 10)) + ax2)])), ph_0[(((ax0 * 130) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1300; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf((ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 13, 10), \"float32\"), ph_3: T.Buffer((10, 13, 10), \"float32\"), compute: T.Buffer((10, 13, 10), \"float32\"), T_mod: T.Buffer((10, 13, 10), \"float32\"), compute_1: T.Buffer((10, 13, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1300,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1300,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_2 = T.Buffer((1300,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1] / (ph_0_1[cse_var_1] + ph_3_1[cse_var_1]))\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(13, 10):\n                cse_var_2: T.int32 = ax0 * 130 + ax1 * 10 + ax2\n                T_mod_1 = T.Buffer((1300,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] / (ph_0_1[cse_var_2] + ph_3_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1300):\n            compute_2 = T.Buffer((1300,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "add",
                "divide",
                "exp",
                "mod",
                "acosh"
            ]
        ],
        "input_shape": [[10, 13, 10], [1, 17, 6], [10, 13, 10]],
        "output_shape": [[1, 17, 6], [10, 13, 10], [10, 13, 10], [10, 13, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 102; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0 * 6) + i2)] = atanf(cosf(ph_0[((i0 * 6) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 17; ++i0_1) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_2[((i0_1 * 6) + i2_1)] = atanhf(ph_0[((i0_1 * 6) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 102; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 1, 6), \"float32\"), compute: T.Buffer((17, 1, 6), \"float32\"), compute_1: T.Buffer((17, 1, 6), \"float32\"), compute_2: T.Buffer((17, 1, 6), \"float32\"), compute_3: T.Buffer((17, 1, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((102,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_4 = T.Buffer((102,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(17):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0 * 6 + i2\n                compute_4 = T.Buffer((102,), data=compute_1.data)\n                compute_4[cse_var_1] = T.atan(T.cos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(17):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0 * 6 + i2\n                compute_4 = T.Buffer((102,), data=compute_2.data)\n                compute_4[cse_var_2] = T.atanh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_4 = T.Buffer((102,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "cos",
                "atan",
                "atanh",
                "asinh"
            ]
        ],
        "input_shape": [[17, 1, 6]],
        "output_shape": [[17, 1, 6], [17, 1, 6], [17, 1, 6], [17, 1, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 120) + (i1 * 15)) + i2)] = sinf(ph_0[(((i0 * 120) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n      compute_2[((i0_i1_fused * 15) + i2_1)] = asinhf(ph_0[((i0_i1_fused * 15) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ceilf(asinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 8, 15), \"float32\"), compute: T.Buffer((8, 8, 15), \"float32\"), compute_1: T.Buffer((8, 8, 15), \"float32\"), compute_2: T.Buffer((8, 8, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(8, 15):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 15 + i2\n                compute_3 = T.Buffer((960,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_3 = T.Buffer((960,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(64):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_3 = T.Buffer((960,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "sin",
                "asin",
                "ceil",
                "asinh"
            ]
        ],
        "input_shape": [[8, 8, 15]],
        "output_shape": [[8, 8, 15], [8, 8, 15], [8, 8, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (asinhf(ph_0[((ax0_ax1_fused * 8) + ax2)]) / ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n        T_subtract[(((ax0 * 16) + (ax1 * 8)) + ax2_1)] = (ph_0[(((ax0 * 16) + (ax1 * 8)) + ax2_1)] - ph_3[(((ax0 * 16) + (ax1 * 8)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 144; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(fmodf(ph_0[i0_i1_fused_i2_fused_1], ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 2, 8), \"float32\"), ph_3: T.Buffer((9, 2, 8), \"float32\"), T_subtract: T.Buffer((9, 2, 8), \"float32\"), compute: T.Buffer((9, 2, 8), \"float32\"), T_divide: T.Buffer((9, 2, 8), \"float32\"), compute_1: T.Buffer((9, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((144,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_2 = T.Buffer((144,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((144,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        ph_3_1 = T.Buffer((144,), data=ph_3.data)\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(2, 8):\n                cse_var_2: T.int32 = ax0 * 16 + ax1 * 8 + ax2\n                T_subtract_1 = T.Buffer((144,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] - ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_2 = T.Buffer((144,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "subtract",
                "asinh",
                "asinh",
                "divide",
                "cos"
            ]
        ],
        "input_shape": [[9, 2, 8], [17, 7, 10], [9, 2, 8]],
        "output_shape": [[17, 7, 10], [9, 2, 8], [9, 2, 8], [9, 2, 8], [9, 2, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 56; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 56; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 14; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute_2[((i0_i1_fused * 4) + i2)] = cosf(acoshf(ph_0[((i0_i1_fused * 4) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 56; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanf((ph_0[i0_i1_fused_i2_fused_2] - ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf(acoshf(ph_0[((int)blockIdx.x)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 2, 4), \"float32\"), ph_3: T.Buffer((7, 2, 4), \"float32\"), compute: T.Buffer((7, 2, 4), \"float32\"), compute_1: T.Buffer((7, 2, 4), \"float32\"), compute_2: T.Buffer((7, 2, 4), \"float32\"), compute_3: T.Buffer((7, 2, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((56,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(56):\n            compute_4 = T.Buffer((56,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(56):\n            compute_4 = T.Buffer((56,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(14):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_4 = T.Buffer((56,), data=compute_2.data)\n                compute_4[cse_var_1] = T.cos(T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(56):\n            compute_4 = T.Buffer((56,), data=compute_3.data)\n            ph_3_1 = T.Buffer((56,), data=ph_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "abs",
                "acosh",
                "atan",
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[7, 2, 4], [7, 7, 1], [7, 2, 4]],
        "output_shape": [[7, 7, 1], [7, 2, 4], [7, 2, 4], [7, 2, 4], [7, 2, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* compute, float* ph_0, float* ph_3, float* ph_5) {\n  float auto_scheduler_layout_transform[1805];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 722; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 19; ++ax4) {\n    for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n      for (int32_t ax8 = 0; ax8 < 19; ++ax8) {\n        auto_scheduler_layout_transform[(((ax4 * 95) + (ax6 * 19)) + ax8)] = ph_3[(((ax8 * 95) + (ax4 * 5)) + ax6)];\n      }\n    }\n  }\n  for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 19; ++b_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 10) + (i_outer_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 19; ++k_outer) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 19; ++b_inner) {\n          T_batch_matmul_NN[(((b_inner * 10) + (i_outer_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_inner * 10) + (i_outer_inner * 5)) + j_outer_inner)] + (ph_0[(((b_inner * 38) + (i_outer_inner * 19)) + k_outer)] * auto_scheduler_layout_transform[(((k_outer * 95) + (j_outer_inner * 19)) + b_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 722; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_5[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_5[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(80) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float ph_3_shared[40];\n  for (int b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 40) {\n      ph_3_shared[((int)threadIdx.x)] = ph_3[((((((int)threadIdx.x) / 5) * 25) + (k_outer_outer * 5)) + (((int)threadIdx.x) % 5))];\n    }\n    __syncthreads();\n    for (int b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (ph_0[(((((((int)threadIdx.x) / 40) * 160) + (b_c_inner * 40)) + (((((int)threadIdx.x) % 40) / 5) * 5)) + k_outer_outer)] * ph_3_shared[((((((int)threadIdx.x) / 40) * 20) + (b_c_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    T_batch_matmul_NN[((((((int)threadIdx.x) / 40) * 160) + (b_inner * 40)) + (((int)threadIdx.x) % 40))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 2, 19), \"float32\"), ph_3: T.Buffer((19, 19, 5), \"float32\"), ph_5: T.Buffer((19, 2, 19), \"float32\"), compute: T.Buffer((19, 2, 19), \"float32\"), T_batch_matmul_NN: T.Buffer((19, 2, 5), \"float32\"), T_multiply: T.Buffer((19, 2, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1805], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((722,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(722):\n            compute_1 = T.Buffer((722,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((1805,), data=auto_scheduler_layout_transform)\n        for ax4, ax6, ax8 in T.grid(19, 5, 19):\n            ph_3_1 = T.Buffer((1805,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax4 * 95 + ax6 * 19 + ax8] = ph_3_1[ax8 * 95 + ax4 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((190,), data=T_batch_matmul_NN.data)\n        for i_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(2, 5, 19):\n            T_batch_matmul_NN_1[b_inner_init * 10 + i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n        for k_outer, i_outer_inner, j_outer_inner, b_inner in T.grid(19, 2, 5, 19):\n            cse_var_1: T.int32 = b_inner * 10 + i_outer_inner * 5 + j_outer_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 38 + i_outer_inner * 19 + k_outer] * auto_scheduler_layout_transform_1[k_outer * 95 + j_outer_inner * 19 + b_inner]\n        for ax0_ax1_fused_ax2_fused in T.parallel(722):\n            T_multiply_1 = T.Buffer((722,), data=T_multiply.data)\n            ph_5_1 = T.Buffer((722,), data=ph_5.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_5_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "exp",
                "batch_matmul",
                "multiply"
            ]
        ],
        "input_shape": [[19, 2, 19], [19, 19, 5], [19, 2, 19]],
        "output_shape": [[19, 2, 19], [19, 2, 5], [19, 2, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 585; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 585; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_1[(((i0 * 45) + (i1 * 3)) + i2)] = acoshf(ph_0[(((i0 * 45) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] - ph_3[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 15, 3), \"float32\"), ph_3: T.Buffer((13, 15, 3), \"float32\"), T_subtract: T.Buffer((13, 15, 3), \"float32\"), compute: T.Buffer((13, 15, 3), \"float32\"), compute_1: T.Buffer((13, 15, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((585,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(585):\n            T_subtract_1 = T.Buffer((585,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((585,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(585):\n            compute_2 = T.Buffer((585,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(15, 3):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 3 + i2\n                compute_2 = T.Buffer((585,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "subtract",
                "acos",
                "acosh"
            ]
        ],
        "input_shape": [[13, 15, 3], [18, 17, 3], [13, 15, 3]],
        "output_shape": [[13, 15, 3], [18, 17, 3], [13, 15, 3], [13, 15, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1190; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_subtract[(((ax0 * 70) + (ax1 * 7)) + ax2)] = (ph_0[(((ax0 * 70) + (ax1 * 7)) + ax2)] - atanf(acosf(ph_0[(((ax0 * 70) + (ax1 * 7)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_1[(((i0 * 70) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 70) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - atanf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 10, 7), \"float32\"), compute: T.Buffer((17, 10, 7), \"float32\"), T_subtract: T.Buffer((17, 10, 7), \"float32\"), compute_1: T.Buffer((17, 10, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1190,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1190):\n            compute_2 = T.Buffer((1190,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(17):\n            for ax1, ax2 in T.grid(10, 7):\n                cse_var_1: T.int32 = ax0 * 70 + ax1 * 7 + ax2\n                T_subtract_1 = T.Buffer((1190,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.atan(T.acos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(10, 7):\n                cse_var_2: T.int32 = i0 * 70 + i1 * 7 + i2\n                compute_2 = T.Buffer((1190,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acosh",
                "acos",
                "atan",
                "subtract",
                "atan"
            ]
        ],
        "input_shape": [[17, 10, 7]],
        "output_shape": [[17, 10, 7], [17, 10, 7], [17, 10, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 728; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 728; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / asinhf(acoshf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 728; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / asinhf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 4, 14), \"float32\"), compute: T.Buffer((13, 4, 14), \"float32\"), T_divide: T.Buffer((13, 4, 14), \"float32\"), compute_1: T.Buffer((13, 4, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((728,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(728):\n            compute_2 = T.Buffer((728,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(728):\n            T_divide_1 = T.Buffer((728,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.asinh(T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(728):\n            compute_2 = T.Buffer((728,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "acosh",
                "asinh",
                "divide",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[13, 4, 14]],
        "output_shape": [[13, 4, 14], [13, 4, 14], [13, 4, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 342; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 342; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 342; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_add[((ax0_ax1_fused * 19) + ax2)] = ((ph_0[((ax0_ax1_fused * 19) + ax2)] - ph_3[((ax0_ax1_fused * 19) + ax2)]) + ph_0[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_2[((i0_i1_fused * 19) + i2)] = sinf((ph_0[((i0_i1_fused * 19) + i2)] - ph_3[((i0_i1_fused * 19) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 18, 19), \"float32\"), ph_3: T.Buffer((1, 18, 19), \"float32\"), T_subtract: T.Buffer((1, 18, 19), \"float32\"), compute: T.Buffer((1, 18, 19), \"float32\"), compute_1: T.Buffer((1, 18, 19), \"float32\"), T_add: T.Buffer((1, 18, 19), \"float32\"), compute_2: T.Buffer((1, 18, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((342,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(342):\n            compute_3 = T.Buffer((342,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(342):\n            compute_3 = T.Buffer((342,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((342,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(342):\n            T_subtract_1 = T.Buffer((342,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_add_1 = T.Buffer((342,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1] + ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((342,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])",
        "op_args": [
            [
                "subtract",
                "subtract",
                "atanh",
                "ceil",
                "acos",
                "add",
                "sin"
            ]
        ],
        "input_shape": [[1, 18, 19], [5, 7, 9], [1, 18, 19]],
        "output_shape": [[5, 7, 9], [1, 18, 19], [1, 18, 19], [1, 18, 19], [1, 18, 19], [1, 18, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 221; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = expf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 221; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 7) + i2_1)] = atanhf(ph_0[((i0_i1_fused_1 * 7) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 17, 7), \"float32\"), compute: T.Buffer((13, 17, 7), \"float32\"), compute_1: T.Buffer((13, 17, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1547,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(221):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1547,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(221):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1547,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "exp",
                "atanh"
            ]
        ],
        "input_shape": [[13, 17, 7]],
        "output_shape": [[13, 17, 7], [13, 17, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 152) + (i1 * 19)) + i2)] = acosf((ph_0[(((i0 * 152) + (i1 * 19)) + i2)] + atanhf(ph_0[(((i0 * 152) + (i1 * 19)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] + atanhf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 8, 19), \"float32\"), compute: T.Buffer((8, 8, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(8, 19):\n                cse_var_1: T.int32 = i0 * 152 + i1 * 19 + i2\n                compute_1 = T.Buffer((1216,), data=compute.data)\n                ph_0_1 = T.Buffer((1216,), data=ph_0.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1] + T.atanh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "atanh",
                "add",
                "acos"
            ]
        ],
        "input_shape": [[8, 8, 19]],
        "output_shape": [[8, 8, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_mod[(((ax0 * 266) + (ax1 * 14)) + ax2)] = fmodf(ph_0[(((ax0 * 266) + (ax1 * 14)) + ax2)], ph_3[(((ax0 * 266) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 10; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 19; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 14; ++ax2_1) {\n        T_mod_1[(((ax0_1 * 266) + (ax1_1 * 14)) + ax2_1)] = fmodf((ph_0[(((ax0_1 * 266) + (ax1_1 * 14)) + ax2_1)] * (ph_0[(((ax0_1 * 266) + (ax1_1 * 14)) + ax2_1)] / ph_3[(((ax0_1 * 266) + (ax1_1 * 14)) + ax2_1)])), ph_0[(((ax0_1 * 266) + (ax1_1 * 14)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2660; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 19, 14), \"float32\"), ph_3: T.Buffer((10, 19, 14), \"float32\"), T_mod: T.Buffer((10, 19, 14), \"float32\"), T_mod_1: T.Buffer((10, 19, 14), \"float32\"), compute: T.Buffer((10, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2660,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2660,), data=ph_3.data)\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(19, 14):\n                cse_var_1: T.int32 = ax0 * 266 + ax1 * 14 + ax2\n                T_mod_2 = T.Buffer((2660,), data=T_mod.data)\n                T_mod_2[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(19, 14):\n                cse_var_2: T.int32 = ax0 * 266 + ax1 * 14 + ax2\n                T_mod_2 = T.Buffer((2660,), data=T_mod_1.data)\n                T_mod_2[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] * (ph_0_1[cse_var_2] / ph_3_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2660):\n            compute_1 = T.Buffer((2660,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "divide",
                "multiply",
                "mod",
                "asin"
            ]
        ],
        "input_shape": [[10, 19, 14], [4, 17, 1], [10, 19, 14]],
        "output_shape": [[10, 19, 14], [4, 17, 1], [10, 19, 14], [10, 19, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = sinf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_1[((i0 * 19) + i2_1)] = atanf(asinhf(ph_0[((i0 * 19) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 76; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 1, 19), \"float32\"), compute: T.Buffer((4, 1, 19), \"float32\"), compute_1: T.Buffer((4, 1, 19), \"float32\"), compute_2: T.Buffer((4, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((76,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((76,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(4):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0 * 19 + i2\n                compute_3 = T.Buffer((76,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(T.asinh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(76):\n            compute_3 = T.Buffer((76,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "asinh",
                "atan",
                "asinh"
            ]
        ],
        "input_shape": [[4, 1, 19]],
        "output_shape": [[4, 1, 19], [4, 1, 19], [4, 1, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 780; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf((ph_0[i0_i1_fused_i2_fused] * acosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 780; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 52; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_divide[((ax0_ax1_fused * 15) + ax2)] = (acosf(ph_0[((ax0_ax1_fused * 15) + ax2)]) / ph_0[((ax0_ax1_fused * 15) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 780; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 4, 15), \"float32\"), compute: T.Buffer((13, 4, 15), \"float32\"), compute_1: T.Buffer((13, 4, 15), \"float32\"), T_divide: T.Buffer((13, 4, 15), \"float32\"), T_subtract: T.Buffer((13, 4, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((780,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(780):\n            compute_2 = T.Buffer((780,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] * T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(780):\n            compute_2 = T.Buffer((780,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(52):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_divide_1 = T.Buffer((780,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.acos(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(780):\n            T_subtract_1 = T.Buffer((780,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acos",
                "multiply",
                "atan",
                "asinh",
                "acos",
                "divide",
                "subtract"
            ]
        ],
        "input_shape": [[13, 4, 15]],
        "output_shape": [[13, 4, 15], [13, 4, 15], [13, 4, 15], [13, 4, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3060; ++i0_i1_fused_i2_fused) {\n    float compute_3[1];\n    compute_3[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute[i0_i1_fused_i2_fused] = sinf(fmodf(ph_0[i0_i1_fused_i2_fused], compute_3[0]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3060; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_2[(((i0 * 255) + (i1 * 15)) + i2)] = asinhf(acoshf(ph_0[(((i0 * 255) + (i1 * 15)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 17, 15), \"float32\"), compute: T.Buffer((12, 17, 15), \"float32\"), compute_1: T.Buffer((12, 17, 15), \"float32\"), compute_2: T.Buffer((12, 17, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3060,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3060):\n            compute_3 = T.allocate([1], \"float32\", \"global\")\n            compute_4 = T.Buffer((1,), data=compute_3, align=4)\n            compute_4[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_5 = T.Buffer((3060,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], compute_4[0]))\n        for i0_i1_fused_i2_fused in T.parallel(3060):\n            compute_3 = T.Buffer((3060,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(17, 15):\n                cse_var_1: T.int32 = i0 * 255 + i1 * 15 + i2\n                compute_3 = T.Buffer((3060,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(T.acosh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "exp",
                "mod",
                "sin",
                "atan",
                "acosh",
                "asinh"
            ]
        ],
        "input_shape": [[12, 17, 15]],
        "output_shape": [[12, 17, 15], [12, 17, 15], [12, 17, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 20) + (i1 * 10)) + i2)] = ceilf(ph_0[(((i0 * 20) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 140; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 2, 10), \"float32\"), compute: T.Buffer((7, 2, 10), \"float32\"), compute_1: T.Buffer((7, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((140,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(2, 10):\n                cse_var_1: T.int32 = i0 * 20 + i1 * 10 + i2\n                compute_2 = T.Buffer((140,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(140):\n            compute_2 = T.Buffer((140,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[7, 2, 10]],
        "output_shape": [[7, 2, 10], [7, 2, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 76; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 18) + ax2)] = (ph_0[((ax0_ax1_fused * 18) + ax2)] * ph_3[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 72) + (i1 * 18)) + i2)] = fabsf(ph_0[(((i0 * 72) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1368; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 4, 18), \"float32\"), ph_3: T.Buffer((19, 4, 18), \"float32\"), T_multiply: T.Buffer((19, 4, 18), \"float32\"), compute: T.Buffer((19, 4, 18), \"float32\"), compute_1: T.Buffer((19, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1368,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(76):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_multiply_1 = T.Buffer((1368,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((1368,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(4, 18):\n                cse_var_2: T.int32 = i0 * 72 + i1 * 18 + i2\n                compute_2 = T.Buffer((1368,), data=compute.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1368):\n            compute_2 = T.Buffer((1368,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "abs",
                "asinh"
            ]
        ],
        "input_shape": [[19, 4, 18], [10, 3, 14], [19, 4, 18]],
        "output_shape": [[19, 4, 18], [10, 3, 14], [19, 4, 18], [19, 4, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2574; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        T_add[(((ax0 * 234) + (ax1 * 13)) + ax2)] = (ph_0[(((ax0 * 234) + (ax1 * 13)) + ax2)] + ph_3[(((ax0 * 234) + (ax1 * 13)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 11; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 18; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 13; ++ax2_1) {\n        T_multiply[(((ax0_1 * 234) + (ax1_1 * 13)) + ax2_1)] = (atanhf(ph_0[(((ax0_1 * 234) + (ax1_1 * 13)) + ax2_1)]) * ph_0[(((ax0_1 * 234) + (ax1_1 * 13)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 18, 13), \"float32\"), ph_3: T.Buffer((11, 18, 13), \"float32\"), T_mod: T.Buffer((11, 18, 13), \"float32\"), T_add: T.Buffer((11, 18, 13), \"float32\"), T_multiply: T.Buffer((11, 18, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2574,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2574,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2574):\n            T_mod_1 = T.Buffer((2574,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(18, 13):\n                cse_var_1: T.int32 = ax0 * 234 + ax1 * 13 + ax2\n                T_add_1 = T.Buffer((2574,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(18, 13):\n                cse_var_2: T.int32 = ax0 * 234 + ax1 * 13 + ax2\n                T_multiply_1 = T.Buffer((2574,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.atanh(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]",
        "op_args": [
            [
                "mod",
                "add",
                "atanh",
                "multiply"
            ]
        ],
        "input_shape": [[11, 18, 13], [15, 11, 7], [11, 18, 13]],
        "output_shape": [[11, 18, 13], [15, 11, 7], [11, 18, 13], [11, 18, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1521; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute_1[(((i0 * 117) + (i1 * 9)) + i2)] = atanhf(fabsf(ph_0[(((i0 * 117) + (i1 * 9)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_subtract[(((ax0 * 117) + (ax1 * 9)) + ax2)] = ((ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)] * acosf(ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)])) - ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 13, 9), \"float32\"), compute: T.Buffer((13, 13, 9), \"float32\"), compute_1: T.Buffer((13, 13, 9), \"float32\"), T_subtract: T.Buffer((13, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1521,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1521):\n            compute_2 = T.Buffer((1521,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(13, 9):\n                cse_var_1: T.int32 = i0 * 117 + i1 * 9 + i2\n                compute_2 = T.Buffer((1521,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(T.fabs(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(13, 9):\n                cse_var_2: T.int32 = ax0 * 117 + ax1 * 9 + ax2\n                T_subtract_1 = T.Buffer((1521,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] * T.acos(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "acosh",
                "abs",
                "atanh",
                "acos",
                "multiply",
                "subtract"
            ]
        ],
        "input_shape": [[13, 13, 9]],
        "output_shape": [[13, 13, 9], [13, 13, 9], [13, 13, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 40; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] - ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 320; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + ceilf(ph_0[ax0_ax1_fused_ax2_fused])) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 320; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] + ceilf(ph_0[ax0_ax1_fused_ax2_fused_1])) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 5, 8), \"float32\"), ph_3: T.Buffer((8, 5, 8), \"float32\"), T_subtract: T.Buffer((8, 5, 8), \"float32\"), T_divide: T.Buffer((8, 5, 8), \"float32\"), T_add: T.Buffer((8, 5, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((320,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(40):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_subtract_1 = T.Buffer((320,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((320,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(320):\n            T_divide_1 = T.Buffer((320,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused])) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(320):\n            T_add_1 = T.Buffer((320,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "subtract",
                "ceil",
                "add",
                "divide",
                "add"
            ]
        ],
        "input_shape": [[8, 5, 8], [7, 7, 20], [8, 5, 8]],
        "output_shape": [[8, 5, 8], [7, 7, 20], [8, 5, 8], [8, 5, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 84; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 84; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(fabsf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_add[(((ax0 * 12) + (ax1 * 6)) + ax2)] = ((ph_0[(((ax0 * 12) + (ax1 * 6)) + ax2)] * asinhf(ph_0[(((ax0 * 12) + (ax1 * 6)) + ax2)])) + ph_0[(((ax0 * 12) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 7; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 6; ++ax2_1) {\n        T_subtract[(((ax0_1 * 12) + (ax1_1 * 6)) + ax2_1)] = ((ph_0[(((ax0_1 * 12) + (ax1_1 * 6)) + ax2_1)] * asinhf(ph_0[(((ax0_1 * 12) + (ax1_1 * 6)) + ax2_1)])) - ph_0[(((ax0_1 * 12) + (ax1_1 * 6)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 2, 6), \"float32\"), compute: T.Buffer((7, 2, 6), \"float32\"), T_mod: T.Buffer((7, 2, 6), \"float32\"), T_add: T.Buffer((7, 2, 6), \"float32\"), T_subtract: T.Buffer((7, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((84,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(84):\n            compute_1 = T.Buffer((84,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(84):\n            T_mod_1 = T.Buffer((84,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(2, 6):\n                cse_var_1: T.int32 = ax0 * 12 + ax1 * 6 + ax2\n                T_add_1 = T.Buffer((84,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] * T.asinh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(2, 6):\n                cse_var_2: T.int32 = ax0 * 12 + ax1 * 6 + ax2\n                T_subtract_1 = T.Buffer((84,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] * T.asinh(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "sin",
                "abs",
                "mod",
                "asinh",
                "multiply",
                "add",
                "subtract"
            ]
        ],
        "input_shape": [[7, 2, 6]],
        "output_shape": [[7, 2, 6], [7, 2, 6], [7, 2, 6], [7, 2, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 490; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 490; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_multiply[(((ax0 * 98) + (ax1 * 14)) + ax2)] = (asinf(ph_0[(((ax0 * 98) + (ax1 * 14)) + ax2)]) * ph_0[(((ax0 * 98) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 14), \"float32\"), ph_3: T.Buffer((5, 7, 14), \"float32\"), T_mod: T.Buffer((5, 7, 14), \"float32\"), compute: T.Buffer((5, 7, 14), \"float32\"), T_multiply: T.Buffer((5, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((490,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(490):\n            T_mod_1 = T.Buffer((490,), data=T_mod.data)\n            ph_3_1 = T.Buffer((490,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(490):\n            compute_1 = T.Buffer((490,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(7, 14):\n                cse_var_1: T.int32 = ax0 * 98 + ax1 * 14 + ax2\n                T_multiply_1 = T.Buffer((490,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]",
        "op_args": [
            [
                "mod",
                "acosh",
                "asin",
                "multiply"
            ]
        ],
        "input_shape": [[5, 7, 14], [4, 10, 17], [5, 7, 14]],
        "output_shape": [[5, 7, 14], [4, 10, 17], [5, 7, 14], [5, 7, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 204; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_mod[((ax0_ax1_fused * 5) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 5) + ax2)], ph_3[((ax0_ax1_fused * 5) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 204; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = acoshf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1020; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 12, 5), \"float32\"), ph_3: T.Buffer((17, 12, 5), \"float32\"), T_mod: T.Buffer((17, 12, 5), \"float32\"), compute: T.Buffer((17, 12, 5), \"float32\"), compute_1: T.Buffer((17, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1020,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(204):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_mod_1 = T.Buffer((1020,), data=T_mod.data)\n                ph_3_1 = T.Buffer((1020,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(204):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((1020,), data=compute.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1020):\n            compute_2 = T.Buffer((1020,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "acosh",
                "cos",
                "exp"
            ]
        ],
        "input_shape": [[17, 12, 5], [8, 18, 13], [17, 12, 5]],
        "output_shape": [[17, 12, 5], [8, 18, 13], [17, 12, 5], [17, 12, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[540];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 270; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_divide[((ax0_ax1_fused * 2) + ax2)] = (ph_0[((ax0_ax1_fused * 2) + ax2)] / sinf((ph_0[((ax0_ax1_fused * 2) + ax2)] * asinhf(ph_0[((ax0_ax1_fused * 2) + ax2)]))));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 540; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 540; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = sinf(compute_2[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 540; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = ceilf(compute_2[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(__expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / __sinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 18, 2), \"float32\"), T_divide: T.Buffer((15, 18, 2), \"float32\"), compute: T.Buffer((15, 18, 2), \"float32\"), compute_1: T.Buffer((15, 18, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([540], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(270):\n            for ax2 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_divide_1 = T.Buffer((540,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.sin(ph_0_1[cse_var_1] * T.asinh(ph_0_1[cse_var_1]))\n        compute_3 = T.Buffer((540,), data=compute_2)\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_4 = T.Buffer((540,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(compute_3[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_4 = T.Buffer((540,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(compute_3[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "multiply",
                "sin",
                "divide",
                "exp",
                "sin",
                "ceil"
            ]
        ],
        "input_shape": [[15, 18, 2]],
        "output_shape": [[15, 18, 2], [15, 18, 2], [15, 18, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[9];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 9; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 9; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 9; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(compute_3[ax0_ax1_fused_ax2_fused], ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 9; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = fabsf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 9; ++i0_i1_fused_i2_fused_3) {\n    compute_2[i0_i1_fused_i2_fused_3] = asinhf((ph_0[i0_i1_fused_i2_fused_3] * ph_3[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 1, 3), \"float32\"), ph_3: T.Buffer((3, 1, 3), \"float32\"), compute: T.Buffer((3, 1, 3), \"float32\"), T_mod: T.Buffer((3, 1, 3), \"float32\"), compute_1: T.Buffer((3, 1, 3), \"float32\"), compute_2: T.Buffer((3, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([9], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((9,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(9):\n            compute_4 = T.Buffer((9,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((9,), data=compute_3, align=32)\n        for i0_i1_fused_i2_fused in T.parallel(9):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(9):\n            T_mod_1 = T.Buffer((9,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(compute_4[ax0_ax1_fused_ax2_fused], ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(9):\n            compute_5 = T.Buffer((9,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.fabs(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(9):\n            compute_5 = T.Buffer((9,), data=compute_2.data)\n            ph_3_1 = T.Buffer((9,), data=ph_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "abs",
                "exp",
                "mod",
                "abs",
                "asinh"
            ]
        ],
        "input_shape": [[3, 1, 3], [2, 4, 11], [3, 1, 3]],
        "output_shape": [[2, 4, 11], [3, 1, 3], [3, 1, 3], [3, 1, 3], [3, 1, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 12; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = asinhf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_mod[((ax0_ax1_fused * 17) + ax2)] = fmodf(sinf(ph_0[((ax0_ax1_fused * 17) + ax2)]), ph_0[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 3, 17), \"float32\"), compute: T.Buffer((4, 3, 17), \"float32\"), T_mod: T.Buffer((4, 3, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((204,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(12):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_1 = T.Buffer((204,), data=compute.data)\n                compute_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(17):\n                cse_var_2: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_mod_1 = T.Buffer((204,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.sin(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "sin",
                "mod"
            ]
        ],
        "input_shape": [[4, 3, 17]],
        "output_shape": [[4, 3, 17], [4, 3, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_add[(((ax0 * 96) + (ax1 * 12)) + ax2)] = (ph_0[(((ax0 * 96) + (ax1 * 12)) + ax2)] + ph_3[(((ax0 * 96) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1248; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1248; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 8, 12), \"float32\"), ph_3: T.Buffer((13, 8, 12), \"float32\"), T_add: T.Buffer((13, 8, 12), \"float32\"), compute: T.Buffer((13, 8, 12), \"float32\"), compute_1: T.Buffer((13, 8, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1248,), data=ph_0.data)\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(8, 12):\n                cse_var_1: T.int32 = ax0 * 96 + ax1 * 12 + ax2\n                T_add_1 = T.Buffer((1248,), data=T_add.data)\n                ph_3_1 = T.Buffer((1248,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1248):\n            compute_2 = T.Buffer((1248,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1248):\n            compute_2 = T.Buffer((1248,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "exp",
                "acosh",
                "sin"
            ]
        ],
        "input_shape": [[13, 8, 12], [5, 13, 20], [13, 8, 12]],
        "output_shape": [[13, 8, 12], [5, 13, 20], [13, 8, 12], [13, 8, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 196; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = cosf((ph_0[((i0_i1_fused * 9) + i2)] - asinf(ph_0[((i0_i1_fused * 9) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 14, 9), \"float32\"), compute: T.Buffer((14, 14, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(196):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((1764,), data=compute.data)\n                ph_0_1 = T.Buffer((1764,), data=ph_0.data)\n                compute_1[cse_var_1] = T.cos(ph_0_1[cse_var_1] - T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asin",
                "subtract",
                "cos"
            ]
        ],
        "input_shape": [[14, 14, 9]],
        "output_shape": [[14, 14, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 60; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_1[((i0 * 3) + i2)] = cosf(cosf(ph_0[((i0 * 3) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 60; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 1, 3), \"float32\"), ph_3: T.Buffer((20, 1, 3), \"float32\"), T_multiply: T.Buffer((20, 1, 3), \"float32\"), compute: T.Buffer((20, 1, 3), \"float32\"), compute_1: T.Buffer((20, 1, 3), \"float32\"), compute_2: T.Buffer((20, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((60,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(60):\n            T_multiply_1 = T.Buffer((60,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((60,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_3 = T.Buffer((60,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(20):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0 * 3 + i2\n                compute_3 = T.Buffer((60,), data=compute_1.data)\n                compute_3[cse_var_1] = T.cos(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_3 = T.Buffer((60,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "abs",
                "cos",
                "cos",
                "atanh"
            ]
        ],
        "input_shape": [[20, 1, 3], [18, 9, 17], [20, 1, 3]],
        "output_shape": [[20, 1, 3], [18, 9, 17], [20, 1, 3], [20, 1, 3], [20, 1, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_add[((ax0_ax1_fused * 20) + ax2)] = (atanf(ph_0[((ax0_ax1_fused * 20) + ax2)]) + ph_0[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_1[((i0 * 20) + i2)] = acosf(ph_0[((i0 * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 1, 20), \"float32\"), compute: T.Buffer((12, 1, 20), \"float32\"), T_add: T.Buffer((12, 1, 20), \"float32\"), compute_1: T.Buffer((12, 1, 20), \"float32\"), T_divide: T.Buffer((12, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_2 = T.Buffer((240,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_add_1 = T.Buffer((240,), data=T_add.data)\n                T_add_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0 in T.parallel(12):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0 * 20 + i2\n                compute_2 = T.Buffer((240,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_divide_1 = T.Buffer((240,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asin",
                "atan",
                "add",
                "acos",
                "cos",
                "divide"
            ]
        ],
        "input_shape": [[12, 1, 20]],
        "output_shape": [[12, 1, 20], [12, 1, 20], [12, 1, 20], [12, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 28; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 17) + ax2)] = (ph_0[((ax0_ax1_fused * 17) + ax2)] * ph_3[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 476; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 476; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 4, 17), \"float32\"), ph_3: T.Buffer((7, 4, 17), \"float32\"), T_multiply: T.Buffer((7, 4, 17), \"float32\"), compute: T.Buffer((7, 4, 17), \"float32\"), compute_1: T.Buffer((7, 4, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((476,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(28):\n            for ax2 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_multiply_1 = T.Buffer((476,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((476,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(476):\n            compute_2 = T.Buffer((476,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(476):\n            compute_2 = T.Buffer((476,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "atanh",
                "asinh",
                "asin"
            ]
        ],
        "input_shape": [[7, 4, 17], [10, 5, 15], [7, 4, 17]],
        "output_shape": [[7, 4, 17], [10, 5, 15], [7, 4, 17], [7, 4, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused], cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_1[(((i0 * 168) + (i1 * 14)) + i2)] = asinhf(ph_0[(((i0 * 168) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 20; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        compute_2[(((i0_1 * 168) + (i1_1 * 14)) + i2_1)] = atanhf(ph_0[(((i0_1 * 168) + (i1_1 * 14)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 12, 14), \"float32\"), compute: T.Buffer((20, 12, 14), \"float32\"), compute_1: T.Buffer((20, 12, 14), \"float32\"), compute_2: T.Buffer((20, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3360,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3360):\n            compute_3 = T.Buffer((3360,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(12, 14):\n                cse_var_1: T.int32 = i0 * 168 + i1 * 14 + i2\n                compute_3 = T.Buffer((3360,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(12, 14):\n                cse_var_2: T.int32 = i0 * 168 + i1 * 14 + i2\n                compute_3 = T.Buffer((3360,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "mod",
                "atanh",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[20, 12, 14]],
        "output_shape": [[20, 12, 14], [20, 12, 14], [20, 12, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 18; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 9; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i0_i1_fused * 2) + i2)] = expf(asinhf(ph_0[((i0_i1_fused * 2) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 18; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 18; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 9, 2), \"float32\"), compute: T.Buffer((1, 9, 2), \"float32\"), compute_1: T.Buffer((1, 9, 2), \"float32\"), compute_2: T.Buffer((1, 9, 2), \"float32\"), compute_3: T.Buffer((1, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((18,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_4 = T.Buffer((18,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(9):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_4 = T.Buffer((18,), data=compute_1.data)\n                compute_4[cse_var_1] = T.exp(T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_4 = T.Buffer((18,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_4 = T.Buffer((18,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "asinh",
                "exp",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[1, 9, 2]],
        "output_shape": [[1, 9, 2], [1, 9, 2], [1, 9, 2], [1, 9, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 150; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf((ph_0[i0_i1_fused_i2_fused] - asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 150; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / cosf(ph_0[ax0_ax1_fused_ax2_fused])) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 150; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf((ph_0[i0_i1_fused_i2_fused_1] / cosf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 10, 1), \"float32\"), compute: T.Buffer((15, 10, 1), \"float32\"), T_divide: T.Buffer((15, 10, 1), \"float32\"), compute_1: T.Buffer((15, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_2 = T.Buffer((150,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] - T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_divide_1 = T.Buffer((150,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_2 = T.Buffer((150,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] / T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asin",
                "subtract",
                "sin",
                "cos",
                "divide",
                "divide",
                "acos"
            ]
        ],
        "input_shape": [[15, 10, 1]],
        "output_shape": [[15, 10, 1], [15, 10, 1], [15, 10, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 135; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 135; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    compute_1[i0_i1_fused] = cosf(ph_0[i0_i1_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 1), \"float32\"), ph_3: T.Buffer((15, 9, 1), \"float32\"), T_add: T.Buffer((15, 9, 1), \"float32\"), compute: T.Buffer((15, 9, 1), \"float32\"), compute_1: T.Buffer((15, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((135,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(135):\n            T_add_1 = T.Buffer((135,), data=T_add.data)\n            ph_3_1 = T.Buffer((135,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(135):\n            compute_2 = T.Buffer((135,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(135):\n            compute_2 = T.Buffer((135,), data=compute_1.data)\n            compute_2[i0_i1_fused] = T.cos(ph_0_1[i0_i1_fused])",
        "op_args": [
            [
                "add",
                "asin",
                "cos"
            ]
        ],
        "input_shape": [[15, 9, 1], [17, 19, 14], [15, 9, 1]],
        "output_shape": [[15, 9, 1], [17, 19, 14], [15, 9, 1], [15, 9, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 18; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 3) + ax2)] = (acosf(ph_0[((ax0_ax1_fused * 3) + ax2)]) - ph_0[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 18; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_2[((i0 * 3) + i2)] = acosf((ph_0[((i0 * 3) + i2)] + ph_3[((i0 * 3) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(acosf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 1, 3), \"float32\"), ph_3: T.Buffer((6, 1, 3), \"float32\"), compute: T.Buffer((6, 1, 3), \"float32\"), T_subtract: T.Buffer((6, 1, 3), \"float32\"), compute_1: T.Buffer((6, 1, 3), \"float32\"), compute_2: T.Buffer((6, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((18,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_3 = T.Buffer((18,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2 in range(3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_subtract_1 = T.Buffer((18,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.acos(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_3 = T.Buffer((18,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(6):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0 * 3 + i2\n                compute_3 = T.Buffer((18,), data=compute_2.data)\n                ph_3_1 = T.Buffer((18,), data=ph_3.data)\n                compute_3[cse_var_2] = T.acos(ph_0_1[cse_var_2] + ph_3_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "atanh",
                "acos",
                "subtract",
                "exp",
                "acos"
            ]
        ],
        "input_shape": [[6, 1, 3], [11, 7, 13], [6, 1, 3]],
        "output_shape": [[11, 7, 13], [6, 1, 3], [6, 1, 3], [6, 1, 3], [6, 1, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2197; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2197; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2197; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 169; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute_2[((i0_i1_fused * 13) + i2)] = cosf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 13, 13), \"float32\"), compute: T.Buffer((13, 13, 13), \"float32\"), T_divide: T.Buffer((13, 13, 13), \"float32\"), compute_1: T.Buffer((13, 13, 13), \"float32\"), compute_2: T.Buffer((13, 13, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2197,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2197):\n            compute_3 = T.Buffer((2197,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2197):\n            T_divide_1 = T.Buffer((2197,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2197):\n            compute_3 = T.Buffer((2197,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(169):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_3 = T.Buffer((2197,), data=compute_2.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acos",
                "asinh",
                "divide",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[13, 13, 13]],
        "output_shape": [[13, 13, 13], [13, 13, 13], [13, 13, 13], [13, 13, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_subtract[((ax1 * 14) + ax2)] = (ph_0[((ax1 * 14) + ax2)] - ph_3[((ax1 * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 224; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 224; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 224; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 16, 14), \"float32\"), ph_3: T.Buffer((1, 16, 14), \"float32\"), T_subtract: T.Buffer((1, 16, 14), \"float32\"), compute: T.Buffer((1, 16, 14), \"float32\"), compute_1: T.Buffer((1, 16, 14), \"float32\"), compute_2: T.Buffer((1, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((224,), data=ph_0.data)\n        for ax1, ax2 in T.grid(16, 14):\n            cse_var_1: T.int32 = ax1 * 14 + ax2\n            T_subtract_1 = T.Buffer((224,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((224,), data=ph_3.data)\n            T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(224):\n            compute_3 = T.Buffer((224,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(224):\n            compute_3 = T.Buffer((224,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(224):\n            compute_3 = T.Buffer((224,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "atan",
                "atan",
                "atanh",
                "atanh"
            ]
        ],
        "input_shape": [[1, 16, 14], [19, 12, 10], [1, 16, 14]],
        "output_shape": [[1, 16, 14], [19, 12, 10], [1, 16, 14], [1, 16, 14], [1, 16, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], cosf(ph_0[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 3, 14), \"float32\"), T_subtract: T.Buffer((16, 3, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            T_subtract_1 = T.Buffer((672,), data=T_subtract.data)\n            ph_0_1 = T.Buffer((672,), data=ph_0.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.cos(ph_0_1[ax0_ax1_fused_ax2_fused])) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "cos",
                "mod",
                "subtract"
            ]
        ],
        "input_shape": [[16, 3, 14]],
        "output_shape": [[16, 3, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_add[((ax0_ax1_fused * 14) + ax2)] = (ph_0[((ax0_ax1_fused * 14) + ax2)] + ph_3[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 56) + (i1 * 14)) + i2)] = asinf(ph_0[(((i0 * 56) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 896; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 4, 14), \"float32\"), ph_3: T.Buffer((16, 4, 14), \"float32\"), T_add: T.Buffer((16, 4, 14), \"float32\"), compute: T.Buffer((16, 4, 14), \"float32\"), compute_1: T.Buffer((16, 4, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((896,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_add_1 = T.Buffer((896,), data=T_add.data)\n                ph_3_1 = T.Buffer((896,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(4, 14):\n                cse_var_2: T.int32 = i0 * 56 + i1 * 14 + i2\n                compute_2 = T.Buffer((896,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(896):\n            compute_2 = T.Buffer((896,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "asin",
                "cos"
            ]
        ],
        "input_shape": [[16, 4, 14], [3, 17, 15], [16, 4, 14]],
        "output_shape": [[16, 4, 14], [3, 17, 15], [16, 4, 14], [16, 4, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1200; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / atanhf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] / atanhf(ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 20, 10), \"float32\"), T_divide: T.Buffer((6, 20, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1200):\n            T_divide_1 = T.Buffer((1200,), data=T_divide.data)\n            ph_0_1 = T.Buffer((1200,), data=ph_0.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "atanh",
                "divide"
            ]
        ],
        "input_shape": [[6, 20, 10]],
        "output_shape": [[6, 20, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 588; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] / atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 98; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0_i1_fused * 6) + i2)] = acoshf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 588; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 7, 6), \"float32\"), compute: T.Buffer((14, 7, 6), \"float32\"), compute_1: T.Buffer((14, 7, 6), \"float32\"), compute_2: T.Buffer((14, 7, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((588,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(588):\n            compute_3 = T.Buffer((588,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] / T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(98):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((588,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(588):\n            compute_3 = T.Buffer((588,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "divide",
                "atanh",
                "acosh",
                "ceil"
            ]
        ],
        "input_shape": [[14, 7, 6]],
        "output_shape": [[14, 7, 6], [14, 7, 6], [14, 7, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 686; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ((ph_0[ax0_ax1_fused_ax2_fused] - cosf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 49) + (i1 * 7)) + i2)] = asinhf(ph_0[(((i0 * 49) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] * ((ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 7, 7), \"float32\"), T_multiply: T.Buffer((14, 7, 7), \"float32\"), compute: T.Buffer((14, 7, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((686,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(686):\n            T_multiply_1 = T.Buffer((686,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ((ph_0_1[ax0_ax1_fused_ax2_fused] - T.cos(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(7, 7):\n                cse_var_1: T.int32 = i0 * 49 + i1 * 7 + i2\n                compute_1 = T.Buffer((686,), data=compute.data)\n                compute_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "subtract",
                "multiply",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[14, 7, 7]],
        "output_shape": [[14, 7, 7], [14, 7, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 13; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = atanf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 26; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 26; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 2), \"float32\"), compute: T.Buffer((13, 1, 2), \"float32\"), compute_1: T.Buffer((13, 1, 2), \"float32\"), compute_2: T.Buffer((13, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((26,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(13):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((26,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(26):\n            compute_3 = T.Buffer((26,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(26):\n            compute_3 = T.Buffer((26,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "cos",
                "asin",
                "sin"
            ]
        ],
        "input_shape": [[13, 1, 2]],
        "output_shape": [[13, 1, 2], [13, 1, 2], [13, 1, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 324; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    float compute_4[18];\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_4[i2] = expf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[((i0_i1_fused * 18) + i2_1)] = expf(compute_4[i2_1]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2_2 = 0; i2_2 < 18; ++i2_2) {\n        compute_2[(((i0 * 36) + (i1 * 18)) + i2_2)] = asinhf(ph_0[(((i0 * 36) + (i1 * 18)) + i2_2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 324; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 2, 18), \"float32\"), compute: T.Buffer((9, 2, 18), \"float32\"), compute_1: T.Buffer((9, 2, 18), \"float32\"), compute_2: T.Buffer((9, 2, 18), \"float32\"), compute_3: T.Buffer((9, 2, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((324,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(324):\n            compute_4 = T.Buffer((324,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(18):\n            compute_4 = T.allocate([18], \"float32\", \"global\")\n            compute_5 = T.Buffer((18,), data=compute_4)\n            for i2 in range(18):\n                compute_5[i2] = T.exp(ph_0_1[i0_i1_fused * 18 + i2])\n            for i2 in range(18):\n                compute_6 = T.Buffer((324,), data=compute_1.data)\n                compute_6[i0_i1_fused * 18 + i2] = T.exp(compute_5[i2])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(2, 18):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 18 + i2\n                compute_4 = T.Buffer((324,), data=compute_2.data)\n                compute_4[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(324):\n            compute_4 = T.Buffer((324,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "exp",
                "exp",
                "asinh",
                "sin"
            ]
        ],
        "input_shape": [[9, 2, 18]],
        "output_shape": [[9, 2, 18], [9, 2, 18], [9, 2, 18], [9, 2, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1235; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1235; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1235; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = fabsf(fmodf(ph_0[i0_i1_fused_i2_fused_2], cosf(ph_0[i0_i1_fused_i2_fused_2])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 13, 5), \"float32\"), compute: T.Buffer((19, 13, 5), \"float32\"), compute_1: T.Buffer((19, 13, 5), \"float32\"), compute_2: T.Buffer((19, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1235,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1235):\n            compute_3 = T.Buffer((1235,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1235):\n            compute_3 = T.Buffer((1235,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1235):\n            compute_3 = T.Buffer((1235,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(ph_0_1[i0_i1_fused_i2_fused])))",
        "op_args": [
            [
                "sin",
                "abs",
                "sin",
                "cos",
                "mod",
                "abs"
            ]
        ],
        "input_shape": [[19, 13, 5]],
        "output_shape": [[19, 13, 5], [19, 13, 5], [19, 13, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 650; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 650; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 650; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_3[((i0_i1_fused * 5) + i2)] = sinf(acoshf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 130; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_4[((i0_i1_fused_1 * 5) + i2_1)] = acoshf(acoshf(ph_0[((i0_i1_fused_1 * 5) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 10, 5), \"float32\"), compute: T.Buffer((13, 10, 5), \"float32\"), compute_1: T.Buffer((13, 10, 5), \"float32\"), compute_2: T.Buffer((13, 10, 5), \"float32\"), compute_3: T.Buffer((13, 10, 5), \"float32\"), compute_4: T.Buffer((13, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((650,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(650):\n            compute_5 = T.Buffer((650,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(650):\n            compute_5 = T.Buffer((650,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.ceil(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(650):\n            compute_5 = T.Buffer((650,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_5 = T.Buffer((650,), data=compute_3.data)\n                compute_5[cse_var_1] = T.sin(T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_5 = T.Buffer((650,), data=compute_4.data)\n                compute_5[cse_var_2] = T.acosh(T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "sin",
                "atanh",
                "ceil",
                "atanh",
                "acosh",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[13, 10, 5]],
        "output_shape": [[13, 10, 5], [13, 10, 5], [13, 10, 5], [13, 10, 5], [13, 10, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 340; ++i0_i1_fused) {\n    compute[i0_i1_fused] = atanhf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 340; ++i0_i1_fused_1) {\n    compute_1[i0_i1_fused_1] = atanhf(sinf(ph_0[i0_i1_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 340; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 1), \"float32\"), compute: T.Buffer((17, 20, 1), \"float32\"), compute_1: T.Buffer((17, 20, 1), \"float32\"), compute_2: T.Buffer((17, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((340,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(340):\n            compute_3 = T.Buffer((340,), data=compute.data)\n            compute_3[i0_i1_fused] = T.atanh(ph_0_1[i0_i1_fused])\n        for i0_i1_fused in T.parallel(340):\n            compute_3 = T.Buffer((340,), data=compute_1.data)\n            compute_3[i0_i1_fused] = T.atanh(T.sin(ph_0_1[i0_i1_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(340):\n            compute_3 = T.Buffer((340,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "sin",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[17, 20, 1]],
        "output_shape": [[17, 20, 1], [17, 20, 1], [17, 20, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* T_subtract_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1260; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_mod[(((ax0 * 180) + (ax1 * 15)) + ax2)] = fmodf(ph_0[(((ax0 * 180) + (ax1 * 15)) + ax2)], ph_3[(((ax0 * 180) + (ax1 * 15)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 15; ++ax2_1) {\n      T_subtract_1[((ax0_ax1_fused * 15) + ax2_1)] = (asinf(ph_0[((ax0_ax1_fused * 15) + ax2_1)]) - ph_0[((ax0_ax1_fused * 15) + ax2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 12, 15), \"float32\"), ph_3: T.Buffer((7, 12, 15), \"float32\"), T_subtract: T.Buffer((7, 12, 15), \"float32\"), T_mod: T.Buffer((7, 12, 15), \"float32\"), T_subtract_1: T.Buffer((7, 12, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1260,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1260):\n            T_subtract_2 = T.Buffer((1260,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(12, 15):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 15 + ax2\n                T_mod_1 = T.Buffer((1260,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(84):\n            for ax2 in range(15):\n                cse_var_2: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_subtract_2 = T.Buffer((1260,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_2] = T.asin(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "subtract",
                "mod",
                "asin",
                "subtract"
            ]
        ],
        "input_shape": [[7, 12, 15], [2, 13, 19], [7, 12, 15]],
        "output_shape": [[7, 12, 15], [2, 13, 19], [7, 12, 15], [7, 12, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 306; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 306; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 102; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_1[((i0_i1_fused * 3) + i2)] = asinhf(asinhf(ph_0[((i0_i1_fused * 3) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 306; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 6, 3), \"float32\"), ph_3: T.Buffer((17, 6, 3), \"float32\"), T_subtract: T.Buffer((17, 6, 3), \"float32\"), compute: T.Buffer((17, 6, 3), \"float32\"), compute_1: T.Buffer((17, 6, 3), \"float32\"), compute_2: T.Buffer((17, 6, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((306,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(306):\n            T_subtract_1 = T.Buffer((306,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((306,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_3 = T.Buffer((306,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(102):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((306,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(306):\n            compute_3 = T.Buffer((306,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "exp",
                "asinh",
                "asinh",
                "sin"
            ]
        ],
        "input_shape": [[17, 6, 3], [18, 4, 1], [17, 6, 3]],
        "output_shape": [[17, 6, 3], [18, 4, 1], [17, 6, 3], [17, 6, 3], [17, 6, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = cosf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 200; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ceilf(acosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 200; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ceilf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(acoshf(ph_0[((int)blockIdx.x)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 8, 5), \"float32\"), compute: T.Buffer((5, 8, 5), \"float32\"), T_mod: T.Buffer((5, 8, 5), \"float32\"), compute_1: T.Buffer((5, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((200,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(40):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((200,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(200):\n            T_mod_1 = T.Buffer((200,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.ceil(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(200):\n            compute_2 = T.Buffer((200,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "cos",
                "acos",
                "ceil",
                "mod",
                "acosh",
                "asinh"
            ]
        ],
        "input_shape": [[5, 8, 5]],
        "output_shape": [[5, 8, 5], [5, 8, 5], [5, 8, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 672; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(fabsf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 672; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 672; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 7, 12), \"float32\"), compute: T.Buffer((8, 7, 12), \"float32\"), T_mod: T.Buffer((8, 7, 12), \"float32\"), compute_1: T.Buffer((8, 7, 12), \"float32\"), compute_2: T.Buffer((8, 7, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((672,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_3 = T.Buffer((672,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            T_mod_1 = T.Buffer((672,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_3 = T.Buffer((672,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_3 = T.Buffer((672,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "abs",
                "mod",
                "ceil",
                "atanh"
            ]
        ],
        "input_shape": [[8, 7, 12]],
        "output_shape": [[8, 7, 12], [8, 7, 12], [8, 7, 12], [8, 7, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4480; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4480; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_3[(((i0 * 280) + (i1 * 20)) + i2)] = atanf(cosf(ph_0[(((i0 * 280) + (i1 * 20)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 4480; ++i0_i1_fused_i2_fused_3) {\n    compute_4[i0_i1_fused_i2_fused_3] = atanf(cosf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 14, 20), \"float32\"), compute: T.Buffer((16, 14, 20), \"float32\"), compute_1: T.Buffer((16, 14, 20), \"float32\"), compute_2: T.Buffer((16, 14, 20), \"float32\"), compute_3: T.Buffer((16, 14, 20), \"float32\"), compute_4: T.Buffer((16, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_5 = T.Buffer((4480,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_5 = T.Buffer((4480,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_5 = T.Buffer((4480,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(14, 20):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 20 + i2\n                compute_5 = T.Buffer((4480,), data=compute_3.data)\n                compute_5[cse_var_1] = T.atan(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_5 = T.Buffer((4480,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "ceil",
                "exp",
                "sin",
                "cos",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[16, 14, 20]],
        "output_shape": [[16, 14, 20], [16, 14, 20], [16, 14, 20], [16, 14, 20], [16, 14, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1260; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 90; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 14) + ax2)] = (ph_0[((ax0_ax1_fused * 14) + ax2)] - ph_3[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 210) + (i1 * 14)) + i2)] = atanf(ph_0[(((i0 * 210) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 15, 14), \"float32\"), ph_3: T.Buffer((6, 15, 14), \"float32\"), T_multiply: T.Buffer((6, 15, 14), \"float32\"), T_subtract: T.Buffer((6, 15, 14), \"float32\"), compute: T.Buffer((6, 15, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1260,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1260):\n            T_multiply_1 = T.Buffer((1260,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(90):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_subtract_1 = T.Buffer((1260,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(15, 14):\n                cse_var_2: T.int32 = i0 * 210 + i1 * 14 + i2\n                compute_1 = T.Buffer((1260,), data=compute.data)\n                compute_1[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "subtract",
                "atan"
            ]
        ],
        "input_shape": [[6, 15, 14], [8, 16, 3], [6, 15, 14]],
        "output_shape": [[6, 15, 14], [8, 16, 3], [6, 15, 14], [6, 15, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1170; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], fabsf(ph_0[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1170; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1170; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 10, 9), \"float32\"), T_subtract: T.Buffer((13, 10, 9), \"float32\"), compute: T.Buffer((13, 10, 9), \"float32\"), compute_1: T.Buffer((13, 10, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1170):\n            T_subtract_1 = T.Buffer((1170,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_2 = T.Buffer((1170,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_2 = T.Buffer((1170,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "mod",
                "subtract",
                "abs",
                "abs",
                "cos"
            ]
        ],
        "input_shape": [[13, 10, 9]],
        "output_shape": [[13, 10, 9], [13, 10, 9], [13, 10, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 154; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 154; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 22; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_2[((i0_i1_fused * 7) + i2)] = sinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 11, 7), \"float32\"), compute: T.Buffer((2, 11, 7), \"float32\"), compute_1: T.Buffer((2, 11, 7), \"float32\"), compute_2: T.Buffer((2, 11, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((154,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(154):\n            compute_3 = T.Buffer((154,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(154):\n            compute_3 = T.Buffer((154,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(22):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((154,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asinh",
                "asin",
                "atanh",
                "sin"
            ]
        ],
        "input_shape": [[2, 11, 7]],
        "output_shape": [[2, 11, 7], [2, 11, 7], [2, 11, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = atanhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1512; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ceilf(acosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1512; ++ax0_ax1_fused_ax2_fused_1) {\n    float compute_1[1];\n    compute_1[0] = expf(ph_0[ax0_ax1_fused_ax2_fused_1]);\n    T_divide[ax0_ax1_fused_ax2_fused_1] = (compute_1[0] / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * ceilf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 9), \"float32\"), compute: T.Buffer((12, 14, 9), \"float32\"), T_multiply: T.Buffer((12, 14, 9), \"float32\"), T_divide: T.Buffer((12, 14, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1512,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(168):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((1512,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1512):\n            T_multiply_1 = T.Buffer((1512,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.ceil(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1512):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.exp(ph_0_1[ax0_ax1_fused_ax2_fused])\n            T_divide_1 = T.Buffer((1512,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = compute_2[0] / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "acos",
                "ceil",
                "multiply",
                "exp",
                "divide"
            ]
        ],
        "input_shape": [[12, 14, 9]],
        "output_shape": [[12, 14, 9], [12, 14, 9], [12, 14, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1120; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1120; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] * ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 56) + (i1 * 7)) + i2)] = acoshf(ph_0[(((i0 * 56) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] / ph_3[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 8, 7), \"float32\"), ph_3: T.Buffer((20, 8, 7), \"float32\"), T_divide: T.Buffer((20, 8, 7), \"float32\"), T_multiply: T.Buffer((20, 8, 7), \"float32\"), compute: T.Buffer((20, 8, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1120,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1120,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1120):\n            T_divide_1 = T.Buffer((1120,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1120):\n            T_multiply_1 = T.Buffer((1120,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(8, 7):\n                cse_var_1: T.int32 = i0 * 56 + i1 * 7 + i2\n                compute_1 = T.Buffer((1120,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "divide",
                "multiply",
                "acosh"
            ]
        ],
        "input_shape": [[20, 8, 7], [14, 20, 10], [20, 8, 7]],
        "output_shape": [[20, 8, 7], [14, 20, 10], [20, 8, 7], [20, 8, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 468; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_1[(((i0 * 36) + (i1 * 3)) + i2)] = asinhf(atanf(ph_0[(((i0 * 36) + (i1 * 3)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 468; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 12, 3), \"float32\"), compute: T.Buffer((13, 12, 3), \"float32\"), compute_1: T.Buffer((13, 12, 3), \"float32\"), compute_2: T.Buffer((13, 12, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_3 = T.Buffer((468,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(12, 3):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 3 + i2\n                compute_3 = T.Buffer((468,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_3 = T.Buffer((468,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "atan",
                "asinh",
                "asinh"
            ]
        ],
        "input_shape": [[13, 12, 3]],
        "output_shape": [[13, 12, 3], [13, 12, 3], [13, 12, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 204; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_divide[((ax0_ax1_fused * 17) + ax2)] = (atanf(ph_0[((ax0_ax1_fused * 17) + ax2)]) / ph_0[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 204; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(fmodf(ph_0[i0_i1_fused_i2_fused_1], sinf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 1, 17), \"float32\"), compute: T.Buffer((12, 1, 17), \"float32\"), T_divide: T.Buffer((12, 1, 17), \"float32\"), compute_1: T.Buffer((12, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((204,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(204):\n            compute_2 = T.Buffer((204,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_divide_1 = T.Buffer((204,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(204):\n            compute_2 = T.Buffer((204,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.sin(ph_0_1[i0_i1_fused_i2_fused])))",
        "op_args": [
            [
                "sin",
                "atan",
                "divide",
                "sin",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[12, 1, 17]],
        "output_shape": [[12, 1, 17], [12, 1, 17], [12, 1, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] / cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 8), \"float32\"), compute: T.Buffer((11, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_1 = T.Buffer((176,), data=compute.data)\n            ph_0_1 = T.Buffer((176,), data=ph_0.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] / T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "cos",
                "divide",
                "atanh"
            ]
        ],
        "input_shape": [[11, 2, 8]],
        "output_shape": [[11, 2, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_9) {\n  float auto_scheduler_layout_transform[1700];\n  float T_batch_matmul_NN[1300];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4420; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4420; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4420; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax4 = 0; ax4 < 17; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 5; ++ax5) {\n        for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n          for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n            auto_scheduler_layout_transform[(((((ax0_ax1_fused_ax2_fused_1 * 850) + (ax4 * 50)) + (ax5 * 10)) + (ax6 * 2)) + ax8)] = ph_3[(((((ax0_ax1_fused_ax2_fused_1 * 850) + (ax5 * 170)) + (ax8 * 85)) + (ax4 * 5)) + ax6)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 26; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 5; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n          T_batch_matmul_NN[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 650) + (b_outer_inner_init * 130)) + (b_inner_init * 65)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1) * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 17; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 5; ++b_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n            T_batch_matmul_NN[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 650) + (b_outer_inner * 130)) + (b_inner * 65)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1) * 5)) + j_outer_inner)] = (T_batch_matmul_NN[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 650) + (b_outer_inner * 130)) + (b_inner * 65)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1) * 5)) + j_outer_inner)] + (ph_0[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 2210) + (b_outer_inner * 442)) + (b_inner * 221)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 1) * 17)) + k_outer)] * auto_scheduler_layout_transform[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 1) * 850) + (k_outer * 50)) + (b_outer_inner * 10)) + (j_outer_inner * 2)) + b_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 1300; ++ax0_ax1_fused_ax2_fused_2) {\n    T_mod[ax0_ax1_fused_ax2_fused_2] = fmodf(T_batch_matmul_NN[ax0_ax1_fused_ax2_fused_2], ph_9[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[16];\n  __shared__ float ph_3_shared[200];\n  for (int b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n    for (int i_outer_inner_init = 0; i_outer_inner_init < 8; ++i_outer_inner_init) {\n      T_batch_matmul_NN[((b_outer_inner_init * 8) + i_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 20) + ((int)threadIdx.x))] = ph_3[((ax0_ax1_fused_ax2_fused_outer_outer * 20) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n    for (int i_outer_inner = 0; i_outer_inner < 8; ++i_outer_inner) {\n      for (int k_inner = 0; k_inner < 5; ++k_inner) {\n        T_batch_matmul_NN[((b_outer_inner * 8) + i_outer_inner)] = (T_batch_matmul_NN[((b_outer_inner * 8) + i_outer_inner)] + (ph_0[(((((((int)threadIdx.x) / 5) * 80) + (b_outer_inner * 40)) + (i_outer_inner * 5)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) / 5) * 50) + (b_outer_inner * 25)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n      }\n    }\n  }\n  for (int ax0_inner = 0; ax0_inner < 2; ++ax0_inner) {\n    for (int ax1_inner = 0; ax1_inner < 8; ++ax1_inner) {\n      T_mod[(((((((int)threadIdx.x) / 5) * 80) + (ax0_inner * 40)) + (ax1_inner * 5)) + (((int)threadIdx.x) % 5))] = fmodf(T_batch_matmul_NN[((ax0_inner * 8) + ax1_inner)], ph_0[(((((((int)threadIdx.x) / 5) * 80) + (ax0_inner * 40)) + (ax1_inner * 5)) + (((int)threadIdx.x) % 5))]);\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 13, 17), \"float32\"), ph_3: T.Buffer((20, 17, 5), \"float32\"), ph_9: T.Buffer((20, 13, 5), \"float32\"), compute: T.Buffer((20, 13, 17), \"float32\"), T_subtract: T.Buffer((20, 13, 17), \"float32\"), compute_1: T.Buffer((20, 13, 17), \"float32\"), T_mod: T.Buffer((20, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1700], \"float32\", \"global\")\n        T_batch_matmul_NN = T.allocate([1300], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((4420,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4420):\n            compute_2 = T.Buffer((4420,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4420):\n            T_subtract_1 = T.Buffer((4420,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(4420):\n            compute_2 = T.Buffer((4420,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((1700,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2):\n            for ax4, ax5, ax6, ax8 in T.grid(17, 5, 5, 2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 850\n                ph_3_1 = T.Buffer((1700,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 50 + ax5 * 10 + ax6 * 2 + ax8] = ph_3_1[cse_var_1 + ax5 * 170 + ax8 * 85 + ax4 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((1300,), data=T_batch_matmul_NN)\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(26):\n            for b_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(5, 5, 2):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 2 * 650 + b_outer_inner_init * 130 + b_inner_init * 65 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2 * 5 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, j_outer_inner, b_inner in T.grid(17, 5, 5, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 2\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 2\n                cse_var_2: T.int32 = cse_var_3 * 650 + b_outer_inner * 130 + b_inner * 65 + cse_var_4 * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_3 * 2210 + b_outer_inner * 442 + b_inner * 221 + cse_var_4 * 17 + k_outer] * auto_scheduler_layout_transform_1[cse_var_3 * 850 + k_outer * 50 + b_outer_inner * 10 + j_outer_inner * 2 + b_inner]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1300):\n            T_mod_1 = T.Buffer((1300,), data=T_mod.data)\n            ph_9_1 = T.Buffer((1300,), data=ph_9.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T_batch_matmul_NN_1[ax0_ax1_fused_ax2_fused], ph_9_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "batch_matmul",
                "exp",
                "acosh",
                "subtract",
                "asin",
                "mod"
            ]
        ],
        "input_shape": [[20, 13, 17], [5, 1, 8], [20, 17, 5], [20, 13, 5]],
        "output_shape": [[5, 1, 8], [20, 13, 17], [20, 13, 17], [20, 13, 17], [20, 13, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 126) + (i1 * 7)) + i2)] = ceilf(ph_0[(((i0 * 126) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_mod[(((ax0 * 126) + (ax1 * 7)) + ax2)] = fmodf(atanhf(ph_0[(((ax0 * 126) + (ax1 * 7)) + ax2)]), ph_0[(((ax0 * 126) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1512; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 18, 7), \"float32\"), compute: T.Buffer((12, 18, 7), \"float32\"), T_mod: T.Buffer((12, 18, 7), \"float32\"), compute_1: T.Buffer((12, 18, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1512,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(18, 7):\n                cse_var_1: T.int32 = i0 * 126 + i1 * 7 + i2\n                compute_2 = T.Buffer((1512,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(18, 7):\n                cse_var_2: T.int32 = ax0 * 126 + ax1 * 7 + ax2\n                T_mod_1 = T.Buffer((1512,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.atanh(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1512):\n            compute_2 = T.Buffer((1512,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "mod",
                "cos"
            ]
        ],
        "input_shape": [[12, 18, 7]],
        "output_shape": [[12, 18, 7], [12, 18, 7], [12, 18, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 76; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 76; ++i0_i1_fused) {\n    compute_1[i0_i1_fused] = acoshf(asinhf(ph_0[i0_i1_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      T_multiply[((ax0 * 4) + ax1)] = (ph_0[((ax0 * 4) + ax1)] * acosf(ph_0[((ax0 * 4) + ax1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 4, 1), \"float32\"), compute: T.Buffer((19, 4, 1), \"float32\"), compute_1: T.Buffer((19, 4, 1), \"float32\"), T_multiply: T.Buffer((19, 4, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((76,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(76):\n            compute_2 = T.Buffer((76,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(76):\n            compute_2 = T.Buffer((76,), data=compute_1.data)\n            compute_2[i0_i1_fused] = T.acosh(T.asinh(ph_0_1[i0_i1_fused]))\n        for ax0 in T.parallel(19):\n            for ax1 in range(4):\n                cse_var_1: T.int32 = ax0 * 4 + ax1\n                T_multiply_1 = T.Buffer((76,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "asinh",
                "acosh",
                "acos",
                "multiply"
            ]
        ],
        "input_shape": [[19, 4, 1]],
        "output_shape": [[19, 4, 1], [19, 4, 1], [19, 4, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 119; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_divide[((ax0_ax1_fused * 3) + ax2)] = (ph_0[((ax0_ax1_fused * 3) + ax2)] / ph_3[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 51) + (i1 * 3)) + i2)] = cosf(ph_0[(((i0 * 51) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 357; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 17, 3), \"float32\"), ph_3: T.Buffer((7, 17, 3), \"float32\"), T_divide: T.Buffer((7, 17, 3), \"float32\"), compute: T.Buffer((7, 17, 3), \"float32\"), compute_1: T.Buffer((7, 17, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((357,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(119):\n            for ax2 in range(3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_divide_1 = T.Buffer((357,), data=T_divide.data)\n                ph_3_1 = T.Buffer((357,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(17, 3):\n                cse_var_2: T.int32 = i0 * 51 + i1 * 3 + i2\n                compute_2 = T.Buffer((357,), data=compute.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(357):\n            compute_2 = T.Buffer((357,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "cos",
                "abs"
            ]
        ],
        "input_shape": [[7, 17, 3], [11, 15, 9], [7, 17, 3]],
        "output_shape": [[7, 17, 3], [11, 15, 9], [7, 17, 3], [7, 17, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 24) + (i1 * 4)) + i2)] = atanf(ph_0[(((i0 * 24) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 10; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 6; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_1[(((i0_1 * 24) + (i1_1 * 4)) + i2_1)] = sinf(atanhf(ph_0[(((i0_1 * 24) + (i1_1 * 4)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 6, 4), \"float32\"), ph_3: T.Buffer((10, 6, 4), \"float32\"), T_mod: T.Buffer((10, 6, 4), \"float32\"), compute: T.Buffer((10, 6, 4), \"float32\"), compute_1: T.Buffer((10, 6, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_mod_1 = T.Buffer((240,), data=T_mod.data)\n            ph_3_1 = T.Buffer((240,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(6, 4):\n                cse_var_1: T.int32 = i0 * 24 + i1 * 4 + i2\n                compute_2 = T.Buffer((240,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(6, 4):\n                cse_var_2: T.int32 = i0 * 24 + i1 * 4 + i2\n                compute_2 = T.Buffer((240,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "atan",
                "atanh",
                "sin"
            ]
        ],
        "input_shape": [[10, 6, 4], [6, 16, 18], [10, 6, 4]],
        "output_shape": [[10, 6, 4], [6, 16, 18], [10, 6, 4], [10, 6, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 77; ++i0_i1_fused) {\n    compute[i0_i1_fused] = asinf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 77; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 77; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] * asinhf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 1), \"float32\"), compute: T.Buffer((7, 11, 1), \"float32\"), compute_1: T.Buffer((7, 11, 1), \"float32\"), T_multiply: T.Buffer((7, 11, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((77,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(77):\n            compute_2 = T.Buffer((77,), data=compute.data)\n            compute_2[i0_i1_fused] = T.asin(ph_0_1[i0_i1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(77):\n            compute_2 = T.Buffer((77,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(77):\n            T_multiply_1 = T.Buffer((77,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asin",
                "asinh",
                "ceil",
                "asinh",
                "multiply",
                "multiply"
            ]
        ],
        "input_shape": [[7, 11, 1]],
        "output_shape": [[7, 11, 1], [7, 11, 1], [7, 11, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 90) + (i1 * 5)) + i2)] = ceilf(fabsf(ph_0[(((i0 * 90) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 360; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] * ph_3[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 360; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] * ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 18, 5), \"float32\"), ph_3: T.Buffer((4, 18, 5), \"float32\"), T_divide: T.Buffer((4, 18, 5), \"float32\"), compute: T.Buffer((4, 18, 5), \"float32\"), compute_1: T.Buffer((4, 18, 5), \"float32\"), T_subtract: T.Buffer((4, 18, 5), \"float32\"), compute_2: T.Buffer((4, 18, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(18, 5):\n                cse_var_1: T.int32 = i0 * 90 + i1 * 5 + i2\n                compute_3 = T.Buffer((360,), data=compute_1.data)\n                compute_3[cse_var_1] = T.ceil(T.fabs(ph_0_1[cse_var_1]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_divide_1 = T.Buffer((360,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "divide",
                "abs",
                "asin",
                "ceil",
                "subtract",
                "asinh"
            ]
        ],
        "input_shape": [[4, 18, 5], [5, 6, 11], [4, 18, 5]],
        "output_shape": [[5, 6, 11], [4, 18, 5], [4, 18, 5], [4, 18, 5], [4, 18, 5], [4, 18, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 280) + (i1 * 20)) + i2)] = fabsf(ph_0[(((i0 * 280) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_1[((i0_i1_fused * 20) + i2_1)] = fabsf(cosf(ph_0[((i0_i1_fused * 20) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 560; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 14, 20), \"float32\"), compute: T.Buffer((2, 14, 20), \"float32\"), compute_1: T.Buffer((2, 14, 20), \"float32\"), compute_2: T.Buffer((2, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((560,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(14, 20):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 20 + i2\n                compute_3 = T.Buffer((560,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(28):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((560,), data=compute_1.data)\n                compute_3[cse_var_2] = T.fabs(T.cos(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(560):\n            compute_3 = T.Buffer((560,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "cos",
                "abs",
                "asinh"
            ]
        ],
        "input_shape": [[2, 14, 20]],
        "output_shape": [[2, 14, 20], [2, 14, 20], [2, 14, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 340; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_add[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] + ph_3[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3400; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 10), \"float32\"), ph_3: T.Buffer((17, 20, 10), \"float32\"), T_add: T.Buffer((17, 20, 10), \"float32\"), compute: T.Buffer((17, 20, 10), \"float32\"), compute_1: T.Buffer((17, 20, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3400,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(340):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_add_1 = T.Buffer((3400,), data=T_add.data)\n                ph_3_1 = T.Buffer((3400,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(3400):\n            compute_2 = T.Buffer((3400,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3400):\n            compute_2 = T.Buffer((3400,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "asinh",
                "sin"
            ]
        ],
        "input_shape": [[17, 20, 10], [1, 2, 15], [17, 20, 10]],
        "output_shape": [[17, 20, 10], [1, 2, 15], [17, 20, 10], [17, 20, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 585; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 585; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] - (ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 585; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 13, 5), \"float32\"), ph_3: T.Buffer((9, 13, 5), \"float32\"), T_subtract: T.Buffer((9, 13, 5), \"float32\"), compute: T.Buffer((9, 13, 5), \"float32\"), compute_1: T.Buffer((9, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((585,), data=ph_0.data)\n        ph_3_1 = T.Buffer((585,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(585):\n            T_subtract_1 = T.Buffer((585,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - (ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(585):\n            compute_2 = T.Buffer((585,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] - (ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(585):\n            compute_2 = T.Buffer((585,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "subtract",
                "subtract",
                "subtract",
                "atanh",
                "abs"
            ]
        ],
        "input_shape": [[9, 13, 5], [8, 11, 7], [9, 13, 5]],
        "output_shape": [[8, 11, 7], [9, 13, 5], [9, 13, 5], [9, 13, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 840; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 840; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_1[(((i0 * 60) + (i1 * 15)) + i2)] = ceilf(ph_0[(((i0 * 60) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n      compute_2[((i0_i1_fused * 15) + i2_1)] = fabsf(asinf(ph_0[((i0_i1_fused * 15) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 840; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = sinf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 4, 15), \"float32\"), compute: T.Buffer((14, 4, 15), \"float32\"), T_multiply: T.Buffer((14, 4, 15), \"float32\"), compute_1: T.Buffer((14, 4, 15), \"float32\"), compute_2: T.Buffer((14, 4, 15), \"float32\"), compute_3: T.Buffer((14, 4, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((840,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            compute_4 = T.Buffer((840,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(840):\n            T_multiply_1 = T.Buffer((840,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(4, 15):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 15 + i2\n                compute_4 = T.Buffer((840,), data=compute_1.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(56):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_4 = T.Buffer((840,), data=compute_2.data)\n                compute_4[cse_var_2] = T.fabs(T.asin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            compute_4 = T.Buffer((840,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "acosh",
                "multiply",
                "ceil",
                "asin",
                "abs",
                "sin"
            ]
        ],
        "input_shape": [[14, 4, 15]],
        "output_shape": [[14, 4, 15], [14, 4, 15], [14, 4, 15], [14, 4, 15], [14, 4, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_divide[(((ax0 * 140) + (ax1 * 20)) + ax2)] = (ph_0[(((ax0 * 140) + (ax1 * 20)) + ax2)] / ph_3[(((ax0 * 140) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_1[(((i0 * 140) + (i1 * 20)) + i2)] = acoshf(ph_0[(((i0 * 140) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 7, 20), \"float32\"), ph_3: T.Buffer((9, 7, 20), \"float32\"), T_divide: T.Buffer((9, 7, 20), \"float32\"), compute: T.Buffer((9, 7, 20), \"float32\"), compute_1: T.Buffer((9, 7, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(7, 20):\n                cse_var_1: T.int32 = ax0 * 140 + ax1 * 20 + ax2\n                T_divide_1 = T.Buffer((1260,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1260,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_2 = T.Buffer((1260,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(7, 20):\n                cse_var_2: T.int32 = i0 * 140 + i1 * 20 + i2\n                compute_2 = T.Buffer((1260,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "cos",
                "acosh"
            ]
        ],
        "input_shape": [[9, 7, 20], [15, 19, 4], [9, 7, 20]],
        "output_shape": [[9, 7, 20], [15, 19, 4], [9, 7, 20], [9, 7, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 120) + (i1 * 12)) + i2)] = fabsf((ph_0[(((i0 * 120) + (i1 * 12)) + i2)] - fabsf(ph_0[(((i0 * 120) + (i1 * 12)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 11; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 10; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n        compute_1[(((i0_1 * 120) + (i1_1 * 12)) + i2_1)] = atanf(ph_0[(((i0_1 * 120) + (i1_1 * 12)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1320; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 10, 12), \"float32\"), compute: T.Buffer((11, 10, 12), \"float32\"), compute_1: T.Buffer((11, 10, 12), \"float32\"), compute_2: T.Buffer((11, 10, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1320,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(10, 12):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 12 + i2\n                compute_3 = T.Buffer((1320,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1] - T.fabs(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(10, 12):\n                cse_var_2: T.int32 = i0 * 120 + i1 * 12 + i2\n                compute_3 = T.Buffer((1320,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1320):\n            compute_3 = T.Buffer((1320,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "subtract",
                "abs",
                "atan",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[11, 10, 12]],
        "output_shape": [[11, 10, 12], [11, 10, 12], [11, 10, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1575; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_add[(((ax0 * 105) + (ax1 * 7)) + ax2)] = (ph_0[(((ax0 * 105) + (ax1 * 7)) + ax2)] + ph_3[(((ax0 * 105) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 105) + (i1 * 7)) + i2)] = ceilf(atanhf(ph_0[(((i0 * 105) + (i1 * 7)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ceilf(atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 15, 7), \"float32\"), ph_3: T.Buffer((15, 15, 7), \"float32\"), T_subtract: T.Buffer((15, 15, 7), \"float32\"), T_add: T.Buffer((15, 15, 7), \"float32\"), compute: T.Buffer((15, 15, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1575,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1575,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1575):\n            T_subtract_1 = T.Buffer((1575,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(15, 7):\n                cse_var_1: T.int32 = ax0 * 105 + ax1 * 7 + ax2\n                T_add_1 = T.Buffer((1575,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(15, 7):\n                cse_var_2: T.int32 = i0 * 105 + i1 * 7 + i2\n                compute_1 = T.Buffer((1575,), data=compute.data)\n                compute_1[cse_var_2] = T.ceil(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "subtract",
                "add",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[15, 15, 7], [17, 15, 7], [15, 15, 7]],
        "output_shape": [[15, 15, 7], [17, 15, 7], [15, 15, 7], [15, 15, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 19; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 16) + ax2)] = (ph_0[((ax0_ax1_fused * 16) + ax2)] * ph_3[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 19; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = atanhf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 304; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 304; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 19, 16), \"float32\"), ph_3: T.Buffer((1, 19, 16), \"float32\"), T_multiply: T.Buffer((1, 19, 16), \"float32\"), compute: T.Buffer((1, 19, 16), \"float32\"), compute_1: T.Buffer((1, 19, 16), \"float32\"), compute_2: T.Buffer((1, 19, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((304,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(19):\n            for ax2 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_multiply_1 = T.Buffer((304,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((304,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(19):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((304,), data=compute.data)\n                compute_3[cse_var_2] = T.atanh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(304):\n            compute_3 = T.Buffer((304,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(304):\n            compute_3 = T.Buffer((304,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "atanh",
                "acos",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[1, 19, 16], [5, 4, 16], [1, 19, 16]],
        "output_shape": [[1, 19, 16], [5, 4, 16], [1, 19, 16], [1, 19, 16], [1, 19, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 200; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] - asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 200; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] - asinhf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 25; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_2[((i0_i1_fused * 8) + i2)] = sinf(fmodf(ph_0[((i0_i1_fused * 8) + i2)], ph_3[((i0_i1_fused * 8) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 25; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_3[((i0_i1_fused_1 * 8) + i2_1)] = atanf(fmodf(ph_0[((i0_i1_fused_1 * 8) + i2_1)], ph_3[((i0_i1_fused_1 * 8) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 5, 8), \"float32\"), ph_3: T.Buffer((5, 5, 8), \"float32\"), compute: T.Buffer((5, 5, 8), \"float32\"), compute_1: T.Buffer((5, 5, 8), \"float32\"), compute_2: T.Buffer((5, 5, 8), \"float32\"), compute_3: T.Buffer((5, 5, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((200,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(200):\n            compute_4 = T.Buffer((200,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(200):\n            compute_4 = T.Buffer((200,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((200,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(25):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((200,), data=compute_2.data)\n                compute_4[cse_var_1] = T.sin(T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(25):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((200,), data=compute_3.data)\n                compute_4[cse_var_2] = T.atan(T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "asinh",
                "subtract",
                "exp",
                "sin",
                "sin",
                "atan"
            ]
        ],
        "input_shape": [[5, 5, 8], [19, 7, 16], [5, 5, 8]],
        "output_shape": [[19, 7, 16], [5, 5, 8], [5, 5, 8], [5, 5, 8], [5, 5, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3900; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3900; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3900; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 3900; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(cosf(ph_0[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 20, 13), \"float32\"), compute: T.Buffer((15, 20, 13), \"float32\"), T_multiply: T.Buffer((15, 20, 13), \"float32\"), compute_1: T.Buffer((15, 20, 13), \"float32\"), T_mod: T.Buffer((15, 20, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3900,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3900):\n            compute_2 = T.Buffer((3900,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3900):\n            T_multiply_1 = T.Buffer((3900,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3900):\n            compute_2 = T.Buffer((3900,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3900):\n            T_mod_1 = T.Buffer((3900,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "acosh",
                "cos",
                "multiply",
                "ceil",
                "cos",
                "mod"
            ]
        ],
        "input_shape": [[15, 20, 13]],
        "output_shape": [[15, 20, 13], [15, 20, 13], [15, 20, 13], [15, 20, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 90) + (i1 * 15)) + i2)] = fabsf(ph_0[(((i0 * 90) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1800; ++i0_i1_fused_i2_fused) {\n    float compute_2[1];\n    compute_2[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute_1[i0_i1_fused_i2_fused] = acoshf(compute_2[0]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(__expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 6, 15), \"float32\"), compute: T.Buffer((20, 6, 15), \"float32\"), compute_1: T.Buffer((20, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1800,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(6, 15):\n                cse_var_1: T.int32 = i0 * 90 + i1 * 15 + i2\n                compute_2 = T.Buffer((1800,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1800):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            compute_3 = T.Buffer((1,), data=compute_2, align=4)\n            compute_3[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_4 = T.Buffer((1800,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(compute_3[0])",
        "op_args": [
            [
                "abs",
                "exp",
                "acosh"
            ]
        ],
        "input_shape": [[20, 6, 15]],
        "output_shape": [[20, 6, 15], [20, 6, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2016; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2016; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf((ph_0[ax0_ax1_fused_ax2_fused_1] + fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1])), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 144; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_divide[((ax0_ax1_fused * 14) + ax2)] = ((ph_0[((ax0_ax1_fused * 14) + ax2)] - ph_3[((ax0_ax1_fused * 14) + ax2)]) / ph_0[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2016; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 18, 14), \"float32\"), ph_3: T.Buffer((8, 18, 14), \"float32\"), T_add: T.Buffer((8, 18, 14), \"float32\"), T_mod: T.Buffer((8, 18, 14), \"float32\"), T_divide: T.Buffer((8, 18, 14), \"float32\"), compute: T.Buffer((8, 18, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2016,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2016,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2016):\n            T_add_1 = T.Buffer((2016,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2016):\n            T_mod_1 = T.Buffer((2016,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] + T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused in T.parallel(144):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_divide_1 = T.Buffer((2016,), data=T_divide.data)\n                T_divide_1[cse_var_1] = (ph_0_1[cse_var_1] - ph_3_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2016):\n            compute_1 = T.Buffer((2016,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "mod",
                "add",
                "add",
                "mod",
                "divide",
                "exp"
            ]
        ],
        "input_shape": [[8, 18, 14], [2, 10, 18], [8, 18, 14]],
        "output_shape": [[2, 10, 18], [8, 18, 14], [8, 18, 14], [8, 18, 14], [8, 18, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 912; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 912; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute_1[(((i0 * 152) + (i1 * 19)) + i2)] = ceilf(atanf(ph_0[(((i0 * 152) + (i1 * 19)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 912; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 8, 19), \"float32\"), ph_3: T.Buffer((6, 8, 19), \"float32\"), T_multiply: T.Buffer((6, 8, 19), \"float32\"), compute: T.Buffer((6, 8, 19), \"float32\"), compute_1: T.Buffer((6, 8, 19), \"float32\"), compute_2: T.Buffer((6, 8, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((912,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(912):\n            T_multiply_1 = T.Buffer((912,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((912,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(912):\n            compute_3 = T.Buffer((912,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(8, 19):\n                cse_var_1: T.int32 = i0 * 152 + i1 * 19 + i2\n                compute_3 = T.Buffer((912,), data=compute_1.data)\n                compute_3[cse_var_1] = T.ceil(T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(912):\n            compute_3 = T.Buffer((912,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "abs",
                "atan",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[6, 8, 19], [19, 7, 7], [6, 8, 19]],
        "output_shape": [[6, 8, 19], [19, 7, 7], [6, 8, 19], [6, 8, 19], [6, 8, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2106; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 117; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = asinf(asinf(ph_0[((i0_i1_fused * 18) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2106; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2106; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanhf(ceilf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 13, 18), \"float32\"), compute: T.Buffer((9, 13, 18), \"float32\"), compute_1: T.Buffer((9, 13, 18), \"float32\"), compute_2: T.Buffer((9, 13, 18), \"float32\"), compute_3: T.Buffer((9, 13, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2106,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2106):\n            compute_4 = T.Buffer((2106,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_4 = T.Buffer((2106,), data=compute_1.data)\n                compute_4[cse_var_1] = T.asin(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2106):\n            compute_4 = T.Buffer((2106,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2106):\n            compute_4 = T.Buffer((2106,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "asin",
                "asin",
                "exp",
                "ceil",
                "atanh"
            ]
        ],
        "input_shape": [[9, 13, 18]],
        "output_shape": [[9, 13, 18], [9, 13, 18], [9, 13, 18], [9, 13, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2040; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_add[(((ax0 * 136) + (ax1 * 17)) + ax2)] = (ph_0[(((ax0 * 136) + (ax1 * 17)) + ax2)] + ph_3[(((ax0 * 136) + (ax1 * 17)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 136) + (i1 * 17)) + i2)] = asinhf(ph_0[(((i0 * 136) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 8, 17), \"float32\"), ph_3: T.Buffer((15, 8, 17), \"float32\"), T_divide: T.Buffer((15, 8, 17), \"float32\"), T_add: T.Buffer((15, 8, 17), \"float32\"), compute: T.Buffer((15, 8, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2040,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2040,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2040):\n            T_divide_1 = T.Buffer((2040,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(8, 17):\n                cse_var_1: T.int32 = ax0 * 136 + ax1 * 17 + ax2\n                T_add_1 = T.Buffer((2040,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(8, 17):\n                cse_var_2: T.int32 = i0 * 136 + i1 * 17 + i2\n                compute_1 = T.Buffer((2040,), data=compute.data)\n                compute_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "add",
                "asinh"
            ]
        ],
        "input_shape": [[15, 8, 17], [1, 15, 5], [15, 8, 17]],
        "output_shape": [[15, 8, 17], [1, 15, 5], [15, 8, 17], [15, 8, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[60];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax5 = 0; ax5 < 3; ++ax5) {\n    for (int32_t ax7 = 0; ax7 < 20; ++ax7) {\n      auto_scheduler_layout_transform[((ax5 * 20) + ax7)] = ph_4[((ax5 * 20) + ax7)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused < 4; ++b_outer_outer_outer_i_outer_outer_outer_fused) {\n    float compute_1[120];\n    for (int32_t i0 = 0; i0 < 3; ++i0) {\n      for (int32_t i1 = 0; i1 < 2; ++i1) {\n        for (int32_t i2 = 0; i2 < 20; ++i2) {\n          compute_1[(((i0 * 40) + (i1 * 20)) + i2)] = expf(ph_0[((((i0 * 160) + (b_outer_outer_outer_i_outer_outer_outer_fused * 40)) + (i1 * 20)) + i2)]);\n        }\n      }\n    }\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 3; ++b_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 8) + (b_outer_outer_outer_i_outer_outer_outer_fused * 2)) + i_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 3; ++b_outer_inner) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 20; ++k_inner) {\n          T_batch_matmul_NN[(((b_outer_inner * 8) + (b_outer_outer_outer_i_outer_outer_outer_fused * 2)) + i_outer_inner)] = (T_batch_matmul_NN[(((b_outer_inner * 8) + (b_outer_outer_outer_i_outer_outer_outer_fused * 2)) + i_outer_inner)] + (compute_1[(((b_outer_inner * 40) + (i_outer_inner * 20)) + k_inner)] * auto_scheduler_layout_transform[((b_outer_inner * 20) + k_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float compute_shared[160];\n  __shared__ float ph_4_shared[40];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_inner_init + 2)] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 20; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((((ax0_ax1_fused_ax2_fused_outer_outer * 2) + (((int)threadIdx.x) >> 2)) / 5) * 40) + (((int)blockIdx.x) * 20)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 8) + ((int)threadIdx.x)) % 20))]);\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 5; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_4_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 8) + ((int)threadIdx.x))] = ph_4[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 8) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (compute_shared[(((((((int)threadIdx.x) >> 1) * 40) + (b_c_inner * 20)) + ((((int)threadIdx.x) & 1) * 5)) + k_outer_inner)] * ph_4_shared[((((((int)threadIdx.x) >> 1) * 10) + (b_c_inner * 5)) + k_outer_inner)]));\n      T_batch_matmul_NN_local[(b_c_inner + 2)] = (T_batch_matmul_NN_local[(b_c_inner + 2)] + (compute_shared[((((((((int)threadIdx.x) >> 1) * 40) + (b_c_inner * 20)) + ((((int)threadIdx.x) & 1) * 5)) + k_outer_inner) + 10)] * ph_4_shared[((((((int)threadIdx.x) >> 1) * 10) + (b_c_inner * 5)) + k_outer_inner)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[(((((((int)threadIdx.x) >> 1) * 16) + (b_inner * 8)) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[((((((((int)threadIdx.x) >> 1) * 16) + (b_inner * 8)) + (((int)blockIdx.x) * 4)) + (((int)threadIdx.x) & 1)) + 2)] = T_batch_matmul_NN_local[(b_inner + 2)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 8, 20), \"float32\"), ph_4: T.Buffer((3, 20, 1), \"float32\"), compute: T.Buffer((3, 8, 20), \"float32\"), T_batch_matmul_NN: T.Buffer((3, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([60], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((60,), data=auto_scheduler_layout_transform)\n        for ax5, ax7 in T.grid(3, 20):\n            cse_var_1: T.int32 = ax5 * 20 + ax7\n            ph_4_1 = T.Buffer((60,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[cse_var_1] = ph_4_1[cse_var_1]\n        for b_outer_outer_outer_i_outer_outer_outer_fused in T.parallel(4):\n            compute_1 = T.allocate([120], \"float32\", \"global\")\n            compute_2 = T.Buffer((120,), data=compute_1)\n            for i0, i1, i2 in T.grid(3, 2, 20):\n                cse_var_2: T.int32 = i1 * 20\n                compute_2[i0 * 40 + cse_var_2 + i2] = T.exp(ph_0_1[i0 * 160 + b_outer_outer_outer_i_outer_outer_outer_fused * 40 + cse_var_2 + i2])\n            T_batch_matmul_NN_1 = T.Buffer((24,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_outer_inner_init in T.grid(3, 2):\n                T_batch_matmul_NN_1[b_outer_inner_init * 8 + b_outer_outer_outer_i_outer_outer_outer_fused * 2 + i_outer_inner_init] = T.float32(0)\n            for b_outer_inner, i_outer_inner, k_inner in T.grid(3, 2, 20):\n                cse_var_3: T.int32 = b_outer_inner * 8 + b_outer_outer_outer_i_outer_outer_outer_fused * 2 + i_outer_inner\n                T_batch_matmul_NN_1[cse_var_3] = T_batch_matmul_NN_1[cse_var_3] + compute_2[b_outer_inner * 40 + i_outer_inner * 20 + k_inner] * auto_scheduler_layout_transform_1[b_outer_inner * 20 + k_inner]",
        "op_args": [
            [
                "asinh",
                "exp",
                "batch_matmul"
            ]
        ],
        "input_shape": [[3, 8, 20], [3, 20, 1]],
        "output_shape": [[3, 8, 20], [3, 8, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 30; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_divide[((ax0_ax1_fused * 13) + ax2)] = (ph_0[((ax0_ax1_fused * 13) + ax2)] / ph_3[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 13; ++ax2_1) {\n        T_subtract[(((ax0 * 78) + (ax1 * 13)) + ax2_1)] = (ph_0[(((ax0 * 78) + (ax1 * 13)) + ax2_1)] - ph_3[(((ax0 * 78) + (ax1 * 13)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 390; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 6, 13), \"float32\"), ph_3: T.Buffer((5, 6, 13), \"float32\"), T_divide: T.Buffer((5, 6, 13), \"float32\"), T_subtract: T.Buffer((5, 6, 13), \"float32\"), compute: T.Buffer((5, 6, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((390,), data=ph_0.data)\n        ph_3_1 = T.Buffer((390,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(30):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_divide_1 = T.Buffer((390,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(6, 13):\n                cse_var_2: T.int32 = ax0 * 78 + ax1 * 13 + ax2\n                T_subtract_1 = T.Buffer((390,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] - ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(390):\n            compute_1 = T.Buffer((390,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "subtract",
                "atanh",
                "abs"
            ]
        ],
        "input_shape": [[5, 6, 13], [19, 10, 13], [5, 6, 13]],
        "output_shape": [[5, 6, 13], [19, 10, 13], [5, 6, 13], [5, 6, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_divide[(((ax0 * 80) + (ax1 * 8)) + ax2)] = (ph_0[(((ax0 * 80) + (ax1 * 8)) + ax2)] / ph_3[(((ax0 * 80) + (ax1 * 8)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 80) + (i1 * 8)) + i2)] = cosf(ph_0[(((i0 * 80) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 10, 8), \"float32\"), ph_3: T.Buffer((6, 10, 8), \"float32\"), T_divide: T.Buffer((6, 10, 8), \"float32\"), compute: T.Buffer((6, 10, 8), \"float32\"), compute_1: T.Buffer((6, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(10, 8):\n                cse_var_1: T.int32 = ax0 * 80 + ax1 * 8 + ax2\n                T_divide_1 = T.Buffer((480,), data=T_divide.data)\n                ph_3_1 = T.Buffer((480,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(10, 8):\n                cse_var_2: T.int32 = i0 * 80 + i1 * 8 + i2\n                compute_2 = T.Buffer((480,), data=compute.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_2 = T.Buffer((480,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "cos",
                "asin"
            ]
        ],
        "input_shape": [[6, 10, 8], [3, 5, 3], [6, 10, 8]],
        "output_shape": [[6, 10, 8], [3, 5, 3], [6, 10, 8], [6, 10, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 10; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_mod[((ax0_ax1_fused * 11) + ax2)] = fmodf((ph_0[((ax0_ax1_fused * 11) + ax2)] + fmodf(ph_0[((ax0_ax1_fused * 11) + ax2)], ph_3[((ax0_ax1_fused * 11) + ax2)])), ph_0[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 110; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] + fmodf(ph_0[i0_i1_fused_i2_fused], ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 110; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 1, 11), \"float32\"), ph_3: T.Buffer((10, 1, 11), \"float32\"), T_mod: T.Buffer((10, 1, 11), \"float32\"), compute: T.Buffer((10, 1, 11), \"float32\"), T_divide: T.Buffer((10, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((110,), data=ph_0.data)\n        ph_3_1 = T.Buffer((110,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(10):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_mod_1 = T.Buffer((110,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1] + T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(110):\n            compute_1 = T.Buffer((110,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] + T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(110):\n            T_divide_1 = T.Buffer((110,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "mod",
                "add",
                "mod",
                "atanh",
                "divide"
            ]
        ],
        "input_shape": [[10, 1, 11], [9, 5, 16], [10, 1, 11]],
        "output_shape": [[9, 5, 16], [10, 1, 11], [10, 1, 11], [10, 1, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1088; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1088; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1088; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 68; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_3[((i0_i1_fused * 16) + i2)] = cosf(asinhf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1088; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 17, 16), \"float32\"), compute: T.Buffer((4, 17, 16), \"float32\"), compute_1: T.Buffer((4, 17, 16), \"float32\"), compute_2: T.Buffer((4, 17, 16), \"float32\"), compute_3: T.Buffer((4, 17, 16), \"float32\"), T_subtract: T.Buffer((4, 17, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1088,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_4 = T.Buffer((1088,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_4 = T.Buffer((1088,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_4 = T.Buffer((1088,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(68):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((1088,), data=compute_3.data)\n                compute_4[cse_var_1] = T.cos(T.asinh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1088):\n            T_subtract_1 = T.Buffer((1088,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "cos",
                "atanh",
                "atanh",
                "asinh",
                "asinh",
                "cos",
                "subtract"
            ]
        ],
        "input_shape": [[4, 17, 16]],
        "output_shape": [[4, 17, 16], [4, 17, 16], [4, 17, 16], [4, 17, 16], [4, 17, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 228) + (i1 * 12)) + i2)] = acosf(ph_0[(((i0 * 228) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 209; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      compute_1[((i0_i1_fused * 12) + i2_1)] = asinhf(ph_0[((i0_i1_fused * 12) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 19, 12), \"float32\"), compute: T.Buffer((11, 19, 12), \"float32\"), compute_1: T.Buffer((11, 19, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2508,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(19, 12):\n                cse_var_1: T.int32 = i0 * 228 + i1 * 12 + i2\n                compute_2 = T.Buffer((2508,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(209):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((2508,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[11, 19, 12]],
        "output_shape": [[11, 19, 12], [11, 19, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_divide[((ax1 * 12) + ax2)] = (acoshf(ph_0[((ax1 * 12) + ax2)]) / ph_0[((ax1 * 12) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 132; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 132; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 132; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf((ph_0[i0_i1_fused_i2_fused_1] * ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((int)blockIdx.x)] = (acoshf(ph_0[((int)blockIdx.x)]) / ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 12), \"float32\"), ph_3: T.Buffer((1, 11, 12), \"float32\"), T_add: T.Buffer((1, 11, 12), \"float32\"), T_divide: T.Buffer((1, 11, 12), \"float32\"), compute: T.Buffer((1, 11, 12), \"float32\"), compute_1: T.Buffer((1, 11, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((132,), data=ph_0.data)\n        for ax1, ax2 in T.grid(11, 12):\n            cse_var_1: T.int32 = ax1 * 12 + ax2\n            T_divide_1 = T.Buffer((132,), data=T_divide.data)\n            T_divide_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(132):\n            compute_2 = T.Buffer((132,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((132,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(132):\n            T_add_1 = T.Buffer((132,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(132):\n            compute_2 = T.Buffer((132,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "add",
                "acosh",
                "divide",
                "acosh",
                "abs"
            ]
        ],
        "input_shape": [[1, 11, 12], [19, 13, 13], [1, 11, 12]],
        "output_shape": [[19, 13, 13], [1, 11, 12], [1, 11, 12], [1, 11, 12], [1, 11, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 510; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] + (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 510; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] + (ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 510; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute_2[(((i0 * 255) + (i1 * 17)) + i2)] = fabsf((ph_0[(((i0 * 255) + (i1 * 17)) + i2)] * ph_3[(((i0 * 255) + (i1 * 17)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 15, 17), \"float32\"), ph_3: T.Buffer((2, 15, 17), \"float32\"), compute: T.Buffer((2, 15, 17), \"float32\"), compute_1: T.Buffer((2, 15, 17), \"float32\"), T_mod: T.Buffer((2, 15, 17), \"float32\"), compute_2: T.Buffer((2, 15, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((510,), data=ph_0.data)\n        ph_3_1 = T.Buffer((510,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(510):\n            compute_3 = T.Buffer((510,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] + ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(510):\n            compute_3 = T.Buffer((510,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] + ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(510):\n            T_mod_1 = T.Buffer((510,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused], ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(15, 17):\n                cse_var_1: T.int32 = i0 * 255 + i1 * 17 + i2\n                compute_3 = T.Buffer((510,), data=compute_2.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1] * ph_3_1[cse_var_1])",
        "op_args": [
            [
                "multiply",
                "divide",
                "add",
                "abs",
                "sin",
                "mod",
                "abs"
            ]
        ],
        "input_shape": [[2, 15, 17], [11, 17, 11], [2, 15, 17]],
        "output_shape": [[11, 17, 11], [2, 15, 17], [2, 15, 17], [2, 15, 17], [2, 15, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_subtract[(((ax0 * 36) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 36) + (ax1 * 9)) + ax2)] - ph_3[(((ax0 * 36) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 28; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 9; ++ax2_1) {\n      T_multiply[((ax0_ax1_fused * 9) + ax2_1)] = ((ph_0[((ax0_ax1_fused * 9) + ax2_1)] * asinf(ph_0[((ax0_ax1_fused * 9) + ax2_1)])) * ph_0[((ax0_ax1_fused * 9) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 252; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf((ph_0[i0_i1_fused_i2_fused] * asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((int)blockIdx.x)] = ((ph_0[((int)blockIdx.x)] * asinf(ph_0[((int)blockIdx.x)])) * ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 4, 9), \"float32\"), ph_3: T.Buffer((7, 4, 9), \"float32\"), T_subtract: T.Buffer((7, 4, 9), \"float32\"), T_multiply: T.Buffer((7, 4, 9), \"float32\"), compute: T.Buffer((7, 4, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((252,), data=ph_0.data)\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(4, 9):\n                cse_var_1: T.int32 = ax0 * 36 + ax1 * 9 + ax2\n                T_subtract_1 = T.Buffer((252,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((252,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(28):\n            for ax2 in range(9):\n                cse_var_2: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_multiply_1 = T.Buffer((252,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] * T.asin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(252):\n            compute_1 = T.Buffer((252,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] * T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "asin",
                "multiply",
                "multiply",
                "sin"
            ]
        ],
        "input_shape": [[7, 4, 9], [18, 13, 12], [7, 4, 9]],
        "output_shape": [[7, 4, 9], [18, 13, 12], [7, 4, 9], [7, 4, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinhf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_divide[(((ax0 * 15) + (ax1 * 5)) + ax2)] = (ph_0[(((ax0 * 15) + (ax1 * 5)) + ax2)] / (atanhf(ph_0[(((ax0 * 15) + (ax1 * 5)) + ax2)]) + ph_0[(((ax0 * 15) + (ax1 * 5)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 300; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / (atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 3, 5), \"float32\"), compute: T.Buffer((20, 3, 5), \"float32\"), T_divide: T.Buffer((20, 3, 5), \"float32\"), compute_1: T.Buffer((20, 3, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((300,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((300,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(3, 5):\n                cse_var_2: T.int32 = ax0 * 15 + ax1 * 5 + ax2\n                T_divide_1 = T.Buffer((300,), data=T_divide.data)\n                T_divide_1[cse_var_2] = ph_0_1[cse_var_2] / (T.atanh(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            compute_2 = T.Buffer((300,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "atanh",
                "add",
                "divide",
                "atan",
                "atanh"
            ]
        ],
        "input_shape": [[20, 3, 5]],
        "output_shape": [[20, 3, 5], [20, 3, 5], [20, 3, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 224; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 14) + ax2)] = (ph_0[((ax0_ax1_fused * 14) + ax2)] - atanf(ph_0[((ax0_ax1_fused * 14) + ax2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 14, 14), \"float32\"), T_subtract: T.Buffer((16, 14, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(224):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_subtract_1 = T.Buffer((3136,), data=T_subtract.data)\n                ph_0_1 = T.Buffer((3136,), data=ph_0.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "subtract"
            ]
        ],
        "input_shape": [[16, 14, 14]],
        "output_shape": [[16, 14, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 27; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = acoshf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_add[(((ax0 * 27) + (ax1 * 3)) + ax2)] = (ceilf(ph_0[(((ax0 * 27) + (ax1 * 3)) + ax2)]) + ph_0[(((ax0 * 27) + (ax1 * 3)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 81; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 9, 3), \"float32\"), compute: T.Buffer((3, 9, 3), \"float32\"), T_add: T.Buffer((3, 9, 3), \"float32\"), compute_1: T.Buffer((3, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((81,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(27):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((81,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(9, 3):\n                cse_var_2: T.int32 = ax0 * 27 + ax1 * 3 + ax2\n                T_add_1 = T.Buffer((81,), data=T_add.data)\n                T_add_1[cse_var_2] = T.ceil(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(81):\n            compute_2 = T.Buffer((81,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "ceil",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[3, 9, 3]],
        "output_shape": [[3, 9, 3], [3, 9, 3], [3, 9, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_multiply[((ax0 * 19) + ax2)] = (ph_0[((ax0 * 19) + ax2)] * ph_3[((ax0 * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 114; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 114; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 1, 19), \"float32\"), ph_3: T.Buffer((6, 1, 19), \"float32\"), T_multiply: T.Buffer((6, 1, 19), \"float32\"), compute: T.Buffer((6, 1, 19), \"float32\"), compute_1: T.Buffer((6, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((114,), data=ph_0.data)\n        for ax0 in T.parallel(6):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0 * 19 + ax2\n                T_multiply_1 = T.Buffer((114,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((114,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(114):\n            compute_2 = T.Buffer((114,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(114):\n            compute_2 = T.Buffer((114,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "atanh",
                "atanh"
            ]
        ],
        "input_shape": [[6, 1, 19], [2, 2, 14], [6, 1, 19]],
        "output_shape": [[6, 1, 19], [2, 2, 14], [6, 1, 19], [6, 1, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    float compute_1[1];\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      compute_1[0] = expf(ph_0[((ax0 * 18) + ax2)]);\n      T_subtract[((ax0 * 18) + ax2)] = ((ph_0[((ax0 * 18) + ax2)] - compute_1[0]) - ph_0[((ax0 * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0 * 18) + i2)] = asinf(ph_0[((i0 * 18) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] - __expf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 1, 18), \"float32\"), T_subtract: T.Buffer((7, 1, 18), \"float32\"), compute: T.Buffer((7, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((126,), data=ph_0.data)\n        for ax0 in T.parallel(7):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0 * 18 + ax2\n                compute_2 = T.Buffer((1,), data=compute_1, align=4)\n                compute_2[0] = T.exp(ph_0_1[cse_var_1])\n                T_subtract_1 = T.Buffer((126,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - compute_2[0] - ph_0_1[cse_var_1]\n        for i0 in T.parallel(7):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0 * 18 + i2\n                compute_1 = T.Buffer((126,), data=compute.data)\n                compute_1[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "exp",
                "subtract",
                "subtract",
                "asin"
            ]
        ],
        "input_shape": [[7, 1, 18]],
        "output_shape": [[7, 1, 18], [7, 1, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 910; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 910; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 65; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_mod[((ax0_ax1_fused * 14) + ax2)] = fmodf(ceilf(ph_0[((ax0_ax1_fused * 14) + ax2)]), ph_0[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 910; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf((ph_0[i0_i1_fused_i2_fused_2] * ph_3[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 910; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = asinf((ph_0[i0_i1_fused_i2_fused_3] * ph_3[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 13, 14), \"float32\"), ph_3: T.Buffer((5, 13, 14), \"float32\"), compute: T.Buffer((5, 13, 14), \"float32\"), compute_1: T.Buffer((5, 13, 14), \"float32\"), T_mod: T.Buffer((5, 13, 14), \"float32\"), compute_2: T.Buffer((5, 13, 14), \"float32\"), compute_3: T.Buffer((5, 13, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((910,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(910):\n            compute_4 = T.Buffer((910,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(910):\n            compute_4 = T.Buffer((910,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(65):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_mod_1 = T.Buffer((910,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.ceil(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        ph_3_1 = T.Buffer((910,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(910):\n            compute_4 = T.Buffer((910,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(910):\n            compute_4 = T.Buffer((910,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "atan",
                "ceil",
                "exp",
                "mod",
                "ceil",
                "asin"
            ]
        ],
        "input_shape": [[5, 13, 14], [2, 12, 3], [5, 13, 14]],
        "output_shape": [[2, 12, 3], [5, 13, 14], [5, 13, 14], [5, 13, 14], [5, 13, 14], [5, 13, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 16; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], acosf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  for (int32_t i1 = 0; i1 < 2; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i1 * 8) + i2)] = atanhf(ph_0[((i1 * 8) + i2)]);\n    }\n  }\n  for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i1_1 * 8) + i2_1)] = atanf(ceilf(ph_0[((i1_1 * 8) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 2, 8), \"float32\"), T_multiply: T.Buffer((1, 2, 8), \"float32\"), compute: T.Buffer((1, 2, 8), \"float32\"), compute_1: T.Buffer((1, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((16,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(16):\n            T_multiply_1 = T.Buffer((16,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i1, i2 in T.grid(2, 8):\n            cse_var_1: T.int32 = i1 * 8 + i2\n            compute_2 = T.Buffer((16,), data=compute.data)\n            compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i1, i2 in T.grid(2, 8):\n            cse_var_2: T.int32 = i1 * 8 + i2\n            compute_2 = T.Buffer((16,), data=compute_1.data)\n            compute_2[cse_var_2] = T.atan(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acos",
                "mod",
                "multiply",
                "atanh",
                "ceil",
                "atan"
            ]
        ],
        "input_shape": [[1, 2, 8]],
        "output_shape": [[1, 2, 8], [1, 2, 8], [1, 2, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 392; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 392; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] + acosf(ph_0[ax0_ax1_fused_ax2_fused_1])) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = fabsf((ph_0[((i0_i1_fused * 7) + i2)] + acosf(ph_0[((i0_i1_fused * 7) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 7, 7), \"float32\"), ph_3: T.Buffer((8, 7, 7), \"float32\"), T_add: T.Buffer((8, 7, 7), \"float32\"), T_multiply: T.Buffer((8, 7, 7), \"float32\"), compute: T.Buffer((8, 7, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((392,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(392):\n            T_add_1 = T.Buffer((392,), data=T_add.data)\n            ph_3_1 = T.Buffer((392,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(392):\n            T_multiply_1 = T.Buffer((392,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(56):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_1 = T.Buffer((392,), data=compute.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "add",
                "acos",
                "add",
                "multiply",
                "abs"
            ]
        ],
        "input_shape": [[8, 7, 7], [16, 18, 5], [8, 7, 7]],
        "output_shape": [[8, 7, 7], [16, 18, 5], [8, 7, 7], [8, 7, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 150; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 150; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 75) + (i1 * 15)) + i2)] = acoshf(atanf(ph_0[(((i0 * 75) + (i1 * 15)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 150; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 5, 15), \"float32\"), ph_3: T.Buffer((2, 5, 15), \"float32\"), T_add: T.Buffer((2, 5, 15), \"float32\"), T_subtract: T.Buffer((2, 5, 15), \"float32\"), compute: T.Buffer((2, 5, 15), \"float32\"), compute_1: T.Buffer((2, 5, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        ph_3_1 = T.Buffer((150,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_add_1 = T.Buffer((150,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_subtract_1 = T.Buffer((150,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(5, 15):\n                cse_var_1: T.int32 = i0 * 75 + i1 * 15 + i2\n                compute_2 = T.Buffer((150,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_2 = T.Buffer((150,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "subtract",
                "atan",
                "acosh",
                "sin"
            ]
        ],
        "input_shape": [[2, 5, 15], [5, 1, 7], [2, 5, 15]],
        "output_shape": [[2, 5, 15], [5, 1, 7], [2, 5, 15], [2, 5, 15], [2, 5, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 21) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 21) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 126; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 6; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_2[(((i0_1 * 21) + (i1_1 * 7)) + i2_1)] = atanf(ph_0[(((i0_1 * 21) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 3, 7), \"float32\"), compute: T.Buffer((6, 3, 7), \"float32\"), compute_1: T.Buffer((6, 3, 7), \"float32\"), compute_2: T.Buffer((6, 3, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((126,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(3, 7):\n                cse_var_1: T.int32 = i0 * 21 + i1 * 7 + i2\n                compute_3 = T.Buffer((126,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_3 = T.Buffer((126,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(3, 7):\n                cse_var_2: T.int32 = i0 * 21 + i1 * 7 + i2\n                compute_3 = T.Buffer((126,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atan",
                "atan",
                "abs",
                "atan"
            ]
        ],
        "input_shape": [[6, 3, 7]],
        "output_shape": [[6, 3, 7], [6, 3, 7], [6, 3, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 36) + (i1 * 2)) + i2)] = expf(ph_0[(((i0 * 36) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_1[((i0_i1_fused * 2) + i2_1)] = acoshf(ceilf(ph_0[((i0_i1_fused * 2) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 18, 2), \"float32\"), compute: T.Buffer((5, 18, 2), \"float32\"), compute_1: T.Buffer((5, 18, 2), \"float32\"), compute_2: T.Buffer((5, 18, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(18, 2):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 2 + i2\n                compute_3 = T.Buffer((180,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(90):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((180,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acosh(T.ceil(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "ceil",
                "acosh",
                "ceil"
            ]
        ],
        "input_shape": [[5, 18, 2]],
        "output_shape": [[5, 18, 2], [5, 18, 2], [5, 18, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_subtract[(((ax0 * 160) + (ax1 * 10)) + ax2)] = (asinf(ph_0[(((ax0 * 160) + (ax1 * 10)) + ax2)]) - ph_0[(((ax0 * 160) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1920; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1920; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 16, 10), \"float32\"), ph_3: T.Buffer((12, 16, 10), \"float32\"), compute: T.Buffer((12, 16, 10), \"float32\"), T_subtract: T.Buffer((12, 16, 10), \"float32\"), compute_1: T.Buffer((12, 16, 10), \"float32\"), compute_2: T.Buffer((12, 16, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1920,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            compute_3 = T.Buffer((1920,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(16, 10):\n                cse_var_1: T.int32 = ax0 * 160 + ax1 * 10 + ax2\n                T_subtract_1 = T.Buffer((1920,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            compute_3 = T.Buffer((1920,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            compute_3 = T.Buffer((1920,), data=compute_2.data)\n            ph_3_1 = T.Buffer((1920,), data=ph_3.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "exp",
                "asin",
                "subtract",
                "atanh",
                "atan"
            ]
        ],
        "input_shape": [[12, 16, 10], [8, 10, 10], [12, 16, 10]],
        "output_shape": [[8, 10, 10], [12, 16, 10], [12, 16, 10], [12, 16, 10], [12, 16, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 32; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_mod[((ax0_ax1_fused * 9) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 9) + ax2)], ph_3[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 288; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 18) + (i1 * 9)) + i2)] = expf(acoshf(ph_0[(((i0 * 18) + (i1 * 9)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 2, 9), \"float32\"), ph_3: T.Buffer((16, 2, 9), \"float32\"), T_mod: T.Buffer((16, 2, 9), \"float32\"), T_add: T.Buffer((16, 2, 9), \"float32\"), compute: T.Buffer((16, 2, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((288,), data=ph_0.data)\n        ph_3_1 = T.Buffer((288,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(32):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_mod_1 = T.Buffer((288,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(288):\n            T_add_1 = T.Buffer((288,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(2, 9):\n                cse_var_2: T.int32 = i0 * 18 + i1 * 9 + i2\n                compute_1 = T.Buffer((288,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "add",
                "acosh",
                "exp"
            ]
        ],
        "input_shape": [[16, 2, 9], [19, 5, 18], [16, 2, 9]],
        "output_shape": [[16, 2, 9], [19, 5, 18], [16, 2, 9], [16, 2, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 594; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 594; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 594; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 594; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = expf(sinf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_4[(((i0 * 66) + (i1 * 6)) + i2)] = asinf(sinf(ph_0[(((i0 * 66) + (i1 * 6)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 11, 6), \"float32\"), compute: T.Buffer((9, 11, 6), \"float32\"), compute_1: T.Buffer((9, 11, 6), \"float32\"), compute_2: T.Buffer((9, 11, 6), \"float32\"), compute_3: T.Buffer((9, 11, 6), \"float32\"), compute_4: T.Buffer((9, 11, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((594,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(11, 6):\n                cse_var_1: T.int32 = i0 * 66 + i1 * 6 + i2\n                compute_5 = T.Buffer((594,), data=compute_4.data)\n                compute_5[cse_var_1] = T.asin(T.sin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "ceil",
                "asin",
                "atanh",
                "acos",
                "sin",
                "exp",
                "asin"
            ]
        ],
        "input_shape": [[9, 11, 6]],
        "output_shape": [[9, 11, 6], [9, 11, 6], [9, 11, 6], [9, 11, 6], [9, 11, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2475; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2475; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf((ph_0[ax0_ax1_fused_ax2_fused_1] / atanhf(ph_0[ax0_ax1_fused_ax2_fused_1])), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 225) + (i1 * 15)) + i2)] = asinhf((ph_0[(((i0 * 225) + (i1 * 15)) + i2)] / atanhf(ph_0[(((i0 * 225) + (i1 * 15)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf((ph_0[((int)blockIdx.x)] / atanhf(ph_0[((int)blockIdx.x)])), ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 15, 15), \"float32\"), ph_3: T.Buffer((11, 15, 15), \"float32\"), T_subtract: T.Buffer((11, 15, 15), \"float32\"), T_mod: T.Buffer((11, 15, 15), \"float32\"), compute: T.Buffer((11, 15, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2475,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2475):\n            T_subtract_1 = T.Buffer((2475,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2475,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2475):\n            T_mod_1 = T.Buffer((2475,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] / T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(15, 15):\n                cse_var_1: T.int32 = i0 * 225 + i1 * 15 + i2\n                compute_1 = T.Buffer((2475,), data=compute.data)\n                compute_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1] / T.atanh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "subtract",
                "atanh",
                "divide",
                "mod",
                "asinh"
            ]
        ],
        "input_shape": [[11, 15, 15], [17, 4, 13], [11, 15, 15]],
        "output_shape": [[11, 15, 15], [17, 4, 13], [11, 15, 15], [11, 15, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 243; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 81; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_divide[((ax0_ax1_fused * 3) + ax2)] = (ph_0[((ax0_ax1_fused * 3) + ax2)] / sinf(sinf(ph_0[((ax0_ax1_fused * 3) + ax2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 243; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __sinf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 9, 3), \"float32\"), compute: T.Buffer((9, 9, 3), \"float32\"), T_divide: T.Buffer((9, 9, 3), \"float32\"), compute_1: T.Buffer((9, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((243,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(243):\n            compute_2 = T.Buffer((243,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(81):\n            for ax2 in range(3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_divide_1 = T.Buffer((243,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.sin(T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(243):\n            compute_2 = T.Buffer((243,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "sin",
                "sin",
                "divide",
                "ceil",
                "abs"
            ]
        ],
        "input_shape": [[9, 9, 3]],
        "output_shape": [[9, 9, 3], [9, 9, 3], [9, 9, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 700; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 35) + (i1 * 5)) + i2)] = sinf(atanhf(ph_0[(((i0 * 35) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 20; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n        compute_2[(((i0_1 * 35) + (i1_1 * 5)) + i2_1)] = ceilf(ph_0[(((i0_1 * 35) + (i1_1 * 5)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 7, 5), \"float32\"), compute: T.Buffer((20, 7, 5), \"float32\"), compute_1: T.Buffer((20, 7, 5), \"float32\"), compute_2: T.Buffer((20, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((700,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(700):\n            compute_3 = T.Buffer((700,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(7, 5):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 5 + i2\n                compute_3 = T.Buffer((700,), data=compute_1.data)\n                compute_3[cse_var_1] = T.sin(T.atanh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(7, 5):\n                cse_var_2: T.int32 = i0 * 35 + i1 * 5 + i2\n                compute_3 = T.Buffer((700,), data=compute_2.data)\n                compute_3[cse_var_2] = T.ceil(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atanh",
                "atanh",
                "sin",
                "ceil"
            ]
        ],
        "input_shape": [[20, 7, 5]],
        "output_shape": [[20, 7, 5], [20, 7, 5], [20, 7, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 320; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 32) + (i1 * 16)) + i2)] = asinf(sinf(ph_0[(((i0 * 32) + (i1 * 16)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 320; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 2, 16), \"float32\"), compute: T.Buffer((10, 2, 16), \"float32\"), compute_1: T.Buffer((10, 2, 16), \"float32\"), compute_2: T.Buffer((10, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((320,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            compute_3 = T.Buffer((320,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(2, 16):\n                cse_var_1: T.int32 = i0 * 32 + i1 * 16 + i2\n                compute_3 = T.Buffer((320,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asin(T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            compute_3 = T.Buffer((320,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "sin",
                "asin",
                "exp"
            ]
        ],
        "input_shape": [[10, 2, 16]],
        "output_shape": [[10, 2, 16], [10, 2, 16], [10, 2, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[18];\n  for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_2[i2] = expf(ph_0[((ax1 * 18) + i2)]);\n    }\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_subtract[((ax1 * 18) + ax2)] = compute_2[ax2];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute[((i0_i1_fused * 18) + i2_1)] = expf((ph_0[((i0_i1_fused * 18) + i2_1)] * fabsf(ph_0[((i0_i1_fused * 18) + i2_1)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 72; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf((ph_0[i0_i1_fused_i2_fused] * fabsf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf((ph_0[((int)blockIdx.x)] * fabsf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((int)blockIdx.x)] = __expf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 18), \"float32\"), T_subtract: T.Buffer((1, 4, 18), \"float32\"), compute: T.Buffer((1, 4, 18), \"float32\"), compute_1: T.Buffer((1, 4, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([18], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((72,), data=ph_0.data)\n        for ax1 in range(4):\n            compute_3 = T.Buffer((18,), data=compute_2)\n            for i2 in range(18):\n                compute_3[i2] = T.exp(ph_0_1[ax1 * 18 + i2])\n            for ax2 in range(18):\n                T_subtract_1 = T.Buffer((72,), data=T_subtract.data)\n                T_subtract_1[ax1 * 18 + ax2] = compute_3[ax2]\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((72,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1] * T.fabs(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(72):\n            compute_3 = T.Buffer((72,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] * T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "add",
                "subtract",
                "abs",
                "multiply",
                "exp",
                "asinh"
            ]
        ],
        "input_shape": [[1, 4, 18]],
        "output_shape": [[1, 4, 18], [1, 4, 18], [1, 4, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 44) + (i1 * 4)) + i2)] = asinhf((ph_0[(((i0 * 44) + (i1 * 4)) + i2)] * sinf(ph_0[(((i0 * 44) + (i1 * 4)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 616; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf((ph_0[i0_i1_fused_i2_fused] * sinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 616; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf((ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * __sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 11, 4), \"float32\"), ph_3: T.Buffer((14, 11, 4), \"float32\"), compute: T.Buffer((14, 11, 4), \"float32\"), compute_1: T.Buffer((14, 11, 4), \"float32\"), compute_2: T.Buffer((14, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((616,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(11, 4):\n                cse_var_1: T.int32 = i0 * 44 + i1 * 4 + i2\n                compute_3 = T.Buffer((616,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1] * T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(616):\n            compute_3 = T.Buffer((616,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] * T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(616):\n            compute_3 = T.Buffer((616,), data=compute_2.data)\n            ph_3_1 = T.Buffer((616,), data=ph_3.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "sin",
                "multiply",
                "asinh",
                "acos",
                "atan"
            ]
        ],
        "input_shape": [[14, 11, 4], [10, 12, 13], [14, 11, 4]],
        "output_shape": [[10, 12, 13], [14, 11, 4], [14, 11, 4], [14, 11, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_multiply[(((ax0 * 26) + (ax1 * 2)) + ax2)] = (ph_0[(((ax0 * 26) + (ax1 * 2)) + ax2)] * ph_3[(((ax0 * 26) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 26) + (i1 * 2)) + i2)] = expf(ph_0[(((i0 * 26) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 468; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 13, 2), \"float32\"), ph_3: T.Buffer((18, 13, 2), \"float32\"), T_multiply: T.Buffer((18, 13, 2), \"float32\"), compute: T.Buffer((18, 13, 2), \"float32\"), compute_1: T.Buffer((18, 13, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(13, 2):\n                cse_var_1: T.int32 = ax0 * 26 + ax1 * 2 + ax2\n                T_multiply_1 = T.Buffer((468,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((468,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(13, 2):\n                cse_var_2: T.int32 = i0 * 26 + i1 * 2 + i2\n                compute_2 = T.Buffer((468,), data=compute.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_2 = T.Buffer((468,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "exp",
                "asinh"
            ]
        ],
        "input_shape": [[18, 13, 2], [20, 7, 19], [18, 13, 2]],
        "output_shape": [[18, 13, 2], [20, 7, 19], [18, 13, 2], [18, 13, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 120) + (i1 * 20)) + i2)] = atanf(ph_0[(((i0 * 120) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2040; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 17; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 6; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n        compute_2[(((i0_1 * 120) + (i1_1 * 20)) + i2_1)] = asinf(ph_0[(((i0_1 * 120) + (i1_1 * 20)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2040; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 6, 20), \"float32\"), compute: T.Buffer((17, 6, 20), \"float32\"), compute_1: T.Buffer((17, 6, 20), \"float32\"), compute_2: T.Buffer((17, 6, 20), \"float32\"), compute_3: T.Buffer((17, 6, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2040,), data=ph_0.data)\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(6, 20):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 20 + i2\n                compute_4 = T.Buffer((2040,), data=compute.data)\n                compute_4[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2040):\n            compute_4 = T.Buffer((2040,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(6, 20):\n                cse_var_2: T.int32 = i0 * 120 + i1 * 20 + i2\n                compute_4 = T.Buffer((2040,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2040):\n            compute_4 = T.Buffer((2040,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "atanh",
                "atanh",
                "asin",
                "exp"
            ]
        ],
        "input_shape": [[17, 6, 20]],
        "output_shape": [[17, 6, 20], [17, 6, 20], [17, 6, 20], [17, 6, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2142; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 238; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0_i1_fused * 9) + i2)] = ceilf(acosf(ph_0[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_2[(((i0 * 153) + (i1 * 9)) + i2_1)] = sinf(ph_0[(((i0 * 153) + (i1 * 9)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 17, 9), \"float32\"), compute: T.Buffer((14, 17, 9), \"float32\"), compute_1: T.Buffer((14, 17, 9), \"float32\"), compute_2: T.Buffer((14, 17, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2142,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2142):\n            compute_3 = T.Buffer((2142,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(238):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((2142,), data=compute_1.data)\n                compute_3[cse_var_1] = T.ceil(T.acos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(17, 9):\n                cse_var_2: T.int32 = i0 * 153 + i1 * 9 + i2\n                compute_3 = T.Buffer((2142,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "acos",
                "ceil",
                "sin"
            ]
        ],
        "input_shape": [[14, 17, 9]],
        "output_shape": [[14, 17, 9], [14, 17, 9], [14, 17, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1950; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] / sinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute_1[(((i0 * 130) + (i1 * 10)) + i2)] = atanhf((ph_0[(((i0 * 130) + (i1 * 10)) + i2)] - atanhf(ph_0[(((i0 * 130) + (i1 * 10)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 195; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_2[((i0_i1_fused * 10) + i2_1)] = asinf((ph_0[((i0_i1_fused * 10) + i2_1)] - atanhf(ph_0[((i0_i1_fused * 10) + i2_1)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf((ph_0[((int)blockIdx.x)] - atanhf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 13, 10), \"float32\"), compute: T.Buffer((15, 13, 10), \"float32\"), compute_1: T.Buffer((15, 13, 10), \"float32\"), compute_2: T.Buffer((15, 13, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1950,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1950):\n            compute_3 = T.Buffer((1950,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] / T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(13, 10):\n                cse_var_1: T.int32 = i0 * 130 + i1 * 10 + i2\n                compute_3 = T.Buffer((1950,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1] - T.atanh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(195):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_3 = T.Buffer((1950,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asin(ph_0_1[cse_var_2] - T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "sin",
                "divide",
                "abs",
                "atanh",
                "subtract",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[15, 13, 10]],
        "output_shape": [[15, 13, 10], [15, 13, 10], [15, 13, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 304; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] - ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 304; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      T_mod[((ax0_ax1_fused_1 * 8) + ax2_1)] = fmodf((ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)] * (ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)] * ph_3[((ax0_ax1_fused_1 * 8) + ax2_1)])), ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2432; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 19, 8), \"float32\"), ph_3: T.Buffer((16, 19, 8), \"float32\"), T_subtract: T.Buffer((16, 19, 8), \"float32\"), T_mod: T.Buffer((16, 19, 8), \"float32\"), compute: T.Buffer((16, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2432,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2432,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_subtract_1 = T.Buffer((2432,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_mod_1 = T.Buffer((2432,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] * (ph_0_1[cse_var_2] * ph_3_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2432):\n            compute_1 = T.Buffer((2432,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "multiply",
                "multiply",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[16, 19, 8], [18, 11, 11], [16, 19, 8]],
        "output_shape": [[16, 19, 8], [18, 11, 11], [16, 19, 8], [16, 19, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 266; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 14; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = ceilf(cosf(ph_0[((i0_i1_fused * 19) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 266; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_mod[((ax0 * 19) + ax2)] = fmodf(fmodf(ph_0[((ax0 * 19) + ax2)], ph_3[((ax0 * 19) + ax2)]), ph_0[((ax0 * 19) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((int)blockIdx.x)] = (__cosf(ph_0[((int)blockIdx.x)]) * ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 1, 19), \"float32\"), ph_3: T.Buffer((14, 1, 19), \"float32\"), T_subtract: T.Buffer((14, 1, 19), \"float32\"), compute: T.Buffer((14, 1, 19), \"float32\"), T_multiply: T.Buffer((14, 1, 19), \"float32\"), T_mod: T.Buffer((14, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((266,), data=ph_0.data)\n        ph_3_1 = T.Buffer((266,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(266):\n            T_subtract_1 = T.Buffer((266,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(14):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_1 = T.Buffer((266,), data=compute.data)\n                compute_1[cse_var_1] = T.ceil(T.cos(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(266):\n            T_multiply_1 = T.Buffer((266,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(14):\n            for ax2 in range(19):\n                cse_var_2: T.int32 = ax0 * 19 + ax2\n                T_mod_1 = T.Buffer((266,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "mod",
                "subtract",
                "cos",
                "ceil",
                "multiply",
                "mod"
            ]
        ],
        "input_shape": [[14, 1, 19], [19, 18, 8], [14, 1, 19]],
        "output_shape": [[19, 18, 8], [14, 1, 19], [14, 1, 19], [14, 1, 19], [14, 1, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 252; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((0.000000e+00f - ph_3[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = expf((0.000000e+00f - ph_3[((i0_i1_fused * 3) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 84; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 3) + i2_1)] = acoshf((ph_0[((i0_i1_fused_1 * 3) + i2_1)] * ph_3[((i0_i1_fused_1 * 3) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 252; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = sinf((ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((0.000000e+00f - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((0.000000e+00f - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 7, 3), \"float32\"), ph_3: T.Buffer((12, 7, 3), \"float32\"), T_add: T.Buffer((12, 7, 3), \"float32\"), compute: T.Buffer((12, 7, 3), \"float32\"), compute_1: T.Buffer((12, 7, 3), \"float32\"), compute_2: T.Buffer((12, 7, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_3_1 = T.Buffer((252,), data=ph_3.data)\n        ph_0_1 = T.Buffer((252,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(252):\n            T_add_1 = T.Buffer((252,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.float32(0) - ph_3_1[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((252,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(T.float32(0) - ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((252,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acosh(ph_0_1[cse_var_2] * ph_3_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(252):\n            compute_3 = T.Buffer((252,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "add",
                "subtract",
                "add",
                "exp",
                "acosh",
                "sin"
            ]
        ],
        "input_shape": [[12, 7, 3], [12, 11, 9], [12, 7, 3]],
        "output_shape": [[12, 11, 9], [12, 7, 3], [12, 7, 3], [12, 7, 3], [12, 7, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 10; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    compute_1[i0_i1_fused] = atanf(sinf(ph_0[i0_i1_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 10; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 2, 1), \"float32\"), compute: T.Buffer((5, 2, 1), \"float32\"), compute_1: T.Buffer((5, 2, 1), \"float32\"), compute_2: T.Buffer((5, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((10,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(10):\n            compute_3 = T.Buffer((10,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(10):\n            compute_3 = T.Buffer((10,), data=compute_1.data)\n            compute_3[i0_i1_fused] = T.atan(T.sin(ph_0_1[i0_i1_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(10):\n            compute_3 = T.Buffer((10,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "sin",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[5, 2, 1]],
        "output_shape": [[5, 2, 1], [5, 2, 1], [5, 2, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = ceilf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 864; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 18, 3), \"float32\"), compute: T.Buffer((16, 18, 3), \"float32\"), T_add: T.Buffer((16, 18, 3), \"float32\"), compute_1: T.Buffer((16, 18, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((864,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((864,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_add_1 = T.Buffer((864,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + (T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_2 = T.Buffer((864,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "acos",
                "subtract",
                "add",
                "atan",
                "acosh"
            ]
        ],
        "input_shape": [[16, 18, 3]],
        "output_shape": [[16, 18, 3], [16, 18, 3], [16, 18, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 144; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_mod[((ax0_ax1_fused * 16) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 16) + ax2)], ph_3[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 128) + (i1 * 16)) + i2)] = cosf(ph_0[(((i0 * 128) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2304; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 8, 16), \"float32\"), ph_3: T.Buffer((18, 8, 16), \"float32\"), T_mod: T.Buffer((18, 8, 16), \"float32\"), compute: T.Buffer((18, 8, 16), \"float32\"), T_multiply: T.Buffer((18, 8, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2304,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(144):\n            for ax2 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_mod_1 = T.Buffer((2304,), data=T_mod.data)\n                ph_3_1 = T.Buffer((2304,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(8, 16):\n                cse_var_2: T.int32 = i0 * 128 + i1 * 16 + i2\n                compute_1 = T.Buffer((2304,), data=compute.data)\n                compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2304):\n            T_multiply_1 = T.Buffer((2304,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "cos",
                "atan",
                "multiply"
            ]
        ],
        "input_shape": [[18, 8, 16], [14, 11, 7], [18, 8, 16]],
        "output_shape": [[18, 8, 16], [14, 11, 7], [18, 8, 16], [18, 8, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* ph_0) {\n  float compute[440];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 440; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 440; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ((ph_0[ax0_ax1_fused_ax2_fused] - compute[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 11, 2), \"float32\"), T_mod: T.Buffer((20, 11, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([440], \"float32\", \"global\")\n        compute_1 = T.Buffer((440,), data=compute)\n        ph_0_1 = T.Buffer((440,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(440):\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(440):\n            T_mod_1 = T.Buffer((440,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_0_1[ax0_ax1_fused_ax2_fused] - compute_1[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "exp",
                "subtract",
                "add",
                "mod"
            ]
        ],
        "input_shape": [[20, 11, 2]],
        "output_shape": [[20, 11, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1710; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1710; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1710; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 190; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_2[((i0_i1_fused * 9) + i2)] = acosf(acoshf(ph_0[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = acosf(acoshf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 19, 9), \"float32\"), ph_3: T.Buffer((10, 19, 9), \"float32\"), T_divide: T.Buffer((10, 19, 9), \"float32\"), compute: T.Buffer((10, 19, 9), \"float32\"), compute_1: T.Buffer((10, 19, 9), \"float32\"), compute_2: T.Buffer((10, 19, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1710,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1710):\n            T_divide_1 = T.Buffer((1710,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1710,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1710):\n            compute_3 = T.Buffer((1710,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1710):\n            compute_3 = T.Buffer((1710,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(190):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((1710,), data=compute_2.data)\n                compute_3[cse_var_1] = T.acos(T.acosh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "divide",
                "atan",
                "acosh",
                "sin",
                "acos"
            ]
        ],
        "input_shape": [[10, 19, 9], [1, 5, 16], [10, 19, 9]],
        "output_shape": [[10, 19, 9], [1, 5, 16], [10, 19, 9], [10, 19, 9], [10, 19, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 17, 20), \"float32\"), compute: T.Buffer((10, 17, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3400):\n            compute_1 = T.Buffer((3400,), data=compute.data)\n            ph_0_1 = T.Buffer((3400,), data=ph_0.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil"
            ]
        ],
        "input_shape": [[10, 17, 20]],
        "output_shape": [[10, 17, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 80) + (i1 * 4)) + i2)] = sinf(ph_0[(((i0 * 80) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 240; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 20, 4), \"float32\"), compute: T.Buffer((3, 20, 4), \"float32\"), T_multiply: T.Buffer((3, 20, 4), \"float32\"), compute_1: T.Buffer((3, 20, 4), \"float32\"), compute_2: T.Buffer((3, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(20, 4):\n                cse_var_1: T.int32 = i0 * 80 + i1 * 4 + i2\n                compute_3 = T.Buffer((240,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_multiply_1 = T.Buffer((240,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "abs",
                "multiply",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[3, 20, 4]],
        "output_shape": [[3, 20, 4], [3, 20, 4], [3, 20, 4], [3, 20, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2200; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 220; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = fabsf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2200; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] - ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 20, 10), \"float32\"), ph_3: T.Buffer((11, 20, 10), \"float32\"), T_subtract: T.Buffer((11, 20, 10), \"float32\"), compute: T.Buffer((11, 20, 10), \"float32\"), compute_1: T.Buffer((11, 20, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2200,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2200):\n            T_subtract_1 = T.Buffer((2200,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2200,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(220):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_2 = T.Buffer((2200,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2200):\n            compute_2 = T.Buffer((2200,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[11, 20, 10], [10, 12, 11], [11, 20, 10]],
        "output_shape": [[11, 20, 10], [10, 12, 11], [11, 20, 10], [11, 20, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 266; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] * ph_3[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2660; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 10; ++ax2_1) {\n        T_add[(((ax0 * 190) + (ax1 * 10)) + ax2_1)] = (fabsf(ph_0[(((ax0 * 190) + (ax1 * 10)) + ax2_1)]) + ph_0[(((ax0 * 190) + (ax1 * 10)) + ax2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] * ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 19, 10), \"float32\"), ph_3: T.Buffer((14, 19, 10), \"float32\"), T_multiply: T.Buffer((14, 19, 10), \"float32\"), T_divide: T.Buffer((14, 19, 10), \"float32\"), T_add: T.Buffer((14, 19, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2660,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2660,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(266):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_multiply_1 = T.Buffer((2660,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2660):\n            T_divide_1 = T.Buffer((2660,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(19, 10):\n                cse_var_2: T.int32 = ax0 * 190 + ax1 * 10 + ax2\n                T_add_1 = T.Buffer((2660,), data=T_add.data)\n                T_add_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "multiply",
                "divide",
                "abs",
                "add"
            ]
        ],
        "input_shape": [[14, 19, 10], [10, 19, 6], [14, 19, 10]],
        "output_shape": [[14, 19, 10], [10, 19, 6], [14, 19, 10], [14, 19, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  float compute_4[3536];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3536; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3536; ++i0_i1_fused_i2_fused_1) {\n    compute_4[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3536; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = sinf(compute_4[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 3536; ++i0_i1_fused_i2_fused_3) {\n    compute_2[i0_i1_fused_i2_fused_3] = acoshf(ph_0[i0_i1_fused_i2_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 3536; ++i0_i1_fused_i2_fused_4) {\n    compute_3[i0_i1_fused_i2_fused_4] = acoshf(ceilf(ph_0[i0_i1_fused_i2_fused_4]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3536; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_4(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 17, 13), \"float32\"), compute: T.Buffer((16, 17, 13), \"float32\"), compute_1: T.Buffer((16, 17, 13), \"float32\"), compute_2: T.Buffer((16, 17, 13), \"float32\"), compute_3: T.Buffer((16, 17, 13), \"float32\"), T_divide: T.Buffer((16, 17, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_4 = T.allocate([3536], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((3536,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_5 = T.Buffer((3536,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        compute_5 = T.Buffer((3536,), data=compute_4)\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_6 = T.Buffer((3536,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.sin(compute_5[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_6 = T.Buffer((3536,), data=compute_2.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_6 = T.Buffer((3536,), data=compute_3.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acosh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(3536):\n            T_divide_1 = T.Buffer((3536,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "exp",
                "exp",
                "sin",
                "acosh",
                "ceil",
                "acosh",
                "divide"
            ]
        ],
        "input_shape": [[16, 17, 13]],
        "output_shape": [[16, 17, 13], [16, 17, 13], [16, 17, 13], [16, 17, 13], [16, 17, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 120) + (i1 * 20)) + i2)] = asinhf(ph_0[(((i0 * 120) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - asinhf(acoshf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_1[((i0_i1_fused * 20) + i2_1)] = asinf(ph_0[((i0_i1_fused * 20) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 20), \"float32\"), compute: T.Buffer((8, 6, 20), \"float32\"), T_subtract: T.Buffer((8, 6, 20), \"float32\"), compute_1: T.Buffer((8, 6, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(6, 20):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 20 + i2\n                compute_2 = T.Buffer((960,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            T_subtract_1 = T.Buffer((960,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.asinh(T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_2 = T.Buffer((960,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "acosh",
                "asinh",
                "subtract",
                "asin"
            ]
        ],
        "input_shape": [[8, 6, 20]],
        "output_shape": [[8, 6, 20], [8, 6, 20], [8, 6, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1152; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 192; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = asinf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1152; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 12, 6), \"float32\"), ph_3: T.Buffer((16, 12, 6), \"float32\"), T_add: T.Buffer((16, 12, 6), \"float32\"), compute: T.Buffer((16, 12, 6), \"float32\"), compute_1: T.Buffer((16, 12, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1152,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1152):\n            T_add_1 = T.Buffer((1152,), data=T_add.data)\n            ph_3_1 = T.Buffer((1152,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(192):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((1152,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1152):\n            compute_2 = T.Buffer((1152,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[16, 12, 6], [1, 8, 14], [16, 12, 6]],
        "output_shape": [[16, 12, 6], [1, 8, 14], [16, 12, 6], [16, 12, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1365; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1365; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], fabsf(cosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 195; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = atanhf(asinhf(ph_0[((i0_i1_fused * 7) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], fabsf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 15, 7), \"float32\"), compute: T.Buffer((13, 15, 7), \"float32\"), T_mod: T.Buffer((13, 15, 7), \"float32\"), compute_1: T.Buffer((13, 15, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1365,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1365):\n            compute_2 = T.Buffer((1365,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1365):\n            T_mod_1 = T.Buffer((1365,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused in T.parallel(195):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1365,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(T.asinh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "atan",
                "cos",
                "abs",
                "mod",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[13, 15, 7]],
        "output_shape": [[13, 15, 7], [13, 15, 7], [13, 15, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 56) + (i1 * 7)) + i2)] = cosf(ph_0[(((i0 * 56) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1064; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 152; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_2[((i0_i1_fused * 7) + i2_1)] = asinf(ph_0[((i0_i1_fused * 7) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 8, 7), \"float32\"), compute: T.Buffer((19, 8, 7), \"float32\"), compute_1: T.Buffer((19, 8, 7), \"float32\"), compute_2: T.Buffer((19, 8, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1064,), data=ph_0.data)\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(8, 7):\n                cse_var_1: T.int32 = i0 * 56 + i1 * 7 + i2\n                compute_3 = T.Buffer((1064,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1064):\n            compute_3 = T.Buffer((1064,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(152):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((1064,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "sin",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[19, 8, 7]],
        "output_shape": [[19, 8, 7], [19, 8, 7], [19, 8, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 49; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = atanhf((ph_0[((i0_i1_fused * 2) + i2)] + cosf(ph_0[((i0_i1_fused * 2) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_1[(((i0 * 14) + (i1 * 2)) + i2_1)] = acoshf(ph_0[(((i0 * 14) + (i1 * 2)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 98; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 7, 2), \"float32\"), compute: T.Buffer((7, 7, 2), \"float32\"), compute_1: T.Buffer((7, 7, 2), \"float32\"), compute_2: T.Buffer((7, 7, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((98,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(49):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((98,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1] + T.cos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(7, 2):\n                cse_var_2: T.int32 = i0 * 14 + i1 * 2 + i2\n                compute_3 = T.Buffer((98,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(98):\n            compute_3 = T.Buffer((98,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "add",
                "atanh",
                "acosh",
                "cos"
            ]
        ],
        "input_shape": [[7, 7, 2]],
        "output_shape": [[7, 7, 2], [7, 7, 2], [7, 7, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_6) {\n  float auto_scheduler_layout_transform[1440];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3456; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3456; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n    for (int32_t ax8 = 0; ax8 < 9; ++ax8) {\n      for (int32_t ax9 = 0; ax9 < 4; ++ax9) {\n        for (int32_t ax11 = 0; ax11 < 2; ++ax11) {\n          for (int32_t ax12 = 0; ax12 < 4; ++ax12) {\n            auto_scheduler_layout_transform[(((((ax3 * 288) + (ax8 * 32)) + (ax9 * 8)) + (ax11 * 4)) + ax12)] = ph_6[(((((ax9 * 360) + (ax12 * 90)) + (ax8 * 10)) + (ax11 * 5)) + ax3)];\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused = 0; i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused < 10; ++i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused) {\n    float T_batch_matmul_NN[96];\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 4; ++b_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 3; ++i_outer_inner_init) {\n        for (int32_t b_inner_init = 0; b_inner_init < 4; ++b_inner_init) {\n          for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n            T_batch_matmul_NN[((((b_outer_inner_init * 24) + (b_inner_init * 6)) + (i_outer_inner_init * 2)) + i_inner_init)] = 0.000000e+00f;\n          }\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 9; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 4; ++b_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 3; ++i_outer_inner) {\n          for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n            for (int32_t b_inner = 0; b_inner < 4; ++b_inner) {\n              for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n                T_batch_matmul_NN[((((b_outer_inner * 24) + (b_inner * 6)) + (i_outer_inner * 2)) + i_inner)] = (T_batch_matmul_NN[((((b_outer_inner * 24) + (b_inner * 6)) + (i_outer_inner * 2)) + i_inner)] + (ph_0[(((((((b_outer_inner * 864) + (b_inner * 216)) + ((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 5) * 108)) + (i_outer_inner * 36)) + (i_inner * 18)) + (k_outer * 2)) + k_inner)] * auto_scheduler_layout_transform[((((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5) * 288) + (k_outer * 32)) + (b_outer_inner * 8)) + (k_inner * 4)) + b_inner)]));\n              }\n            }\n          }\n        }\n      }\n    }\n    for (int32_t i0_inner = 0; i0_inner < 16; ++i0_inner) {\n      for (int32_t i1_inner = 0; i1_inner < 6; ++i1_inner) {\n        compute_1[((((i0_inner * 60) + ((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 5) * 30)) + (i1_inner * 5)) + (i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5))] = fabsf(T_batch_matmul_NN[((i0_inner * 6) + i1_inner)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN[9];\n  __shared__ float ph_6_shared[40];\n  for (int i_inner_init = 0; i_inner_init < 9; ++i_inner_init) {\n    T_batch_matmul_NN[i_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_6_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 5) + ((int)threadIdx.x))] = ph_6[((ax0_ax1_fused_ax2_fused_outer_outer * 5) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n    for (int k_inner = 0; k_inner < 4; ++k_inner) {\n      for (int i_inner = 0; i_inner < 9; ++i_inner) {\n        T_batch_matmul_NN[i_inner] = (T_batch_matmul_NN[i_inner] + (ph_0[(((i_inner * 8) + (k_outer_inner * 4)) + k_inner)] * ph_6_shared[(((k_outer_inner * 20) + (k_inner * 5)) + ((int)threadIdx.x))]));\n      }\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 9; ++i1_inner) {\n    compute[((i1_inner * 5) + ((int)threadIdx.x))] = fabsf(T_batch_matmul_NN[i1_inner]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 12, 18), \"float32\"), ph_6: T.Buffer((16, 18, 5), \"float32\"), compute: T.Buffer((16, 12, 18), \"float32\"), T_add: T.Buffer((16, 12, 18), \"float32\"), compute_1: T.Buffer((16, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1440], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((3456,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3456):\n            compute_2 = T.Buffer((3456,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3456):\n            T_add_1 = T.Buffer((3456,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((1440,), data=auto_scheduler_layout_transform)\n        for ax3, ax8, ax9, ax11, ax12 in T.grid(5, 9, 4, 2, 4):\n            ph_6_1 = T.Buffer((1440,), data=ph_6.data)\n            auto_scheduler_layout_transform_1[ax3 * 288 + ax8 * 32 + ax9 * 8 + ax11 * 4 + ax12] = ph_6_1[ax9 * 360 + ax12 * 90 + ax8 * 10 + ax11 * 5 + ax3]\n        for i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(10):\n            T_batch_matmul_NN = T.allocate([96], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((96,), data=T_batch_matmul_NN)\n            for b_outer_inner_init, i_outer_inner_init, b_inner_init, i_inner_init in T.grid(4, 3, 4, 2):\n                T_batch_matmul_NN_1[b_outer_inner_init * 24 + b_inner_init * 6 + i_outer_inner_init * 2 + i_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, i_outer_inner, k_inner, b_inner, i_inner in T.grid(9, 4, 3, 2, 4, 2):\n                cse_var_1: T.int32 = b_outer_inner * 24 + b_inner * 6 + i_outer_inner * 2 + i_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 864 + b_inner * 216 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 5 * 108 + i_outer_inner * 36 + i_inner * 18 + k_outer * 2 + k_inner] * auto_scheduler_layout_transform_1[i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5 * 288 + k_outer * 32 + b_outer_inner * 8 + k_inner * 4 + b_inner]\n            for i0_inner, i1_inner in T.grid(16, 6):\n                compute_2 = T.Buffer((960,), data=compute_1.data)\n                compute_2[i0_inner * 60 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 5 * 30 + i1_inner * 5 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5] = T.fabs(T_batch_matmul_NN_1[i0_inner * 6 + i1_inner])",
        "op_args": [
            [
                "acosh",
                "atanh",
                "divide",
                "add",
                "batch_matmul",
                "abs"
            ]
        ],
        "input_shape": [[16, 12, 18], [16, 18, 5]],
        "output_shape": [[16, 12, 18], [16, 12, 18], [16, 12, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0_i1_fused * 6) + i2)] = fabsf(acoshf(ph_0[((i0_i1_fused * 6) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n        compute_2[(((i0 * 90) + (i1 * 6)) + i2_1)] = atanhf(ph_0[(((i0 * 90) + (i1 * 6)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 360; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add_1[ax0_ax1_fused_ax2_fused_1] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_4(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 15, 6), \"float32\"), compute: T.Buffer((4, 15, 6), \"float32\"), compute_1: T.Buffer((4, 15, 6), \"float32\"), compute_2: T.Buffer((4, 15, 6), \"float32\"), T_add: T.Buffer((4, 15, 6), \"float32\"), T_add_1: T.Buffer((4, 15, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((360,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(T.acosh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(15, 6):\n                cse_var_2: T.int32 = i0 * 90 + i1 * 6 + i2\n                compute_3 = T.Buffer((360,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atanh(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_2 = T.Buffer((360,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_add_2 = T.Buffer((360,), data=T_add_1.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atan",
                "acosh",
                "abs",
                "atanh",
                "asinh",
                "add",
                "add"
            ]
        ],
        "input_shape": [[4, 15, 6]],
        "output_shape": [[4, 15, 6], [4, 15, 6], [4, 15, 6], [4, 15, 6], [4, 15, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    float compute_1[1];\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      compute_1[0] = expf(ph_0[((i0 * 7) + i1)]);\n      compute[((i0 * 7) + i1)] = cosf((ph_0[((i0 * 7) + i1)] + compute_1[0]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      T_multiply[((ax0 * 7) + ax1)] = (ph_0[((ax0 * 7) + ax1)] * atanhf(ph_0[((ax0 * 7) + ax1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 7, 1), \"float32\"), compute: T.Buffer((6, 7, 1), \"float32\"), T_multiply: T.Buffer((6, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((42,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            for i1 in range(7):\n                cse_var_1: T.int32 = i0 * 7 + i1\n                compute_2 = T.Buffer((1,), data=compute_1, align=4)\n                compute_2[0] = T.exp(ph_0_1[cse_var_1])\n                compute_3 = T.Buffer((42,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1] + compute_2[0])\n        for ax0 in T.parallel(6):\n            for ax1 in range(7):\n                cse_var_2: T.int32 = ax0 * 7 + ax1\n                T_multiply_1 = T.Buffer((42,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] * T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "exp",
                "add",
                "cos",
                "atanh",
                "multiply"
            ]
        ],
        "input_shape": [[6, 7, 1]],
        "output_shape": [[6, 7, 1], [6, 7, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 72) + (i1 * 9)) + i2)] = fabsf(ph_0[(((i0 * 72) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 648; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 648; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * sinf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 8, 9), \"float32\"), compute: T.Buffer((9, 8, 9), \"float32\"), compute_1: T.Buffer((9, 8, 9), \"float32\"), T_multiply: T.Buffer((9, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((648,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(8, 9):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 9 + i2\n                compute_2 = T.Buffer((648,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(648):\n            compute_2 = T.Buffer((648,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(648):\n            T_multiply_1 = T.Buffer((648,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.sin(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "abs",
                "acosh",
                "ceil",
                "sin",
                "multiply"
            ]
        ],
        "input_shape": [[9, 8, 9]],
        "output_shape": [[9, 8, 9], [9, 8, 9], [9, 8, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 19; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = expf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 209; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 209; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 19; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 11) + i2_1)] = atanhf(ceilf(ph_0[((i0_i1_fused_1 * 11) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 1, 11), \"float32\"), compute: T.Buffer((19, 1, 11), \"float32\"), T_divide: T.Buffer((19, 1, 11), \"float32\"), compute_1: T.Buffer((19, 1, 11), \"float32\"), compute_2: T.Buffer((19, 1, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((209,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(19):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((209,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(209):\n            T_divide_1 = T.Buffer((209,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(209):\n            compute_3 = T.Buffer((209,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(19):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((209,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atanh(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "exp",
                "atanh",
                "divide",
                "asin",
                "ceil",
                "atanh"
            ]
        ],
        "input_shape": [[19, 1, 11]],
        "output_shape": [[19, 1, 11], [19, 1, 11], [19, 1, 11], [19, 1, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0 * 18) + i2)] = atanhf((ph_0[((i0 * 18) + i2)] + cosf(ph_0[((i0 * 18) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 1, 18), \"float32\"), compute: T.Buffer((8, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0 * 18 + i2\n                compute_1 = T.Buffer((144,), data=compute.data)\n                ph_0_1 = T.Buffer((144,), data=ph_0.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1] + T.cos(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "cos",
                "add",
                "atanh"
            ]
        ],
        "input_shape": [[8, 1, 18]],
        "output_shape": [[8, 1, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_subtract[(((ax0 * 108) + (ax1 * 6)) + ax2)] = (ph_0[(((ax0 * 108) + (ax1 * 6)) + ax2)] - ph_3[(((ax0 * 108) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = ceilf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 432; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 18, 6), \"float32\"), ph_3: T.Buffer((4, 18, 6), \"float32\"), T_subtract: T.Buffer((4, 18, 6), \"float32\"), compute: T.Buffer((4, 18, 6), \"float32\"), compute_1: T.Buffer((4, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((432,), data=ph_0.data)\n        for ax0 in T.parallel(4):\n            for ax1, ax2 in T.grid(18, 6):\n                cse_var_1: T.int32 = ax0 * 108 + ax1 * 6 + ax2\n                T_subtract_1 = T.Buffer((432,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((432,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(72):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((432,), data=compute.data)\n                compute_2[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(432):\n            compute_2 = T.Buffer((432,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "ceil",
                "cos",
                "cos"
            ]
        ],
        "input_shape": [[4, 18, 6], [19, 10, 13], [4, 18, 6]],
        "output_shape": [[4, 18, 6], [19, 10, 13], [4, 18, 6], [4, 18, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 65; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = atanhf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1235; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / acosf(atanhf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_1[(((i0 * 95) + (i1 * 19)) + i2_1)] = sinf(ph_0[(((i0 * 95) + (i1 * 19)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / acosf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 5, 19), \"float32\"), compute: T.Buffer((13, 5, 19), \"float32\"), T_divide: T.Buffer((13, 5, 19), \"float32\"), compute_1: T.Buffer((13, 5, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1235,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(65):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((1235,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1235):\n            T_divide_1 = T.Buffer((1235,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.acos(T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(5, 19):\n                cse_var_2: T.int32 = i0 * 95 + i1 * 19 + i2\n                compute_2 = T.Buffer((1235,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atanh",
                "atanh",
                "acos",
                "divide",
                "sin"
            ]
        ],
        "input_shape": [[13, 5, 19]],
        "output_shape": [[13, 5, 19], [13, 5, 19], [13, 5, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4800; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_multiply[(((ax0 * 320) + (ax1 * 16)) + ax2)] = (atanf(ph_0[(((ax0 * 320) + (ax1 * 16)) + ax2)]) * ph_0[(((ax0 * 320) + (ax1 * 16)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 320) + (i1 * 16)) + i2)] = sinf(atanf(ph_0[(((i0 * 320) + (i1 * 16)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4800; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf((ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 20, 16), \"float32\"), ph_3: T.Buffer((15, 20, 16), \"float32\"), compute: T.Buffer((15, 20, 16), \"float32\"), T_multiply: T.Buffer((15, 20, 16), \"float32\"), compute_1: T.Buffer((15, 20, 16), \"float32\"), compute_2: T.Buffer((15, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4800,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(20, 16):\n                cse_var_1: T.int32 = ax0 * 320 + ax1 * 16 + ax2\n                T_multiply_1 = T.Buffer((4800,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(20, 16):\n                cse_var_2: T.int32 = i0 * 320 + i1 * 16 + i2\n                compute_3 = T.Buffer((4800,), data=compute_1.data)\n                compute_3[cse_var_2] = T.sin(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute_2.data)\n            ph_3_1 = T.Buffer((4800,), data=ph_3.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "cos",
                "atan",
                "multiply",
                "sin",
                "cos"
            ]
        ],
        "input_shape": [[15, 20, 16], [6, 2, 6], [15, 20, 16]],
        "output_shape": [[6, 2, 6], [15, 20, 16], [15, 20, 16], [15, 20, 16], [15, 20, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 120) + (i1 * 6)) + i2)] = acosf(ph_0[(((i0 * 120) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_multiply[(((ax0 * 120) + (ax1 * 6)) + ax2)] = (ph_0[(((ax0 * 120) + (ax1 * 6)) + ax2)] * fmodf(fabsf(ph_0[(((ax0 * 120) + (ax1 * 6)) + ax2)]), ph_0[(((ax0 * 120) + (ax1 * 6)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1800; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * fmodf(fabsf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 20, 6), \"float32\"), compute: T.Buffer((15, 20, 6), \"float32\"), T_multiply: T.Buffer((15, 20, 6), \"float32\"), T_multiply_1: T.Buffer((15, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1800,), data=ph_0.data)\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(20, 6):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 6 + i2\n                compute_1 = T.Buffer((1800,), data=compute.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(20, 6):\n                cse_var_2: T.int32 = ax0 * 120 + ax1 * 6 + ax2\n                T_multiply_2 = T.Buffer((1800,), data=T_multiply.data)\n                T_multiply_2[cse_var_2] = ph_0_1[cse_var_2] * T.truncmod(T.fabs(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1800):\n            T_multiply_2 = T.Buffer((1800,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acos",
                "abs",
                "mod",
                "multiply",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[15, 20, 6]],
        "output_shape": [[15, 20, 6], [15, 20, 6], [15, 20, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 399; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_multiply[(((ax0 * 57) + (ax1 * 19)) + ax2)] = (atanf(ph_0[(((ax0 * 57) + (ax1 * 19)) + ax2)]) * ph_0[(((ax0 * 57) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 21; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_1[((i0_i1_fused * 19) + i2)] = asinf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 3, 19), \"float32\"), compute: T.Buffer((7, 3, 19), \"float32\"), T_multiply: T.Buffer((7, 3, 19), \"float32\"), compute_1: T.Buffer((7, 3, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((399,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(399):\n            compute_2 = T.Buffer((399,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(3, 19):\n                cse_var_1: T.int32 = ax0 * 57 + ax1 * 19 + ax2\n                T_multiply_1 = T.Buffer((399,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(21):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((399,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asin",
                "atan",
                "multiply",
                "asin"
            ]
        ],
        "input_shape": [[7, 3, 19]],
        "output_shape": [[7, 3, 19], [7, 3, 19], [7, 3, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[150];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1050; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n    for (int32_t ax7 = 0; ax7 < 15; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n        auto_scheduler_layout_transform[(((ax5 * 75) + (ax7 * 5)) + ax8)] = ph_4[(((ax5 * 75) + (ax8 * 15)) + ax7)];\n      }\n    }\n  }\n  for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n    for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 7; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 35) + (b_inner_init * 7)) + i_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n    for (int32_t k_inner = 0; k_inner < 15; ++k_inner) {\n      for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 7; ++i_inner) {\n          T_batch_matmul_NN[(((b_outer_inner * 35) + (b_inner * 7)) + i_inner)] = (T_batch_matmul_NN[(((b_outer_inner * 35) + (b_inner * 7)) + i_inner)] + (sinf(ph_0[((((b_outer_inner * 525) + (b_inner * 105)) + (i_inner * 15)) + k_inner)]) * auto_scheduler_layout_transform[(((b_outer_inner * 75) + (k_inner * 5)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float compute_shared[160];\n  __shared__ float ph_4_shared[20];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 4; ++b_c_outer_inner_init) {\n    for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n      T_batch_matmul_NN_local[((b_c_outer_inner_init * 2) + i_c_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 40; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))] = __sinf(ph_0[(((((int)blockIdx.x) * 160) + (ax0_ax1_fused_ax2_fused_outer_outer * 4)) + ((int)threadIdx.x))]);\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 5; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_4_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 4) + ((int)threadIdx.x))] = ph_4[(((((int)blockIdx.x) * 20) + (ax0_ax1_fused_ax2_fused_outer_outer_1 * 4)) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 4; ++b_c_outer_inner) {\n      for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n        T_batch_matmul_NN_local[((b_c_outer_inner * 2) + i_c_outer_inner)] = (T_batch_matmul_NN_local[((b_c_outer_inner * 2) + i_c_outer_inner)] + (compute_shared[((((b_c_outer_inner * 40) + (((int)threadIdx.x) * 10)) + (i_c_outer_inner * 5)) + k_outer_inner)] * ph_4_shared[((b_c_outer_inner * 5) + k_outer_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    for (int i_inner = 0; i_inner < 2; ++i_inner) {\n      T_batch_matmul_NN[((((((int)blockIdx.x) * 32) + (b_inner * 8)) + (((int)threadIdx.x) * 2)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 2) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 7, 15), \"float32\"), ph_4: T.Buffer((10, 15, 1), \"float32\"), compute: T.Buffer((10, 7, 15), \"float32\"), T_batch_matmul_NN: T.Buffer((10, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([150], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1050,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1050):\n            compute_1 = T.Buffer((1050,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((150,), data=auto_scheduler_layout_transform)\n        for ax5, ax7, ax8 in T.grid(2, 15, 5):\n            cse_var_1: T.int32 = ax5 * 75\n            ph_4_1 = T.Buffer((150,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[cse_var_1 + ax7 * 5 + ax8] = ph_4_1[cse_var_1 + ax8 * 15 + ax7]\n        T_batch_matmul_NN_1 = T.Buffer((70,), data=T_batch_matmul_NN.data)\n        for b_outer_inner_init, b_inner_init, i_inner_init in T.grid(2, 5, 7):\n            T_batch_matmul_NN_1[b_outer_inner_init * 35 + b_inner_init * 7 + i_inner_init] = T.float32(0)\n        for b_outer_inner, k_inner, b_inner, i_inner in T.grid(2, 15, 5, 7):\n            cse_var_2: T.int32 = b_outer_inner * 35 + b_inner * 7 + i_inner\n            T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.sin(ph_0_1[b_outer_inner * 525 + b_inner * 105 + i_inner * 15 + k_inner]) * auto_scheduler_layout_transform_1[b_outer_inner * 75 + k_inner * 5 + b_inner]",
        "op_args": [
            [
                "cos",
                "sin",
                "batch_matmul"
            ]
        ],
        "input_shape": [[10, 7, 15], [10, 15, 1]],
        "output_shape": [[10, 7, 15], [10, 7, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 342; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      T_add[((ax0 * 19) + ax1)] = (ceilf(ph_0[((ax0 * 19) + ax1)]) + ph_0[((ax0 * 19) + ax1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 342; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - cosf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 19, 1), \"float32\"), compute: T.Buffer((18, 19, 1), \"float32\"), T_add: T.Buffer((18, 19, 1), \"float32\"), T_subtract: T.Buffer((18, 19, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((342,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(342):\n            compute_1 = T.Buffer((342,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(18):\n            for ax1 in range(19):\n                cse_var_1: T.int32 = ax0 * 19 + ax1\n                T_add_1 = T.Buffer((342,), data=T_add.data)\n                T_add_1[cse_var_1] = T.ceil(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(342):\n            T_subtract_1 = T.Buffer((342,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.cos(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "exp",
                "ceil",
                "add",
                "cos",
                "subtract"
            ]
        ],
        "input_shape": [[18, 19, 1]],
        "output_shape": [[18, 19, 1], [18, 19, 1], [18, 19, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 792; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 792; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute_2[(((i0 * 72) + (i1 * 9)) + i2)] = fabsf(ph_0[(((i0 * 72) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 792; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 792; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_4(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((int)blockIdx.x)] = (fabsf(ph_0[((int)blockIdx.x)]) + ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 8, 9), \"float32\"), compute: T.Buffer((11, 8, 9), \"float32\"), compute_1: T.Buffer((11, 8, 9), \"float32\"), compute_2: T.Buffer((11, 8, 9), \"float32\"), compute_3: T.Buffer((11, 8, 9), \"float32\"), T_add: T.Buffer((11, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((792,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(792):\n            compute_4 = T.Buffer((792,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(792):\n            compute_4 = T.Buffer((792,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(8, 9):\n                cse_var_1: T.int32 = i0 * 72 + i1 * 9 + i2\n                compute_4 = T.Buffer((792,), data=compute_2.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(792):\n            compute_4 = T.Buffer((792,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(792):\n            T_add_1 = T.Buffer((792,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acos",
                "acos",
                "atan",
                "abs",
                "abs",
                "cos",
                "add"
            ]
        ],
        "input_shape": [[11, 8, 9]],
        "output_shape": [[11, 8, 9], [11, 8, 9], [11, 8, 9], [11, 8, 9], [11, 8, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 342) + (i1 * 18)) + i2)] = fabsf(ph_0[(((i0 * 342) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2052; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 6; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 19; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_2[(((i0_1 * 342) + (i1_1 * 18)) + i2_1)] = sinf(ph_0[(((i0_1 * 342) + (i1_1 * 18)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2052; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 19, 18), \"float32\"), compute: T.Buffer((6, 19, 18), \"float32\"), compute_1: T.Buffer((6, 19, 18), \"float32\"), compute_2: T.Buffer((6, 19, 18), \"float32\"), compute_3: T.Buffer((6, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2052,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(19, 18):\n                cse_var_1: T.int32 = i0 * 342 + i1 * 18 + i2\n                compute_4 = T.Buffer((2052,), data=compute.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2052):\n            compute_4 = T.Buffer((2052,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(19, 18):\n                cse_var_2: T.int32 = i0 * 342 + i1 * 18 + i2\n                compute_4 = T.Buffer((2052,), data=compute_2.data)\n                compute_4[cse_var_2] = T.sin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2052):\n            compute_4 = T.Buffer((2052,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "cos",
                "asin",
                "sin",
                "sin"
            ]
        ],
        "input_shape": [[6, 19, 18]],
        "output_shape": [[6, 19, 18], [6, 19, 18], [6, 19, 18], [6, 19, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 98) + (i1 * 14)) + i2)] = acoshf(ph_0[(((i0 * 98) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 784; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 784; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 7, 14), \"float32\"), compute: T.Buffer((8, 7, 14), \"float32\"), compute_1: T.Buffer((8, 7, 14), \"float32\"), compute_2: T.Buffer((8, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((784,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(7, 14):\n                cse_var_1: T.int32 = i0 * 98 + i1 * 14 + i2\n                compute_3 = T.Buffer((784,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(784):\n            compute_3 = T.Buffer((784,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(784):\n            compute_3 = T.Buffer((784,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "acosh",
                "atan",
                "cos"
            ]
        ],
        "input_shape": [[8, 7, 14]],
        "output_shape": [[8, 7, 14], [8, 7, 14], [8, 7, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2040; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2040; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_1[(((i0 * 120) + (i1 * 6)) + i2)] = atanf(ph_0[(((i0 * 120) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 6), \"float32\"), ph_3: T.Buffer((17, 20, 6), \"float32\"), T_subtract: T.Buffer((17, 20, 6), \"float32\"), compute: T.Buffer((17, 20, 6), \"float32\"), compute_1: T.Buffer((17, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2040,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2040):\n            T_subtract_1 = T.Buffer((2040,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2040,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2040):\n            compute_2 = T.Buffer((2040,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(20, 6):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 6 + i2\n                compute_2 = T.Buffer((2040,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "subtract",
                "asin",
                "atan"
            ]
        ],
        "input_shape": [[17, 20, 6], [12, 6, 10], [17, 20, 6]],
        "output_shape": [[17, 20, 6], [12, 6, 10], [17, 20, 6], [17, 20, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_8) {\n  float auto_scheduler_layout_transform[12];\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      T_mod[((ax0 * 6) + ax2)] = fmodf(ph_0[((ax0 * 6) + ax2)], ph_3[((ax0 * 6) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 12; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 12; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 2; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 6; ++ax2_1) {\n      auto_scheduler_layout_transform[((ax0_ax1_fused * 6) + ax2_1)] = ph_8[((ax0_ax1_fused * 6) + ax2_1)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_i_fused_j_fused = 0; b_i_fused_j_fused < 2; ++b_i_fused_j_fused) {\n    T_batch_matmul_NN[b_i_fused_j_fused] = 0.000000e+00f;\n    for (int32_t k = 0; k < 6; ++k) {\n      T_batch_matmul_NN[b_i_fused_j_fused] = (T_batch_matmul_NN[b_i_fused_j_fused] + (atanhf(ph_0[((b_i_fused_j_fused * 6) + k)]) * auto_scheduler_layout_transform[((b_i_fused_j_fused * 6) + k)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[4];\n  __shared__ float ph_8_shared[8];\n  for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n    T_batch_matmul_NN_local[i_c_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(i_c_inner_init + 2)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    ph_8_shared[((int)threadIdx.x)] = ph_8[((((int)threadIdx.x) * 5) + k_outer_outer)];\n    __syncthreads();\n    for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n      T_batch_matmul_NN_local[i_c_inner] = (T_batch_matmul_NN_local[i_c_inner] + (atanhf(ph_0[((((((int)threadIdx.x) * 40) + (((int)blockIdx.x) * 20)) + (i_c_inner * 5)) + k_outer_outer)]) * ph_8_shared[((int)threadIdx.x)]));\n      T_batch_matmul_NN_local[(i_c_inner + 2)] = (T_batch_matmul_NN_local[(i_c_inner + 2)] + (atanhf(ph_0[(((((((int)threadIdx.x) * 40) + (((int)blockIdx.x) * 20)) + (i_c_inner * 5)) + k_outer_outer) + 10)]) * ph_8_shared[((int)threadIdx.x)]));\n    }\n  }\n  for (int i_inner = 0; i_inner < 2; ++i_inner) {\n    T_batch_matmul_NN[(((((int)threadIdx.x) * 8) + (((int)blockIdx.x) * 4)) + i_inner)] = T_batch_matmul_NN_local[i_inner];\n    T_batch_matmul_NN[((((((int)threadIdx.x) * 8) + (((int)blockIdx.x) * 4)) + i_inner) + 2)] = T_batch_matmul_NN_local[(i_inner + 2)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 1, 6), \"float32\"), ph_3: T.Buffer((2, 1, 6), \"float32\"), ph_8: T.Buffer((2, 6, 1), \"float32\"), T_mod: T.Buffer((2, 1, 6), \"float32\"), compute: T.Buffer((2, 1, 6), \"float32\"), compute_1: T.Buffer((2, 1, 6), \"float32\"), T_batch_matmul_NN: T.Buffer((2, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([12], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((12,), data=ph_0.data)\n        for ax0 in T.parallel(2):\n            for ax2 in range(6):\n                cse_var_1: T.int32 = ax0 * 6 + ax2\n                T_mod_1 = T.Buffer((12,), data=T_mod.data)\n                ph_3_1 = T.Buffer((12,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(12):\n            compute_2 = T.Buffer((12,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(12):\n            compute_2 = T.Buffer((12,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((12,), data=auto_scheduler_layout_transform, align=32)\n        for ax0_ax1_fused in T.parallel(2):\n            for ax2 in range(6):\n                cse_var_2: T.int32 = ax0_ax1_fused * 6 + ax2\n                ph_8_1 = T.Buffer((12,), data=ph_8.data)\n                auto_scheduler_layout_transform_1[cse_var_2] = ph_8_1[cse_var_2]\n        for b_i_fused_j_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((2,), data=T_batch_matmul_NN.data)\n            T_batch_matmul_NN_1[b_i_fused_j_fused] = T.float32(0)\n            for k in range(6):\n                cse_var_3: T.int32 = b_i_fused_j_fused * 6 + k\n                T_batch_matmul_NN_1[b_i_fused_j_fused] = T_batch_matmul_NN_1[b_i_fused_j_fused] + T.atanh(ph_0_1[cse_var_3]) * auto_scheduler_layout_transform_1[cse_var_3]",
        "op_args": [
            [
                "mod",
                "atan",
                "atanh",
                "atanh",
                "batch_matmul"
            ]
        ],
        "input_shape": [[2, 1, 6], [17, 16, 5], [2, 1, 6], [2, 6, 1]],
        "output_shape": [[2, 1, 6], [17, 16, 5], [2, 1, 6], [2, 1, 6], [2, 1, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 88; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] - ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 704; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 704; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 8, 8), \"float32\"), ph_3: T.Buffer((11, 8, 8), \"float32\"), T_subtract: T.Buffer((11, 8, 8), \"float32\"), compute: T.Buffer((11, 8, 8), \"float32\"), compute_1: T.Buffer((11, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((704,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(88):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_subtract_1 = T.Buffer((704,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((704,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(704):\n            compute_2 = T.Buffer((704,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(704):\n            compute_2 = T.Buffer((704,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "atan",
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[11, 8, 8], [6, 12, 6], [11, 8, 8]],
        "output_shape": [[11, 8, 8], [6, 12, 6], [11, 8, 8], [11, 8, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_divide[(((ax0 * 140) + (ax1 * 14)) + ax2)] = (ph_0[(((ax0 * 140) + (ax1 * 14)) + ax2)] / ph_3[(((ax0 * 140) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 140) + (i1 * 14)) + i2)] = atanf(ph_0[(((i0 * 140) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1540; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1540; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 10, 14), \"float32\"), ph_3: T.Buffer((11, 10, 14), \"float32\"), T_divide: T.Buffer((11, 10, 14), \"float32\"), compute: T.Buffer((11, 10, 14), \"float32\"), compute_1: T.Buffer((11, 10, 14), \"float32\"), compute_2: T.Buffer((11, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1540,), data=ph_0.data)\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(10, 14):\n                cse_var_1: T.int32 = ax0 * 140 + ax1 * 14 + ax2\n                T_divide_1 = T.Buffer((1540,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1540,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(10, 14):\n                cse_var_2: T.int32 = i0 * 140 + i1 * 14 + i2\n                compute_3 = T.Buffer((1540,), data=compute.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1540):\n            compute_3 = T.Buffer((1540,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1540):\n            compute_3 = T.Buffer((1540,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "atan",
                "ceil",
                "sin",
                "abs"
            ]
        ],
        "input_shape": [[11, 10, 14], [9, 19, 6], [11, 10, 14]],
        "output_shape": [[11, 10, 14], [9, 19, 6], [11, 10, 14], [11, 10, 14], [11, 10, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 75) + (i1 * 15)) + i2)] = asinhf(ph_0[(((i0 * 75) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1200; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n      compute_2[((i0_i1_fused * 15) + i2_1)] = ceilf(ph_0[((i0_i1_fused * 15) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_2 = 0; i2_2 < 15; ++i2_2) {\n        compute_3[(((i0_1 * 75) + (i1_1 * 15)) + i2_2)] = asinhf(fabsf(ph_0[(((i0_1 * 75) + (i1_1 * 15)) + i2_2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1200; ++i0_i1_fused_i2_fused_1) {\n    compute_4[i0_i1_fused_i2_fused_1] = atanf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 5, 15), \"float32\"), compute: T.Buffer((16, 5, 15), \"float32\"), compute_1: T.Buffer((16, 5, 15), \"float32\"), compute_2: T.Buffer((16, 5, 15), \"float32\"), compute_3: T.Buffer((16, 5, 15), \"float32\"), compute_4: T.Buffer((16, 5, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1200,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(5, 15):\n                cse_var_1: T.int32 = i0 * 75 + i1 * 15 + i2\n                compute_5 = T.Buffer((1200,), data=compute.data)\n                compute_5[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1200):\n            compute_5 = T.Buffer((1200,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(80):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_5 = T.Buffer((1200,), data=compute_2.data)\n                compute_5[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(5, 15):\n                cse_var_3: T.int32 = i0 * 75 + i1 * 15 + i2\n                compute_5 = T.Buffer((1200,), data=compute_3.data)\n                compute_5[cse_var_3] = T.asinh(T.fabs(ph_0_1[cse_var_3]))\n        for i0_i1_fused_i2_fused in T.parallel(1200):\n            compute_5 = T.Buffer((1200,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "ceil",
                "atanh",
                "ceil",
                "abs",
                "asinh",
                "atan"
            ]
        ],
        "input_shape": [[16, 5, 15]],
        "output_shape": [[16, 5, 15], [16, 5, 15], [16, 5, 15], [16, 5, 15], [16, 5, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 720; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 720; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_2[((i0_i1_fused * 6) + i2)] = acosf((ph_0[((i0_i1_fused * 6) + i2)] - ph_3[((i0_i1_fused * 6) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 6, 6), \"float32\"), ph_3: T.Buffer((20, 6, 6), \"float32\"), compute: T.Buffer((20, 6, 6), \"float32\"), compute_1: T.Buffer((20, 6, 6), \"float32\"), T_multiply: T.Buffer((20, 6, 6), \"float32\"), compute_2: T.Buffer((20, 6, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_3 = T.Buffer((720,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_3 = T.Buffer((720,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(720):\n            T_multiply_1 = T.Buffer((720,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(120):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((720,), data=compute_2.data)\n                ph_3_1 = T.Buffer((720,), data=ph_3.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1] - ph_3_1[cse_var_1])",
        "op_args": [
            [
                "subtract",
                "asinh",
                "atanh",
                "cos",
                "multiply",
                "acos"
            ]
        ],
        "input_shape": [[20, 6, 6], [14, 14, 9], [20, 6, 6]],
        "output_shape": [[14, 14, 9], [20, 6, 6], [20, 6, 6], [20, 6, 6], [20, 6, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_subtract[(((ax0 * 42) + (ax1 * 6)) + ax2)] = (ph_0[(((ax0 * 42) + (ax1 * 6)) + ax2)] - ph_3[(((ax0 * 42) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 42) + (i1 * 6)) + i2)] = acosf(ph_0[(((i0 * 42) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 798; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 7, 6), \"float32\"), ph_3: T.Buffer((19, 7, 6), \"float32\"), T_subtract: T.Buffer((19, 7, 6), \"float32\"), compute: T.Buffer((19, 7, 6), \"float32\"), compute_1: T.Buffer((19, 7, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((798,), data=ph_0.data)\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(7, 6):\n                cse_var_1: T.int32 = ax0 * 42 + ax1 * 6 + ax2\n                T_subtract_1 = T.Buffer((798,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((798,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(7, 6):\n                cse_var_2: T.int32 = i0 * 42 + i1 * 6 + i2\n                compute_2 = T.Buffer((798,), data=compute.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(798):\n            compute_2 = T.Buffer((798,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[19, 7, 6], [7, 8, 19], [19, 7, 6]],
        "output_shape": [[19, 7, 6], [7, 8, 19], [19, 7, 6], [19, 7, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 70; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 70; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 70; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_2[(((i0 * 35) + (i1 * 5)) + i2)] = acosf(ph_0[(((i0 * 35) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 7, 5), \"float32\"), compute: T.Buffer((2, 7, 5), \"float32\"), T_subtract: T.Buffer((2, 7, 5), \"float32\"), compute_1: T.Buffer((2, 7, 5), \"float32\"), compute_2: T.Buffer((2, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((70,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(70):\n            compute_3 = T.Buffer((70,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(70):\n            T_subtract_1 = T.Buffer((70,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(70):\n            compute_3 = T.Buffer((70,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(7, 5):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 5 + i2\n                compute_3 = T.Buffer((70,), data=compute_2.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atanh",
                "cos",
                "subtract",
                "acos",
                "acos"
            ]
        ],
        "input_shape": [[2, 7, 5]],
        "output_shape": [[2, 7, 5], [2, 7, 5], [2, 7, 5], [2, 7, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 140; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 140; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 140; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - atanf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 2, 14), \"float32\"), compute: T.Buffer((5, 2, 14), \"float32\"), compute_1: T.Buffer((5, 2, 14), \"float32\"), T_subtract: T.Buffer((5, 2, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((140,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(140):\n            compute_2 = T.Buffer((140,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(140):\n            compute_2 = T.Buffer((140,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(140):\n            T_subtract_1 = T.Buffer((140,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.atan(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "exp",
                "cos",
                "asinh",
                "atan",
                "subtract"
            ]
        ],
        "input_shape": [[5, 2, 14]],
        "output_shape": [[5, 2, 14], [5, 2, 14], [5, 2, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 110; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] - ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute_1[((i0_i1_fused * 10) + i2)] = asinf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 11; ++i1) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_2[((i1 * 10) + i2_1)] = cosf(asinf(ph_0[((i1 * 10) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 110; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = fabsf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 10), \"float32\"), compute: T.Buffer((1, 11, 10), \"float32\"), compute_1: T.Buffer((1, 11, 10), \"float32\"), compute_2: T.Buffer((1, 11, 10), \"float32\"), compute_3: T.Buffer((1, 11, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((110,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(110):\n            compute_4 = T.Buffer((110,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(11):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_4 = T.Buffer((110,), data=compute_1.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i1, i2 in T.grid(11, 10):\n            cse_var_2: T.int32 = i1 * 10 + i2\n            compute_4 = T.Buffer((110,), data=compute_2.data)\n            compute_4[cse_var_2] = T.cos(T.asin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(110):\n            compute_4 = T.Buffer((110,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "subtract",
                "exp",
                "asin",
                "asin",
                "cos",
                "abs"
            ]
        ],
        "input_shape": [[1, 11, 10]],
        "output_shape": [[1, 11, 10], [1, 11, 10], [1, 11, 10], [1, 11, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 133; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_add[((ax0_ax1_fused * 19) + ax2)] = (ph_0[((ax0_ax1_fused * 19) + ax2)] + ph_3[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2527; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2527; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2527; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 7, 19), \"float32\"), ph_3: T.Buffer((19, 7, 19), \"float32\"), T_add: T.Buffer((19, 7, 19), \"float32\"), compute: T.Buffer((19, 7, 19), \"float32\"), compute_1: T.Buffer((19, 7, 19), \"float32\"), T_divide: T.Buffer((19, 7, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2527,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(133):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_add_1 = T.Buffer((2527,), data=T_add.data)\n                ph_3_1 = T.Buffer((2527,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2527):\n            compute_2 = T.Buffer((2527,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2527):\n            compute_2 = T.Buffer((2527,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(2527):\n            T_divide_1 = T.Buffer((2527,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "acos",
                "atan",
                "atanh",
                "divide"
            ]
        ],
        "input_shape": [[19, 7, 19], [3, 2, 13], [19, 7, 19]],
        "output_shape": [[19, 7, 19], [3, 2, 13], [19, 7, 19], [19, 7, 19], [19, 7, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_8) {\n  float auto_scheduler_layout_transform[18];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 144; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 144; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused_1] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], asinf(ph_0[ax0_ax1_fused_ax2_fused_1])) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  for (int32_t ax5 = 0; ax5 < 6; ++ax5) {\n    for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n      auto_scheduler_layout_transform[((ax5 * 3) + ax7)] = ph_8[((ax5 * 3) + ax7)];\n    }\n  }\n  for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 6; ++b_outer_inner_init) {\n    for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 4; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 8) + (i_outer_inner_init * 4)) + i_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t b_outer_inner = 0; b_outer_inner < 6; ++b_outer_inner) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 3; ++k_inner) {\n        for (int32_t i_inner = 0; i_inner < 4; ++i_inner) {\n          T_batch_matmul_NN[(((b_outer_inner * 8) + (i_outer_inner * 4)) + i_inner)] = (T_batch_matmul_NN[(((b_outer_inner * 8) + (i_outer_inner * 4)) + i_inner)] + (fmodf(ph_0[((((b_outer_inner * 24) + (i_outer_inner * 12)) + (i_inner * 3)) + k_inner)], asinf(ph_0[((((b_outer_inner * 24) + (i_outer_inner * 12)) + (i_inner * 3)) + k_inner)])) * auto_scheduler_layout_transform[((b_outer_inner * 3) + k_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_8_shared[20];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    T_batch_matmul_NN_local[b_c_outer_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_8_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))] = ph_8[((((((int)blockIdx.x) >> 2) * 20) + (ax0_ax1_fused_ax2_fused_outer_outer * 4)) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      T_batch_matmul_NN_local[b_c_outer_inner] = (T_batch_matmul_NN_local[b_c_outer_inner] + (fmodf(ph_0[(((((((((int)blockIdx.x) >> 2) * 160) + ((((int)threadIdx.x) >> 1) * 80)) + (b_c_outer_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + ((((int)threadIdx.x) & 1) * 5)) + k_outer_inner)], asinf(ph_0[(((((((((int)blockIdx.x) >> 2) * 160) + ((((int)threadIdx.x) >> 1) * 80)) + (b_c_outer_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + ((((int)threadIdx.x) & 1) * 5)) + k_outer_inner)])) * ph_8_shared[((((((int)threadIdx.x) >> 1) * 10) + (b_c_outer_inner * 5)) + k_outer_inner)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[((((((((int)blockIdx.x) >> 2) * 32) + ((((int)threadIdx.x) >> 1) * 16)) + (b_inner * 8)) + ((((int)blockIdx.x) & 3) * 2)) + (((int)threadIdx.x) & 1))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 8, 3), \"float32\"), ph_8: T.Buffer((6, 3, 1), \"float32\"), compute: T.Buffer((6, 8, 3), \"float32\"), T_multiply: T.Buffer((6, 8, 3), \"float32\"), T_multiply_1: T.Buffer((6, 8, 3), \"float32\"), T_batch_matmul_NN: T.Buffer((6, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([18], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((144,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_1 = T.Buffer((144,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            T_multiply_2 = T.Buffer((144,), data=T_multiply.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            T_multiply_2 = T.Buffer((144,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((18,), data=auto_scheduler_layout_transform)\n        for ax5, ax7 in T.grid(6, 3):\n            cse_var_1: T.int32 = ax5 * 3 + ax7\n            ph_8_1 = T.Buffer((18,), data=ph_8.data)\n            auto_scheduler_layout_transform_1[cse_var_1] = ph_8_1[cse_var_1]\n        T_batch_matmul_NN_1 = T.Buffer((48,), data=T_batch_matmul_NN.data)\n        for b_outer_inner_init, i_outer_inner_init, i_inner_init in T.grid(6, 2, 4):\n            T_batch_matmul_NN_1[b_outer_inner_init * 8 + i_outer_inner_init * 4 + i_inner_init] = T.float32(0)\n        for b_outer_inner, i_outer_inner, k_inner, i_inner in T.grid(6, 2, 3, 4):\n            cse_var_3: T.int32 = b_outer_inner * 8 + i_outer_inner * 4 + i_inner\n            cse_var_2: T.int32 = b_outer_inner * 24 + i_outer_inner * 12 + i_inner * 3 + k_inner\n            T_batch_matmul_NN_1[cse_var_3] = T_batch_matmul_NN_1[cse_var_3] + T.truncmod(ph_0_1[cse_var_2], T.asin(ph_0_1[cse_var_2])) * auto_scheduler_layout_transform_1[b_outer_inner * 3 + k_inner]",
        "op_args": [
            [
                "abs",
                "acosh",
                "multiply",
                "asin",
                "mod",
                "multiply",
                "batch_matmul"
            ]
        ],
        "input_shape": [[6, 8, 3], [6, 3, 1]],
        "output_shape": [[6, 8, 3], [6, 8, 3], [6, 8, 3], [6, 8, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1089; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 99) + (i1 * 11)) + i2)] = fabsf(ph_0[(((i0 * 99) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1089; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 9, 11), \"float32\"), ph_3: T.Buffer((11, 9, 11), \"float32\"), T_add: T.Buffer((11, 9, 11), \"float32\"), compute: T.Buffer((11, 9, 11), \"float32\"), compute_1: T.Buffer((11, 9, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1089,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1089):\n            T_add_1 = T.Buffer((1089,), data=T_add.data)\n            ph_3_1 = T.Buffer((1089,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(9, 11):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 11 + i2\n                compute_2 = T.Buffer((1089,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1089):\n            compute_2 = T.Buffer((1089,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "abs",
                "asinh",
                "acosh"
            ]
        ],
        "input_shape": [[11, 9, 11], [13, 5, 12], [11, 9, 11]],
        "output_shape": [[11, 9, 11], [13, 5, 12], [11, 9, 11], [11, 9, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 52; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_add[((ax0_ax1_fused * 5) + ax2)] = (ph_0[((ax0_ax1_fused * 5) + ax2)] + ph_3[((ax0_ax1_fused * 5) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 260; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 260; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 4, 5), \"float32\"), ph_3: T.Buffer((13, 4, 5), \"float32\"), T_add: T.Buffer((13, 4, 5), \"float32\"), T_divide: T.Buffer((13, 4, 5), \"float32\"), compute: T.Buffer((13, 4, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((260,), data=ph_0.data)\n        ph_3_1 = T.Buffer((260,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(52):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_add_1 = T.Buffer((260,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(260):\n            T_divide_1 = T.Buffer((260,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(260):\n            compute_1 = T.Buffer((260,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "divide",
                "acosh"
            ]
        ],
        "input_shape": [[13, 4, 5], [6, 2, 3], [13, 4, 5]],
        "output_shape": [[13, 4, 5], [6, 2, 3], [13, 4, 5], [13, 4, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 336; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 336; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 336; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 4, 12), \"float32\"), ph_3: T.Buffer((7, 4, 12), \"float32\"), T_add: T.Buffer((7, 4, 12), \"float32\"), compute: T.Buffer((7, 4, 12), \"float32\"), compute_1: T.Buffer((7, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((336,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(336):\n            T_add_1 = T.Buffer((336,), data=T_add.data)\n            ph_3_1 = T.Buffer((336,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(336):\n            compute_2 = T.Buffer((336,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(336):\n            compute_2 = T.Buffer((336,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "acosh",
                "abs",
                "atan"
            ]
        ],
        "input_shape": [[7, 4, 12], [2, 2, 14], [7, 4, 12]],
        "output_shape": [[7, 4, 12], [2, 2, 14], [7, 4, 12], [7, 4, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 800; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 100; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 8) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 8) + ax2)]) * ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 800; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 800; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 5, 8), \"float32\"), ph_3: T.Buffer((20, 5, 8), \"float32\"), compute: T.Buffer((20, 5, 8), \"float32\"), T_multiply: T.Buffer((20, 5, 8), \"float32\"), T_divide: T.Buffer((20, 5, 8), \"float32\"), compute_1: T.Buffer((20, 5, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((800,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(800):\n            compute_2 = T.Buffer((800,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(100):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_multiply_1 = T.Buffer((800,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(800):\n            T_divide_1 = T.Buffer((800,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(800):\n            compute_2 = T.Buffer((800,), data=compute_1.data)\n            ph_3_1 = T.Buffer((800,), data=ph_3.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "exp",
                "asin",
                "multiply",
                "divide",
                "atan"
            ]
        ],
        "input_shape": [[20, 5, 8], [5, 15, 10], [20, 5, 8]],
        "output_shape": [[5, 15, 10], [20, 5, 8], [20, 5, 8], [20, 5, 8], [20, 5, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 76; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = acoshf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 456; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n        compute_2[(((i0 * 114) + (i1 * 6)) + i2_1)] = asinf(asinhf(ph_0[(((i0 * 114) + (i1 * 6)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 456; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanhf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = ceilf(asinhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 19, 6), \"float32\"), ph_3: T.Buffer((4, 19, 6), \"float32\"), compute: T.Buffer((4, 19, 6), \"float32\"), compute_1: T.Buffer((4, 19, 6), \"float32\"), compute_2: T.Buffer((4, 19, 6), \"float32\"), compute_3: T.Buffer((4, 19, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((456,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(76):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_4 = T.Buffer((456,), data=compute.data)\n                compute_4[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(456):\n            compute_4 = T.Buffer((456,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(19, 6):\n                cse_var_2: T.int32 = i0 * 114 + i1 * 6 + i2\n                compute_4 = T.Buffer((456,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(T.asinh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(456):\n            compute_4 = T.Buffer((456,), data=compute_3.data)\n            ph_3_1 = T.Buffer((456,), data=ph_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "acosh",
                "asinh",
                "ceil",
                "asin",
                "atanh"
            ]
        ],
        "input_shape": [[4, 19, 6], [10, 14, 1], [4, 19, 6]],
        "output_shape": [[10, 14, 1], [4, 19, 6], [4, 19, 6], [4, 19, 6], [4, 19, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 30) + (i1 * 3)) + i2)] = acoshf(ph_0[(((i0 * 30) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 60; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 10, 3), \"float32\"), compute: T.Buffer((2, 10, 3), \"float32\"), compute_1: T.Buffer((2, 10, 3), \"float32\"), compute_2: T.Buffer((2, 10, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((60,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(10, 3):\n                cse_var_1: T.int32 = i0 * 30 + i1 * 3 + i2\n                compute_3 = T.Buffer((60,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_3 = T.Buffer((60,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_3 = T.Buffer((60,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "abs",
                "sin",
                "asin"
            ]
        ],
        "input_shape": [[2, 10, 3]],
        "output_shape": [[2, 10, 3], [2, 10, 3], [2, 10, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 210; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / acosf(ph_0[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = asinf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 210; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(acosf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 5, 6), \"float32\"), T_add: T.Buffer((7, 5, 6), \"float32\"), compute: T.Buffer((7, 5, 6), \"float32\"), compute_1: T.Buffer((7, 5, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((210,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(210):\n            T_add_1 = T.Buffer((210,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(35):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((210,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(210):\n            compute_2 = T.Buffer((210,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acos",
                "divide",
                "add",
                "asin",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[7, 5, 6]],
        "output_shape": [[7, 5, 6], [7, 5, 6], [7, 5, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 13; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = acoshf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 52; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 52; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 13, 4), \"float32\"), compute: T.Buffer((1, 13, 4), \"float32\"), T_multiply: T.Buffer((1, 13, 4), \"float32\"), compute_1: T.Buffer((1, 13, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((52,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(13):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_2 = T.Buffer((52,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(52):\n            T_multiply_1 = T.Buffer((52,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(52):\n            compute_2 = T.Buffer((52,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "abs",
                "multiply",
                "abs"
            ]
        ],
        "input_shape": [[1, 13, 4]],
        "output_shape": [[1, 13, 4], [1, 13, 4], [1, 13, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 672; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_divide[(((ax0 * 56) + (ax1 * 4)) + ax2)] = (acoshf(ph_0[(((ax0 * 56) + (ax1 * 4)) + ax2)]) / ph_0[(((ax0 * 56) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 168; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute_1[((i0_i1_fused * 4) + i2)] = sinf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 14, 4), \"float32\"), compute: T.Buffer((12, 14, 4), \"float32\"), T_divide: T.Buffer((12, 14, 4), \"float32\"), compute_1: T.Buffer((12, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((672,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_2 = T.Buffer((672,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(14, 4):\n                cse_var_1: T.int32 = ax0 * 56 + ax1 * 4 + ax2\n                T_divide_1 = T.Buffer((672,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(168):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_2 = T.Buffer((672,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atanh",
                "acosh",
                "divide",
                "sin"
            ]
        ],
        "input_shape": [[12, 14, 4]],
        "output_shape": [[12, 14, 4], [12, 14, 4], [12, 14, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 6) + (i1 * 2)) + i2)] = atanhf(fmodf(ph_0[(((i0 * 6) + (i1 * 2)) + i2)], cosf(ph_0[(((i0 * 6) + (i1 * 2)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 3, 2), \"float32\"), compute: T.Buffer((7, 3, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(3, 2):\n                cse_var_1: T.int32 = i0 * 6 + i1 * 2 + i2\n                compute_1 = T.Buffer((42,), data=compute.data)\n                ph_0_1 = T.Buffer((42,), data=ph_0.data)\n                compute_1[cse_var_1] = T.atanh(T.truncmod(ph_0_1[cse_var_1], T.cos(ph_0_1[cse_var_1])))",
        "op_args": [
            [
                "cos",
                "mod",
                "atanh"
            ]
        ],
        "input_shape": [[7, 3, 2]],
        "output_shape": [[7, 3, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[3536];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 208; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_3[((i0_i1_fused * 17) + i2)] = expf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3536; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf((ph_0[i0_i1_fused_i2_fused] / compute_3[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3536; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf((ph_0[i0_i1_fused_i2_fused_1] / compute_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3536; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3536; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf((ph_0[i0_i1_fused_i2_fused_2] / ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 13, 17), \"float32\"), ph_3: T.Buffer((16, 13, 17), \"float32\"), compute: T.Buffer((16, 13, 17), \"float32\"), compute_1: T.Buffer((16, 13, 17), \"float32\"), T_subtract: T.Buffer((16, 13, 17), \"float32\"), compute_2: T.Buffer((16, 13, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([3536], \"float32\", \"global\")\n        compute_4 = T.Buffer((3536,), data=compute_3)\n        ph_0_1 = T.Buffer((3536,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(208):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_5 = T.Buffer((3536,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] / compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_5 = T.Buffer((3536,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] / compute_4[i0_i1_fused_i2_fused])\n        ph_3_1 = T.Buffer((3536,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3536):\n            T_subtract_1 = T.Buffer((3536,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3536):\n            compute_5 = T.Buffer((3536,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "exp",
                "divide",
                "atan",
                "acos",
                "subtract",
                "asinh"
            ]
        ],
        "input_shape": [[16, 13, 17], [19, 6, 8], [16, 13, 17]],
        "output_shape": [[19, 6, 8], [16, 13, 17], [16, 13, 17], [16, 13, 17], [16, 13, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1352; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 104) + (i1 * 8)) + i2)] = atanhf(acoshf(ph_0[(((i0 * 104) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1352; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + asinf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 13, 8), \"float32\"), compute: T.Buffer((13, 13, 8), \"float32\"), compute_1: T.Buffer((13, 13, 8), \"float32\"), T_add: T.Buffer((13, 13, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1352,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1352):\n            compute_2 = T.Buffer((1352,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(13, 8):\n                cse_var_1: T.int32 = i0 * 104 + i1 * 8 + i2\n                compute_2 = T.Buffer((1352,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(T.acosh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1352):\n            T_add_1 = T.Buffer((1352,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "ceil",
                "acosh",
                "atanh",
                "asin",
                "add"
            ]
        ],
        "input_shape": [[13, 13, 8]],
        "output_shape": [[13, 13, 8], [13, 13, 8], [13, 13, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 225; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 225; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(5) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 5, 15), \"float32\"), ph_3: T.Buffer((3, 5, 15), \"float32\"), T_divide: T.Buffer((3, 5, 15), \"float32\"), T_subtract: T.Buffer((3, 5, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((225,), data=ph_0.data)\n        ph_3_1 = T.Buffer((225,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(225):\n            T_divide_1 = T.Buffer((225,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(225):\n            T_subtract_1 = T.Buffer((225,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "subtract"
            ]
        ],
        "input_shape": [[3, 5, 15], [19, 19, 7], [3, 5, 15]],
        "output_shape": [[3, 5, 15], [19, 19, 7], [3, 5, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_mod[(((ax0 * 40) + (ax1 * 20)) + ax2)] = fmodf(atanhf(ph_0[(((ax0 * 40) + (ax1 * 20)) + ax2)]), ph_0[(((ax0 * 40) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_1[((i0_i1_fused * 20) + i2)] = fabsf(ph_0[((i0_i1_fused * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 160; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 2, 20), \"float32\"), compute: T.Buffer((4, 2, 20), \"float32\"), T_mod: T.Buffer((4, 2, 20), \"float32\"), compute_1: T.Buffer((4, 2, 20), \"float32\"), compute_2: T.Buffer((4, 2, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((160,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            compute_3 = T.Buffer((160,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(4):\n            for ax1, ax2 in T.grid(2, 20):\n                cse_var_1: T.int32 = ax0 * 40 + ax1 * 20 + ax2\n                T_mod_1 = T.Buffer((160,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.atanh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(8):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((160,), data=compute_1.data)\n                compute_3[cse_var_2] = T.fabs(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            compute_3 = T.Buffer((160,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "atanh",
                "mod",
                "abs",
                "cos"
            ]
        ],
        "input_shape": [[4, 2, 20]],
        "output_shape": [[4, 2, 20], [4, 2, 20], [4, 2, 20], [4, 2, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1216; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1216; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 64) + (i1 * 8)) + i2)] = ceilf(atanf(ph_0[(((i0 * 64) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 8, 8), \"float32\"), ph_3: T.Buffer((19, 8, 8), \"float32\"), T_subtract: T.Buffer((19, 8, 8), \"float32\"), compute: T.Buffer((19, 8, 8), \"float32\"), compute_1: T.Buffer((19, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1216,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1216):\n            T_subtract_1 = T.Buffer((1216,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((1216,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1216):\n            compute_2 = T.Buffer((1216,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(8, 8):\n                cse_var_1: T.int32 = i0 * 64 + i1 * 8 + i2\n                compute_2 = T.Buffer((1216,), data=compute_1.data)\n                compute_2[cse_var_1] = T.ceil(T.atan(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "subtract",
                "cos",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[19, 8, 8], [1, 14, 3], [19, 8, 8]],
        "output_shape": [[19, 8, 8], [1, 14, 3], [19, 8, 8], [19, 8, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 135; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 135; ++ax0_ax1_fused) {\n    T_add[ax0_ax1_fused] = (sinf(ph_0[ax0_ax1_fused]) + ph_0[ax0_ax1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 135; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      compute_2[((i0 * 9) + i1)] = asinhf(ph_0[((i0 * 9) + i1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 1), \"float32\"), compute: T.Buffer((15, 9, 1), \"float32\"), T_add: T.Buffer((15, 9, 1), \"float32\"), compute_1: T.Buffer((15, 9, 1), \"float32\"), compute_2: T.Buffer((15, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((135,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(135):\n            compute_3 = T.Buffer((135,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(135):\n            T_add_1 = T.Buffer((135,), data=T_add.data)\n            T_add_1[ax0_ax1_fused] = T.sin(ph_0_1[ax0_ax1_fused]) + ph_0_1[ax0_ax1_fused]\n        for i0_i1_fused_i2_fused in T.parallel(135):\n            compute_3 = T.Buffer((135,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(15):\n            for i1 in range(9):\n                cse_var_1: T.int32 = i0 * 9 + i1\n                compute_3 = T.Buffer((135,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "abs",
                "sin",
                "add",
                "asinh",
                "asinh"
            ]
        ],
        "input_shape": [[15, 9, 1]],
        "output_shape": [[15, 9, 1], [15, 9, 1], [15, 9, 1], [15, 9, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 195; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 195; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 39) + (i1 * 13)) + i2)] = expf(asinhf(ph_0[(((i0 * 39) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        T_divide[(((ax0 * 39) + (ax1 * 13)) + ax2)] = (asinhf(ph_0[(((ax0 * 39) + (ax1 * 13)) + ax2)]) / ph_0[(((ax0 * 39) + (ax1 * 13)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 3, 13), \"float32\"), ph_3: T.Buffer((5, 3, 13), \"float32\"), T_mod: T.Buffer((5, 3, 13), \"float32\"), compute: T.Buffer((5, 3, 13), \"float32\"), compute_1: T.Buffer((5, 3, 13), \"float32\"), T_divide: T.Buffer((5, 3, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((195,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(195):\n            T_mod_1 = T.Buffer((195,), data=T_mod.data)\n            ph_3_1 = T.Buffer((195,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(195):\n            compute_2 = T.Buffer((195,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(3, 13):\n                cse_var_1: T.int32 = i0 * 39 + i1 * 13 + i2\n                compute_2 = T.Buffer((195,), data=compute_1.data)\n                compute_2[cse_var_1] = T.exp(T.asinh(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(3, 13):\n                cse_var_2: T.int32 = ax0 * 39 + ax1 * 13 + ax2\n                T_divide_1 = T.Buffer((195,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]",
        "op_args": [
            [
                "mod",
                "acosh",
                "asinh",
                "exp",
                "divide"
            ]
        ],
        "input_shape": [[5, 3, 13], [18, 20, 19], [5, 3, 13]],
        "output_shape": [[5, 3, 13], [18, 20, 19], [5, 3, 13], [5, 3, 13], [5, 3, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 810; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0_i1_fused * 6) + i2)] = fabsf(fmodf(ph_0[((i0_i1_fused * 6) + i2)], (ph_0[((i0_i1_fused * 6) + i2)] - cosf(asinf(ph_0[((i0_i1_fused * 6) + i2)])))));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 810; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused_1], (ph_0[i0_i1_fused_i2_fused_1] - cosf(asinf(ph_0[i0_i1_fused_i2_fused_1])))));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - __cosf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 6), \"float32\"), compute: T.Buffer((15, 9, 6), \"float32\"), compute_1: T.Buffer((15, 9, 6), \"float32\"), compute_2: T.Buffer((15, 9, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((810,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(810):\n            compute_3 = T.Buffer((810,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((810,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] - T.cos(T.asin(ph_0_1[cse_var_1]))))\n        for i0_i1_fused_i2_fused in T.parallel(810):\n            compute_3 = T.Buffer((810,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_0_1[i0_i1_fused_i2_fused] - T.cos(T.asin(ph_0_1[i0_i1_fused_i2_fused]))))",
        "op_args": [
            [
                "ceil",
                "asin",
                "cos",
                "subtract",
                "mod",
                "abs",
                "acosh"
            ]
        ],
        "input_shape": [[15, 9, 6]],
        "output_shape": [[15, 9, 6], [15, 9, 6], [15, 9, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = asinf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 60; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 3) + i2_1)] = atanhf(atanf(ph_0[((i0_i1_fused_1 * 3) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 180; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 4, 3), \"float32\"), compute: T.Buffer((15, 4, 3), \"float32\"), compute_1: T.Buffer((15, 4, 3), \"float32\"), compute_2: T.Buffer((15, 4, 3), \"float32\"), compute_3: T.Buffer((15, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_4 = T.Buffer((180,), data=compute.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0_i1_fused * 3 + i2\n                compute_4 = T.Buffer((180,), data=compute_1.data)\n                compute_4[cse_var_2] = T.atanh(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_4 = T.Buffer((180,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_4 = T.Buffer((180,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "atan",
                "atanh",
                "cos",
                "ceil"
            ]
        ],
        "input_shape": [[15, 4, 3]],
        "output_shape": [[15, 4, 3], [15, 4, 3], [15, 4, 3], [15, 4, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 600; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 60) + (i1 * 5)) + i2)] = fabsf(acoshf(ph_0[(((i0 * 60) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 600; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 600; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 12, 5), \"float32\"), compute: T.Buffer((10, 12, 5), \"float32\"), compute_1: T.Buffer((10, 12, 5), \"float32\"), compute_2: T.Buffer((10, 12, 5), \"float32\"), compute_3: T.Buffer((10, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((600,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(600):\n            compute_4 = T.Buffer((600,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(12, 5):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 5 + i2\n                compute_4 = T.Buffer((600,), data=compute_1.data)\n                compute_4[cse_var_1] = T.fabs(T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(600):\n            compute_4 = T.Buffer((600,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(600):\n            compute_4 = T.Buffer((600,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "acosh",
                "abs",
                "exp",
                "atanh"
            ]
        ],
        "input_shape": [[10, 12, 5]],
        "output_shape": [[10, 12, 5], [10, 12, 5], [10, 12, 5], [10, 12, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2907; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_add[(((ax0 * 153) + (ax1 * 17)) + ax2)] = (ph_0[(((ax0 * 153) + (ax1 * 17)) + ax2)] + asinf(atanhf(ph_0[(((ax0 * 153) + (ax1 * 17)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute_1[(((i0 * 153) + (i1 * 17)) + i2)] = asinhf(cosf(ph_0[(((i0 * 153) + (i1 * 17)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] + asinf(atanhf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 9, 17), \"float32\"), compute: T.Buffer((19, 9, 17), \"float32\"), T_add: T.Buffer((19, 9, 17), \"float32\"), compute_1: T.Buffer((19, 9, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2907,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2907):\n            compute_2 = T.Buffer((2907,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(9, 17):\n                cse_var_1: T.int32 = ax0 * 153 + ax1 * 17 + ax2\n                T_add_1 = T.Buffer((2907,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.asin(T.atanh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(9, 17):\n                cse_var_2: T.int32 = i0 * 153 + i1 * 17 + i2\n                compute_2 = T.Buffer((2907,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "atan",
                "atanh",
                "asin",
                "add",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[19, 9, 17]],
        "output_shape": [[19, 9, 17], [19, 9, 17], [19, 9, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 4; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_divide[(((ax0 * 16) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 16) + (ax1 * 4)) + ax2)] / ph_3[(((ax0 * 16) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 32; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 32; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 32; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] / ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 4, 4), \"float32\"), ph_3: T.Buffer((2, 4, 4), \"float32\"), T_divide: T.Buffer((2, 4, 4), \"float32\"), T_add: T.Buffer((2, 4, 4), \"float32\"), compute: T.Buffer((2, 4, 4), \"float32\"), T_multiply: T.Buffer((2, 4, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        ph_3_1 = T.Buffer((32,), data=ph_3.data)\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(4, 4):\n                cse_var_1: T.int32 = ax0 * 16 + ax1 * 4 + ax2\n                T_divide_1 = T.Buffer((32,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(32):\n            T_add_1 = T.Buffer((32,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_1 = T.Buffer((32,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atan(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(32):\n            T_multiply_1 = T.Buffer((32,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "add",
                "acosh",
                "atan",
                "multiply"
            ]
        ],
        "input_shape": [[2, 4, 4], [20, 5, 4], [2, 4, 4]],
        "output_shape": [[2, 4, 4], [20, 5, 4], [2, 4, 4], [2, 4, 4], [2, 4, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[32];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 320; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax5 = 0; ax5 < 16; ++ax5) {\n    for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n      auto_scheduler_layout_transform[((ax5 * 2) + ax7)] = ph_4[((ax5 * 2) + ax7)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 16; ++b_outer_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 10) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + i_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 16; ++b_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n        for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n          T_batch_matmul_NN[(((b_outer_inner * 10) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + i_inner)] = (T_batch_matmul_NN[(((b_outer_inner * 10) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + i_inner)] + (acoshf(ph_0[((((b_outer_inner * 20) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (i_inner * 2)) + k_inner)]) * auto_scheduler_layout_transform[((b_outer_inner * 2) + k_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 320; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 320; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[3];\n  __shared__ float compute_shared[12];\n  __shared__ float ph_4_shared[4];\n  for (int i_c_inner_init = 0; i_c_inner_init < 3; ++i_c_inner_init) {\n    T_batch_matmul_NN_local[i_c_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 12; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      compute_shared[ax0_ax1_fused_ax2_fused_outer_outer] = acoshf(ph_0[((((((int)blockIdx.x) * 24) + ((ax0_ax1_fused_ax2_fused_outer_outer >> 2) * 8)) + (k_outer_outer * 4)) + (ax0_ax1_fused_ax2_fused_outer_outer & 3))]);\n    }\n    for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 4; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n      ph_4_shared[ax0_ax1_fused_ax2_fused_outer_outer_1] = ph_4[((k_outer_outer * 4) + ax0_ax1_fused_ax2_fused_outer_outer_1)];\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 4; ++k_inner) {\n      for (int i_c_inner = 0; i_c_inner < 3; ++i_c_inner) {\n        T_batch_matmul_NN_local[i_c_inner] = (T_batch_matmul_NN_local[i_c_inner] + (compute_shared[((i_c_inner * 4) + k_inner)] * ph_4_shared[k_inner]));\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 3; ++i_inner) {\n    T_batch_matmul_NN[((((int)blockIdx.x) * 3) + i_inner)] = T_batch_matmul_NN_local[i_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 10, 2), \"float32\"), ph_4: T.Buffer((16, 2, 1), \"float32\"), compute: T.Buffer((16, 10, 2), \"float32\"), T_batch_matmul_NN: T.Buffer((16, 10, 1), \"float32\"), compute_1: T.Buffer((16, 10, 2), \"float32\"), compute_2: T.Buffer((16, 10, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([32], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((320,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            compute_3 = T.Buffer((320,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((32,), data=auto_scheduler_layout_transform)\n        for ax5, ax7 in T.grid(16, 2):\n            cse_var_1: T.int32 = ax5 * 2 + ax7\n            ph_4_1 = T.Buffer((32,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[cse_var_1] = ph_4_1[cse_var_1]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((160,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_inner_init in T.grid(16, 5):\n                T_batch_matmul_NN_1[b_outer_inner_init * 10 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + i_inner_init] = T.float32(0)\n            for b_outer_inner, k_inner, i_inner in T.grid(16, 2, 5):\n                cse_var_2: T.int32 = b_outer_inner * 10 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + i_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.acosh(ph_0_1[b_outer_inner * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + i_inner * 2 + k_inner]) * auto_scheduler_layout_transform_1[b_outer_inner * 2 + k_inner]\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            compute_3 = T.Buffer((320,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(320):\n            compute_3 = T.Buffer((320,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asin",
                "acosh",
                "batch_matmul",
                "abs",
                "asinh",
                "ceil"
            ]
        ],
        "input_shape": [[16, 10, 2], [16, 2, 1]],
        "output_shape": [[16, 10, 2], [16, 10, 1], [16, 10, 2], [16, 10, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 108) + (i1 * 6)) + i2)] = ceilf(ph_0[(((i0 * 108) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_multiply[(((ax0 * 108) + (ax1 * 6)) + ax2)] = (sinf(ph_0[(((ax0 * 108) + (ax1 * 6)) + ax2)]) * ph_0[(((ax0 * 108) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 15; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 18; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 6; ++ax2_1) {\n        T_mod[(((ax0_1 * 108) + (ax1_1 * 6)) + ax2_1)] = fmodf(ph_0[(((ax0_1 * 108) + (ax1_1 * 6)) + ax2_1)], asinf(ph_0[(((ax0_1 * 108) + (ax1_1 * 6)) + ax2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 18, 6), \"float32\"), compute: T.Buffer((15, 18, 6), \"float32\"), T_multiply: T.Buffer((15, 18, 6), \"float32\"), T_mod: T.Buffer((15, 18, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(18, 6):\n                cse_var_1: T.int32 = i0 * 108 + i1 * 6 + i2\n                compute_1 = T.Buffer((1620,), data=compute.data)\n                compute_1[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(18, 6):\n                cse_var_2: T.int32 = ax0 * 108 + ax1 * 6 + ax2\n                T_multiply_1 = T.Buffer((1620,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.sin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(18, 6):\n                cse_var_3: T.int32 = ax0 * 108 + ax1 * 6 + ax2\n                T_mod_1 = T.Buffer((1620,), data=T_mod.data)\n                T_mod_1[cse_var_3] = T.truncmod(ph_0_1[cse_var_3], T.asin(ph_0_1[cse_var_3]))",
        "op_args": [
            [
                "ceil",
                "sin",
                "multiply",
                "asin",
                "mod"
            ]
        ],
        "input_shape": [[15, 18, 6]],
        "output_shape": [[15, 18, 6], [15, 18, 6], [15, 18, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1188; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 66; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 18) + ax2)] = (ph_0[((ax0_ax1_fused * 18) + ax2)] - cosf(atanhf(ph_0[((ax0_ax1_fused * 18) + ax2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1188; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] - __cosf(atanhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 6, 18), \"float32\"), compute: T.Buffer((11, 6, 18), \"float32\"), T_subtract: T.Buffer((11, 6, 18), \"float32\"), compute_1: T.Buffer((11, 6, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1188,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            compute_2 = T.Buffer((1188,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(66):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_subtract_1 = T.Buffer((1188,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.cos(T.atanh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            compute_2 = T.Buffer((1188,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "atanh",
                "cos",
                "subtract",
                "cos"
            ]
        ],
        "input_shape": [[11, 6, 18]],
        "output_shape": [[11, 6, 18], [11, 6, 18], [11, 6, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 513; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf((ph_0[i0_i1_fused_i2_fused] / asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 513; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_2[(((i0 * 27) + (i1 * 3)) + i2)] = asinhf(asinhf(ph_0[(((i0 * 27) + (i1 * 3)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 9, 3), \"float32\"), compute: T.Buffer((19, 9, 3), \"float32\"), compute_1: T.Buffer((19, 9, 3), \"float32\"), compute_2: T.Buffer((19, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((513,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(513):\n            compute_3 = T.Buffer((513,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] / T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(513):\n            compute_3 = T.Buffer((513,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(9, 3):\n                cse_var_1: T.int32 = i0 * 27 + i1 * 3 + i2\n                compute_3 = T.Buffer((513,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(T.asinh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asin",
                "divide",
                "asinh",
                "ceil",
                "asinh",
                "asinh"
            ]
        ],
        "input_shape": [[19, 9, 3]],
        "output_shape": [[19, 9, 3], [19, 9, 3], [19, 9, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 540; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 540; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(asinhf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 540; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 540; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fmodf(asinhf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 4), \"float32\"), compute: T.Buffer((15, 9, 4), \"float32\"), T_mod: T.Buffer((15, 9, 4), \"float32\"), compute_1: T.Buffer((15, 9, 4), \"float32\"), compute_2: T.Buffer((15, 9, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_3 = T.Buffer((540,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(540):\n            T_mod_1 = T.Buffer((540,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_3 = T.Buffer((540,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_3 = T.Buffer((540,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "asinh",
                "mod",
                "sin",
                "sin"
            ]
        ],
        "input_shape": [[15, 9, 4]],
        "output_shape": [[15, 9, 4], [15, 9, 4], [15, 9, 4], [15, 9, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 660; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] - atanhf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 55) + (i1 * 11)) + i2)] = atanhf(ph_0[(((i0 * 55) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 660; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 660; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 5, 11), \"float32\"), T_mod: T.Buffer((12, 5, 11), \"float32\"), compute: T.Buffer((12, 5, 11), \"float32\"), T_subtract: T.Buffer((12, 5, 11), \"float32\"), compute_1: T.Buffer((12, 5, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((660,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(660):\n            T_mod_1 = T.Buffer((660,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] - T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(5, 11):\n                cse_var_1: T.int32 = i0 * 55 + i1 * 11 + i2\n                compute_2 = T.Buffer((660,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(660):\n            T_subtract_1 = T.Buffer((660,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(660):\n            compute_2 = T.Buffer((660,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "subtract",
                "mod",
                "atanh",
                "acos",
                "subtract",
                "acos"
            ]
        ],
        "input_shape": [[12, 5, 11]],
        "output_shape": [[12, 5, 11], [12, 5, 11], [12, 5, 11], [12, 5, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0) {\n  float auto_scheduler_layout_transform[1280];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(fmodf(ph_0[i0_i1_fused_i2_fused], ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n        for (int32_t ax6 = 0; ax6 < 8; ++ax6) {\n          for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n            for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n              auto_scheduler_layout_transform[((((((ax0_ax1_fused_ax2_fused * 256) + (ax4 * 64)) + (ax5 * 32)) + (ax6 * 4)) + (ax7 * 2)) + ax8)] = atanf(ph_0[((((((ax0_ax1_fused_ax2_fused * 256) + (ax5 * 128)) + (ax8 * 64)) + (ax4 * 16)) + (ax7 * 8)) + ax6)]);\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 10; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n        for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 8; ++j_outer_inner_init) {\n          for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n            for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n              T_batch_matmul_NN[((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner_init * 128)) + (b_inner_init * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner_init * 16)) + (i_inner_init * 8)) + j_outer_inner_init)] = 0.000000e+00f;\n            }\n          }\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 4; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n          for (int32_t j_outer_inner = 0; j_outer_inner < 8; ++j_outer_inner) {\n            for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n              for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n                for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n                  T_batch_matmul_NN[((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner * 128)) + (b_inner * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner * 16)) + (i_inner * 8)) + j_outer_inner)] = (T_batch_matmul_NN[((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner * 128)) + (b_inner * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner * 16)) + (i_inner * 8)) + j_outer_inner)] + (ph_0[(((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner * 128)) + (b_inner * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner * 16)) + (i_inner * 8)) + (k_outer * 2)) + k_inner)] * auto_scheduler_layout_transform[(((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (k_outer * 64)) + (b_outer_inner * 32)) + (j_outer_inner * 4)) + (k_inner * 2)) + b_inner)]));\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_6_shared[50];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 4) + (((int)threadIdx.x) / 10)) < 5) {\n      ph_6_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 40) + ((int)threadIdx.x))] = ph_6[(((((int)blockIdx.x) * 50) + (ax0_ax1_fused_ax2_fused_outer_outer * 40)) + ((int)threadIdx.x))];\n    }\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 5; ++k_inner) {\n    for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (ph_0[((((((int)blockIdx.x) * 80) + (b_c_inner * 40)) + ((((int)threadIdx.x) / 5) * 5)) + k_inner)] * ph_6_shared[(((b_c_inner * 25) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[(((((int)blockIdx.x) * 80) + (b_inner * 40)) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 8, 8), \"float32\"), compute: T.Buffer((20, 8, 8), \"float32\"), T_batch_matmul_NN: T.Buffer((20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1280], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1280):\n            compute_1 = T.Buffer((1280,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asinh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.ceil(ph_0_1[i0_i1_fused_i2_fused])))\n        auto_scheduler_layout_transform_1 = T.Buffer((1280,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax4, ax5, ax6, ax7, ax8 in T.grid(4, 2, 8, 2, 2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 256\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 64 + ax5 * 32 + ax6 * 4 + ax7 * 2 + ax8] = T.atan(ph_0_1[cse_var_1 + ax5 * 128 + ax8 * 64 + ax4 * 16 + ax7 * 8 + ax6])\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(10):\n            T_batch_matmul_NN_1 = T.Buffer((1280,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_outer_inner_init, j_outer_inner_init, b_inner_init, i_inner_init in T.grid(2, 2, 8, 2, 2):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5 * 256 + b_outer_inner_init * 128 + b_inner_init * 64 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 5 * 32 + i_outer_inner_init * 16 + i_inner_init * 8 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, i_outer_inner, j_outer_inner, k_inner, b_inner, i_inner in T.grid(4, 2, 2, 8, 2, 2, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5 * 256\n                cse_var_4: T.int32 = cse_var_3 + b_outer_inner * 128 + b_inner * 64 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 5 * 32 + i_outer_inner * 16 + i_inner * 8\n                cse_var_2: T.int32 = cse_var_4 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_4 + k_outer * 2 + k_inner] * auto_scheduler_layout_transform_1[cse_var_3 + k_outer * 64 + b_outer_inner * 32 + j_outer_inner * 4 + k_inner * 2 + b_inner]",
        "op_args": [
            [
                "ceil",
                "mod",
                "asinh",
                "atan",
                "batch_matmul"
            ]
        ],
        "input_shape": [[20, 8, 8]],
        "output_shape": [[20, 8, 8], [20, 8, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  float compute_1[272];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 272; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_1[(((i0 * 16) + (i1 * 2)) + i2)] = expf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 272; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - (compute_1[ax0_ax1_fused_ax2_fused] / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] - (__expf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 8, 2), \"float32\"), compute: T.Buffer((17, 8, 2), \"float32\"), T_subtract: T.Buffer((17, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([272], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((272,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(272):\n            compute_2 = T.Buffer((272,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_2 = T.Buffer((272,), data=compute_1)\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(272):\n            T_subtract_1 = T.Buffer((272,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - compute_2[ax0_ax1_fused_ax2_fused] / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asin",
                "exp",
                "divide",
                "subtract"
            ]
        ],
        "input_shape": [[17, 8, 2]],
        "output_shape": [[17, 8, 2], [17, 8, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4800; ++ax0_ax1_fused_ax2_fused) {\n    float compute[1];\n    compute[0] = expf(ph_0[ax0_ax1_fused_ax2_fused]);\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], compute[0]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 20), \"float32\"), T_multiply: T.Buffer((12, 20, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(4800):\n            compute = T.allocate([1], \"float32\", \"global\")\n            compute_1 = T.Buffer((1,), data=compute, align=4)\n            ph_0_1 = T.Buffer((4800,), data=ph_0.data)\n            compute_1[0] = T.exp(ph_0_1[ax0_ax1_fused_ax2_fused])\n            T_multiply_1 = T.Buffer((4800,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], compute_1[0]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "exp",
                "mod",
                "multiply"
            ]
        ],
        "input_shape": [[12, 20, 20]],
        "output_shape": [[12, 20, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  float compute_3[196];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = sinf((ph_0[((i0_i1_fused * 7) + i2)] - acosf(ph_0[((i0_i1_fused * 7) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 196; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 196; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 196; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 14, 7), \"float32\"), compute: T.Buffer((2, 14, 7), \"float32\"), compute_1: T.Buffer((2, 14, 7), \"float32\"), compute_2: T.Buffer((2, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([196], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((196,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(28):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_4 = T.Buffer((196,), data=compute.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1] - T.acos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(196):\n            compute_4 = T.Buffer((196,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((196,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(196):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(196):\n            compute_5 = T.Buffer((196,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(compute_4[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "subtract",
                "sin",
                "sin",
                "exp",
                "exp"
            ]
        ],
        "input_shape": [[2, 14, 7]],
        "output_shape": [[2, 14, 7], [2, 14, 7], [2, 14, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 210; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = asinf((ph_0[((i0_i1_fused * 6) + i2)] / ceilf(ph_0[((i0_i1_fused * 6) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 210; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 6) + i2_1)] = atanf(ph_0[((i0_i1_fused_1 * 6) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 15, 6), \"float32\"), compute: T.Buffer((14, 15, 6), \"float32\"), compute_1: T.Buffer((14, 15, 6), \"float32\"), compute_2: T.Buffer((14, 15, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(210):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((1260,), data=compute.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1] / T.ceil(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(210):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((1260,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_3 = T.Buffer((1260,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "divide",
                "asin",
                "atan",
                "asin",
                "ceil"
            ]
        ],
        "input_shape": [[14, 15, 6]],
        "output_shape": [[14, 15, 6], [14, 15, 6], [14, 15, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 380; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_add[((ax0_ax1_fused * 11) + ax2)] = (ph_0[((ax0_ax1_fused * 11) + ax2)] + ceilf(ph_0[((ax0_ax1_fused * 11) + ax2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 19, 11), \"float32\"), T_add: T.Buffer((20, 19, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(380):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_add_1 = T.Buffer((4180,), data=T_add.data)\n                ph_0_1 = T.Buffer((4180,), data=ph_0.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.ceil(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "ceil",
                "add"
            ]
        ],
        "input_shape": [[20, 19, 11]],
        "output_shape": [[20, 19, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    float compute_4[1];\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_4[0] = expf(ph_0[((i0_i1_fused * 12) + i2)]);\n      compute_1[((i0_i1_fused * 12) + i2)] = atanhf(compute_4[0]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 720; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 720; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = acoshf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 15, 12), \"float32\"), compute: T.Buffer((4, 15, 12), \"float32\"), compute_1: T.Buffer((4, 15, 12), \"float32\"), compute_2: T.Buffer((4, 15, 12), \"float32\"), compute_3: T.Buffer((4, 15, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(60):\n            compute_4 = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_5 = T.Buffer((1,), data=compute_4, align=4)\n                compute_5[0] = T.exp(ph_0_1[cse_var_1])\n                compute_6 = T.Buffer((720,), data=compute_1.data)\n                compute_6[cse_var_1] = T.atanh(compute_5[0])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_4 = T.Buffer((720,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "exp",
                "atanh",
                "acos",
                "acosh"
            ]
        ],
        "input_shape": [[4, 15, 12]],
        "output_shape": [[4, 15, 12], [4, 15, 12], [4, 15, 12], [4, 15, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 693; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute_1[(((i0 * 99) + (i1 * 9)) + i2)] = atanhf(asinf(ph_0[(((i0 * 99) + (i1 * 9)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 77; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_2[((i0_i1_fused * 9) + i2_1)] = asinf(asinf(ph_0[((i0_i1_fused * 9) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 693; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanf((ph_0[i0_i1_fused_i2_fused_1] * ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 9), \"float32\"), ph_3: T.Buffer((7, 11, 9), \"float32\"), compute: T.Buffer((7, 11, 9), \"float32\"), compute_1: T.Buffer((7, 11, 9), \"float32\"), compute_2: T.Buffer((7, 11, 9), \"float32\"), compute_3: T.Buffer((7, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((693,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(693):\n            compute_4 = T.Buffer((693,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(11, 9):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 9 + i2\n                compute_4 = T.Buffer((693,), data=compute_1.data)\n                compute_4[cse_var_1] = T.atanh(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(77):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_4 = T.Buffer((693,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(T.asin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(693):\n            compute_4 = T.Buffer((693,), data=compute_3.data)\n            ph_3_1 = T.Buffer((693,), data=ph_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "acosh",
                "asin",
                "atanh",
                "asin",
                "atan"
            ]
        ],
        "input_shape": [[7, 11, 9], [17, 16, 20], [7, 11, 9]],
        "output_shape": [[17, 16, 20], [7, 11, 9], [7, 11, 9], [7, 11, 9], [7, 11, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 45; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_divide[((ax0_ax1_fused * 20) + ax2)] = (ph_0[((ax0_ax1_fused * 20) + ax2)] / ph_3[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 300) + (i1 * 20)) + i2)] = ceilf(ph_0[(((i0 * 300) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 900; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 15, 20), \"float32\"), ph_3: T.Buffer((3, 15, 20), \"float32\"), T_divide: T.Buffer((3, 15, 20), \"float32\"), compute: T.Buffer((3, 15, 20), \"float32\"), T_subtract: T.Buffer((3, 15, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(45):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_divide_1 = T.Buffer((900,), data=T_divide.data)\n                ph_3_1 = T.Buffer((900,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(15, 20):\n                cse_var_2: T.int32 = i0 * 300 + i1 * 20 + i2\n                compute_1 = T.Buffer((900,), data=compute.data)\n                compute_1[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            T_subtract_1 = T.Buffer((900,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "ceil",
                "sin",
                "subtract"
            ]
        ],
        "input_shape": [[3, 15, 20], [6, 14, 6], [3, 15, 20]],
        "output_shape": [[3, 15, 20], [6, 14, 6], [3, 15, 20], [3, 15, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 16) + (i1 * 2)) + i2)] = fabsf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 80; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 8, 2), \"float32\"), compute: T.Buffer((5, 8, 2), \"float32\"), T_multiply: T.Buffer((5, 8, 2), \"float32\"), compute_1: T.Buffer((5, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((80,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_2 = T.Buffer((80,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(80):\n            T_multiply_1 = T.Buffer((80,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            compute_2 = T.Buffer((80,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "atan",
                "multiply",
                "asin"
            ]
        ],
        "input_shape": [[5, 8, 2]],
        "output_shape": [[5, 8, 2], [5, 8, 2], [5, 8, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0, float* ph_8) {\n  float auto_scheduler_layout_transform[119];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1785; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(fmodf(ph_0[i0_i1_fused_i2_fused], (ph_0[i0_i1_fused_i2_fused] + ((ph_0[i0_i1_fused_i2_fused] * fabsf(ph_0[i0_i1_fused_i2_fused])) / ph_0[i0_i1_fused_i2_fused]))));\n  }\n  for (int32_t ax4 = 0; ax4 < 7; ++ax4) {\n    for (int32_t ax8 = 0; ax8 < 17; ++ax8) {\n      auto_scheduler_layout_transform[((ax4 * 17) + ax8)] = ph_8[((ax8 * 7) + ax4)];\n    }\n  }\n  for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 5; ++i_outer_inner_init) {\n    for (int32_t b_inner_init = 0; b_inner_init < 17; ++b_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 3; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 15) + (i_outer_inner_init * 3)) + i_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 7; ++k_outer) {\n    for (int32_t i_outer_inner = 0; i_outer_inner < 5; ++i_outer_inner) {\n      for (int32_t b_inner = 0; b_inner < 17; ++b_inner) {\n        for (int32_t i_inner = 0; i_inner < 3; ++i_inner) {\n          T_batch_matmul_NN[(((b_inner * 15) + (i_outer_inner * 3)) + i_inner)] = (T_batch_matmul_NN[(((b_inner * 15) + (i_outer_inner * 3)) + i_inner)] + (fmodf(ph_0[((((b_inner * 105) + (i_outer_inner * 21)) + (i_inner * 7)) + k_outer)], (ph_0[((((b_inner * 105) + (i_outer_inner * 21)) + (i_inner * 7)) + k_outer)] + ((ph_0[((((b_inner * 105) + (i_outer_inner * 21)) + (i_inner * 7)) + k_outer)] * fabsf(ph_0[((((b_inner * 105) + (i_outer_inner * 21)) + (i_inner * 7)) + k_outer)])) / ph_0[((((b_inner * 105) + (i_outer_inner * 21)) + (i_inner * 7)) + k_outer)]))) * auto_scheduler_layout_transform[((k_outer * 17) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float ph_8_shared[20];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 2; ++i_c_outer_inner_init) {\n    for (int b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_inner_init * 2) + i_c_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 20; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_8_shared[ax0_ax1_fused_ax2_fused_outer_outer] = ph_8[(((((int)blockIdx.x) >> 2) * 20) + ax0_ax1_fused_ax2_fused_outer_outer)];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 2; ++i_c_outer_inner) {\n      for (int b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n        T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_outer_inner)] = (T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_outer_inner)] + (fmodf(ph_0[((((((((int)blockIdx.x) >> 2) * 160) + (b_c_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + (i_c_outer_inner * 5)) + k_outer_inner)], (ph_0[((((((((int)blockIdx.x) >> 2) * 160) + (b_c_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + (i_c_outer_inner * 5)) + k_outer_inner)] + ((ph_0[((((((((int)blockIdx.x) >> 2) * 160) + (b_c_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + (i_c_outer_inner * 5)) + k_outer_inner)] * fabsf(ph_0[((((((((int)blockIdx.x) >> 2) * 160) + (b_c_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + (i_c_outer_inner * 5)) + k_outer_inner)])) / ph_0[((((((((int)blockIdx.x) >> 2) * 160) + (b_c_inner * 40)) + ((((int)blockIdx.x) & 3) * 10)) + (i_c_outer_inner * 5)) + k_outer_inner)]))) * ph_8_shared[((b_c_inner * 5) + k_outer_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    for (int i_inner = 0; i_inner < 2; ++i_inner) {\n      T_batch_matmul_NN[(((((((int)blockIdx.x) >> 2) * 32) + (b_inner * 8)) + ((((int)blockIdx.x) & 3) * 2)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 2) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] + ((ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 15, 7), \"float32\"), ph_8: T.Buffer((17, 7, 1), \"float32\"), compute: T.Buffer((17, 15, 7), \"float32\"), T_batch_matmul_NN: T.Buffer((17, 15, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([119], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1785,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1785):\n            compute_1 = T.Buffer((1785,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asinh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_0_1[i0_i1_fused_i2_fused] + ph_0_1[i0_i1_fused_i2_fused] * T.fabs(ph_0_1[i0_i1_fused_i2_fused]) / ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((119,), data=auto_scheduler_layout_transform)\n        for ax4, ax8 in T.grid(7, 17):\n            ph_8_1 = T.Buffer((119,), data=ph_8.data)\n            auto_scheduler_layout_transform_1[ax4 * 17 + ax8] = ph_8_1[ax8 * 7 + ax4]\n        T_batch_matmul_NN_1 = T.Buffer((255,), data=T_batch_matmul_NN.data)\n        for i_outer_inner_init, b_inner_init, i_inner_init in T.grid(5, 17, 3):\n            T_batch_matmul_NN_1[b_inner_init * 15 + i_outer_inner_init * 3 + i_inner_init] = T.float32(0)\n        for k_outer, i_outer_inner, b_inner, i_inner in T.grid(7, 5, 17, 3):\n            cse_var_2: T.int32 = b_inner * 15 + i_outer_inner * 3 + i_inner\n            cse_var_1: T.int32 = b_inner * 105 + i_outer_inner * 21 + i_inner * 7 + k_outer\n            T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] + ph_0_1[cse_var_1] * T.fabs(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]) * auto_scheduler_layout_transform_1[k_outer * 17 + b_inner]",
        "op_args": [
            [
                "abs",
                "multiply",
                "divide",
                "add",
                "mod",
                "asinh",
                "batch_matmul"
            ]
        ],
        "input_shape": [[17, 15, 7], [17, 7, 1]],
        "output_shape": [[17, 15, 7], [17, 15, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2100; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 210) + (i1 * 14)) + i2)] = asinf(ph_0[(((i0 * 210) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2100; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 15, 14), \"float32\"), ph_3: T.Buffer((10, 15, 14), \"float32\"), T_subtract: T.Buffer((10, 15, 14), \"float32\"), compute: T.Buffer((10, 15, 14), \"float32\"), compute_1: T.Buffer((10, 15, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2100,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2100):\n            T_subtract_1 = T.Buffer((2100,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2100,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(15, 14):\n                cse_var_1: T.int32 = i0 * 210 + i1 * 14 + i2\n                compute_2 = T.Buffer((2100,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2100):\n            compute_2 = T.Buffer((2100,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "asin",
                "ceil",
                "acosh"
            ]
        ],
        "input_shape": [[10, 15, 14], [11, 18, 9], [10, 15, 14]],
        "output_shape": [[10, 15, 14], [11, 18, 9], [10, 15, 14], [10, 15, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_8) {\n  float auto_scheduler_layout_transform[70];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 630; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 630; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / fabsf(ceilf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 630; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ceilf(ph_0[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 7; ++ax5) {\n      for (int32_t ax7 = 0; ax7 < 5; ++ax7) {\n        auto_scheduler_layout_transform[(((ax4 * 35) + (ax5 * 5)) + ax7)] = ph_8[(((ax5 * 10) + (ax4 * 5)) + ax7)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 3; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 7; ++b_outer_inner_init) {\n      for (int32_t i_inner_init = 0; i_inner_init < 3; ++i_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 9) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 3)) + i_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 7; ++b_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 5; ++k_inner) {\n          for (int32_t i_inner = 0; i_inner < 3; ++i_inner) {\n            T_batch_matmul_NN[(((b_outer_inner * 9) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 3)) + i_inner)] = (T_batch_matmul_NN[(((b_outer_inner * 9) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 3)) + i_inner)] + (ceilf(ph_0[(((((b_outer_inner * 90) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 30)) + (i_inner * 10)) + (k_outer * 5)) + k_inner)]) * auto_scheduler_layout_transform[(((k_outer * 35) + (b_outer_inner * 5)) + k_inner)]));\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float ph_8_shared[10];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_inner_init * 2) + i_c_inner_init)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_inner_init * 2) + i_c_inner_init) + 4)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_8_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 2) + ((int)threadIdx.x))] = ph_8[(((((int)blockIdx.x) * 10) + (ax0_ax1_fused_ax2_fused_outer_outer * 2)) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n      for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n        T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_inner)] = (T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_inner)] + (ceilf(ph_0[(((((((int)blockIdx.x) * 80) + (b_c_inner * 40)) + (((int)threadIdx.x) * 10)) + (i_c_inner * 5)) + k_outer_inner)]) * ph_8_shared[((b_c_inner * 5) + k_outer_inner)]));\n        T_batch_matmul_NN_local[(((b_c_inner * 2) + i_c_inner) + 4)] = (T_batch_matmul_NN_local[(((b_c_inner * 2) + i_c_inner) + 4)] + (ceilf(ph_0[((((((((int)blockIdx.x) * 80) + (b_c_inner * 40)) + (((int)threadIdx.x) * 10)) + (i_c_inner * 5)) + k_outer_inner) + 20)]) * ph_8_shared[((b_c_inner * 5) + k_outer_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int i_inner = 0; i_inner < 2; ++i_inner) {\n      T_batch_matmul_NN[((((((int)blockIdx.x) * 16) + (b_inner * 8)) + (((int)threadIdx.x) * 2)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 2) + i_inner)];\n      T_batch_matmul_NN[(((((((int)blockIdx.x) * 16) + (b_inner * 8)) + (((int)threadIdx.x) * 2)) + i_inner) + 4)] = T_batch_matmul_NN_local[(((b_inner * 2) + i_inner) + 4)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / fabsf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 9, 10), \"float32\"), ph_8: T.Buffer((7, 10, 1), \"float32\"), compute: T.Buffer((7, 9, 10), \"float32\"), T_divide: T.Buffer((7, 9, 10), \"float32\"), T_mod: T.Buffer((7, 9, 10), \"float32\"), T_batch_matmul_NN: T.Buffer((7, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([70], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((630,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(630):\n            compute_1 = T.Buffer((630,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(630):\n            T_divide_1 = T.Buffer((630,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.fabs(T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(630):\n            T_mod_1 = T.Buffer((630,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((70,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax7 in T.grid(2, 7, 5):\n            ph_8_1 = T.Buffer((70,), data=ph_8.data)\n            auto_scheduler_layout_transform_1[ax4 * 35 + ax5 * 5 + ax7] = ph_8_1[ax5 * 10 + ax4 * 5 + ax7]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(3):\n            T_batch_matmul_NN_1 = T.Buffer((63,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_inner_init in T.grid(7, 3):\n                T_batch_matmul_NN_1[b_outer_inner_init * 9 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 3 + i_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, k_inner, i_inner in T.grid(2, 7, 5, 3):\n                cse_var_1: T.int32 = b_outer_inner * 9 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 3 + i_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + T.ceil(ph_0_1[b_outer_inner * 90 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 30 + i_inner * 10 + k_outer * 5 + k_inner]) * auto_scheduler_layout_transform_1[k_outer * 35 + b_outer_inner * 5 + k_inner]",
        "op_args": [
            [
                "acos",
                "ceil",
                "abs",
                "divide",
                "ceil",
                "mod",
                "batch_matmul"
            ]
        ],
        "input_shape": [[7, 9, 10], [7, 10, 1]],
        "output_shape": [[7, 9, 10], [7, 9, 10], [7, 9, 10], [7, 9, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_add[(((ax0 * 68) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 68) + (ax1 * 4)) + ax2)] + ph_3[(((ax0 * 68) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 340; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 340; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 17, 4), \"float32\"), ph_3: T.Buffer((5, 17, 4), \"float32\"), T_add: T.Buffer((5, 17, 4), \"float32\"), compute: T.Buffer((5, 17, 4), \"float32\"), compute_1: T.Buffer((5, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((340,), data=ph_0.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(17, 4):\n                cse_var_1: T.int32 = ax0 * 68 + ax1 * 4 + ax2\n                T_add_1 = T.Buffer((340,), data=T_add.data)\n                ph_3_1 = T.Buffer((340,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(340):\n            compute_2 = T.Buffer((340,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(340):\n            compute_2 = T.Buffer((340,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[5, 17, 4], [3, 8, 10], [5, 17, 4]],
        "output_shape": [[5, 17, 4], [3, 8, 10], [5, 17, 4], [5, 17, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 32) + (i1 * 2)) + i2)] = acoshf(ph_0[(((i0 * 32) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 448; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 224; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_2[((i0_i1_fused * 2) + i2_1)] = asinf(ph_0[((i0_i1_fused * 2) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 448; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = acoshf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 16, 2), \"float32\"), compute: T.Buffer((14, 16, 2), \"float32\"), compute_1: T.Buffer((14, 16, 2), \"float32\"), compute_2: T.Buffer((14, 16, 2), \"float32\"), compute_3: T.Buffer((14, 16, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((448,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(16, 2):\n                cse_var_1: T.int32 = i0 * 32 + i1 * 2 + i2\n                compute_4 = T.Buffer((448,), data=compute.data)\n                compute_4[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            compute_4 = T.Buffer((448,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(224):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_4 = T.Buffer((448,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            compute_4 = T.Buffer((448,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "cos",
                "asinh",
                "asin",
                "asin",
                "acosh"
            ]
        ],
        "input_shape": [[14, 16, 2]],
        "output_shape": [[14, 16, 2], [14, 16, 2], [14, 16, 2], [14, 16, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    compute[i0_i1_fused] = cosf(fmodf(ph_0[i0_i1_fused], asinf(ph_0[i0_i1_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 108; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 108; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 6, 1), \"float32\"), compute: T.Buffer((18, 6, 1), \"float32\"), compute_1: T.Buffer((18, 6, 1), \"float32\"), compute_2: T.Buffer((18, 6, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((108,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(108):\n            compute_3 = T.Buffer((108,), data=compute.data)\n            compute_3[i0_i1_fused] = T.cos(T.truncmod(ph_0_1[i0_i1_fused], T.asin(ph_0_1[i0_i1_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(108):\n            compute_3 = T.Buffer((108,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(108):\n            compute_3 = T.Buffer((108,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "mod",
                "cos",
                "abs",
                "exp"
            ]
        ],
        "input_shape": [[18, 6, 1]],
        "output_shape": [[18, 6, 1], [18, 6, 1], [18, 6, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 60; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 60; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(asinf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  for (int32_t i1 = 0; i1 < 4; ++i1) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_2[((i1 * 15) + i2)] = acoshf((ph_0[((i1 * 15) + i2)] * ph_3[((i1 * 15) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 4, 15), \"float32\"), ph_3: T.Buffer((1, 4, 15), \"float32\"), compute: T.Buffer((1, 4, 15), \"float32\"), compute_1: T.Buffer((1, 4, 15), \"float32\"), T_mod: T.Buffer((1, 4, 15), \"float32\"), compute_2: T.Buffer((1, 4, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((60,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_3 = T.Buffer((60,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_3 = T.Buffer((60,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(60):\n            T_mod_1 = T.Buffer((60,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i1, i2 in T.grid(4, 15):\n            cse_var_1: T.int32 = i1 * 15 + i2\n            compute_3 = T.Buffer((60,), data=compute_2.data)\n            ph_3_1 = T.Buffer((60,), data=ph_3.data)\n            compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1] * ph_3_1[cse_var_1])",
        "op_args": [
            [
                "multiply",
                "sin",
                "asin",
                "abs",
                "mod",
                "acosh"
            ]
        ],
        "input_shape": [[1, 4, 15], [12, 18, 10], [1, 4, 15]],
        "output_shape": [[12, 18, 10], [1, 4, 15], [1, 4, 15], [1, 4, 15], [1, 4, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 78) + (i1 * 6)) + i2)] = sinf(ph_0[(((i0 * 78) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 780; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 780; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 13, 6), \"float32\"), compute: T.Buffer((10, 13, 6), \"float32\"), compute_1: T.Buffer((10, 13, 6), \"float32\"), compute_2: T.Buffer((10, 13, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((780,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(13, 6):\n                cse_var_1: T.int32 = i0 * 78 + i1 * 6 + i2\n                compute_3 = T.Buffer((780,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(780):\n            compute_3 = T.Buffer((780,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(780):\n            compute_3 = T.Buffer((780,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "ceil",
                "exp",
                "atan"
            ]
        ],
        "input_shape": [[10, 13, 6]],
        "output_shape": [[10, 13, 6], [10, 13, 6], [10, 13, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 40) + (i1 * 8)) + i2)] = cosf(ph_0[(((i0 * 40) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 280; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 5, 8), \"float32\"), compute: T.Buffer((7, 5, 8), \"float32\"), T_subtract: T.Buffer((7, 5, 8), \"float32\"), compute_1: T.Buffer((7, 5, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((280,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(5, 8):\n                cse_var_1: T.int32 = i0 * 40 + i1 * 8 + i2\n                compute_2 = T.Buffer((280,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(280):\n            T_subtract_1 = T.Buffer((280,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_2 = T.Buffer((280,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "ceil",
                "subtract",
                "sin"
            ]
        ],
        "input_shape": [[7, 5, 8]],
        "output_shape": [[7, 5, 8], [7, 5, 8], [7, 5, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 34; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 34; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_2[((i0 * 2) + i2)] = acoshf(ph_0[((i0 * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 34; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinhf(acosf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 1, 2), \"float32\"), compute: T.Buffer((17, 1, 2), \"float32\"), compute_1: T.Buffer((17, 1, 2), \"float32\"), compute_2: T.Buffer((17, 1, 2), \"float32\"), compute_3: T.Buffer((17, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((34,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(34):\n            compute_4 = T.Buffer((34,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(34):\n            compute_4 = T.Buffer((34,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(17):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i2\n                compute_4 = T.Buffer((34,), data=compute_2.data)\n                compute_4[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(34):\n            compute_4 = T.Buffer((34,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "atan",
                "acos",
                "acosh",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[17, 1, 2]],
        "output_shape": [[17, 1, 2], [17, 1, 2], [17, 1, 2], [17, 1, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      T_mod[((ax0 * 20) + ax1)] = fmodf(fmodf(ph_0[((ax0 * 20) + ax1)], (0.000000e+00f - acosf(ph_0[((ax0 * 20) + ax1)]))), ph_0[((ax0 * 20) + ax1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 60; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(fmodf(ph_0[i0_i1_fused_i2_fused_1], (0.000000e+00f - acosf(ph_0[i0_i1_fused_i2_fused_1]))));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (0.000000e+00f - acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (0.000000e+00f - acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]))));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 20, 1), \"float32\"), compute: T.Buffer((3, 20, 1), \"float32\"), T_mod: T.Buffer((3, 20, 1), \"float32\"), compute_1: T.Buffer((3, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((60,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_2 = T.Buffer((60,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(3):\n            for ax1 in range(20):\n                cse_var_1: T.int32 = ax0 * 20 + ax1\n                T_mod_1 = T.Buffer((60,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.truncmod(ph_0_1[cse_var_1], T.float32(0) - T.acos(ph_0_1[cse_var_1])), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_2 = T.Buffer((60,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.float32(0) - T.acos(ph_0_1[i0_i1_fused_i2_fused])))",
        "op_args": [
            [
                "ceil",
                "acos",
                "add",
                "subtract",
                "mod",
                "mod",
                "cos"
            ]
        ],
        "input_shape": [[3, 20, 1]],
        "output_shape": [[3, 20, 1], [3, 20, 1], [3, 20, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1140; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 60; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_mod[((ax0_ax1_fused * 19) + ax2)] = fmodf(atanhf(ph_0[((ax0_ax1_fused * 19) + ax2)]), ph_0[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute_1[(((i0 * 57) + (i1 * 19)) + i2)] = atanf(ph_0[(((i0 * 57) + (i1 * 19)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 3, 19), \"float32\"), compute: T.Buffer((20, 3, 19), \"float32\"), T_mod: T.Buffer((20, 3, 19), \"float32\"), compute_1: T.Buffer((20, 3, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1140,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1140):\n            compute_2 = T.Buffer((1140,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(60):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_mod_1 = T.Buffer((1140,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.atanh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(3, 19):\n                cse_var_2: T.int32 = i0 * 57 + i1 * 19 + i2\n                compute_2 = T.Buffer((1140,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "sin",
                "atanh",
                "mod",
                "atan"
            ]
        ],
        "input_shape": [[20, 3, 19]],
        "output_shape": [[20, 3, 19], [20, 3, 19], [20, 3, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_multiply[(((ax0 * 60) + (ax1 * 12)) + ax2)] = (ph_0[(((ax0 * 60) + (ax1 * 12)) + ax2)] * ph_3[(((ax0 * 60) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1140; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 95; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_1[((i0_i1_fused * 12) + i2)] = atanf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 5, 12), \"float32\"), ph_3: T.Buffer((19, 5, 12), \"float32\"), T_multiply: T.Buffer((19, 5, 12), \"float32\"), compute: T.Buffer((19, 5, 12), \"float32\"), compute_1: T.Buffer((19, 5, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1140,), data=ph_0.data)\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(5, 12):\n                cse_var_1: T.int32 = ax0 * 60 + ax1 * 12 + ax2\n                T_multiply_1 = T.Buffer((1140,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((1140,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1140):\n            compute_2 = T.Buffer((1140,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(95):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((1140,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "asinh",
                "atan"
            ]
        ],
        "input_shape": [[19, 5, 12], [2, 20, 14], [19, 5, 12]],
        "output_shape": [[19, 5, 12], [2, 20, 14], [19, 5, 12], [19, 5, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 540; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_multiply_1[(((ax0 * 180) + (ax1 * 18)) + ax2)] = ((ph_0[(((ax0 * 180) + (ax1 * 18)) + ax2)] - sinf(ph_0[(((ax0 * 180) + (ax1 * 18)) + ax2)])) * ph_0[(((ax0 * 180) + (ax1 * 18)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = sinf((ph_0[((i0_i1_fused * 18) + i2)] - sinf(ph_0[((i0_i1_fused * 18) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] - __sinf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 10, 18), \"float32\"), ph_3: T.Buffer((3, 10, 18), \"float32\"), T_multiply: T.Buffer((3, 10, 18), \"float32\"), T_multiply_1: T.Buffer((3, 10, 18), \"float32\"), compute: T.Buffer((3, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(540):\n            T_multiply_2 = T.Buffer((540,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((540,), data=ph_3.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(10, 18):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 18 + ax2\n                T_multiply_2 = T.Buffer((540,), data=T_multiply_1.data)\n                T_multiply_2[cse_var_1] = (ph_0_1[cse_var_1] - T.sin(ph_0_1[cse_var_1])) * ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_1 = T.Buffer((540,), data=compute.data)\n                compute_1[cse_var_2] = T.sin(ph_0_1[cse_var_2] - T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "multiply",
                "sin",
                "subtract",
                "multiply",
                "sin"
            ]
        ],
        "input_shape": [[3, 10, 18], [17, 1, 16], [3, 10, 18]],
        "output_shape": [[3, 10, 18], [17, 1, 16], [3, 10, 18], [3, 10, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = asinhf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n        compute_1[(((i0 * 24) + (i1 * 3)) + i2_1)] = fabsf(acoshf(ph_0[(((i0 * 24) + (i1 * 3)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 8, 3), \"float32\"), compute: T.Buffer((6, 8, 3), \"float32\"), compute_1: T.Buffer((6, 8, 3), \"float32\"), compute_2: T.Buffer((6, 8, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((144,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((144,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(8, 3):\n                cse_var_2: T.int32 = i0 * 24 + i1 * 3 + i2\n                compute_3 = T.Buffer((144,), data=compute_1.data)\n                compute_3[cse_var_2] = T.fabs(T.acosh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_3 = T.Buffer((144,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "acosh",
                "abs",
                "asinh"
            ]
        ],
        "input_shape": [[6, 8, 3]],
        "output_shape": [[6, 8, 3], [6, 8, 3], [6, 8, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 78; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = cosf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1014; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n        compute_2[(((i0 * 78) + (i1 * 13)) + i2_1)] = asinf(ph_0[(((i0 * 78) + (i1 * 13)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 6, 13), \"float32\"), compute: T.Buffer((13, 6, 13), \"float32\"), compute_1: T.Buffer((13, 6, 13), \"float32\"), compute_2: T.Buffer((13, 6, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1014,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(78):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_3 = T.Buffer((1014,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1014):\n            compute_3 = T.Buffer((1014,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(6, 13):\n                cse_var_2: T.int32 = i0 * 78 + i1 * 13 + i2\n                compute_3 = T.Buffer((1014,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "atanh",
                "acosh",
                "asin"
            ]
        ],
        "input_shape": [[13, 6, 13]],
        "output_shape": [[13, 6, 13], [13, 6, 13], [13, 6, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  float compute_4[462];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_4[((i0_i1_fused * 11) + i2)] = expf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 462; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(compute_4[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 462; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(compute_4[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 462; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 462; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 462; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = sinf(fmodf(ph_0[i0_i1_fused_i2_fused_3], ph_3[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 14, 11), \"float32\"), ph_3: T.Buffer((3, 14, 11), \"float32\"), T_subtract: T.Buffer((3, 14, 11), \"float32\"), compute: T.Buffer((3, 14, 11), \"float32\"), compute_1: T.Buffer((3, 14, 11), \"float32\"), compute_2: T.Buffer((3, 14, 11), \"float32\"), compute_3: T.Buffer((3, 14, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_4 = T.allocate([462], \"float32\", \"global\")\n        compute_5 = T.Buffer((462,), data=compute_4)\n        ph_0_1 = T.Buffer((462,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(42):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_5[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_6 = T.Buffer((462,), data=compute.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acos(compute_5[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_6 = T.Buffer((462,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.exp(compute_5[i0_i1_fused_i2_fused])\n        ph_3_1 = T.Buffer((462,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(462):\n            T_subtract_1 = T.Buffer((462,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_6 = T.Buffer((462,), data=compute_2.data)\n            compute_6[i0_i1_fused_i2_fused] = T.ceil(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_6 = T.Buffer((462,), data=compute_3.data)\n            compute_6[i0_i1_fused_i2_fused] = T.sin(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "subtract",
                "exp",
                "acos",
                "exp",
                "ceil",
                "sin"
            ]
        ],
        "input_shape": [[3, 14, 11], [9, 2, 5], [3, 14, 11]],
        "output_shape": [[9, 2, 5], [3, 14, 11], [3, 14, 11], [3, 14, 11], [3, 14, 11], [3, 14, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 110; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 110; ++i0_i1_fused) {\n    compute[i0_i1_fused] = expf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 110; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 11, 1), \"float32\"), ph_3: T.Buffer((10, 11, 1), \"float32\"), T_multiply: T.Buffer((10, 11, 1), \"float32\"), compute: T.Buffer((10, 11, 1), \"float32\"), compute_1: T.Buffer((10, 11, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((110,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(110):\n            T_multiply_1 = T.Buffer((110,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((110,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(110):\n            compute_2 = T.Buffer((110,), data=compute.data)\n            compute_2[i0_i1_fused] = T.exp(ph_0_1[i0_i1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(110):\n            compute_2 = T.Buffer((110,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "exp",
                "acosh"
            ]
        ],
        "input_shape": [[10, 11, 1], [2, 2, 5], [10, 11, 1]],
        "output_shape": [[10, 11, 1], [2, 2, 5], [10, 11, 1], [10, 11, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_mod[(((ax0 * 240) + (ax1 * 20)) + ax2)] = fmodf(asinhf(ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)]), ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 2; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n        T_mod_1[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)] = fmodf(ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)], cosf(ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 12, 20), \"float32\"), compute: T.Buffer((2, 12, 20), \"float32\"), T_mod: T.Buffer((2, 12, 20), \"float32\"), T_mod_1: T.Buffer((2, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_mod_2 = T.Buffer((480,), data=T_mod.data)\n                T_mod_2[cse_var_1] = T.truncmod(T.asinh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_2: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_mod_2 = T.Buffer((480,), data=T_mod_1.data)\n                T_mod_2[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acosh",
                "asinh",
                "mod",
                "cos",
                "mod"
            ]
        ],
        "input_shape": [[2, 12, 20]],
        "output_shape": [[2, 12, 20], [2, 12, 20], [2, 12, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1560; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        T_divide[(((ax0 * 156) + (ax1 * 13)) + ax2)] = (cosf(ph_0[(((ax0 * 156) + (ax1 * 13)) + ax2)]) / ph_0[(((ax0 * 156) + (ax1 * 13)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute_1[((i0_i1_fused * 13) + i2)] = acosf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 12, 13), \"float32\"), compute: T.Buffer((10, 12, 13), \"float32\"), T_divide: T.Buffer((10, 12, 13), \"float32\"), compute_1: T.Buffer((10, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1560,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_2 = T.Buffer((1560,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(12, 13):\n                cse_var_1: T.int32 = ax0 * 156 + ax1 * 13 + ax2\n                T_divide_1 = T.Buffer((1560,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.cos(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(120):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((1560,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "ceil",
                "cos",
                "divide",
                "acos"
            ]
        ],
        "input_shape": [[10, 12, 13]],
        "output_shape": [[10, 12, 13], [10, 12, 13], [10, 12, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 126) + (i1 * 14)) + i2)] = asinhf(ph_0[(((i0 * 126) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2268; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(atanf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 18; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 9; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        compute_1[(((i0_1 * 126) + (i1_1 * 14)) + i2_1)] = expf(ph_0[(((i0_1 * 126) + (i1_1 * 14)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 9, 14), \"float32\"), compute: T.Buffer((18, 9, 14), \"float32\"), T_mod: T.Buffer((18, 9, 14), \"float32\"), compute_1: T.Buffer((18, 9, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2268,), data=ph_0.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(9, 14):\n                cse_var_1: T.int32 = i0 * 126 + i1 * 14 + i2\n                compute_2 = T.Buffer((2268,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2268):\n            T_mod_1 = T.Buffer((2268,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(9, 14):\n                cse_var_2: T.int32 = i0 * 126 + i1 * 14 + i2\n                compute_2 = T.Buffer((2268,), data=compute_1.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "atan",
                "mod",
                "exp"
            ]
        ],
        "input_shape": [[18, 9, 14]],
        "output_shape": [[18, 9, 14], [18, 9, 14], [18, 9, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = acoshf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_1[(((i0 * 18) + (i1 * 2)) + i2_1)] = acosf(fabsf(ph_0[(((i0 * 18) + (i1 * 2)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 270; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 2), \"float32\"), compute: T.Buffer((15, 9, 2), \"float32\"), compute_1: T.Buffer((15, 9, 2), \"float32\"), compute_2: T.Buffer((15, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((270,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((270,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(9, 2):\n                cse_var_2: T.int32 = i0 * 18 + i1 * 2 + i2\n                compute_3 = T.Buffer((270,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acos(T.fabs(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(270):\n            compute_3 = T.Buffer((270,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "abs",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[15, 9, 2]],
        "output_shape": [[15, 9, 2], [15, 9, 2], [15, 9, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1900; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1900; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1900; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 190; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_divide[((ax0_ax1_fused * 10) + ax2)] = (asinhf(ph_0[((ax0_ax1_fused * 10) + ax2)]) / ph_0[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 19, 10), \"float32\"), compute: T.Buffer((10, 19, 10), \"float32\"), T_multiply: T.Buffer((10, 19, 10), \"float32\"), compute_1: T.Buffer((10, 19, 10), \"float32\"), T_divide: T.Buffer((10, 19, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1900,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1900):\n            compute_2 = T.Buffer((1900,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1900):\n            T_multiply_1 = T.Buffer((1900,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1900):\n            compute_2 = T.Buffer((1900,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(190):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_divide_1 = T.Buffer((1900,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]",
        "op_args": [
            [
                "asin",
                "acos",
                "multiply",
                "asin",
                "asinh",
                "divide"
            ]
        ],
        "input_shape": [[10, 19, 10]],
        "output_shape": [[10, 19, 10], [10, 19, 10], [10, 19, 10], [10, 19, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 126; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = fabsf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 126; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 17) + ax2)] = (fabsf(ph_0[((ax0_ax1_fused * 17) + ax2)]) * ph_0[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2142; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2142; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 7, 17), \"float32\"), compute: T.Buffer((18, 7, 17), \"float32\"), T_multiply: T.Buffer((18, 7, 17), \"float32\"), compute_1: T.Buffer((18, 7, 17), \"float32\"), compute_2: T.Buffer((18, 7, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2142,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(126):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_3 = T.Buffer((2142,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(126):\n            for ax2 in range(17):\n                cse_var_2: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_multiply_1 = T.Buffer((2142,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(2142):\n            compute_3 = T.Buffer((2142,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2142):\n            compute_3 = T.Buffer((2142,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "abs",
                "multiply",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[18, 7, 17]],
        "output_shape": [[18, 7, 17], [18, 7, 17], [18, 7, 17], [18, 7, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1080; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 216) + (i1 * 12)) + i2)] = asinf(ph_0[(((i0 * 216) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1080; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] / ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 18, 12), \"float32\"), ph_3: T.Buffer((5, 18, 12), \"float32\"), T_divide: T.Buffer((5, 18, 12), \"float32\"), compute: T.Buffer((5, 18, 12), \"float32\"), compute_1: T.Buffer((5, 18, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1080,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1080):\n            T_divide_1 = T.Buffer((1080,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1080,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(18, 12):\n                cse_var_1: T.int32 = i0 * 216 + i1 * 12 + i2\n                compute_2 = T.Buffer((1080,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_2 = T.Buffer((1080,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "asin",
                "ceil"
            ]
        ],
        "input_shape": [[5, 18, 12], [19, 16, 19], [5, 18, 12]],
        "output_shape": [[5, 18, 12], [19, 16, 19], [5, 18, 12], [5, 18, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 32; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 32; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i0 * 2) + i2)] = acoshf((ph_0[((i0 * 2) + i2)] - acoshf(ph_0[((i0 * 2) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_2[((i0_1 * 2) + i2_1)] = atanf((ph_0[((i0_1 * 2) + i2_1)] - acoshf(ph_0[((i0_1 * 2) + i2_1)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 1, 2), \"float32\"), compute: T.Buffer((16, 1, 2), \"float32\"), T_divide: T.Buffer((16, 1, 2), \"float32\"), compute_1: T.Buffer((16, 1, 2), \"float32\"), compute_2: T.Buffer((16, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((32,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(32):\n            compute_3 = T.Buffer((32,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(32):\n            T_divide_1 = T.Buffer((32,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(16):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i2\n                compute_3 = T.Buffer((32,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1] - T.acosh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(16):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0 * 2 + i2\n                compute_3 = T.Buffer((32,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2] - T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "sin",
                "cos",
                "divide",
                "acosh",
                "subtract",
                "acosh",
                "atan"
            ]
        ],
        "input_shape": [[16, 1, 2]],
        "output_shape": [[16, 1, 2], [16, 1, 2], [16, 1, 2], [16, 1, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 600; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 600; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_2[(((i0 * 40) + (i1 * 5)) + i2)] = sinf(cosf(ph_0[(((i0 * 40) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_3[((i0_i1_fused * 5) + i2_1)] = acoshf(fmodf(ph_0[((i0_i1_fused * 5) + i2_1)], ph_3[((i0_i1_fused * 5) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 8, 5), \"float32\"), ph_3: T.Buffer((15, 8, 5), \"float32\"), compute: T.Buffer((15, 8, 5), \"float32\"), compute_1: T.Buffer((15, 8, 5), \"float32\"), compute_2: T.Buffer((15, 8, 5), \"float32\"), compute_3: T.Buffer((15, 8, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((600,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(600):\n            compute_4 = T.Buffer((600,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(600):\n            compute_4 = T.Buffer((600,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(8, 5):\n                cse_var_1: T.int32 = i0 * 40 + i1 * 5 + i2\n                compute_4 = T.Buffer((600,), data=compute_2.data)\n                compute_4[cse_var_1] = T.sin(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(120):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_4 = T.Buffer((600,), data=compute_3.data)\n                ph_3_1 = T.Buffer((600,), data=ph_3.data)\n                compute_4[cse_var_2] = T.acosh(T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "abs",
                "cos",
                "cos",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[15, 8, 5], [19, 10, 5], [15, 8, 5]],
        "output_shape": [[19, 10, 5], [15, 8, 5], [15, 8, 5], [15, 8, 5], [15, 8, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_4) {\n  float auto_scheduler_layout_transform[36];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 612; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax7 = 0; ax7 < 12; ++ax7) {\n      auto_scheduler_layout_transform[((ax0_ax1_fused_ax2_fused * 12) + ax7)] = ph_4[((ax0_ax1_fused_ax2_fused * 12) + ax7)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 3; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t i_inner_init = 0; i_inner_init < 17; ++i_inner_init) {\n      T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 17) + i_inner_init)] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 12; ++k_inner) {\n      for (int32_t i_inner = 0; i_inner < 17; ++i_inner) {\n        T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 17) + i_inner)] = (T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 17) + i_inner)] + (cosf(ph_0[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 204) + (i_inner * 12)) + k_inner)]) * auto_scheduler_layout_transform[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 12) + k_inner)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 612; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_4) {\n  float T_batch_matmul_NN_local[9];\n  __shared__ float compute_shared[18];\n  __shared__ float ph_4_shared[2];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 9; ++i_c_outer_inner_init) {\n    T_batch_matmul_NN_local[i_c_outer_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 4; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 18; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      compute_shared[ax0_ax1_fused_ax2_fused_outer_outer] = __cosf(ph_0[((((ax0_ax1_fused_ax2_fused_outer_outer >> 1) * 8) + (k_outer_outer * 2)) + (ax0_ax1_fused_ax2_fused_outer_outer & 1))]);\n    }\n    *(float2*)(ph_4_shared + 0) = *(float2*)(ph_4 + (k_outer_outer * 2));\n    __syncthreads();\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 9; ++i_c_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_batch_matmul_NN_local[i_c_outer_inner] = (T_batch_matmul_NN_local[i_c_outer_inner] + (compute_shared[((i_c_outer_inner * 2) + k_inner)] * ph_4_shared[k_inner]));\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 9; ++i_inner) {\n    T_batch_matmul_NN[i_inner] = T_batch_matmul_NN_local[i_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 17, 12), \"float32\"), ph_4: T.Buffer((3, 12, 1), \"float32\"), compute: T.Buffer((3, 17, 12), \"float32\"), T_batch_matmul_NN: T.Buffer((3, 17, 1), \"float32\"), compute_1: T.Buffer((3, 17, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([36], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((612,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(612):\n            compute_2 = T.Buffer((612,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((36,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3):\n            for ax7 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 12 + ax7\n                ph_4_1 = T.Buffer((36,), data=ph_4.data)\n                auto_scheduler_layout_transform_1[cse_var_1] = ph_4_1[cse_var_1]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(3):\n            T_batch_matmul_NN_1 = T.Buffer((51,), data=T_batch_matmul_NN.data)\n            for i_inner_init in range(17):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 17 + i_inner_init] = T.float32(0)\n            for k_inner, i_inner in T.grid(12, 17):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 17 + i_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.cos(ph_0_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 204 + i_inner * 12 + k_inner]) * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 12 + k_inner]\n        for i0_i1_fused_i2_fused in T.parallel(612):\n            compute_2 = T.Buffer((612,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "cos",
                "batch_matmul",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[3, 17, 12], [3, 12, 1]],
        "output_shape": [[3, 17, 12], [3, 17, 1], [3, 17, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 156; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 52) + (i1 * 13)) + i2)] = fabsf(acoshf(ph_0[(((i0 * 52) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 156; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 3; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 4; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n        compute_2[(((i0_1 * 52) + (i1_1 * 13)) + i2_1)] = asinhf(fmodf(ph_0[(((i0_1 * 52) + (i1_1 * 13)) + i2_1)], ph_3[(((i0_1 * 52) + (i1_1 * 13)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 4, 13), \"float32\"), ph_3: T.Buffer((3, 4, 13), \"float32\"), compute: T.Buffer((3, 4, 13), \"float32\"), compute_1: T.Buffer((3, 4, 13), \"float32\"), T_multiply: T.Buffer((3, 4, 13), \"float32\"), compute_2: T.Buffer((3, 4, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((156,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(156):\n            compute_3 = T.Buffer((156,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(4, 13):\n                cse_var_1: T.int32 = i0 * 52 + i1 * 13 + i2\n                compute_3 = T.Buffer((156,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(T.acosh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(156):\n            T_multiply_1 = T.Buffer((156,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(4, 13):\n                cse_var_2: T.int32 = i0 * 52 + i1 * 13 + i2\n                compute_3 = T.Buffer((156,), data=compute_2.data)\n                ph_3_1 = T.Buffer((156,), data=ph_3.data)\n                compute_3[cse_var_2] = T.asinh(T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "cos",
                "acosh",
                "abs",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[3, 4, 13], [12, 7, 4], [3, 4, 13]],
        "output_shape": [[12, 7, 4], [3, 4, 13], [3, 4, 13], [3, 4, 13], [3, 4, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 300) + (i1 * 15)) + i2)] = ceilf((ph_0[(((i0 * 300) + (i1 * 15)) + i2)] + atanf(ph_0[(((i0 * 300) + (i1 * 15)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4200; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] - asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 280; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n      compute_2[((i0_i1_fused * 15) + i2_1)] = fabsf((ph_0[((i0_i1_fused * 15) + i2_1)] - asinf(ph_0[((i0_i1_fused * 15) + i2_1)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 20, 15), \"float32\"), compute: T.Buffer((14, 20, 15), \"float32\"), compute_1: T.Buffer((14, 20, 15), \"float32\"), compute_2: T.Buffer((14, 20, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4200,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(20, 15):\n                cse_var_1: T.int32 = i0 * 300 + i1 * 15 + i2\n                compute_3 = T.Buffer((4200,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1] + T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4200):\n            compute_3 = T.Buffer((4200,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(280):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_3 = T.Buffer((4200,), data=compute_2.data)\n                compute_3[cse_var_2] = T.fabs(ph_0_1[cse_var_2] - T.asin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "atan",
                "add",
                "ceil",
                "asin",
                "subtract",
                "exp",
                "abs"
            ]
        ],
        "input_shape": [[14, 20, 15]],
        "output_shape": [[14, 20, 15], [14, 20, 15], [14, 20, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 6; ++i1) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i1 * 14) + i2)] = atanf((ph_0[((i1 * 14) + i2)] - acosf(ph_0[((i1 * 14) + i2)])));\n    }\n  }\n  for (int32_t i1_1 = 0; i1_1 < 6; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      compute_1[((i1_1 * 14) + i2_1)] = sinf(ph_0[((i1_1 * 14) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 84; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 6, 14), \"float32\"), compute: T.Buffer((1, 6, 14), \"float32\"), compute_1: T.Buffer((1, 6, 14), \"float32\"), compute_2: T.Buffer((1, 6, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((84,), data=ph_0.data)\n        for i1, i2 in T.grid(6, 14):\n            cse_var_1: T.int32 = i1 * 14 + i2\n            compute_3 = T.Buffer((84,), data=compute.data)\n            compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1] - T.acos(ph_0_1[cse_var_1]))\n        for i1, i2 in T.grid(6, 14):\n            cse_var_2: T.int32 = i1 * 14 + i2\n            compute_3 = T.Buffer((84,), data=compute_1.data)\n            compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(84):\n            compute_3 = T.Buffer((84,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "subtract",
                "atan",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[1, 6, 14]],
        "output_shape": [[1, 6, 14], [1, 6, 14], [1, 6, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 6840; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6840; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_1[(((i0 * 360) + (i1 * 18)) + i2)] = atanhf(ph_0[(((i0 * 360) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 20, 18), \"float32\"), ph_3: T.Buffer((19, 20, 18), \"float32\"), T_multiply: T.Buffer((19, 20, 18), \"float32\"), compute: T.Buffer((19, 20, 18), \"float32\"), compute_1: T.Buffer((19, 20, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((6840,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(6840):\n            T_multiply_1 = T.Buffer((6840,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((6840,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(6840):\n            compute_2 = T.Buffer((6840,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(20, 18):\n                cse_var_1: T.int32 = i0 * 360 + i1 * 18 + i2\n                compute_2 = T.Buffer((6840,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "multiply",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[19, 20, 18], [9, 19, 17], [19, 20, 18]],
        "output_shape": [[19, 20, 18], [9, 19, 17], [19, 20, 18], [19, 20, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[120];\n  float T_batch_matmul_NN[20];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 24; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 24; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 24; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t ax4 = 0; ax4 < 3; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 4; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n        for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n          auto_scheduler_layout_transform[((((ax4 * 40) + (ax5 * 10)) + (ax6 * 2)) + ax7)] = ph_3[((((ax5 * 30) + (ax4 * 10)) + (ax7 * 5)) + ax6)];\n        }\n      }\n    }\n  }\n  for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 4; ++b_outer_inner_init) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      T_batch_matmul_NN[((b_outer_inner_init * 5) + j_outer_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int32_t k_outer = 0; k_outer < 3; ++k_outer) {\n    for (int32_t b_outer_inner = 0; b_outer_inner < 4; ++b_outer_inner) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n          T_batch_matmul_NN[((b_outer_inner * 5) + j_outer_inner)] = (T_batch_matmul_NN[((b_outer_inner * 5) + j_outer_inner)] + (ph_0[(((b_outer_inner * 6) + (k_outer * 2)) + k_inner)] * auto_scheduler_layout_transform[((((k_outer * 40) + (b_outer_inner * 10)) + (j_outer_inner * 2)) + k_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 20; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(T_batch_matmul_NN[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(120) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[3];\n  __shared__ float ph_3_shared[360];\n  for (int b_inner_init = 0; b_inner_init < 3; ++b_inner_init) {\n    T_batch_matmul_NN[b_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 120) + ((int)threadIdx.x))] = ph_3[((ax0_ax1_fused_ax2_fused_outer_outer * 120) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 8; ++k_inner) {\n    for (int b_inner = 0; b_inner < 3; ++b_inner) {\n      T_batch_matmul_NN[b_inner] = (T_batch_matmul_NN[b_inner] + (ph_0[(((((((int)threadIdx.x) / 40) * 192) + (b_inner * 64)) + (((((int)threadIdx.x) % 40) / 5) * 8)) + k_inner)] * ph_3_shared[(((((((int)threadIdx.x) / 40) * 120) + (b_inner * 40)) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  for (int i0_inner = 0; i0_inner < 3; ++i0_inner) {\n    compute[((((((int)threadIdx.x) / 40) * 120) + (i0_inner * 40)) + (((int)threadIdx.x) % 40))] = __expf(T_batch_matmul_NN[i0_inner]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 1, 6), \"float32\"), ph_3: T.Buffer((4, 6, 5), \"float32\"), compute: T.Buffer((4, 1, 6), \"float32\"), T_subtract: T.Buffer((4, 1, 6), \"float32\"), compute_1: T.Buffer((4, 1, 6), \"float32\"), compute_2: T.Buffer((4, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([120], \"float32\", \"global\")\n        T_batch_matmul_NN = T.allocate([20], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((24,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(24):\n            compute_3 = T.Buffer((24,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(24):\n            T_subtract_1 = T.Buffer((24,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(24):\n            compute_3 = T.Buffer((24,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((120,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax6, ax7 in T.grid(3, 4, 5, 2):\n            ph_3_1 = T.Buffer((120,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax4 * 40 + ax5 * 10 + ax6 * 2 + ax7] = ph_3_1[ax5 * 30 + ax4 * 10 + ax7 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((20,), data=T_batch_matmul_NN)\n        for b_outer_inner_init, j_outer_inner_init in T.grid(4, 5):\n            T_batch_matmul_NN_1[b_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n        for k_outer, b_outer_inner, j_outer_inner, k_inner in T.grid(3, 4, 5, 2):\n            cse_var_1: T.int32 = b_outer_inner * 5 + j_outer_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 6 + k_outer * 2 + k_inner] * auto_scheduler_layout_transform_1[k_outer * 40 + b_outer_inner * 10 + j_outer_inner * 2 + k_inner]\n        for i0_i1_fused_i2_fused in T.parallel(20):\n            compute_3 = T.Buffer((20,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T_batch_matmul_NN_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "batch_matmul",
                "cos",
                "asinh",
                "subtract",
                "asinh",
                "exp"
            ]
        ],
        "input_shape": [[4, 1, 6], [15, 18, 20], [4, 6, 5]],
        "output_shape": [[15, 18, 20], [4, 1, 6], [4, 1, 6], [4, 1, 6], [4, 1, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 114; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = acosf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 570; ++i0_i1_fused_i2_fused) {\n    float compute_4[1];\n    compute_4[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute_1[i0_i1_fused_i2_fused] = fabsf(compute_4[0]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 570; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 570; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = cosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = fabsf(__expf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 19, 5), \"float32\"), compute: T.Buffer((6, 19, 5), \"float32\"), compute_1: T.Buffer((6, 19, 5), \"float32\"), compute_2: T.Buffer((6, 19, 5), \"float32\"), compute_3: T.Buffer((6, 19, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((570,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(114):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_4 = T.Buffer((570,), data=compute.data)\n                compute_4[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(570):\n            compute_4 = T.allocate([1], \"float32\", \"global\")\n            compute_5 = T.Buffer((1,), data=compute_4, align=4)\n            compute_5[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_6 = T.Buffer((570,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.fabs(compute_5[0])\n        for i0_i1_fused_i2_fused in T.parallel(570):\n            compute_4 = T.Buffer((570,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(570):\n            compute_4 = T.Buffer((570,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "exp",
                "abs",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[6, 19, 5]],
        "output_shape": [[6, 19, 5], [6, 19, 5], [6, 19, 5], [6, 19, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 15; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = fabsf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 30; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - asinhf(cosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 15; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 2) + i2_1)] = asinf(atanhf(ph_0[((i0_i1_fused_1 * 2) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 15, 2), \"float32\"), compute: T.Buffer((1, 15, 2), \"float32\"), T_subtract: T.Buffer((1, 15, 2), \"float32\"), compute_1: T.Buffer((1, 15, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((30,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(15):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((30,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(30):\n            T_subtract_1 = T.Buffer((30,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.asinh(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused in T.parallel(15):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((30,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "cos",
                "asinh",
                "subtract",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[1, 15, 2]],
        "output_shape": [[1, 15, 2], [1, 15, 2], [1, 15, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 704; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 44) + (i1 * 4)) + i2)] = ceilf(asinf(ph_0[(((i0 * 44) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 704; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 11, 4), \"float32\"), compute: T.Buffer((16, 11, 4), \"float32\"), compute_1: T.Buffer((16, 11, 4), \"float32\"), compute_2: T.Buffer((16, 11, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((704,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(704):\n            compute_3 = T.Buffer((704,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(11, 4):\n                cse_var_1: T.int32 = i0 * 44 + i1 * 4 + i2\n                compute_3 = T.Buffer((704,), data=compute_1.data)\n                compute_3[cse_var_1] = T.ceil(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(704):\n            compute_3 = T.Buffer((704,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "asin",
                "ceil",
                "atan"
            ]
        ],
        "input_shape": [[16, 11, 4]],
        "output_shape": [[16, 11, 4], [16, 11, 4], [16, 11, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 342; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 342; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_1[((i0_i1_fused * 19) + i2)] = asinf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 1, 19), \"float32\"), compute: T.Buffer((18, 1, 19), \"float32\"), T_divide: T.Buffer((18, 1, 19), \"float32\"), compute_1: T.Buffer((18, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((342,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(342):\n            compute_2 = T.Buffer((342,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(342):\n            T_divide_1 = T.Buffer((342,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((342,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acosh",
                "cos",
                "divide",
                "asin"
            ]
        ],
        "input_shape": [[18, 1, 19]],
        "output_shape": [[18, 1, 19], [18, 1, 19], [18, 1, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3072; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 256; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = ceilf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 256; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_multiply_1[((ax0_ax1_fused * 12) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 12) + ax2)]) * ph_0[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 16, 12), \"float32\"), ph_3: T.Buffer((16, 16, 12), \"float32\"), T_multiply: T.Buffer((16, 16, 12), \"float32\"), compute: T.Buffer((16, 16, 12), \"float32\"), T_multiply_1: T.Buffer((16, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3072,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3072):\n            T_multiply_2 = T.Buffer((3072,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((3072,), data=ph_3.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(256):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_1 = T.Buffer((3072,), data=compute.data)\n                compute_1[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(256):\n            for ax2 in range(12):\n                cse_var_2: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_multiply_2 = T.Buffer((3072,), data=T_multiply_1.data)\n                T_multiply_2[cse_var_2] = T.asin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]",
        "op_args": [
            [
                "multiply",
                "ceil",
                "asin",
                "multiply"
            ]
        ],
        "input_shape": [[16, 16, 12], [4, 6, 12], [16, 16, 12]],
        "output_shape": [[16, 16, 12], [4, 6, 12], [16, 16, 12], [16, 16, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0 * 17) + i2)] = acoshf(ph_0[((i0 * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 68; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - acoshf(acoshf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_subtract_1[((ax0 * 17) + ax2)] = (asinf(ph_0[((ax0 * 17) + ax2)]) - ph_0[((ax0 * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 68; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (asinf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - acoshf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 1, 17), \"float32\"), compute: T.Buffer((4, 1, 17), \"float32\"), T_subtract: T.Buffer((4, 1, 17), \"float32\"), T_subtract_1: T.Buffer((4, 1, 17), \"float32\"), T_multiply: T.Buffer((4, 1, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((68,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0 * 17 + i2\n                compute_1 = T.Buffer((68,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(68):\n            T_subtract_2 = T.Buffer((68,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.acosh(T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for ax0 in T.parallel(4):\n            for ax2 in range(17):\n                cse_var_2: T.int32 = ax0 * 17 + ax2\n                T_subtract_2 = T.Buffer((68,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_2] = T.asin(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(68):\n            T_multiply_1 = T.Buffer((68,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acosh",
                "acosh",
                "acosh",
                "subtract",
                "asin",
                "subtract",
                "multiply"
            ]
        ],
        "input_shape": [[4, 1, 17]],
        "output_shape": [[4, 1, 17], [4, 1, 17], [4, 1, 17], [4, 1, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 408; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 408; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 408; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 408; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (asinf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 3, 17), \"float32\"), ph_3: T.Buffer((8, 3, 17), \"float32\"), T_add: T.Buffer((8, 3, 17), \"float32\"), compute: T.Buffer((8, 3, 17), \"float32\"), compute_1: T.Buffer((8, 3, 17), \"float32\"), T_subtract: T.Buffer((8, 3, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((408,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(408):\n            T_add_1 = T.Buffer((408,), data=T_add.data)\n            ph_3_1 = T.Buffer((408,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(408):\n            compute_2 = T.Buffer((408,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(408):\n            compute_2 = T.Buffer((408,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(408):\n            T_subtract_1 = T.Buffer((408,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "acosh",
                "asin",
                "acos",
                "subtract"
            ]
        ],
        "input_shape": [[8, 3, 17], [15, 6, 9], [8, 3, 17]],
        "output_shape": [[8, 3, 17], [15, 6, 9], [8, 3, 17], [8, 3, 17], [8, 3, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 13; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_divide[((ax0_ax1_fused * 9) + ax2)] = (ph_0[((ax0_ax1_fused * 9) + ax2)] / ph_3[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 117; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] - acoshf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0 * 9) + i2)] = atanhf((ph_0[((i0 * 9) + i2)] - acoshf(ph_0[((i0 * 9) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 9), \"float32\"), ph_3: T.Buffer((13, 1, 9), \"float32\"), T_divide: T.Buffer((13, 1, 9), \"float32\"), compute: T.Buffer((13, 1, 9), \"float32\"), compute_1: T.Buffer((13, 1, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((117,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(13):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_divide_1 = T.Buffer((117,), data=T_divide.data)\n                ph_3_1 = T.Buffer((117,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(117):\n            compute_2 = T.Buffer((117,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] - T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(13):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0 * 9 + i2\n                compute_2 = T.Buffer((117,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2] - T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "acosh",
                "subtract",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[13, 1, 9], [14, 7, 11], [13, 1, 9]],
        "output_shape": [[13, 1, 9], [14, 7, 11], [13, 1, 9], [13, 1, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 520; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 40; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_divide[((ax0_ax1_fused * 13) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 13) + ax2)]) / ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 520; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 8, 13), \"float32\"), compute: T.Buffer((5, 8, 13), \"float32\"), T_divide: T.Buffer((5, 8, 13), \"float32\"), compute_1: T.Buffer((5, 8, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((520,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(520):\n            compute_2 = T.Buffer((520,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(40):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_divide_1 = T.Buffer((520,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(520):\n            compute_2 = T.Buffer((520,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "asin",
                "divide",
                "atan"
            ]
        ],
        "input_shape": [[5, 8, 13]],
        "output_shape": [[5, 8, 13], [5, 8, 13], [5, 8, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_1[((i0 * 20) + i2)] = asinf(asinhf(ph_0[((i0 * 20) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 14; ++i0_1) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_2[((i0_1 * 20) + i2_1)] = cosf(ph_0[((i0_1 * 20) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    float compute_3[20];\n    for (int32_t i2_2 = 0; i2_2 < 20; ++i2_2) {\n      compute_3[i2_2] = expf(ph_0[((ax0 * 20) + i2_2)]);\n    }\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_add[((ax0 * 20) + ax2)] = (compute_3[ax2] + ph_0[((ax0 * 20) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 1, 20), \"float32\"), compute: T.Buffer((14, 1, 20), \"float32\"), compute_1: T.Buffer((14, 1, 20), \"float32\"), compute_2: T.Buffer((14, 1, 20), \"float32\"), T_add: T.Buffer((14, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_3 = T.Buffer((280,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(14):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0 * 20 + i2\n                compute_3 = T.Buffer((280,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asin(T.asinh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(14):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0 * 20 + i2\n                compute_3 = T.Buffer((280,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for ax0 in T.parallel(14):\n            compute_3 = T.allocate([20], \"float32\", \"global\")\n            compute_4 = T.Buffer((20,), data=compute_3)\n            for i2 in range(20):\n                compute_4[i2] = T.exp(ph_0_1[ax0 * 20 + i2])\n            for ax2 in range(20):\n                cse_var_3: T.int32 = ax0 * 20 + ax2\n                T_add_1 = T.Buffer((280,), data=T_add.data)\n                T_add_1[cse_var_3] = compute_4[ax2] + ph_0_1[cse_var_3]",
        "op_args": [
            [
                "abs",
                "asinh",
                "asin",
                "cos",
                "exp",
                "add"
            ]
        ],
        "input_shape": [[14, 1, 20]],
        "output_shape": [[14, 1, 20], [14, 1, 20], [14, 1, 20], [14, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1260; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1260; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 126; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_mod[((ax0_ax1_fused * 10) + ax2)] = fmodf(asinhf(ph_0[((ax0_ax1_fused * 10) + ax2)]), ph_0[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 1260; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = ceilf(asinhf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 7, 10), \"float32\"), compute: T.Buffer((18, 7, 10), \"float32\"), compute_1: T.Buffer((18, 7, 10), \"float32\"), compute_2: T.Buffer((18, 7, 10), \"float32\"), T_mod: T.Buffer((18, 7, 10), \"float32\"), compute_3: T.Buffer((18, 7, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(126):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_mod_1 = T.Buffer((1260,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.asinh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "sin",
                "acos",
                "acos",
                "asinh",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[18, 7, 10]],
        "output_shape": [[18, 7, 10], [18, 7, 10], [18, 7, 10], [18, 7, 10], [18, 7, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 340) + (i1 * 20)) + i2)] = sinf(ph_0[(((i0 * 340) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 20; ++i0_1) {\n    float compute_3[1];\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n        compute_3[0] = expf(ph_0[(((i0_1 * 340) + (i1_1 * 20)) + i2_1)]);\n        compute_1[(((i0_1 * 340) + (i1_1 * 20)) + i2_1)] = atanf(compute_3[0]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6800; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 17, 20), \"float32\"), compute: T.Buffer((20, 17, 20), \"float32\"), compute_1: T.Buffer((20, 17, 20), \"float32\"), compute_2: T.Buffer((20, 17, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((6800,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(17, 20):\n                cse_var_1: T.int32 = i0 * 340 + i1 * 20 + i2\n                compute_3 = T.Buffer((6800,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(20):\n            compute_3 = T.allocate([1], \"float32\", \"global\")\n            for i1, i2 in T.grid(17, 20):\n                cse_var_2: T.int32 = i0 * 340 + i1 * 20 + i2\n                compute_4 = T.Buffer((1,), data=compute_3, align=4)\n                compute_4[0] = T.exp(ph_0_1[cse_var_2])\n                compute_5 = T.Buffer((6800,), data=compute_1.data)\n                compute_5[cse_var_2] = T.atan(compute_4[0])\n        for i0_i1_fused_i2_fused in T.parallel(6800):\n            compute_3 = T.Buffer((6800,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "exp",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[20, 17, 20]],
        "output_shape": [[20, 17, 20], [20, 17, 20], [20, 17, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = ceilf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 168; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + sinf(atanhf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 168; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __sinf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 2, 7), \"float32\"), compute: T.Buffer((12, 2, 7), \"float32\"), T_add: T.Buffer((12, 2, 7), \"float32\"), compute_1: T.Buffer((12, 2, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((168,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(24):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((168,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(168):\n            T_add_1 = T.Buffer((168,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.sin(T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(168):\n            compute_2 = T.Buffer((168,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "sin",
                "add",
                "ceil"
            ]
        ],
        "input_shape": [[12, 2, 7]],
        "output_shape": [[12, 2, 7], [12, 2, 7], [12, 2, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 684; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * fmodf((ph_0[ax0_ax1_fused_ax2_fused] + acoshf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 684; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 57; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_divide[((ax0_ax1_fused * 12) + ax2)] = (asinhf(ph_0[((ax0_ax1_fused * 12) + ax2)]) / ph_0[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 3, 12), \"float32\"), T_multiply: T.Buffer((19, 3, 12), \"float32\"), compute: T.Buffer((19, 3, 12), \"float32\"), T_divide: T.Buffer((19, 3, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((684,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(684):\n            T_multiply_1 = T.Buffer((684,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] + T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(684):\n            compute_1 = T.Buffer((684,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(57):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_divide_1 = T.Buffer((684,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]",
        "op_args": [
            [
                "acosh",
                "add",
                "mod",
                "multiply",
                "asinh",
                "acosh",
                "divide"
            ]
        ],
        "input_shape": [[19, 3, 12]],
        "output_shape": [[19, 3, 12], [19, 3, 12], [19, 3, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 484; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 484; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 484; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = fabsf(__cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 11, 11), \"float32\"), ph_3: T.Buffer((4, 11, 11), \"float32\"), T_divide: T.Buffer((4, 11, 11), \"float32\"), compute: T.Buffer((4, 11, 11), \"float32\"), compute_1: T.Buffer((4, 11, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((484,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(484):\n            T_divide_1 = T.Buffer((484,), data=T_divide.data)\n            ph_3_1 = T.Buffer((484,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(484):\n            compute_2 = T.Buffer((484,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(484):\n            compute_2 = T.Buffer((484,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "abs",
                "cos",
                "abs"
            ]
        ],
        "input_shape": [[4, 11, 11], [19, 4, 19], [4, 11, 11]],
        "output_shape": [[4, 11, 11], [19, 4, 19], [4, 11, 11], [4, 11, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1920; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 160) + (i1 * 10)) + i2)] = acosf((ph_0[(((i0 * 160) + (i1 * 10)) + i2)] * acosf(ph_0[(((i0 * 160) + (i1 * 10)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] * acosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 16, 10), \"float32\"), ph_3: T.Buffer((12, 16, 10), \"float32\"), T_subtract: T.Buffer((12, 16, 10), \"float32\"), compute: T.Buffer((12, 16, 10), \"float32\"), compute_1: T.Buffer((12, 16, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1920,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1920):\n            T_subtract_1 = T.Buffer((1920,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((1920,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(16, 10):\n                cse_var_1: T.int32 = i0 * 160 + i1 * 10 + i2\n                compute_2 = T.Buffer((1920,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1] * T.acos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            compute_2 = T.Buffer((1920,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] * T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "acos",
                "multiply",
                "acos",
                "ceil"
            ]
        ],
        "input_shape": [[12, 16, 10], [15, 6, 17], [12, 16, 10]],
        "output_shape": [[12, 16, 10], [15, 6, 17], [12, 16, 10], [12, 16, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 60) + (i1 * 15)) + i2)] = expf(ph_0[(((i0 * 60) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 780; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 780; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 13; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 4; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n        compute_2[(((i0_1 * 60) + (i1_1 * 15)) + i2_1)] = sinf(asinf(ph_0[(((i0_1 * 60) + (i1_1 * 15)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 4, 15), \"float32\"), compute: T.Buffer((13, 4, 15), \"float32\"), T_multiply: T.Buffer((13, 4, 15), \"float32\"), compute_1: T.Buffer((13, 4, 15), \"float32\"), compute_2: T.Buffer((13, 4, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((780,), data=ph_0.data)\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(4, 15):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 15 + i2\n                compute_3 = T.Buffer((780,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(780):\n            T_multiply_1 = T.Buffer((780,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(780):\n            compute_3 = T.Buffer((780,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(4, 15):\n                cse_var_2: T.int32 = i0 * 60 + i1 * 15 + i2\n                compute_3 = T.Buffer((780,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(T.asin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "exp",
                "acosh",
                "multiply",
                "acos",
                "asin",
                "sin"
            ]
        ],
        "input_shape": [[13, 4, 15]],
        "output_shape": [[13, 4, 15], [13, 4, 15], [13, 4, 15], [13, 4, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = fabsf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_1[(((i0 * 180) + (i1 * 9)) + i2_1)] = atanf(atanf(ph_0[(((i0 * 180) + (i1 * 9)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 720; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 20, 9), \"float32\"), compute: T.Buffer((4, 20, 9), \"float32\"), compute_1: T.Buffer((4, 20, 9), \"float32\"), compute_2: T.Buffer((4, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(80):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((720,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(20, 9):\n                cse_var_2: T.int32 = i0 * 180 + i1 * 9 + i2\n                compute_3 = T.Buffer((720,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(720):\n            compute_3 = T.Buffer((720,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "atan",
                "atan",
                "cos"
            ]
        ],
        "input_shape": [[4, 20, 9]],
        "output_shape": [[4, 20, 9], [4, 20, 9], [4, 20, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 208; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 208; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_subtract[((ax1 * 13) + ax2)] = (asinhf(ph_0[((ax1 * 13) + ax2)]) - ph_0[((ax1 * 13) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 16, 13), \"float32\"), ph_3: T.Buffer((1, 16, 13), \"float32\"), T_mod: T.Buffer((1, 16, 13), \"float32\"), compute: T.Buffer((1, 16, 13), \"float32\"), T_subtract: T.Buffer((1, 16, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((208,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(208):\n            T_mod_1 = T.Buffer((208,), data=T_mod.data)\n            ph_3_1 = T.Buffer((208,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(208):\n            compute_1 = T.Buffer((208,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax1, ax2 in T.grid(16, 13):\n            cse_var_1: T.int32 = ax1 * 13 + ax2\n            T_subtract_1 = T.Buffer((208,), data=T_subtract.data)\n            T_subtract_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]",
        "op_args": [
            [
                "mod",
                "cos",
                "asinh",
                "subtract"
            ]
        ],
        "input_shape": [[1, 16, 13], [1, 6, 15], [1, 16, 13]],
        "output_shape": [[1, 16, 13], [1, 6, 15], [1, 16, 13], [1, 16, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 48) + (i1 * 6)) + i2)] = acoshf(ph_0[(((i0 * 48) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 136; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_1[((i0_i1_fused * 6) + i2_1)] = sinf(asinhf(ph_0[((i0_i1_fused * 6) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 816; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * acoshf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 8, 6), \"float32\"), compute: T.Buffer((17, 8, 6), \"float32\"), compute_1: T.Buffer((17, 8, 6), \"float32\"), T_multiply: T.Buffer((17, 8, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((816,), data=ph_0.data)\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(8, 6):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 6 + i2\n                compute_2 = T.Buffer((816,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(136):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((816,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(T.asinh(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(816):\n            T_multiply_1 = T.Buffer((816,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "acosh",
                "asinh",
                "sin",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[17, 8, 6]],
        "output_shape": [[17, 8, 6], [17, 8, 6], [17, 8, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused], acosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1260; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused_1], acosf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 70; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_2[((i0_i1_fused * 18) + i2)] = asinhf((ph_0[((i0_i1_fused * 18) + i2)] - ph_3[((i0_i1_fused * 18) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1260; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = sinf((ph_0[i0_i1_fused_i2_fused_2] - ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 10, 18), \"float32\"), ph_3: T.Buffer((7, 10, 18), \"float32\"), compute: T.Buffer((7, 10, 18), \"float32\"), compute_1: T.Buffer((7, 10, 18), \"float32\"), compute_2: T.Buffer((7, 10, 18), \"float32\"), compute_3: T.Buffer((7, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.acos(ph_0_1[i0_i1_fused_i2_fused])))\n        ph_3_1 = T.Buffer((1260,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(70):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_4 = T.Buffer((1260,), data=compute_2.data)\n                compute_4[cse_var_1] = T.asinh(ph_0_1[cse_var_1] - ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_4 = T.Buffer((1260,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "acos",
                "mod",
                "atanh",
                "atanh",
                "asinh",
                "sin"
            ]
        ],
        "input_shape": [[7, 10, 18], [13, 6, 15], [7, 10, 18]],
        "output_shape": [[13, 6, 15], [7, 10, 18], [7, 10, 18], [7, 10, 18], [7, 10, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1170; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 130) + (i1 * 13)) + i2)] = expf(asinf(ph_0[(((i0 * 130) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1170; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1170; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanf((ph_0[i0_i1_fused_i2_fused_2] + ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __expf(asinf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 10, 13), \"float32\"), ph_3: T.Buffer((9, 10, 13), \"float32\"), compute: T.Buffer((9, 10, 13), \"float32\"), compute_1: T.Buffer((9, 10, 13), \"float32\"), compute_2: T.Buffer((9, 10, 13), \"float32\"), compute_3: T.Buffer((9, 10, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_4 = T.Buffer((1170,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(10, 13):\n                cse_var_1: T.int32 = i0 * 130 + i1 * 13 + i2\n                compute_4 = T.Buffer((1170,), data=compute_1.data)\n                compute_4[cse_var_1] = T.exp(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_4 = T.Buffer((1170,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_4 = T.Buffer((1170,), data=compute_3.data)\n            ph_3_1 = T.Buffer((1170,), data=ph_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "acosh",
                "asin",
                "exp",
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[9, 10, 13], [2, 5, 8], [9, 10, 13]],
        "output_shape": [[2, 5, 8], [9, 10, 13], [9, 10, 13], [9, 10, 13], [9, 10, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1440; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 90; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_add[((ax0_ax1_fused * 16) + ax2)] = (ceilf(ph_0[((ax0_ax1_fused * 16) + ax2)]) + ph_0[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1440; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1440; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 18, 16), \"float32\"), ph_3: T.Buffer((5, 18, 16), \"float32\"), T_mod: T.Buffer((5, 18, 16), \"float32\"), T_multiply: T.Buffer((5, 18, 16), \"float32\"), T_add: T.Buffer((5, 18, 16), \"float32\"), compute: T.Buffer((5, 18, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1440,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1440):\n            T_multiply_1 = T.Buffer((1440,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused in T.parallel(90):\n            for ax2 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_add_1 = T.Buffer((1440,), data=T_add.data)\n                T_add_1[cse_var_1] = T.ceil(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        ph_3_1 = T.Buffer((1440,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1440):\n            T_mod_1 = T.Buffer((1440,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1440):\n            compute_1 = T.Buffer((1440,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "mod",
                "ceil",
                "multiply",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[5, 18, 16], [18, 4, 7], [5, 18, 16]],
        "output_shape": [[18, 4, 7], [5, 18, 16], [5, 18, 16], [5, 18, 16], [5, 18, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 247; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = expf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3458; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3458; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3458; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 19, 14), \"float32\"), compute: T.Buffer((13, 19, 14), \"float32\"), compute_1: T.Buffer((13, 19, 14), \"float32\"), compute_2: T.Buffer((13, 19, 14), \"float32\"), compute_3: T.Buffer((13, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3458,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(247):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_4 = T.Buffer((3458,), data=compute.data)\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3458):\n            compute_4 = T.Buffer((3458,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(3458):\n            compute_4 = T.Buffer((3458,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3458):\n            compute_4 = T.Buffer((3458,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "cos",
                "asinh",
                "exp",
                "sin"
            ]
        ],
        "input_shape": [[13, 19, 14]],
        "output_shape": [[13, 19, 14], [13, 19, 14], [13, 19, 14], [13, 19, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 234) + (i1 * 18)) + i2)] = acoshf((ph_0[(((i0 * 234) + (i1 * 18)) + i2)] + ceilf(ph_0[(((i0 * 234) + (i1 * 18)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 17; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 13; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_1[(((i0_1 * 234) + (i1_1 * 18)) + i2_1)] = ceilf(ph_0[(((i0_1 * 234) + (i1_1 * 18)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3978; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 13, 18), \"float32\"), compute: T.Buffer((17, 13, 18), \"float32\"), compute_1: T.Buffer((17, 13, 18), \"float32\"), compute_2: T.Buffer((17, 13, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3978,), data=ph_0.data)\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(13, 18):\n                cse_var_1: T.int32 = i0 * 234 + i1 * 18 + i2\n                compute_3 = T.Buffer((3978,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1] + T.ceil(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(13, 18):\n                cse_var_2: T.int32 = i0 * 234 + i1 * 18 + i2\n                compute_3 = T.Buffer((3978,), data=compute_1.data)\n                compute_3[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(3978):\n            compute_3 = T.Buffer((3978,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "add",
                "acosh",
                "ceil",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[17, 13, 18]],
        "output_shape": [[17, 13, 18], [17, 13, 18], [17, 13, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_subtract, float* compute, float* ph_0, float* ph_3, float* ph_5) {\n  float auto_scheduler_layout_transform[25];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 15; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n      auto_scheduler_layout_transform[((ax0_ax1_fused_ax2_fused * 5) + ax6)] = ph_3[((ax0_ax1_fused_ax2_fused * 5) + ax6)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 15; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5) + j_outer_inner_init)] = 0.000000e+00f;\n    }\n    for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n      T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5) + j_outer_inner)] = (T_batch_matmul_NN[((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5) + j_outer_inner)] + (ph_0[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 3) * 5) + j_outer_inner)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 15; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - ph_5[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_5[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_3_shared[200];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    T_batch_matmul_NN_local[b_c_outer_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 3; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    for (int ax0_ax1_fused_ax2_fused_inner_s = 0; ax0_ax1_fused_ax2_fused_inner_s < 2; ++ax0_ax1_fused_ax2_fused_inner_s) {\n      if (((ax0_ax1_fused_ax2_fused_outer_outer * 2) + (((int)threadIdx.x) / 20)) < 5) {\n        ph_3_shared[(((ax0_ax1_fused_ax2_fused_outer_outer * 80) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_inner_s)] = ph_3[(((ax0_ax1_fused_ax2_fused_outer_outer * 80) + (((int)threadIdx.x) * 2)) + ax0_ax1_fused_ax2_fused_inner_s)];\n      }\n    }\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      T_batch_matmul_NN_local[b_c_outer_inner] = (T_batch_matmul_NN_local[b_c_outer_inner] + (ph_0[((((((((int)threadIdx.x) / 10) * 80) + (b_c_outer_inner * 40)) + (((int)blockIdx.x) * 10)) + (((((int)threadIdx.x) % 10) / 5) * 5)) + k_outer_inner)] * ph_3_shared[(((((((int)threadIdx.x) / 10) * 50) + (b_c_outer_inner * 25)) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[(((((((int)threadIdx.x) / 10) * 80) + (b_inner * 40)) + (((int)blockIdx.x) * 10)) + (((int)threadIdx.x) % 10))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 3, 1), \"float32\"), ph_3: T.Buffer((5, 1, 5), \"float32\"), ph_5: T.Buffer((5, 3, 1), \"float32\"), compute: T.Buffer((5, 3, 1), \"float32\"), T_batch_matmul_NN: T.Buffer((5, 3, 5), \"float32\"), T_subtract: T.Buffer((5, 3, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([25], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((15,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(15):\n            compute_1 = T.Buffer((15,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((25,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax6 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 5 + ax6\n                ph_3_1 = T.Buffer((25,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[cse_var_1] = ph_3_1[cse_var_1]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(15):\n            T_batch_matmul_NN_1 = T.Buffer((75,), data=T_batch_matmul_NN.data)\n            for j_outer_inner_init in range(5):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner_init] = T.float32(0)\n            for j_outer_inner in range(5):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 3 * 5 + j_outer_inner]\n        for ax0_ax1_fused_ax2_fused in T.parallel(15):\n            T_subtract_1 = T.Buffer((15,), data=T_subtract.data)\n            ph_5_1 = T.Buffer((15,), data=ph_5.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_5_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "exp",
                "batch_matmul",
                "subtract"
            ]
        ],
        "input_shape": [[5, 3, 1], [5, 1, 5], [5, 3, 1]],
        "output_shape": [[5, 3, 1], [5, 3, 5], [5, 3, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 140) + (i1 * 14)) + i2)] = ceilf(ph_0[(((i0 * 140) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 280; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 280; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = fabsf((ph_0[i0_i1_fused_i2_fused_2] / ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 10, 14), \"float32\"), ph_3: T.Buffer((2, 10, 14), \"float32\"), compute: T.Buffer((2, 10, 14), \"float32\"), compute_1: T.Buffer((2, 10, 14), \"float32\"), compute_2: T.Buffer((2, 10, 14), \"float32\"), compute_3: T.Buffer((2, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((280,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(10, 14):\n                cse_var_1: T.int32 = i0 * 140 + i1 * 14 + i2\n                compute_4 = T.Buffer((280,), data=compute.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_4 = T.Buffer((280,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_4 = T.Buffer((280,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_4 = T.Buffer((280,), data=compute_3.data)\n            ph_3_1 = T.Buffer((280,), data=ph_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "ceil",
                "asinh",
                "asin",
                "asinh",
                "abs"
            ]
        ],
        "input_shape": [[2, 10, 14], [18, 6, 18], [2, 10, 14]],
        "output_shape": [[18, 6, 18], [2, 10, 14], [2, 10, 14], [2, 10, 14], [2, 10, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 36; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = sinf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 6; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 6) + i2_1)] = acoshf(ph_0[((i0_i1_fused_1 * 6) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 6), \"float32\"), ph_3: T.Buffer((3, 2, 6), \"float32\"), T_multiply: T.Buffer((3, 2, 6), \"float32\"), compute: T.Buffer((3, 2, 6), \"float32\"), compute_1: T.Buffer((3, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((36,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(36):\n            T_multiply_1 = T.Buffer((36,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((36,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((36,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((36,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[3, 2, 6], [17, 4, 17], [3, 2, 6]],
        "output_shape": [[3, 2, 6], [17, 4, 17], [3, 2, 6], [3, 2, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 63) + (i1 * 9)) + i2)] = fabsf(ph_0[(((i0 * 63) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 12; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_1[(((i0_1 * 63) + (i1_1 * 9)) + i2_1)] = asinf(atanhf(ph_0[(((i0_1 * 63) + (i1_1 * 9)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 756; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], cosf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 7, 9), \"float32\"), compute: T.Buffer((12, 7, 9), \"float32\"), compute_1: T.Buffer((12, 7, 9), \"float32\"), T_mod: T.Buffer((12, 7, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((756,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(7, 9):\n                cse_var_1: T.int32 = i0 * 63 + i1 * 9 + i2\n                compute_2 = T.Buffer((756,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(7, 9):\n                cse_var_2: T.int32 = i0 * 63 + i1 * 9 + i2\n                compute_2 = T.Buffer((756,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(T.atanh(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(756):\n            T_mod_1 = T.Buffer((756,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]))",
        "op_args": [
            [
                "abs",
                "atanh",
                "asin",
                "cos",
                "mod"
            ]
        ],
        "input_shape": [[12, 7, 9]],
        "output_shape": [[12, 7, 9], [12, 7, 9], [12, 7, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 765; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_divide[(((ax0 * 45) + (ax1 * 5)) + ax2)] = (acosf(ph_0[(((ax0 * 45) + (ax1 * 5)) + ax2)]) / ph_0[(((ax0 * 45) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 153; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = acosf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 765; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 9, 5), \"float32\"), compute: T.Buffer((17, 9, 5), \"float32\"), T_divide: T.Buffer((17, 9, 5), \"float32\"), compute_1: T.Buffer((17, 9, 5), \"float32\"), compute_2: T.Buffer((17, 9, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((765,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(765):\n            compute_3 = T.Buffer((765,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(17):\n            for ax1, ax2 in T.grid(9, 5):\n                cse_var_1: T.int32 = ax0 * 45 + ax1 * 5 + ax2\n                T_divide_1 = T.Buffer((765,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.acos(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(153):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((765,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(765):\n            compute_3 = T.Buffer((765,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asin",
                "acos",
                "divide",
                "acos",
                "abs",
                "abs"
            ]
        ],
        "input_shape": [[17, 9, 5]],
        "output_shape": [[17, 9, 5], [17, 9, 5], [17, 9, 5], [17, 9, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 900; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 900; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 900; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_3[((i0_i1_fused * 5) + i2)] = atanf(ceilf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 15, 5), \"float32\"), compute: T.Buffer((12, 15, 5), \"float32\"), compute_1: T.Buffer((12, 15, 5), \"float32\"), compute_2: T.Buffer((12, 15, 5), \"float32\"), compute_3: T.Buffer((12, 15, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_4 = T.Buffer((900,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_4 = T.Buffer((900,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_4 = T.Buffer((900,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_4 = T.Buffer((900,), data=compute_3.data)\n                compute_4[cse_var_1] = T.atan(T.ceil(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "sin",
                "atanh",
                "asin",
                "acos",
                "ceil",
                "atan"
            ]
        ],
        "input_shape": [[12, 15, 5]],
        "output_shape": [[12, 15, 5], [12, 15, 5], [12, 15, 5], [12, 15, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 162) + (i1 * 18)) + i2)] = cosf((ph_0[(((i0 * 162) + (i1 * 18)) + i2)] + atanhf(ph_0[(((i0 * 162) + (i1 * 18)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[((i0_i1_fused * 18) + i2_1)] = ceilf(ph_0[((i0_i1_fused * 18) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2592; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2592; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = sinf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 9, 18), \"float32\"), compute: T.Buffer((16, 9, 18), \"float32\"), compute_1: T.Buffer((16, 9, 18), \"float32\"), compute_2: T.Buffer((16, 9, 18), \"float32\"), compute_3: T.Buffer((16, 9, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2592,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(9, 18):\n                cse_var_1: T.int32 = i0 * 162 + i1 * 18 + i2\n                compute_4 = T.Buffer((2592,), data=compute.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1] + T.atanh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(144):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_4 = T.Buffer((2592,), data=compute_1.data)\n                compute_4[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2592):\n            compute_4 = T.Buffer((2592,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2592):\n            compute_4 = T.Buffer((2592,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "add",
                "cos",
                "ceil",
                "cos",
                "asin",
                "sin"
            ]
        ],
        "input_shape": [[16, 9, 18]],
        "output_shape": [[16, 9, 18], [16, 9, 18], [16, 9, 18], [16, 9, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    compute[i0_i1_fused] = atanhf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      compute_1[((i0 * 16) + i1)] = acosf(ph_0[((i0 * 16) + i1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 16, 1), \"float32\"), compute: T.Buffer((8, 16, 1), \"float32\"), compute_1: T.Buffer((8, 16, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((128,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(128):\n            compute_2 = T.Buffer((128,), data=compute.data)\n            compute_2[i0_i1_fused] = T.atanh(ph_0_1[i0_i1_fused])\n        for i0 in T.parallel(8):\n            for i1 in range(16):\n                cse_var_1: T.int32 = i0 * 16 + i1\n                compute_2 = T.Buffer((128,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atanh",
                "acos"
            ]
        ],
        "input_shape": [[8, 16, 1]],
        "output_shape": [[8, 16, 1], [8, 16, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 15) + (i1 * 5)) + i2)] = acosf(ph_0[(((i0 * 15) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 240; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 3, 5), \"float32\"), compute: T.Buffer((16, 3, 5), \"float32\"), compute_1: T.Buffer((16, 3, 5), \"float32\"), compute_2: T.Buffer((16, 3, 5), \"float32\"), T_divide: T.Buffer((16, 3, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(3, 5):\n                cse_var_1: T.int32 = i0 * 15 + i1 * 5 + i2\n                compute_3 = T.Buffer((240,), data=compute.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_divide_1 = T.Buffer((240,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acos",
                "ceil",
                "asinh",
                "asin",
                "abs",
                "divide"
            ]
        ],
        "input_shape": [[16, 3, 5]],
        "output_shape": [[16, 3, 5], [16, 3, 5], [16, 3, 5], [16, 3, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 34) + (i1 * 2)) + i2)] = asinf((ph_0[(((i0 * 34) + (i1 * 2)) + i2)] / ceilf(ph_0[(((i0 * 34) + (i1 * 2)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_multiply[(((ax0 * 34) + (ax1 * 2)) + ax2)] = ((ph_0[(((ax0 * 34) + (ax1 * 2)) + ax2)] / ceilf(ph_0[(((ax0 * 34) + (ax1 * 2)) + ax2)])) * ph_0[(((ax0 * 34) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 102; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 102; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 17, 2), \"float32\"), ph_3: T.Buffer((3, 17, 2), \"float32\"), compute: T.Buffer((3, 17, 2), \"float32\"), T_multiply: T.Buffer((3, 17, 2), \"float32\"), compute_1: T.Buffer((3, 17, 2), \"float32\"), compute_2: T.Buffer((3, 17, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((102,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(17, 2):\n                cse_var_1: T.int32 = i0 * 34 + i1 * 2 + i2\n                compute_3 = T.Buffer((102,), data=compute.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1] / T.ceil(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(17, 2):\n                cse_var_2: T.int32 = ax0 * 34 + ax1 * 2 + ax2\n                T_multiply_1 = T.Buffer((102,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] / T.ceil(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        ph_3_1 = T.Buffer((102,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_3 = T.Buffer((102,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_3 = T.Buffer((102,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "ceil",
                "divide",
                "asin",
                "multiply",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[3, 17, 2], [9, 5, 8], [3, 17, 2]],
        "output_shape": [[9, 5, 8], [3, 17, 2], [3, 17, 2], [3, 17, 2], [3, 17, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    float compute_1[17];\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute_1[i2] = expf(ph_0[(((ax0 * 136) + (ax1 * 17)) + i2)]);\n      }\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_mod[(((ax0 * 136) + (ax1 * 17)) + ax2)] = fmodf(ph_0[(((ax0 * 136) + (ax1 * 17)) + ax2)], (ph_0[(((ax0 * 136) + (ax1 * 17)) + ax2)] + (compute_1[ax2] / ph_0[(((ax0 * 136) + (ax1 * 17)) + ax2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] + (__expf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 8, 17), \"float32\"), compute: T.Buffer((16, 8, 17), \"float32\"), T_mod: T.Buffer((16, 8, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2176,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2176):\n            compute_1 = T.Buffer((2176,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(16):\n            compute_1 = T.allocate([17], \"float32\", \"global\")\n            for ax1 in range(8):\n                compute_2 = T.Buffer((17,), data=compute_1)\n                for i2 in range(17):\n                    compute_2[i2] = T.exp(ph_0_1[ax0 * 136 + ax1 * 17 + i2])\n                for ax2 in range(17):\n                    cse_var_1: T.int32 = ax0 * 136 + ax1 * 17 + ax2\n                    T_mod_1 = T.Buffer((2176,), data=T_mod.data)\n                    T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] + compute_2[ax2] / ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "exp",
                "divide",
                "add",
                "mod"
            ]
        ],
        "input_shape": [[16, 8, 17]],
        "output_shape": [[16, 8, 17], [16, 8, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1368; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1368; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_2[(((i0 * 152) + (i1 * 8)) + i2)] = acosf(ph_0[(((i0 * 152) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1368; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = fabsf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 19, 8), \"float32\"), compute: T.Buffer((9, 19, 8), \"float32\"), compute_1: T.Buffer((9, 19, 8), \"float32\"), compute_2: T.Buffer((9, 19, 8), \"float32\"), compute_3: T.Buffer((9, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1368,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1368):\n            compute_4 = T.Buffer((1368,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1368):\n            compute_4 = T.Buffer((1368,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(19, 8):\n                cse_var_1: T.int32 = i0 * 152 + i1 * 8 + i2\n                compute_4 = T.Buffer((1368,), data=compute_2.data)\n                compute_4[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1368):\n            compute_4 = T.Buffer((1368,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "asin",
                "acos",
                "acos",
                "abs"
            ]
        ],
        "input_shape": [[9, 19, 8]],
        "output_shape": [[9, 19, 8], [9, 19, 8], [9, 19, 8], [9, 19, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  for (int32_t i1 = 0; i1 < 18; ++i1) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i1 * 5) + i2)] = acosf(fmodf(ph_0[((i1 * 5) + i2)], (ph_0[((i1 * 5) + i2)] / ph_3[((i1 * 5) + i2)])));\n    }\n  }\n  for (int32_t i1_1 = 0; i1_1 < 18; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_1[((i1_1 * 5) + i2_1)] = fabsf(fmodf(ph_0[((i1_1 * 5) + i2_1)], (ph_0[((i1_1 * 5) + i2_1)] / ph_3[((i1_1 * 5) + i2_1)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 90; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 18, 5), \"float32\"), ph_3: T.Buffer((1, 18, 5), \"float32\"), compute: T.Buffer((1, 18, 5), \"float32\"), compute_1: T.Buffer((1, 18, 5), \"float32\"), T_subtract: T.Buffer((1, 18, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((90,), data=ph_0.data)\n        ph_3_1 = T.Buffer((90,), data=ph_3.data)\n        for i1, i2 in T.grid(18, 5):\n            cse_var_1: T.int32 = i1 * 5 + i2\n            compute_2 = T.Buffer((90,), data=compute.data)\n            compute_2[cse_var_1] = T.acos(T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] / ph_3_1[cse_var_1]))\n        for i1, i2 in T.grid(18, 5):\n            cse_var_2: T.int32 = i1 * 5 + i2\n            compute_2 = T.Buffer((90,), data=compute_1.data)\n            compute_2[cse_var_2] = T.fabs(T.truncmod(ph_0_1[cse_var_2], ph_0_1[cse_var_2] / ph_3_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(90):\n            T_subtract_1 = T.Buffer((90,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "multiply",
                "divide",
                "mod",
                "acos",
                "abs",
                "subtract"
            ]
        ],
        "input_shape": [[1, 18, 5], [3, 1, 15], [1, 18, 5]],
        "output_shape": [[3, 1, 15], [1, 18, 5], [1, 18, 5], [1, 18, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = acosf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 60; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 6; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 6) + i2_1)] = atanf(ph_0[((i0_i1_fused_1 * 6) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 15, 6), \"float32\"), compute: T.Buffer((4, 15, 6), \"float32\"), compute_1: T.Buffer((4, 15, 6), \"float32\"), compute_2: T.Buffer((4, 15, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((360,), data=compute.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((360,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acos",
                "atanh",
                "asin",
                "atan"
            ]
        ],
        "input_shape": [[4, 15, 6]],
        "output_shape": [[4, 15, 6], [4, 15, 6], [4, 15, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 8; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = acosf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 8; ++i1) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_1[((i1 * 13) + i2_1)] = asinf(asinf(ph_0[((i1 * 13) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 104; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ceilf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 8, 13), \"float32\"), compute: T.Buffer((1, 8, 13), \"float32\"), compute_1: T.Buffer((1, 8, 13), \"float32\"), T_add: T.Buffer((1, 8, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((104,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(8):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((104,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i1, i2 in T.grid(8, 13):\n            cse_var_2: T.int32 = i1 * 13 + i2\n            compute_2 = T.Buffer((104,), data=compute_1.data)\n            compute_2[cse_var_2] = T.asin(T.asin(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(104):\n            T_add_1 = T.Buffer((104,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "acos",
                "asin",
                "asin",
                "ceil",
                "add"
            ]
        ],
        "input_shape": [[1, 8, 13]],
        "output_shape": [[1, 8, 13], [1, 8, 13], [1, 8, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4160; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] * atanhf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 320) + (i1 * 16)) + i2)] = fabsf((ph_0[(((i0 * 320) + (i1 * 16)) + i2)] * atanhf(ph_0[(((i0 * 320) + (i1 * 16)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_1[((i0_i1_fused * 16) + i2_1)] = sinf((ph_0[((i0_i1_fused * 16) + i2_1)] * ph_3[((i0_i1_fused * 16) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 20, 16), \"float32\"), ph_3: T.Buffer((13, 20, 16), \"float32\"), T_mod: T.Buffer((13, 20, 16), \"float32\"), compute: T.Buffer((13, 20, 16), \"float32\"), compute_1: T.Buffer((13, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4160,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4160):\n            T_mod_1 = T.Buffer((4160,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] * T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(20, 16):\n                cse_var_1: T.int32 = i0 * 320 + i1 * 16 + i2\n                compute_2 = T.Buffer((4160,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1] * T.atanh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(260):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((4160,), data=compute_1.data)\n                ph_3_1 = T.Buffer((4160,), data=ph_3.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2] * ph_3_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "atanh",
                "multiply",
                "mod",
                "abs",
                "sin"
            ]
        ],
        "input_shape": [[13, 20, 16], [3, 13, 19], [13, 20, 16]],
        "output_shape": [[3, 13, 19], [13, 20, 16], [13, 20, 16], [13, 20, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 24; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_mod[((ax0_ax1_fused * 14) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 14) + ax2)], ph_3[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 168) + (i1 * 14)) + i2)] = expf(ph_0[(((i0 * 168) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 336; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 12, 14), \"float32\"), ph_3: T.Buffer((2, 12, 14), \"float32\"), T_mod: T.Buffer((2, 12, 14), \"float32\"), compute: T.Buffer((2, 12, 14), \"float32\"), T_divide: T.Buffer((2, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((336,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(24):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_mod_1 = T.Buffer((336,), data=T_mod.data)\n                ph_3_1 = T.Buffer((336,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(12, 14):\n                cse_var_2: T.int32 = i0 * 168 + i1 * 14 + i2\n                compute_1 = T.Buffer((336,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(336):\n            T_divide_1 = T.Buffer((336,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "exp",
                "cos",
                "divide"
            ]
        ],
        "input_shape": [[2, 12, 14], [16, 2, 10], [2, 12, 14]],
        "output_shape": [[2, 12, 14], [16, 2, 10], [2, 12, 14], [2, 12, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = fabsf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 40; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 16) + i2_1)] = sinf(ph_0[((i0_i1_fused_1 * 16) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 2, 16), \"float32\"), compute: T.Buffer((20, 2, 16), \"float32\"), compute_1: T.Buffer((20, 2, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((640,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(40):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((640,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(40):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((640,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "sin"
            ]
        ],
        "input_shape": [[20, 2, 16]],
        "output_shape": [[20, 2, 16], [20, 2, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 280) + (i1 * 20)) + i2)] = sinf((ph_0[(((i0 * 280) + (i1 * 20)) + i2)] - cosf(ph_0[(((i0 * 280) + (i1 * 20)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4200; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf((ph_0[i0_i1_fused_i2_fused] - cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 210; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_2[((i0_i1_fused * 20) + i2_1)] = ceilf(fmodf(ph_0[((i0_i1_fused * 20) + i2_1)], ph_3[((i0_i1_fused * 20) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 14, 20), \"float32\"), ph_3: T.Buffer((15, 14, 20), \"float32\"), compute: T.Buffer((15, 14, 20), \"float32\"), compute_1: T.Buffer((15, 14, 20), \"float32\"), compute_2: T.Buffer((15, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4200,), data=ph_0.data)\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(14, 20):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 20 + i2\n                compute_3 = T.Buffer((4200,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1] - T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4200):\n            compute_3 = T.Buffer((4200,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] - T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(210):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((4200,), data=compute_2.data)\n                ph_3_1 = T.Buffer((4200,), data=ph_3.data)\n                compute_3[cse_var_2] = T.ceil(T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "cos",
                "subtract",
                "sin",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[15, 14, 20], [6, 7, 18], [15, 14, 20]],
        "output_shape": [[6, 7, 18], [15, 14, 20], [15, 14, 20], [15, 14, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 95; ++i0_i1_fused) {\n    compute[i0_i1_fused] = cosf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 95; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 95; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      compute_3[((i0 * 5) + i1)] = atanf(ph_0[((i0 * 5) + i1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 5, 1), \"float32\"), compute: T.Buffer((19, 5, 1), \"float32\"), compute_1: T.Buffer((19, 5, 1), \"float32\"), compute_2: T.Buffer((19, 5, 1), \"float32\"), compute_3: T.Buffer((19, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((95,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(95):\n            compute_4 = T.Buffer((95,), data=compute.data)\n            compute_4[i0_i1_fused] = T.cos(ph_0_1[i0_i1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(95):\n            compute_4 = T.Buffer((95,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(95):\n            compute_4 = T.Buffer((95,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1 in range(5):\n                cse_var_1: T.int32 = i0 * 5 + i1\n                compute_4 = T.Buffer((95,), data=compute_3.data)\n                compute_4[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "atan",
                "sin",
                "acos",
                "atan"
            ]
        ],
        "input_shape": [[19, 5, 1]],
        "output_shape": [[19, 5, 1], [19, 5, 1], [19, 5, 1], [19, 5, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3800; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3800; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 3800; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], fabsf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 20, 10), \"float32\"), compute: T.Buffer((19, 20, 10), \"float32\"), T_subtract: T.Buffer((19, 20, 10), \"float32\"), T_mod: T.Buffer((19, 20, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3800,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3800):\n            compute_1 = T.Buffer((3800,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3800):\n            T_subtract_1 = T.Buffer((3800,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(3800):\n            T_mod_1 = T.Buffer((3800,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]))",
        "op_args": [
            [
                "exp",
                "acos",
                "subtract",
                "abs",
                "mod"
            ]
        ],
        "input_shape": [[19, 20, 10]],
        "output_shape": [[19, 20, 10], [19, 20, 10], [19, 20, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_add[((ax1 * 17) + ax2)] = (ph_0[((ax1 * 17) + ax2)] + ph_3[((ax1 * 17) + ax2)]);\n    }\n  }\n  for (int32_t ax1_1 = 0; ax1_1 < 2; ++ax1_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 17; ++ax2_1) {\n      T_mod[((ax1_1 * 17) + ax2_1)] = fmodf(ph_0[((ax1_1 * 17) + ax2_1)], ph_3[((ax1_1 * 17) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 34; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 2, 17), \"float32\"), ph_3: T.Buffer((1, 2, 17), \"float32\"), T_add: T.Buffer((1, 2, 17), \"float32\"), T_mod: T.Buffer((1, 2, 17), \"float32\"), compute: T.Buffer((1, 2, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((34,), data=ph_0.data)\n        ph_3_1 = T.Buffer((34,), data=ph_3.data)\n        for ax1, ax2 in T.grid(2, 17):\n            cse_var_1: T.int32 = ax1 * 17 + ax2\n            T_add_1 = T.Buffer((34,), data=T_add.data)\n            T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax1, ax2 in T.grid(2, 17):\n            cse_var_2: T.int32 = ax1 * 17 + ax2\n            T_mod_1 = T.Buffer((34,), data=T_mod.data)\n            T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(34):\n            compute_1 = T.Buffer((34,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "mod",
                "sin"
            ]
        ],
        "input_shape": [[1, 2, 17], [1, 15, 11], [1, 2, 17]],
        "output_shape": [[1, 2, 17], [1, 15, 11], [1, 2, 17], [1, 2, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 112) + (i1 * 7)) + i2)] = fabsf(ph_0[(((i0 * 112) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * acosf(asinhf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_1[((i0_i1_fused * 7) + i2_1)] = acosf(ph_0[((i0_i1_fused * 7) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * acosf(asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 16, 7), \"float32\"), compute: T.Buffer((6, 16, 7), \"float32\"), T_multiply: T.Buffer((6, 16, 7), \"float32\"), compute_1: T.Buffer((6, 16, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((672,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(16, 7):\n                cse_var_1: T.int32 = i0 * 112 + i1 * 7 + i2\n                compute_2 = T.Buffer((672,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            T_multiply_1 = T.Buffer((672,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.acos(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((672,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "asinh",
                "acos",
                "multiply",
                "acos"
            ]
        ],
        "input_shape": [[6, 16, 7]],
        "output_shape": [[6, 16, 7], [6, 16, 7], [6, 16, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1200; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1200; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    float compute_2[1];\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_2[0] = expf(ph_0[((i0_i1_fused * 15) + i2)]);\n      compute_1[((i0_i1_fused * 15) + i2)] = atanhf(compute_2[0]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 16, 15), \"float32\"), ph_3: T.Buffer((5, 16, 15), \"float32\"), T_subtract: T.Buffer((5, 16, 15), \"float32\"), compute: T.Buffer((5, 16, 15), \"float32\"), compute_1: T.Buffer((5, 16, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1200,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1200):\n            T_subtract_1 = T.Buffer((1200,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((1200,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1200):\n            compute_2 = T.Buffer((1200,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(80):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_3 = T.Buffer((1,), data=compute_2, align=4)\n                compute_3[0] = T.exp(ph_0_1[cse_var_1])\n                compute_4 = T.Buffer((1200,), data=compute_1.data)\n                compute_4[cse_var_1] = T.atanh(compute_3[0])",
        "op_args": [
            [
                "subtract",
                "atan",
                "exp",
                "atanh"
            ]
        ],
        "input_shape": [[5, 16, 15], [2, 1, 9], [5, 16, 15]],
        "output_shape": [[5, 16, 15], [2, 1, 9], [5, 16, 15], [5, 16, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2736; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_mod[(((ax0 * 144) + (ax1 * 8)) + ax2)] = fmodf(cosf(ph_0[(((ax0 * 144) + (ax1 * 8)) + ax2)]), ph_0[(((ax0 * 144) + (ax1 * 8)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2736; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2736; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((int)blockIdx.x)] = ((ph_0[((int)blockIdx.x)] - ph_3[((int)blockIdx.x)]) - ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 18, 8), \"float32\"), ph_3: T.Buffer((19, 18, 8), \"float32\"), compute: T.Buffer((19, 18, 8), \"float32\"), T_mod: T.Buffer((19, 18, 8), \"float32\"), T_divide: T.Buffer((19, 18, 8), \"float32\"), T_subtract: T.Buffer((19, 18, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2736,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2736):\n            compute_1 = T.Buffer((2736,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(19):\n            for ax1, ax2 in T.grid(18, 8):\n                cse_var_1: T.int32 = ax0 * 144 + ax1 * 8 + ax2\n                T_mod_1 = T.Buffer((2736,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.cos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2736):\n            T_divide_1 = T.Buffer((2736,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2736):\n            T_subtract_1 = T.Buffer((2736,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2736,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "subtract",
                "asin",
                "cos",
                "mod",
                "divide",
                "subtract"
            ]
        ],
        "input_shape": [[19, 18, 8], [8, 11, 13], [19, 18, 8]],
        "output_shape": [[8, 11, 13], [19, 18, 8], [19, 18, 8], [19, 18, 8], [19, 18, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0, float* ph_3) {\n  float compute_5[5440];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5440; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5440; ++i0_i1_fused_i2_fused_1) {\n    compute_5[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 5440; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = sinf(compute_5[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 320; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_2[((i0_i1_fused * 17) + i2)] = sinf(compute_5[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 5440; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = acosf(fmodf(ph_0[i0_i1_fused_i2_fused_3], ph_3[i0_i1_fused_i2_fused_3]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 5440; ++i0_i1_fused_i2_fused_4) {\n    compute_4[i0_i1_fused_i2_fused_4] = ceilf(fmodf(ph_0[i0_i1_fused_i2_fused_4], ph_3[i0_i1_fused_i2_fused_4]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 16, 17), \"float32\"), ph_3: T.Buffer((20, 16, 17), \"float32\"), compute: T.Buffer((20, 16, 17), \"float32\"), compute_1: T.Buffer((20, 16, 17), \"float32\"), compute_2: T.Buffer((20, 16, 17), \"float32\"), compute_3: T.Buffer((20, 16, 17), \"float32\"), compute_4: T.Buffer((20, 16, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_5 = T.allocate([5440], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((5440,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(5440):\n            compute_6 = T.Buffer((5440,), data=compute.data)\n            compute_6[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        compute_6 = T.Buffer((5440,), data=compute_5)\n        for i0_i1_fused_i2_fused in T.parallel(5440):\n            compute_6[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(5440):\n            compute_7 = T.Buffer((5440,), data=compute_1.data)\n            compute_7[i0_i1_fused_i2_fused] = T.sin(compute_6[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(320):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_7 = T.Buffer((5440,), data=compute_2.data)\n                compute_7[cse_var_1] = T.sin(compute_6[cse_var_1])\n        ph_3_1 = T.Buffer((5440,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(5440):\n            compute_7 = T.Buffer((5440,), data=compute_3.data)\n            compute_7[i0_i1_fused_i2_fused] = T.acos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(5440):\n            compute_7 = T.Buffer((5440,), data=compute_4.data)\n            compute_7[i0_i1_fused_i2_fused] = T.ceil(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "acosh",
                "exp",
                "sin",
                "sin",
                "acos",
                "ceil"
            ]
        ],
        "input_shape": [[20, 16, 17], [13, 2, 7], [20, 16, 17]],
        "output_shape": [[13, 2, 7], [20, 16, 17], [20, 16, 17], [20, 16, 17], [20, 16, 17], [20, 16, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 192; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 192; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_2[(((i0 * 24) + (i1 * 6)) + i2)] = expf(ph_0[(((i0 * 24) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 192; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(atanhf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 4, 6), \"float32\"), compute: T.Buffer((8, 4, 6), \"float32\"), compute_1: T.Buffer((8, 4, 6), \"float32\"), compute_2: T.Buffer((8, 4, 6), \"float32\"), compute_3: T.Buffer((8, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((192,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(192):\n            compute_4 = T.Buffer((192,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(192):\n            compute_4 = T.Buffer((192,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(4, 6):\n                cse_var_1: T.int32 = i0 * 24 + i1 * 6 + i2\n                compute_4 = T.Buffer((192,), data=compute_2.data)\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(192):\n            compute_4 = T.Buffer((192,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "sin",
                "exp",
                "asinh"
            ]
        ],
        "input_shape": [[8, 4, 6]],
        "output_shape": [[8, 4, 6], [8, 4, 6], [8, 4, 6], [8, 4, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_subtract[((ax0 * 20) + ax2)] = (ph_0[((ax0 * 20) + ax2)] - ph_3[((ax0 * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i0_i1_fused * 20) + i2)] = cosf(ph_0[((i0_i1_fused * 20) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 1, 20), \"float32\"), ph_3: T.Buffer((18, 1, 20), \"float32\"), T_subtract: T.Buffer((18, 1, 20), \"float32\"), compute: T.Buffer((18, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0 in T.parallel(18):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0 * 20 + ax2\n                T_subtract_1 = T.Buffer((360,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((360,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_1 = T.Buffer((360,), data=compute.data)\n                compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "subtract",
                "cos"
            ]
        ],
        "input_shape": [[18, 1, 20], [16, 12, 7], [18, 1, 20]],
        "output_shape": [[18, 1, 20], [16, 12, 7], [18, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_multiply[(((ax0 * 252) + (ax1 * 14)) + ax2)] = ((ph_0[(((ax0 * 252) + (ax1 * 14)) + ax2)] * (ph_0[(((ax0 * 252) + (ax1 * 14)) + ax2)] / ph_3[(((ax0 * 252) + (ax1 * 14)) + ax2)])) * ph_0[(((ax0 * 252) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5040; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5040; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 18, 14), \"float32\"), ph_3: T.Buffer((20, 18, 14), \"float32\"), T_multiply: T.Buffer((20, 18, 14), \"float32\"), compute: T.Buffer((20, 18, 14), \"float32\"), compute_1: T.Buffer((20, 18, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5040,), data=ph_0.data)\n        ph_3_1 = T.Buffer((5040,), data=ph_3.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(18, 14):\n                cse_var_1: T.int32 = ax0 * 252 + ax1 * 14 + ax2\n                T_multiply_1 = T.Buffer((5040,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * (ph_0_1[cse_var_1] / ph_3_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(5040):\n            compute_2 = T.Buffer((5040,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(5040):\n            compute_2 = T.Buffer((5040,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "divide",
                "multiply",
                "multiply",
                "cos",
                "atanh"
            ]
        ],
        "input_shape": [[20, 18, 14], [1, 8, 10], [20, 18, 14]],
        "output_shape": [[1, 8, 10], [20, 18, 14], [20, 18, 14], [20, 18, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 119; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = ceilf((ph_0[((i0_i1_fused * 13) + i2)] / atanf(ph_0[((i0_i1_fused * 13) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 7, 13), \"float32\"), compute: T.Buffer((17, 7, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(119):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_1 = T.Buffer((1547,), data=compute.data)\n                ph_0_1 = T.Buffer((1547,), data=ph_0.data)\n                compute_1[cse_var_1] = T.ceil(ph_0_1[cse_var_1] / T.atan(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "atan",
                "divide",
                "ceil"
            ]
        ],
        "input_shape": [[17, 7, 13]],
        "output_shape": [[17, 7, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] * atanf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute_1[(((i0 * 56) + (i1 * 7)) + i2)] = acosf(ph_0[(((i0 * 56) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 8, 7), \"float32\"), T_mod: T.Buffer((9, 8, 7), \"float32\"), compute: T.Buffer((9, 8, 7), \"float32\"), compute_1: T.Buffer((9, 8, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((504,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(504):\n            T_mod_1 = T.Buffer((504,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] * T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_2 = T.Buffer((504,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(8, 7):\n                cse_var_1: T.int32 = i0 * 56 + i1 * 7 + i2\n                compute_2 = T.Buffer((504,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "multiply",
                "mod",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[9, 8, 7]],
        "output_shape": [[9, 8, 7], [9, 8, 7], [9, 8, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 11; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_subtract[(((ax0 * 120) + (ax1 * 12)) + ax2)] = (0.000000e+00f - (ph_0[(((ax0 * 120) + (ax1 * 12)) + ax2)] - ceilf(ph_0[(((ax0 * 120) + (ax1 * 12)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 110; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = asinhf(acosf(ph_0[((i0_i1_fused * 12) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1320; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (0.000000e+00f - (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 10, 12), \"float32\"), T_subtract: T.Buffer((11, 10, 12), \"float32\"), compute: T.Buffer((11, 10, 12), \"float32\"), compute_1: T.Buffer((11, 10, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1320,), data=ph_0.data)\n        for ax0 in T.parallel(11):\n            for ax1, ax2 in T.grid(10, 12):\n                cse_var_1: T.int32 = ax0 * 120 + ax1 * 12 + ax2\n                T_subtract_1 = T.Buffer((1320,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.float32(0) - (ph_0_1[cse_var_1] - T.ceil(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(110):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((1320,), data=compute.data)\n                compute_2[cse_var_2] = T.asinh(T.acos(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1320):\n            compute_2 = T.Buffer((1320,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "subtract",
                "add",
                "subtract",
                "acos",
                "asinh",
                "acosh"
            ]
        ],
        "input_shape": [[11, 10, 12]],
        "output_shape": [[11, 10, 12], [11, 10, 12], [11, 10, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 220) + (i1 * 11)) + i2)] = asinhf(ph_0[(((i0 * 220) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 660; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / (sinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 660; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / (__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 20, 11), \"float32\"), compute: T.Buffer((3, 20, 11), \"float32\"), T_divide: T.Buffer((3, 20, 11), \"float32\"), compute_1: T.Buffer((3, 20, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((660,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(20, 11):\n                cse_var_1: T.int32 = i0 * 220 + i1 * 11 + i2\n                compute_2 = T.Buffer((660,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(660):\n            T_divide_1 = T.Buffer((660,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / (T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(660):\n            compute_2 = T.Buffer((660,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "sin",
                "divide",
                "divide",
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[3, 20, 11]],
        "output_shape": [[3, 20, 11], [3, 20, 11], [3, 20, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_5) {\n  float auto_scheduler_layout_transform[70];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 112; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n    for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n      for (int32_t ax8 = 0; ax8 < 7; ++ax8) {\n        auto_scheduler_layout_transform[(((ax3 * 14) + (ax5 * 7)) + ax8)] = ph_3[(((ax5 * 35) + (ax8 * 5)) + ax3)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 5; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 8; ++i_outer_inner_init) {\n        for (int32_t b_inner_init = 0; b_inner_init < 7; ++b_inner_init) {\n          T_batch_matmul_NN[((((b_outer_inner_init * 280) + (b_inner_init * 40)) + (i_outer_inner_init * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 8; ++i_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 7; ++b_inner) {\n          T_batch_matmul_NN[((((b_outer_inner * 280) + (b_inner * 40)) + (i_outer_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[((((b_outer_inner * 280) + (b_inner * 40)) + (i_outer_inner * 5)) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (ph_0[(((b_outer_inner * 56) + (b_inner * 8)) + i_outer_inner)] * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 14) + (b_outer_inner * 7)) + b_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 112; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_5[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 112; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(ph_5[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(15) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[1];\n  __shared__ float ph_3_shared[20];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 10) {\n      *(float2*)(ph_3_shared + (((int)threadIdx.x) * 2)) = *(float2*)(ph_3 + ((k_outer_outer * 20) + (((int)threadIdx.x) * 2)));\n    }\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 2; ++k_outer_inner) {\n      for (int k_inner = 0; k_inner < 2; ++k_inner) {\n        T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (ph_0[(((((((int)blockIdx.x) * 24) + ((((int)threadIdx.x) / 5) * 8)) + (k_outer_outer * 4)) + (k_outer_inner * 2)) + k_inner)] * ph_3_shared[(((k_outer_inner * 10) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n      }\n    }\n  }\n  T_batch_matmul_NN[((((int)blockIdx.x) * 15) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_5[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_5) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_5[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 8, 1), \"float32\"), ph_3: T.Buffer((14, 1, 5), \"float32\"), ph_5: T.Buffer((14, 8, 1), \"float32\"), compute: T.Buffer((14, 8, 1), \"float32\"), T_batch_matmul_NN: T.Buffer((14, 8, 5), \"float32\"), T_multiply: T.Buffer((14, 8, 1), \"float32\"), compute_1: T.Buffer((14, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([70], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((112,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_2 = T.Buffer((112,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((70,), data=auto_scheduler_layout_transform)\n        for ax3, ax5, ax8 in T.grid(5, 2, 7):\n            ph_3_1 = T.Buffer((70,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax3 * 14 + ax5 * 7 + ax8] = ph_3_1[ax5 * 35 + ax8 * 5 + ax3]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(5):\n            T_batch_matmul_NN_1 = T.Buffer((560,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_outer_inner_init, b_inner_init in T.grid(2, 8, 7):\n                T_batch_matmul_NN_1[b_outer_inner_init * 280 + b_inner_init * 40 + i_outer_inner_init * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for b_outer_inner, i_outer_inner, b_inner in T.grid(2, 8, 7):\n                cse_var_1: T.int32 = b_outer_inner * 280 + b_inner * 40 + i_outer_inner * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 56 + b_inner * 8 + i_outer_inner] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 14 + b_outer_inner * 7 + b_inner]\n        ph_5_1 = T.Buffer((112,), data=ph_5.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(112):\n            T_multiply_1 = T.Buffer((112,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_5_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(112):\n            compute_2 = T.Buffer((112,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_5_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "batch_matmul",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[14, 8, 1], [14, 1, 5], [14, 8, 1]],
        "output_shape": [[14, 8, 1], [14, 8, 5], [14, 8, 1], [14, 8, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0, float* ph_4, float* ph_6) {\n  float auto_scheduler_layout_transform[19];\n  for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n    auto_scheduler_layout_transform[ax2] = ph_4[ax2];\n  }\n  #pragma omp parallel for\n  for (int32_t b = 0; b < 4; ++b) {\n    T_batch_matmul_NN[b] = 0.000000e+00f;\n    for (int32_t k = 0; k < 19; ++k) {\n      T_batch_matmul_NN[b] = (T_batch_matmul_NN[b] + (acoshf(ph_0[((b * 19) + k)]) * auto_scheduler_layout_transform[k]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0 * 19) + i2)] = fabsf((ph_0[((i0 * 19) + i2)] - ph_6[((i0 * 19) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 76; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(72) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  float T_batch_matmul_NN[1];\n  __shared__ float compute_shared[576];\n  T_batch_matmul_NN[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 8; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 72) + ((int)threadIdx.x))] = acoshf(ph_0[((ax0_ax1_fused_ax2_fused_outer_outer * 72) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 8; ++k_inner) {\n    T_batch_matmul_NN[0] = (T_batch_matmul_NN[0] + (compute_shared[((((int)threadIdx.x) * 8) + k_inner)] * ph_0[((((((int)threadIdx.x) >> 3) * 64) + (k_inner * 8)) + ((int)blockIdx.x))]));\n  }\n  T_subtract[((((int)threadIdx.x) * 8) + ((int)blockIdx.x))] = (ph_0[((((int)threadIdx.x) * 8) + ((int)blockIdx.x))] - T_batch_matmul_NN[0]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 1, 19), \"float32\"), ph_4: T.Buffer((4, 19, 1), \"float32\"), ph_6: T.Buffer((4, 1, 19), \"float32\"), T_batch_matmul_NN: T.Buffer((4, 1, 1), \"float32\"), compute: T.Buffer((4, 1, 19), \"float32\"), compute_1: T.Buffer((4, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([19], \"float32\", \"global\")\n        auto_scheduler_layout_transform_1 = T.Buffer((19,), data=auto_scheduler_layout_transform)\n        for ax2 in range(19):\n            ph_4_1 = T.Buffer((76,), data=ph_4.data)\n            auto_scheduler_layout_transform_1[ax2] = ph_4_1[ax2]\n        ph_0_1 = T.Buffer((76,), data=ph_0.data)\n        for b in T.parallel(4):\n            T_batch_matmul_NN_1 = T.Buffer((4,), data=T_batch_matmul_NN.data)\n            T_batch_matmul_NN_1[b] = T.float32(0)\n            for k in range(19):\n                T_batch_matmul_NN_1[b] = T_batch_matmul_NN_1[b] + T.acosh(ph_0_1[b * 19 + k]) * auto_scheduler_layout_transform_1[k]\n        for i0 in T.parallel(4):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0 * 19 + i2\n                compute_2 = T.Buffer((76,), data=compute.data)\n                ph_6_1 = T.Buffer((76,), data=ph_6.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1] - ph_6_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(76):\n            compute_2 = T.Buffer((76,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "acosh",
                "batch_matmul",
                "subtract",
                "abs",
                "atan"
            ]
        ],
        "input_shape": [[4, 1, 19], [4, 19, 1], [4, 1, 19]],
        "output_shape": [[4, 1, 1], [4, 1, 19], [4, 1, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 320) + (i1 * 16)) + i2)] = cosf(ph_0[(((i0 * 320) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 640; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 640; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 20, 16), \"float32\"), compute: T.Buffer((2, 20, 16), \"float32\"), T_multiply: T.Buffer((2, 20, 16), \"float32\"), compute_1: T.Buffer((2, 20, 16), \"float32\"), compute_2: T.Buffer((2, 20, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((640,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(20, 16):\n                cse_var_1: T.int32 = i0 * 320 + i1 * 16 + i2\n                compute_3 = T.Buffer((640,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(640):\n            T_multiply_1 = T.Buffer((640,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            compute_3 = T.Buffer((640,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            compute_3 = T.Buffer((640,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "atanh",
                "multiply",
                "acosh",
                "ceil"
            ]
        ],
        "input_shape": [[2, 20, 16]],
        "output_shape": [[2, 20, 16], [2, 20, 16], [2, 20, 16], [2, 20, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_mod[(((ax0 * 45) + (ax1 * 3)) + ax2)] = fmodf(ph_0[(((ax0 * 45) + (ax1 * 3)) + ax2)], ph_3[(((ax0 * 45) + (ax1 * 3)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = asinf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 540; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 15, 3), \"float32\"), ph_3: T.Buffer((12, 15, 3), \"float32\"), T_mod: T.Buffer((12, 15, 3), \"float32\"), compute: T.Buffer((12, 15, 3), \"float32\"), compute_1: T.Buffer((12, 15, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for ax0 in T.parallel(12):\n            for ax1, ax2 in T.grid(15, 3):\n                cse_var_1: T.int32 = ax0 * 45 + ax1 * 3 + ax2\n                T_mod_1 = T.Buffer((540,), data=T_mod.data)\n                ph_3_1 = T.Buffer((540,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((540,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(540):\n            compute_2 = T.Buffer((540,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "mod",
                "asin",
                "ceil"
            ]
        ],
        "input_shape": [[12, 15, 3], [15, 19, 16], [12, 15, 3]],
        "output_shape": [[12, 15, 3], [15, 19, 16], [12, 15, 3], [12, 15, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 13; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_add[((ax0_ax1_fused * 12) + ax2)] = ((ph_0[((ax0_ax1_fused * 12) + ax2)] - atanhf(ph_0[((ax0_ax1_fused * 12) + ax2)])) + ph_0[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 13; ++i1) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i1 * 12) + i2)] = cosf(ph_0[((i1 * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 156; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 13, 12), \"float32\"), T_add: T.Buffer((1, 13, 12), \"float32\"), compute: T.Buffer((1, 13, 12), \"float32\"), T_subtract: T.Buffer((1, 13, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((156,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(13):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_add_1 = T.Buffer((156,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] - T.atanh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i1, i2 in T.grid(13, 12):\n            cse_var_2: T.int32 = i1 * 12 + i2\n            compute_1 = T.Buffer((156,), data=compute.data)\n            compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(156):\n            T_subtract_1 = T.Buffer((156,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "subtract",
                "add",
                "cos",
                "sin",
                "subtract"
            ]
        ],
        "input_shape": [[1, 13, 12]],
        "output_shape": [[1, 13, 12], [1, 13, 12], [1, 13, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = ceilf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 816; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 816; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 3, 17), \"float32\"), compute: T.Buffer((16, 3, 17), \"float32\"), compute_1: T.Buffer((16, 3, 17), \"float32\"), compute_2: T.Buffer((16, 3, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((816,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_3 = T.Buffer((816,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(816):\n            compute_3 = T.Buffer((816,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(816):\n            compute_3 = T.Buffer((816,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "asin",
                "ceil"
            ]
        ],
        "input_shape": [[16, 3, 17]],
        "output_shape": [[16, 3, 17], [16, 3, 17], [16, 3, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2160; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 9), \"float32\"), ph_3: T.Buffer((16, 15, 9), \"float32\"), T_divide: T.Buffer((16, 15, 9), \"float32\"), compute: T.Buffer((16, 15, 9), \"float32\"), compute_1: T.Buffer((16, 15, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_divide_1 = T.Buffer((2160,), data=T_divide.data)\n            ph_3_1 = T.Buffer((2160,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_2 = T.Buffer((2160,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_2 = T.Buffer((2160,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "acos",
                "acos"
            ]
        ],
        "input_shape": [[16, 15, 9], [15, 11, 6], [16, 15, 9]],
        "output_shape": [[16, 15, 9], [15, 11, 6], [16, 15, 9], [16, 15, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 30) + (i1 * 10)) + i2)] = cosf(ph_0[(((i0 * 30) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 150; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * (cosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 5; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n        compute_1[(((i0_1 * 30) + (i1_1 * 10)) + i2_1)] = expf(ph_0[(((i0_1 * 30) + (i1_1 * 10)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 3, 10), \"float32\"), compute: T.Buffer((5, 3, 10), \"float32\"), T_multiply: T.Buffer((5, 3, 10), \"float32\"), compute_1: T.Buffer((5, 3, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(3, 10):\n                cse_var_1: T.int32 = i0 * 30 + i1 * 10 + i2\n                compute_2 = T.Buffer((150,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(150):\n            T_multiply_1 = T.Buffer((150,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * (T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(3, 10):\n                cse_var_2: T.int32 = i0 * 30 + i1 * 10 + i2\n                compute_2 = T.Buffer((150,), data=compute_1.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "cos",
                "divide",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[5, 3, 10]],
        "output_shape": [[5, 3, 10], [5, 3, 10], [5, 3, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 255) + (i1 * 15)) + i2)] = sinf(ph_0[(((i0 * 255) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2295; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2295; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2295; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_subtract[(((ax0 * 255) + (ax1 * 15)) + ax2)] = (fmodf(ph_0[(((ax0 * 255) + (ax1 * 15)) + ax2)], ph_3[(((ax0 * 255) + (ax1 * 15)) + ax2)]) - ph_0[(((ax0 * 255) + (ax1 * 15)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(__cosf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 17, 15), \"float32\"), ph_3: T.Buffer((9, 17, 15), \"float32\"), compute: T.Buffer((9, 17, 15), \"float32\"), compute_1: T.Buffer((9, 17, 15), \"float32\"), compute_2: T.Buffer((9, 17, 15), \"float32\"), compute_3: T.Buffer((9, 17, 15), \"float32\"), T_subtract: T.Buffer((9, 17, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2295,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(17, 15):\n                cse_var_1: T.int32 = i0 * 255 + i1 * 15 + i2\n                compute_4 = T.Buffer((2295,), data=compute.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2295):\n            compute_4 = T.Buffer((2295,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2295):\n            compute_4 = T.Buffer((2295,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((2295,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(2295):\n            compute_4 = T.Buffer((2295,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(17, 15):\n                cse_var_2: T.int32 = ax0 * 255 + ax1 * 15 + ax2\n                T_subtract_1 = T.Buffer((2295,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "mod",
                "sin",
                "cos",
                "acos",
                "asinh",
                "asin",
                "subtract"
            ]
        ],
        "input_shape": [[9, 17, 15], [7, 12, 1], [9, 17, 15]],
        "output_shape": [[7, 12, 1], [9, 17, 15], [9, 17, 15], [9, 17, 15], [9, 17, 15], [9, 17, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* ph_0) {\n  float auto_scheduler_layout_transform[2880];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 16; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n      for (int32_t ax6 = 0; ax6 < 3; ++ax6) {\n        for (int32_t ax7 = 0; ax7 < 3; ++ax7) {\n          for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n            auto_scheduler_layout_transform[(((((ax0_ax1_fused_ax2_fused * 180) + (ax4 * 45)) + (ax6 * 15)) + (ax7 * 5)) + ax8)] = ceilf(ph_0[(((((((ax0_ax1_fused_ax2_fused >> 2) * 720) + (ax8 * 144)) + (ax4 * 36)) + (ax7 * 12)) + ((ax0_ax1_fused_ax2_fused & 3) * 3)) + ax6)]);\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 96; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 3; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n          T_batch_matmul_NN[(((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 24) * 720) + (b_inner_init * 144)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 6) * 24)) + (i_inner_init * 12)) + (((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 24) / 6) * 3)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 4; ++k_outer) {\n      for (int32_t j_outer_inner = 0; j_outer_inner < 3; ++j_outer_inner) {\n        for (int32_t k_inner = 0; k_inner < 3; ++k_inner) {\n          for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n            for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n              T_batch_matmul_NN[(((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 24) * 720) + (b_inner * 144)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 6) * 24)) + (i_inner * 12)) + (((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 24) / 6) * 3)) + j_outer_inner)] = (T_batch_matmul_NN[(((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 24) * 720) + (b_inner * 144)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 6) * 24)) + (i_inner * 12)) + (((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 24) / 6) * 3)) + j_outer_inner)] + (ph_0[(((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 24) * 720) + (b_inner * 144)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 6) * 24)) + (i_inner * 12)) + (k_outer * 3)) + k_inner)] * auto_scheduler_layout_transform[((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 6) * 180) + (k_outer * 45)) + (j_outer_inner * 15)) + (k_inner * 5)) + b_inner)]));\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[1];\n  __shared__ float ph_3_shared[50];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 4) + (((int)threadIdx.x) / 10)) < 5) {\n      ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 40) + ((int)threadIdx.x))] = ph_3[((((((int)blockIdx.x) >> 1) * 50) + (ax0_ax1_fused_ax2_fused_outer_outer * 40)) + ((int)threadIdx.x))];\n    }\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (ph_0[((((((((int)blockIdx.x) >> 1) * 80) + ((((int)threadIdx.x) / 20) * 40)) + ((((int)blockIdx.x) & 1) * 20)) + (((((int)threadIdx.x) % 20) / 5) * 5)) + k_outer_inner)] * ph_3_shared[((((((int)threadIdx.x) / 20) * 25) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5))]));\n  }\n  T_batch_matmul_NN[(((((((int)blockIdx.x) >> 1) * 80) + ((((int)threadIdx.x) / 20) * 40)) + ((((int)blockIdx.x) & 1) * 20)) + (((int)threadIdx.x) % 20))] = T_batch_matmul_NN_local[0];\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 12, 12), \"float32\"), T_batch_matmul_NN: T.Buffer((20, 12, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([2880], \"float32\", \"global\")\n        auto_scheduler_layout_transform_1 = T.Buffer((2880,), data=auto_scheduler_layout_transform)\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(16):\n            for ax4, ax6, ax7, ax8 in T.grid(4, 3, 3, 5):\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 180 + ax4 * 45 + ax6 * 15 + ax7 * 5 + ax8] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused // 4 * 720 + ax8 * 144 + ax4 * 36 + ax7 * 12 + ax0_ax1_fused_ax2_fused % 4 * 3 + ax6])\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(96):\n            T_batch_matmul_NN_1 = T.Buffer((2880,), data=T_batch_matmul_NN.data)\n            for j_outer_inner_init, b_inner_init, i_inner_init in T.grid(3, 5, 2):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 24 * 720 + b_inner_init * 144 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 6 * 24 + i_inner_init * 12 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 24 // 6 * 3 + j_outer_inner_init] = T.float32(0)\n            for k_outer, j_outer_inner, k_inner, b_inner, i_inner in T.grid(4, 3, 3, 5, 2):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 24 * 720 + b_inner * 144 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 6 * 24 + i_inner * 12\n                cse_var_1: T.int32 = cse_var_2 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 24 // 6 * 3 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[cse_var_2 + k_outer * 3 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 6 * 180 + k_outer * 45 + j_outer_inner * 15 + k_inner * 5 + b_inner]",
        "op_args": [
            [
                "ceil",
                "batch_matmul"
            ]
        ],
        "input_shape": [[20, 12, 12]],
        "output_shape": [[20, 12, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_divide_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_divide[(((ax0 * 90) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 90) + (ax1 * 9)) + ax2)] / ph_3[(((ax0 * 90) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1620; ++ax0_ax1_fused_ax2_fused) {\n    T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 90) + (i1 * 9)) + i2)] = fabsf(ph_0[(((i0 * 90) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 10, 9), \"float32\"), ph_3: T.Buffer((18, 10, 9), \"float32\"), T_divide: T.Buffer((18, 10, 9), \"float32\"), T_divide_1: T.Buffer((18, 10, 9), \"float32\"), compute: T.Buffer((18, 10, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1620,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1620,), data=ph_3.data)\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(10, 9):\n                cse_var_1: T.int32 = ax0 * 90 + ax1 * 9 + ax2\n                T_divide_2 = T.Buffer((1620,), data=T_divide.data)\n                T_divide_2[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1620):\n            T_divide_2 = T.Buffer((1620,), data=T_divide_1.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(10, 9):\n                cse_var_2: T.int32 = i0 * 90 + i1 * 9 + i2\n                compute_1 = T.Buffer((1620,), data=compute.data)\n                compute_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "divide",
                "abs"
            ]
        ],
        "input_shape": [[18, 10, 9], [6, 14, 9], [18, 10, 9]],
        "output_shape": [[18, 10, 9], [6, 14, 9], [18, 10, 9], [18, 10, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_mod[((ax0_ax1_fused * 5) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 5) + ax2)], (atanhf(ph_0[((ax0_ax1_fused * 5) + ax2)]) / ph_0[((ax0_ax1_fused * 5) + ax2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 60; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 4, 5), \"float32\"), compute: T.Buffer((3, 4, 5), \"float32\"), T_mod: T.Buffer((3, 4, 5), \"float32\"), compute_1: T.Buffer((3, 4, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((60,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_2 = T.Buffer((60,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_mod_1 = T.Buffer((60,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_2 = T.Buffer((60,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "atanh",
                "divide",
                "mod",
                "abs",
                "ceil"
            ]
        ],
        "input_shape": [[3, 4, 5]],
        "output_shape": [[3, 4, 5], [3, 4, 5], [3, 4, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 52; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_add[((ax0_ax1_fused * 9) + ax2)] = (ph_0[((ax0_ax1_fused * 9) + ax2)] + ph_3[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 468; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 52; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = expf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] + ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 4, 9), \"float32\"), ph_3: T.Buffer((13, 4, 9), \"float32\"), T_add: T.Buffer((13, 4, 9), \"float32\"), T_subtract: T.Buffer((13, 4, 9), \"float32\"), compute: T.Buffer((13, 4, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        ph_3_1 = T.Buffer((468,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(52):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_add_1 = T.Buffer((468,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(468):\n            T_subtract_1 = T.Buffer((468,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(52):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_1 = T.Buffer((468,), data=compute.data)\n                compute_1[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "subtract",
                "exp"
            ]
        ],
        "input_shape": [[13, 4, 9], [16, 16, 1], [13, 4, 9]],
        "output_shape": [[13, 4, 9], [16, 16, 1], [13, 4, 9], [13, 4, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 16) + (i1 * 2)) + i2)] = fabsf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_add[((ax0_ax1_fused * 2) + ax2)] = (acoshf(ph_0[((ax0_ax1_fused * 2) + ax2)]) + ph_0[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 256; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 256; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 8, 2), \"float32\"), compute: T.Buffer((16, 8, 2), \"float32\"), T_add: T.Buffer((16, 8, 2), \"float32\"), compute_1: T.Buffer((16, 8, 2), \"float32\"), compute_2: T.Buffer((16, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((256,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_3 = T.Buffer((256,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2 in range(2):\n                cse_var_2: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_add_1 = T.Buffer((256,), data=T_add.data)\n                T_add_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(256):\n            compute_3 = T.Buffer((256,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(256):\n            compute_3 = T.Buffer((256,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "acosh",
                "add",
                "exp",
                "sin",
                "acos"
            ]
        ],
        "input_shape": [[16, 8, 2]],
        "output_shape": [[16, 8, 2], [16, 8, 2], [16, 8, 2], [16, 8, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 80; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_1[((i0_i1_fused * 20) + i2)] = cosf(ph_0[((i0_i1_fused * 20) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 1, 20), \"float32\"), compute: T.Buffer((4, 1, 20), \"float32\"), T_multiply: T.Buffer((4, 1, 20), \"float32\"), compute_1: T.Buffer((4, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((80,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            compute_2 = T.Buffer((80,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(80):\n            T_multiply_1 = T.Buffer((80,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2\n                compute_2 = T.Buffer((80,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "asinh",
                "multiply",
                "cos"
            ]
        ],
        "input_shape": [[4, 1, 20]],
        "output_shape": [[4, 1, 20], [4, 1, 20], [4, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[1792];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1792; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1792; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_1[((i0_i1_fused * 14) + i2)] = atanhf(fmodf(ph_0[((i0_i1_fused * 14) + i2)], (ph_0[((i0_i1_fused * 14) + i2)] / compute_2[((i0_i1_fused * 14) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_mod[(((ax0 * 112) + (ax1 * 14)) + ax2)] = fmodf(fmodf(ph_0[(((ax0 * 112) + (ax1 * 14)) + ax2)], (ph_0[(((ax0 * 112) + (ax1 * 14)) + ax2)] / compute_2[(((ax0 * 112) + (ax1 * 14)) + ax2)])), ph_0[(((ax0 * 112) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / __expf(acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])))), ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / __expf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 8, 14), \"float32\"), compute: T.Buffer((16, 8, 14), \"float32\"), compute_1: T.Buffer((16, 8, 14), \"float32\"), T_mod: T.Buffer((16, 8, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([1792], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1792,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1792):\n            compute_3 = T.Buffer((1792,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        compute_3 = T.Buffer((1792,), data=compute_2)\n        for i0_i1_fused_i2_fused in T.parallel(1792):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(128):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_4 = T.Buffer((1792,), data=compute_1.data)\n                compute_4[cse_var_1] = T.atanh(T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] / compute_3[cse_var_1]))\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(8, 14):\n                cse_var_2: T.int32 = ax0 * 112 + ax1 * 14 + ax2\n                T_mod_1 = T.Buffer((1792,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.truncmod(ph_0_1[cse_var_2], ph_0_1[cse_var_2] / compute_3[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "acos",
                "exp",
                "divide",
                "mod",
                "atanh",
                "mod"
            ]
        ],
        "input_shape": [[16, 8, 14]],
        "output_shape": [[16, 8, 14], [16, 8, 14], [16, 8, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 16; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_mod[(((ax0 * 48) + (ax1 * 3)) + ax2)] = fmodf(ph_0[(((ax0 * 48) + (ax1 * 3)) + ax2)], ph_3[(((ax0 * 48) + (ax1 * 3)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 80; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 3; ++ax2_1) {\n      T_subtract[((ax0_ax1_fused * 3) + ax2_1)] = (ph_0[((ax0_ax1_fused * 3) + ax2_1)] - ph_3[((ax0_ax1_fused * 3) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 240; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 16, 3), \"float32\"), ph_3: T.Buffer((5, 16, 3), \"float32\"), T_mod: T.Buffer((5, 16, 3), \"float32\"), T_subtract: T.Buffer((5, 16, 3), \"float32\"), compute: T.Buffer((5, 16, 3), \"float32\"), compute_1: T.Buffer((5, 16, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        ph_3_1 = T.Buffer((240,), data=ph_3.data)\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(16, 3):\n                cse_var_1: T.int32 = ax0 * 48 + ax1 * 3 + ax2\n                T_mod_1 = T.Buffer((240,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(80):\n            for ax2 in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_subtract_1 = T.Buffer((240,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] - ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_2 = T.Buffer((240,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_2 = T.Buffer((240,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "subtract",
                "abs",
                "atan",
                "asin"
            ]
        ],
        "input_shape": [[5, 16, 3], [3, 20, 14], [5, 16, 3]],
        "output_shape": [[5, 16, 3], [3, 20, 14], [5, 16, 3], [5, 16, 3], [5, 16, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = fabsf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n      compute_1[((i0 * 3) + i2_1)] = fabsf(ph_0[((i0 * 3) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 1, 3), \"float32\"), compute: T.Buffer((18, 1, 3), \"float32\"), compute_1: T.Buffer((18, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((54,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((54,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0 in T.parallel(18):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0 * 3 + i2\n                compute_2 = T.Buffer((54,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "abs"
            ]
        ],
        "input_shape": [[18, 1, 3]],
        "output_shape": [[18, 1, 3], [18, 1, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 88; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = expf((ph_0[((i0_i1_fused * 3) + i2)] * asinhf(ph_0[((i0_i1_fused * 3) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 8, 3), \"float32\"), compute: T.Buffer((11, 8, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(88):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_1 = T.Buffer((264,), data=compute.data)\n                ph_0_1 = T.Buffer((264,), data=ph_0.data)\n                compute_1[cse_var_1] = T.exp(ph_0_1[cse_var_1] * T.asinh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asinh",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[11, 8, 3]],
        "output_shape": [[11, 8, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_mod[((ax0_ax1_fused * 11) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 11) + ax2)], ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = asinhf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 16, 11), \"float32\"), ph_3: T.Buffer((8, 16, 11), \"float32\"), T_mod: T.Buffer((8, 16, 11), \"float32\"), compute: T.Buffer((8, 16, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1408,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_mod_1 = T.Buffer((1408,), data=T_mod.data)\n                ph_3_1 = T.Buffer((1408,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused in T.parallel(128):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_1 = T.Buffer((1408,), data=compute.data)\n                compute_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "mod",
                "asinh"
            ]
        ],
        "input_shape": [[8, 16, 11], [12, 16, 9], [8, 16, 11]],
        "output_shape": [[8, 16, 11], [12, 16, 9], [8, 16, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 196; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 28; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_mod_1[((ax0_ax1_fused * 7) + ax2)] = fmodf((ph_0[((ax0_ax1_fused * 7) + ax2)] - ceilf(ph_0[((ax0_ax1_fused * 7) + ax2)])), ph_0[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 28; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 7; ++ax2_1) {\n      T_multiply[((ax0_ax1_fused_1 * 7) + ax2_1)] = ((ph_0[((ax0_ax1_fused_1 * 7) + ax2_1)] - ceilf(ph_0[((ax0_ax1_fused_1 * 7) + ax2_1)])) * ph_0[((ax0_ax1_fused_1 * 7) + ax2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 4, 7), \"float32\"), ph_3: T.Buffer((7, 4, 7), \"float32\"), T_mod: T.Buffer((7, 4, 7), \"float32\"), T_mod_1: T.Buffer((7, 4, 7), \"float32\"), T_multiply: T.Buffer((7, 4, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((196,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(196):\n            T_mod_2 = T.Buffer((196,), data=T_mod.data)\n            ph_3_1 = T.Buffer((196,), data=ph_3.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused in T.parallel(28):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_mod_2 = T.Buffer((196,), data=T_mod_1.data)\n                T_mod_2[cse_var_1] = T.truncmod(ph_0_1[cse_var_1] - T.ceil(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(28):\n            for ax2 in range(7):\n                cse_var_2: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_multiply_1 = T.Buffer((196,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = (ph_0_1[cse_var_2] - T.ceil(ph_0_1[cse_var_2])) * ph_0_1[cse_var_2]",
        "op_args": [
            [
                "mod",
                "ceil",
                "subtract",
                "mod",
                "multiply"
            ]
        ],
        "input_shape": [[7, 4, 7], [20, 15, 5], [7, 4, 7]],
        "output_shape": [[7, 4, 7], [20, 15, 5], [7, 4, 7], [7, 4, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_add_1, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3952; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3952; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3952; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 304; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_add_1[((ax0_ax1_fused * 13) + ax2)] = ((ph_0[((ax0_ax1_fused * 13) + ax2)] * ph_3[((ax0_ax1_fused * 13) + ax2)]) + ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 19, 13), \"float32\"), ph_3: T.Buffer((16, 19, 13), \"float32\"), compute: T.Buffer((16, 19, 13), \"float32\"), T_add: T.Buffer((16, 19, 13), \"float32\"), compute_1: T.Buffer((16, 19, 13), \"float32\"), T_add_1: T.Buffer((16, 19, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3952,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3952):\n            compute_2 = T.Buffer((3952,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3952):\n            T_add_2 = T.Buffer((3952,), data=T_add.data)\n            T_add_2[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3952):\n            compute_2 = T.Buffer((3952,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_add_2 = T.Buffer((3952,), data=T_add_1.data)\n                ph_3_1 = T.Buffer((3952,), data=ph_3.data)\n                T_add_2[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1] + ph_0_1[cse_var_1]",
        "op_args": [
            [
                "multiply",
                "ceil",
                "atan",
                "add",
                "atan",
                "add"
            ]
        ],
        "input_shape": [[16, 19, 13], [15, 20, 5], [16, 19, 13]],
        "output_shape": [[15, 20, 5], [16, 19, 13], [16, 19, 13], [16, 19, 13], [16, 19, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 156) + (i1 * 13)) + i2)] = atanhf(ph_0[(((i0 * 156) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1560; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 12, 13), \"float32\"), compute: T.Buffer((10, 12, 13), \"float32\"), compute_1: T.Buffer((10, 12, 13), \"float32\"), compute_2: T.Buffer((10, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1560,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(12, 13):\n                cse_var_1: T.int32 = i0 * 156 + i1 * 13 + i2\n                compute_3 = T.Buffer((1560,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_3 = T.Buffer((1560,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1560):\n            compute_3 = T.Buffer((1560,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "abs",
                "asinh",
                "ceil"
            ]
        ],
        "input_shape": [[10, 12, 13]],
        "output_shape": [[10, 12, 13], [10, 12, 13], [10, 12, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 140; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_add[((ax0_ax1_fused * 12) + ax2)] = (ph_0[((ax0_ax1_fused * 12) + ax2)] + ph_3[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1680; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_1[((i0_i1_fused * 12) + i2)] = asinf(cosf(ph_0[((i0_i1_fused * 12) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1680; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf(__cosf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 7, 12), \"float32\"), ph_3: T.Buffer((20, 7, 12), \"float32\"), T_add: T.Buffer((20, 7, 12), \"float32\"), compute: T.Buffer((20, 7, 12), \"float32\"), compute_1: T.Buffer((20, 7, 12), \"float32\"), compute_2: T.Buffer((20, 7, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1680,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(140):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_add_1 = T.Buffer((1680,), data=T_add.data)\n                ph_3_1 = T.Buffer((1680,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_3 = T.Buffer((1680,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(140):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((1680,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asin(T.cos(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_3 = T.Buffer((1680,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "asinh",
                "cos",
                "asin",
                "exp"
            ]
        ],
        "input_shape": [[20, 7, 12], [16, 7, 9], [20, 7, 12]],
        "output_shape": [[20, 7, 12], [16, 7, 9], [20, 7, 12], [20, 7, 12], [20, 7, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4032; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] - ceilf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 252) + (i1 * 14)) + i2)] = ceilf(ph_0[(((i0 * 252) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4032; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 18, 14), \"float32\"), T_mod: T.Buffer((16, 18, 14), \"float32\"), compute: T.Buffer((16, 18, 14), \"float32\"), compute_1: T.Buffer((16, 18, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4032,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4032):\n            T_mod_1 = T.Buffer((4032,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] - T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(18, 14):\n                cse_var_1: T.int32 = i0 * 252 + i1 * 14 + i2\n                compute_2 = T.Buffer((4032,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(4032):\n            compute_2 = T.Buffer((4032,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "subtract",
                "mod",
                "ceil",
                "exp"
            ]
        ],
        "input_shape": [[16, 18, 14]],
        "output_shape": [[16, 18, 14], [16, 18, 14], [16, 18, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1458; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1458; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1458; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 9, 18), \"float32\"), ph_3: T.Buffer((9, 9, 18), \"float32\"), T_subtract: T.Buffer((9, 9, 18), \"float32\"), T_add: T.Buffer((9, 9, 18), \"float32\"), compute: T.Buffer((9, 9, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1458,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1458,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1458):\n            T_subtract_1 = T.Buffer((1458,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1458):\n            T_add_1 = T.Buffer((1458,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1458):\n            compute_1 = T.Buffer((1458,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asinh(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "add",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[9, 9, 18], [17, 17, 7], [9, 9, 18]],
        "output_shape": [[9, 9, 18], [17, 17, 7], [9, 9, 18], [9, 9, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  float compute_1[1120];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1120; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 70) + (i1 * 10)) + i2)] = acoshf(fmodf(ph_0[(((i0 * 70) + (i1 * 10)) + i2)], compute_1[(((i0 * 70) + (i1 * 10)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_add[(((ax0 * 70) + (ax1 * 10)) + ax2)] = (fmodf(ph_0[(((ax0 * 70) + (ax1 * 10)) + ax2)], compute_1[(((ax0 * 70) + (ax1 * 10)) + ax2)]) + ph_0[(((ax0 * 70) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1120; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(fmodf(ph_0[((int)blockIdx.x)], __expf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 7, 10), \"float32\"), ph_3: T.Buffer((16, 7, 10), \"float32\"), compute: T.Buffer((16, 7, 10), \"float32\"), T_add: T.Buffer((16, 7, 10), \"float32\"), T_subtract: T.Buffer((16, 7, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([1120], \"float32\", \"global\")\n        compute_2 = T.Buffer((1120,), data=compute_1)\n        ph_0_1 = T.Buffer((1120,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1120):\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(7, 10):\n                cse_var_1: T.int32 = i0 * 70 + i1 * 10 + i2\n                compute_3 = T.Buffer((1120,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(T.truncmod(ph_0_1[cse_var_1], compute_2[cse_var_1]))\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(7, 10):\n                cse_var_2: T.int32 = ax0 * 70 + ax1 * 10 + ax2\n                T_add_1 = T.Buffer((1120,), data=T_add.data)\n                T_add_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], compute_2[cse_var_2]) + ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1120):\n            T_subtract_1 = T.Buffer((1120,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((1120,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "subtract",
                "exp",
                "mod",
                "acosh",
                "add",
                "subtract"
            ]
        ],
        "input_shape": [[16, 7, 10], [6, 15, 7], [16, 7, 10]],
        "output_shape": [[6, 15, 7], [16, 7, 10], [16, 7, 10], [16, 7, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 720; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = atanhf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 48; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 15) + i2_1)] = atanf(atanf(ph_0[((i0_i1_fused_1 * 15) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 15), \"float32\"), ph_3: T.Buffer((8, 6, 15), \"float32\"), T_divide: T.Buffer((8, 6, 15), \"float32\"), compute: T.Buffer((8, 6, 15), \"float32\"), compute_1: T.Buffer((8, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((720,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(720):\n            T_divide_1 = T.Buffer((720,), data=T_divide.data)\n            ph_3_1 = T.Buffer((720,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((720,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((720,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "atanh",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[8, 6, 15], [1, 10, 11], [8, 6, 15]],
        "output_shape": [[8, 6, 15], [1, 10, 11], [8, 6, 15], [8, 6, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4284; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 306; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_1[((i0_i1_fused * 14) + i2)] = asinf(fabsf(ph_0[((i0_i1_fused * 14) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        compute_2[(((i0 * 238) + (i1 * 14)) + i2_1)] = atanhf(ph_0[(((i0 * 238) + (i1 * 14)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 17, 14), \"float32\"), compute: T.Buffer((18, 17, 14), \"float32\"), compute_1: T.Buffer((18, 17, 14), \"float32\"), compute_2: T.Buffer((18, 17, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4284,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4284):\n            compute_3 = T.Buffer((4284,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(306):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((4284,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asin(T.fabs(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(17, 14):\n                cse_var_2: T.int32 = i0 * 238 + i1 * 14 + i2\n                compute_3 = T.Buffer((4284,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "ceil",
                "abs",
                "asin",
                "atanh"
            ]
        ],
        "input_shape": [[18, 17, 14]],
        "output_shape": [[18, 17, 14], [18, 17, 14], [18, 17, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 24; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 24; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 24; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] * ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute_1[(((i0 * 8) + (i1 * 2)) + i2)] = atanf(fmodf(ph_0[(((i0 * 8) + (i1 * 2)) + i2)], ph_3[(((i0 * 8) + (i1 * 2)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 24; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(fmodf(ph_0[i0_i1_fused_i2_fused_1], ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 4, 2), \"float32\"), ph_3: T.Buffer((3, 4, 2), \"float32\"), T_multiply: T.Buffer((3, 4, 2), \"float32\"), compute: T.Buffer((3, 4, 2), \"float32\"), T_add: T.Buffer((3, 4, 2), \"float32\"), compute_1: T.Buffer((3, 4, 2), \"float32\"), compute_2: T.Buffer((3, 4, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((24,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(24):\n            compute_3 = T.Buffer((24,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(24):\n            T_add_1 = T.Buffer((24,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        ph_3_1 = T.Buffer((24,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(24):\n            T_multiply_1 = T.Buffer((24,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(4, 2):\n                cse_var_1: T.int32 = i0 * 8 + i1 * 2 + i2\n                compute_3 = T.Buffer((24,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(24):\n            compute_3 = T.Buffer((24,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "multiply",
                "ceil",
                "cos",
                "add",
                "atan",
                "cos"
            ]
        ],
        "input_shape": [[3, 4, 2], [4, 15, 15], [3, 4, 2]],
        "output_shape": [[4, 15, 15], [3, 4, 2], [3, 4, 2], [3, 4, 2], [3, 4, 2], [3, 4, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 960; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 960; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_3[(((i0 * 48) + (i1 * 16)) + i2)] = acosf(asinhf(ph_0[(((i0 * 48) + (i1 * 16)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 3, 16), \"float32\"), compute: T.Buffer((20, 3, 16), \"float32\"), compute_1: T.Buffer((20, 3, 16), \"float32\"), compute_2: T.Buffer((20, 3, 16), \"float32\"), compute_3: T.Buffer((20, 3, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_4 = T.Buffer((960,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_4 = T.Buffer((960,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_4 = T.Buffer((960,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(3, 16):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 16 + i2\n                compute_4 = T.Buffer((960,), data=compute_3.data)\n                compute_4[cse_var_1] = T.acos(T.asinh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "atan",
                "atan",
                "atanh",
                "exp",
                "asinh",
                "acos"
            ]
        ],
        "input_shape": [[20, 3, 16]],
        "output_shape": [[20, 3, 16], [20, 3, 16], [20, 3, 16], [20, 3, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1224; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 72; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_divide[((ax0_ax1_fused * 17) + ax2)] = (sinf(ph_0[((ax0_ax1_fused * 17) + ax2)]) / ph_0[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1224; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1224; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 12, 17), \"float32\"), compute: T.Buffer((6, 12, 17), \"float32\"), T_divide: T.Buffer((6, 12, 17), \"float32\"), compute_1: T.Buffer((6, 12, 17), \"float32\"), compute_2: T.Buffer((6, 12, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1224,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1224):\n            compute_3 = T.Buffer((1224,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(72):\n            for ax2 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_divide_1 = T.Buffer((1224,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.sin(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1224):\n            compute_3 = T.Buffer((1224,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1224):\n            compute_3 = T.Buffer((1224,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asinh",
                "sin",
                "divide",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[6, 12, 17]],
        "output_shape": [[6, 12, 17], [6, 12, 17], [6, 12, 17], [6, 12, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf((ph_0[i0_i1_fused_i2_fused] * asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 80; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_1[((i0_i1_fused * 12) + i2)] = atanf((ph_0[((i0_i1_fused * 12) + i2)] * acoshf(ph_0[((i0_i1_fused * 12) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 960; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf((ph_0[i0_i1_fused_i2_fused_1] * acoshf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 10, 12), \"float32\"), compute: T.Buffer((8, 10, 12), \"float32\"), compute_1: T.Buffer((8, 10, 12), \"float32\"), compute_2: T.Buffer((8, 10, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_3 = T.Buffer((960,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] * T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(80):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((960,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1] * T.acosh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(960):\n            compute_3 = T.Buffer((960,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] * T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "multiply",
                "atan",
                "acosh",
                "multiply",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[8, 10, 12]],
        "output_shape": [[8, 10, 12], [8, 10, 12], [8, 10, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 143; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 7) + ax2)] = ((ph_0[((ax0_ax1_fused * 7) + ax2)] / atanhf(ph_0[((ax0_ax1_fused * 7) + ax2)])) - ph_0[((ax0_ax1_fused * 7) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1001; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] / atanhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1001; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 13, 7), \"float32\"), ph_3: T.Buffer((11, 13, 7), \"float32\"), T_subtract: T.Buffer((11, 13, 7), \"float32\"), compute: T.Buffer((11, 13, 7), \"float32\"), T_divide: T.Buffer((11, 13, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1001,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(143):\n            for ax2 in range(7):\n                cse_var_1: T.int32 = ax0_ax1_fused * 7 + ax2\n                T_subtract_1 = T.Buffer((1001,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] / T.atanh(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1001):\n            compute_1 = T.Buffer((1001,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] / T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1001):\n            T_divide_1 = T.Buffer((1001,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1001,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused] / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "multiply",
                "atanh",
                "divide",
                "subtract",
                "exp",
                "divide"
            ]
        ],
        "input_shape": [[11, 13, 7], [5, 1, 14], [11, 13, 7]],
        "output_shape": [[5, 1, 14], [11, 13, 7], [11, 13, 7], [11, 13, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 154; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = expf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 154; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 1, 14), \"float32\"), ph_3: T.Buffer((11, 1, 14), \"float32\"), T_add: T.Buffer((11, 1, 14), \"float32\"), compute: T.Buffer((11, 1, 14), \"float32\"), compute_1: T.Buffer((11, 1, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((154,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(154):\n            T_add_1 = T.Buffer((154,), data=T_add.data)\n            ph_3_1 = T.Buffer((154,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(11):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_2 = T.Buffer((154,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(154):\n            compute_2 = T.Buffer((154,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "exp",
                "abs"
            ]
        ],
        "input_shape": [[11, 1, 14], [10, 15, 5], [11, 1, 14]],
        "output_shape": [[11, 1, 14], [10, 15, 5], [11, 1, 14], [11, 1, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_mod[(((ax0 * 340) + (ax1 * 20)) + ax2)] = fmodf(ph_0[(((ax0 * 340) + (ax1 * 20)) + ax2)], ph_3[(((ax0 * 340) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 8; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 17; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n        T_mod_1[(((ax0_1 * 340) + (ax1_1 * 20)) + ax2_1)] = fmodf(ph_0[(((ax0_1 * 340) + (ax1_1 * 20)) + ax2_1)], ph_3[(((ax0_1 * 340) + (ax1_1 * 20)) + ax2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 17, 20), \"float32\"), ph_3: T.Buffer((8, 17, 20), \"float32\"), T_mod: T.Buffer((8, 17, 20), \"float32\"), T_mod_1: T.Buffer((8, 17, 20), \"float32\"), compute: T.Buffer((8, 17, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2720,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2720,), data=ph_3.data)\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(17, 20):\n                cse_var_1: T.int32 = ax0 * 340 + ax1 * 20 + ax2\n                T_mod_2 = T.Buffer((2720,), data=T_mod.data)\n                T_mod_2[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(17, 20):\n                cse_var_2: T.int32 = ax0 * 340 + ax1 * 20 + ax2\n                T_mod_2 = T.Buffer((2720,), data=T_mod_1.data)\n                T_mod_2[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2720):\n            compute_1 = T.Buffer((2720,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "mod",
                "mod",
                "exp"
            ]
        ],
        "input_shape": [[8, 17, 20], [10, 7, 6], [8, 17, 20]],
        "output_shape": [[8, 17, 20], [10, 7, 6], [8, 17, 20], [8, 17, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[504];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 504; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 56; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = atanhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    compute_3[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 504; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 504; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(__expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 8, 9), \"float32\"), ph_3: T.Buffer((7, 8, 9), \"float32\"), T_divide: T.Buffer((7, 8, 9), \"float32\"), compute: T.Buffer((7, 8, 9), \"float32\"), compute_1: T.Buffer((7, 8, 9), \"float32\"), compute_2: T.Buffer((7, 8, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([504], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((504,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(504):\n            T_divide_1 = T.Buffer((504,), data=T_divide.data)\n            ph_3_1 = T.Buffer((504,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(56):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_4 = T.Buffer((504,), data=compute.data)\n                compute_4[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        compute_4 = T.Buffer((504,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_5 = T.Buffer((504,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.cos(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_5 = T.Buffer((504,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(compute_4[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "atanh",
                "exp",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[7, 8, 9], [2, 11, 15], [7, 8, 9]],
        "output_shape": [[7, 8, 9], [2, 11, 15], [7, 8, 9], [7, 8, 9], [7, 8, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 608; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 608; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 608; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 608; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(fabsf(ph_0[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 8, 19), \"float32\"), compute: T.Buffer((4, 8, 19), \"float32\"), T_subtract: T.Buffer((4, 8, 19), \"float32\"), compute_1: T.Buffer((4, 8, 19), \"float32\"), T_mod: T.Buffer((4, 8, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((608,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(608):\n            compute_2 = T.Buffer((608,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(608):\n            T_subtract_1 = T.Buffer((608,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(608):\n            compute_2 = T.Buffer((608,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(608):\n            T_mod_1 = T.Buffer((608,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "exp",
                "ceil",
                "subtract",
                "sin",
                "abs",
                "mod"
            ]
        ],
        "input_shape": [[4, 8, 19]],
        "output_shape": [[4, 8, 19], [4, 8, 19], [4, 8, 19], [4, 8, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1680; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] / asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1680; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_2[(((i0 * 105) + (i1 * 15)) + i2)] = acosf(ph_0[(((i0 * 105) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 7, 15), \"float32\"), compute: T.Buffer((16, 7, 15), \"float32\"), compute_1: T.Buffer((16, 7, 15), \"float32\"), compute_2: T.Buffer((16, 7, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1680,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_3 = T.Buffer((1680,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] / T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1680):\n            compute_3 = T.Buffer((1680,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(7, 15):\n                cse_var_1: T.int32 = i0 * 105 + i1 * 15 + i2\n                compute_3 = T.Buffer((1680,), data=compute_2.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asinh",
                "divide",
                "ceil",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[16, 7, 15]],
        "output_shape": [[16, 7, 15], [16, 7, 15], [16, 7, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3800; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] / atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute_1[(((i0 * 380) + (i1 * 19)) + i2)] = acosf((ph_0[(((i0 * 380) + (i1 * 19)) + i2)] / atanf(ph_0[(((i0 * 380) + (i1 * 19)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_2[((i0_i1_fused * 19) + i2_1)] = asinf((ph_0[((i0_i1_fused * 19) + i2_1)] - ph_3[((i0_i1_fused * 19) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3800; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 20, 19), \"float32\"), ph_3: T.Buffer((10, 20, 19), \"float32\"), compute: T.Buffer((10, 20, 19), \"float32\"), compute_1: T.Buffer((10, 20, 19), \"float32\"), compute_2: T.Buffer((10, 20, 19), \"float32\"), T_divide: T.Buffer((10, 20, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3800,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3800):\n            compute_3 = T.Buffer((3800,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] / T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(20, 19):\n                cse_var_1: T.int32 = i0 * 380 + i1 * 19 + i2\n                compute_3 = T.Buffer((3800,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1] / T.atan(ph_0_1[cse_var_1]))\n        ph_3_1 = T.Buffer((3800,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(200):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((3800,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asin(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3800):\n            T_divide_1 = T.Buffer((3800,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "subtract",
                "atan",
                "divide",
                "cos",
                "acos",
                "asin",
                "divide"
            ]
        ],
        "input_shape": [[10, 20, 19], [10, 20, 8], [10, 20, 19]],
        "output_shape": [[10, 20, 8], [10, 20, 19], [10, 20, 19], [10, 20, 19], [10, 20, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_add[(((ax0 * 12) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 12) + (ax1 * 4)) + ax2)] + ph_3[(((ax0 * 12) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 9; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = expf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 36; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 3, 4), \"float32\"), ph_3: T.Buffer((3, 3, 4), \"float32\"), T_add: T.Buffer((3, 3, 4), \"float32\"), compute: T.Buffer((3, 3, 4), \"float32\"), compute_1: T.Buffer((3, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((36,), data=ph_0.data)\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(3, 4):\n                cse_var_1: T.int32 = ax0 * 12 + ax1 * 4 + ax2\n                T_add_1 = T.Buffer((36,), data=T_add.data)\n                ph_3_1 = T.Buffer((36,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(9):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_2 = T.Buffer((36,), data=compute.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_2 = T.Buffer((36,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "exp",
                "sin"
            ]
        ],
        "input_shape": [[3, 3, 4], [19, 7, 19], [3, 3, 4]],
        "output_shape": [[3, 3, 4], [19, 7, 19], [3, 3, 4], [3, 3, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 64; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (ph_0[ax0_ax1_fused_ax2_fused] * atanf(fmodf(ph_0[ax0_ax1_fused_ax2_fused], atanf(ph_0[ax0_ax1_fused_ax2_fused])))));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] * atanf(fmodf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))], atanf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))])))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 4, 2), \"float32\"), T_add: T.Buffer((8, 4, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(64):\n            T_add_1 = T.Buffer((64,), data=T_add.data)\n            ph_0_1 = T.Buffer((64,), data=ph_0.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused] * T.atan(T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.atan(ph_0_1[ax0_ax1_fused_ax2_fused])))",
        "op_args": [
            [
                "atan",
                "mod",
                "atan",
                "multiply",
                "add"
            ]
        ],
        "input_shape": [[8, 4, 2]],
        "output_shape": [[8, 4, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 156) + (i1 * 13)) + i2)] = asinf(fmodf(ph_0[(((i0 * 156) + (i1 * 13)) + i2)], acoshf(ph_0[(((i0 * 156) + (i1 * 13)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2496; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(fmodf(ph_0[i0_i1_fused_i2_fused], acoshf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        T_mod[(((ax0 * 156) + (ax1 * 13)) + ax2)] = fmodf((ph_0[(((ax0 * 156) + (ax1 * 13)) + ax2)] / ph_3[(((ax0 * 156) + (ax1 * 13)) + ax2)]), ph_0[(((ax0 * 156) + (ax1 * 13)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf(fmodf(ph_0[((int)blockIdx.x)], acoshf(ph_0[((int)blockIdx.x)])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 12, 13), \"float32\"), ph_3: T.Buffer((16, 12, 13), \"float32\"), compute: T.Buffer((16, 12, 13), \"float32\"), compute_1: T.Buffer((16, 12, 13), \"float32\"), T_mod: T.Buffer((16, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2496,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(12, 13):\n                cse_var_1: T.int32 = i0 * 156 + i1 * 13 + i2\n                compute_2 = T.Buffer((2496,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(T.truncmod(ph_0_1[cse_var_1], T.acosh(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(2496):\n            compute_2 = T.Buffer((2496,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.acosh(ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(12, 13):\n                cse_var_2: T.int32 = ax0 * 156 + ax1 * 13 + ax2\n                T_mod_1 = T.Buffer((2496,), data=T_mod.data)\n                ph_3_1 = T.Buffer((2496,), data=ph_3.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] / ph_3_1[cse_var_2], ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "acosh",
                "mod",
                "asin",
                "asinh",
                "mod"
            ]
        ],
        "input_shape": [[16, 12, 13], [8, 13, 4], [16, 12, 13]],
        "output_shape": [[8, 13, 4], [16, 12, 13], [16, 12, 13], [16, 12, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 819; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 91) + (i1 * 13)) + i2)] = ceilf(fabsf(ph_0[(((i0 * 91) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 7, 13), \"float32\"), compute: T.Buffer((9, 7, 13), \"float32\"), compute_1: T.Buffer((9, 7, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((819,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(819):\n            compute_2 = T.Buffer((819,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(7, 13):\n                cse_var_1: T.int32 = i0 * 91 + i1 * 13 + i2\n                compute_2 = T.Buffer((819,), data=compute_1.data)\n                compute_2[cse_var_1] = T.ceil(T.fabs(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asinh",
                "abs",
                "ceil"
            ]
        ],
        "input_shape": [[9, 7, 13]],
        "output_shape": [[9, 7, 13], [9, 7, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 960; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 960; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] - ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 48) + (i1 * 12)) + i2)] = fabsf(atanf(ph_0[(((i0 * 48) + (i1 * 12)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 4, 12), \"float32\"), ph_3: T.Buffer((20, 4, 12), \"float32\"), T_multiply: T.Buffer((20, 4, 12), \"float32\"), T_subtract: T.Buffer((20, 4, 12), \"float32\"), compute: T.Buffer((20, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((960,), data=ph_0.data)\n        ph_3_1 = T.Buffer((960,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            T_multiply_1 = T.Buffer((960,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(960):\n            T_subtract_1 = T.Buffer((960,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(4, 12):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 12 + i2\n                compute_1 = T.Buffer((960,), data=compute.data)\n                compute_1[cse_var_1] = T.fabs(T.atan(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "multiply",
                "subtract",
                "atan",
                "abs"
            ]
        ],
        "input_shape": [[20, 4, 12], [3, 12, 12], [20, 4, 12]],
        "output_shape": [[20, 4, 12], [3, 12, 12], [20, 4, 12], [20, 4, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 432; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 432; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_2[((i0_i1_fused * 9) + i2)] = atanhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_3[(((i0 * 144) + (i1 * 9)) + i2_1)] = sinf(acosf(ph_0[(((i0 * 144) + (i1 * 9)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 16, 9), \"float32\"), compute: T.Buffer((3, 16, 9), \"float32\"), compute_1: T.Buffer((3, 16, 9), \"float32\"), compute_2: T.Buffer((3, 16, 9), \"float32\"), compute_3: T.Buffer((3, 16, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((432,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(432):\n            compute_4 = T.Buffer((432,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(432):\n            compute_4 = T.Buffer((432,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_4 = T.Buffer((432,), data=compute_2.data)\n                compute_4[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(16, 9):\n                cse_var_2: T.int32 = i0 * 144 + i1 * 9 + i2\n                compute_4 = T.Buffer((432,), data=compute_3.data)\n                compute_4[cse_var_2] = T.sin(T.acos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "sin",
                "atan",
                "atanh",
                "acos",
                "sin"
            ]
        ],
        "input_shape": [[3, 16, 9]],
        "output_shape": [[3, 16, 9], [3, 16, 9], [3, 16, 9], [3, 16, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 170; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = ceilf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_mod[(((ax0 * 85) + (ax1 * 5)) + ax2)] = fmodf(atanhf(ph_0[(((ax0 * 85) + (ax1 * 5)) + ax2)]), ph_0[(((ax0 * 85) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 850; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 850; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(atanhf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 17, 5), \"float32\"), compute: T.Buffer((10, 17, 5), \"float32\"), T_mod: T.Buffer((10, 17, 5), \"float32\"), compute_1: T.Buffer((10, 17, 5), \"float32\"), compute_2: T.Buffer((10, 17, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((850,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(170):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((850,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(17, 5):\n                cse_var_2: T.int32 = ax0 * 85 + ax1 * 5 + ax2\n                T_mod_1 = T.Buffer((850,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.atanh(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(850):\n            compute_3 = T.Buffer((850,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(850):\n            compute_3 = T.Buffer((850,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "atanh",
                "mod",
                "atan",
                "exp"
            ]
        ],
        "input_shape": [[10, 17, 5]],
        "output_shape": [[10, 17, 5], [10, 17, 5], [10, 17, 5], [10, 17, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = sinf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_add[(((ax0 * 120) + (ax1 * 6)) + ax2)] = (ph_0[(((ax0 * 120) + (ax1 * 6)) + ax2)] + (asinhf(ph_0[(((ax0 * 120) + (ax1 * 6)) + ax2)]) * ph_0[(((ax0 * 120) + (ax1 * 6)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 840; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 20, 6), \"float32\"), compute: T.Buffer((7, 20, 6), \"float32\"), T_add: T.Buffer((7, 20, 6), \"float32\"), compute_1: T.Buffer((7, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((840,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(140):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((840,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(20, 6):\n                cse_var_2: T.int32 = ax0 * 120 + ax1 * 6 + ax2\n                T_add_1 = T.Buffer((840,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] + T.asinh(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            compute_2 = T.Buffer((840,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "asinh",
                "multiply",
                "add",
                "atan",
                "asinh"
            ]
        ],
        "input_shape": [[7, 20, 6]],
        "output_shape": [[7, 20, 6], [7, 20, 6], [7, 20, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 112; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = expf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_mod[(((ax0 * 28) + (ax1 * 2)) + ax2)] = fmodf(asinhf(ph_0[(((ax0 * 28) + (ax1 * 2)) + ax2)]), ph_0[(((ax0 * 28) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 224; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf((ph_0[i0_i1_fused_i2_fused] * asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf((ph_0[((int)blockIdx.x)] * asinhf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 14, 2), \"float32\"), compute: T.Buffer((8, 14, 2), \"float32\"), T_mod: T.Buffer((8, 14, 2), \"float32\"), compute_1: T.Buffer((8, 14, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((224,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(112):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((224,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(14, 2):\n                cse_var_2: T.int32 = ax0 * 28 + ax1 * 2 + ax2\n                T_mod_1 = T.Buffer((224,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.asinh(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(224):\n            compute_2 = T.Buffer((224,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] * T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "asinh",
                "mod",
                "asinh",
                "multiply",
                "acosh"
            ]
        ],
        "input_shape": [[8, 14, 2]],
        "output_shape": [[8, 14, 2], [8, 14, 2], [8, 14, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2565; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 285; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = asinhf((ph_0[((i0_i1_fused * 9) + i2)] * (ph_0[((i0_i1_fused * 9) + i2)] - ph_3[((i0_i1_fused * 9) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2565; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 15, 9), \"float32\"), ph_3: T.Buffer((19, 15, 9), \"float32\"), T_divide: T.Buffer((19, 15, 9), \"float32\"), compute: T.Buffer((19, 15, 9), \"float32\"), compute_1: T.Buffer((19, 15, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2565,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2565,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2565):\n            T_divide_1 = T.Buffer((2565,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(285):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((2565,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1] * (ph_0_1[cse_var_1] - ph_3_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2565):\n            compute_2 = T.Buffer((2565,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "subtract",
                "multiply",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[19, 15, 9], [19, 12, 17], [19, 15, 9]],
        "output_shape": [[19, 15, 9], [19, 12, 17], [19, 15, 9], [19, 15, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = ceilf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n        compute_1[(((i0 * 55) + (i1 * 11)) + i2_1)] = acosf(asinhf(ph_0[(((i0 * 55) + (i1 * 11)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 440; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 440; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 440; ++i0_i1_fused_i2_fused_2) {\n    compute_4[i0_i1_fused_i2_fused_2] = fabsf((ph_0[i0_i1_fused_i2_fused_2] + ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 5, 11), \"float32\"), ph_3: T.Buffer((8, 5, 11), \"float32\"), compute: T.Buffer((8, 5, 11), \"float32\"), compute_1: T.Buffer((8, 5, 11), \"float32\"), compute_2: T.Buffer((8, 5, 11), \"float32\"), compute_3: T.Buffer((8, 5, 11), \"float32\"), compute_4: T.Buffer((8, 5, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((440,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(40):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_5 = T.Buffer((440,), data=compute.data)\n                compute_5[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(5, 11):\n                cse_var_2: T.int32 = i0 * 55 + i1 * 11 + i2\n                compute_5 = T.Buffer((440,), data=compute_1.data)\n                compute_5[cse_var_2] = T.acos(T.asinh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(440):\n            compute_5 = T.Buffer((440,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((440,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(440):\n            compute_5 = T.Buffer((440,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(440):\n            compute_5 = T.Buffer((440,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "ceil",
                "asinh",
                "acos",
                "exp",
                "sin",
                "abs"
            ]
        ],
        "input_shape": [[8, 5, 11], [11, 16, 8], [8, 5, 11]],
        "output_shape": [[11, 16, 8], [8, 5, 11], [8, 5, 11], [8, 5, 11], [8, 5, 11], [8, 5, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 420; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 420; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] * ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 5; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_add[(((ax0 * 84) + (ax1 * 14)) + ax2)] = (atanhf(ph_0[(((ax0 * 84) + (ax1 * 14)) + ax2)]) + ph_0[(((ax0 * 84) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 6, 14), \"float32\"), ph_3: T.Buffer((5, 6, 14), \"float32\"), T_mod: T.Buffer((5, 6, 14), \"float32\"), T_multiply: T.Buffer((5, 6, 14), \"float32\"), T_add: T.Buffer((5, 6, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((420,), data=ph_0.data)\n        ph_3_1 = T.Buffer((420,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(420):\n            T_mod_1 = T.Buffer((420,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(420):\n            T_multiply_1 = T.Buffer((420,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(5):\n            for ax1, ax2 in T.grid(6, 14):\n                cse_var_1: T.int32 = ax0 * 84 + ax1 * 14 + ax2\n                T_add_1 = T.Buffer((420,), data=T_add.data)\n                T_add_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]",
        "op_args": [
            [
                "mod",
                "multiply",
                "atanh",
                "add"
            ]
        ],
        "input_shape": [[5, 6, 14], [4, 7, 6], [5, 6, 14]],
        "output_shape": [[5, 6, 14], [4, 7, 6], [5, 6, 14], [5, 6, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = acoshf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_divide[(((ax0 * 60) + (ax1 * 10)) + ax2)] = ((ph_0[(((ax0 * 60) + (ax1 * 10)) + ax2)] - acosf(ph_0[(((ax0 * 60) + (ax1 * 10)) + ax2)])) / ph_0[(((ax0 * 60) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 6, 10), \"float32\"), compute: T.Buffer((3, 6, 10), \"float32\"), compute_1: T.Buffer((3, 6, 10), \"float32\"), T_divide: T.Buffer((3, 6, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_2 = T.Buffer((180,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_2 = T.Buffer((180,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(6, 10):\n                cse_var_2: T.int32 = ax0 * 60 + ax1 * 10 + ax2\n                T_divide_1 = T.Buffer((180,), data=T_divide.data)\n                T_divide_1[cse_var_2] = (ph_0_1[cse_var_2] - T.acos(ph_0_1[cse_var_2])) / ph_0_1[cse_var_2]",
        "op_args": [
            [
                "acosh",
                "acos",
                "ceil",
                "acos",
                "subtract",
                "divide"
            ]
        ],
        "input_shape": [[3, 6, 10]],
        "output_shape": [[3, 6, 10], [3, 6, 10], [3, 6, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 560; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 70; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = sinf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 560; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = ceilf(atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 7, 8), \"float32\"), ph_3: T.Buffer((10, 7, 8), \"float32\"), T_divide: T.Buffer((10, 7, 8), \"float32\"), compute: T.Buffer((10, 7, 8), \"float32\"), compute_1: T.Buffer((10, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((560,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(560):\n            T_divide_1 = T.Buffer((560,), data=T_divide.data)\n            ph_3_1 = T.Buffer((560,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(70):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((560,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(560):\n            compute_2 = T.Buffer((560,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "sin",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[10, 7, 8], [14, 4, 16], [10, 7, 8]],
        "output_shape": [[10, 7, 8], [14, 4, 16], [10, 7, 8], [10, 7, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3971; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3971; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 361; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_1[((i0_i1_fused * 11) + i2)] = atanhf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n        compute_2[(((i0 * 209) + (i1 * 11)) + i2_1)] = atanhf(ph_0[(((i0 * 209) + (i1 * 11)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 19, 11), \"float32\"), compute: T.Buffer((19, 19, 11), \"float32\"), T_subtract: T.Buffer((19, 19, 11), \"float32\"), compute_1: T.Buffer((19, 19, 11), \"float32\"), compute_2: T.Buffer((19, 19, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3971,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3971):\n            compute_3 = T.Buffer((3971,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3971):\n            T_subtract_1 = T.Buffer((3971,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(361):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((3971,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(19, 11):\n                cse_var_2: T.int32 = i0 * 209 + i1 * 11 + i2\n                compute_3 = T.Buffer((3971,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atanh",
                "atan",
                "subtract",
                "atanh",
                "atanh"
            ]
        ],
        "input_shape": [[19, 19, 11]],
        "output_shape": [[19, 19, 11], [19, 19, 11], [19, 19, 11], [19, 19, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0 * 18) + i2)] = asinf(ph_0[((i0 * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 11; ++i0_1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[((i0_1 * 18) + i2_1)] = atanf(atanhf(ph_0[((i0_1 * 18) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 198; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 198; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = asinf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 198; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 1, 18), \"float32\"), compute: T.Buffer((11, 1, 18), \"float32\"), compute_1: T.Buffer((11, 1, 18), \"float32\"), compute_2: T.Buffer((11, 1, 18), \"float32\"), compute_3: T.Buffer((11, 1, 18), \"float32\"), T_subtract: T.Buffer((11, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((198,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0 * 18 + i2\n                compute_4 = T.Buffer((198,), data=compute.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(11):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0 * 18 + i2\n                compute_4 = T.Buffer((198,), data=compute_1.data)\n                compute_4[cse_var_2] = T.atan(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(198):\n            compute_4 = T.Buffer((198,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(198):\n            compute_4 = T.Buffer((198,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(198):\n            T_subtract_1 = T.Buffer((198,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asin",
                "atanh",
                "atan",
                "atan",
                "atan",
                "asin",
                "subtract"
            ]
        ],
        "input_shape": [[11, 1, 18]],
        "output_shape": [[11, 1, 18], [11, 1, 18], [11, 1, 18], [11, 1, 18], [11, 1, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 120; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_1[((i0_i1_fused * 3) + i2)] = expf(asinhf(ph_0[((i0_i1_fused * 3) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_multiply[(((ax0 * 6) + (ax1 * 3)) + ax2)] = (ph_0[(((ax0 * 6) + (ax1 * 3)) + ax2)] * acoshf(ph_0[(((ax0 * 6) + (ax1 * 3)) + ax2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 2, 3), \"float32\"), compute: T.Buffer((20, 2, 3), \"float32\"), compute_1: T.Buffer((20, 2, 3), \"float32\"), T_multiply: T.Buffer((20, 2, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((120,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(120):\n            compute_2 = T.Buffer((120,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(40):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((120,), data=compute_1.data)\n                compute_2[cse_var_1] = T.exp(T.asinh(ph_0_1[cse_var_1]))\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(2, 3):\n                cse_var_2: T.int32 = ax0 * 6 + ax1 * 3 + ax2\n                T_multiply_1 = T.Buffer((120,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] * T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "asinh",
                "exp",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[20, 2, 3]],
        "output_shape": [[20, 2, 3], [20, 2, 3], [20, 2, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_6) {\n  float auto_scheduler_layout_transform[640];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 512; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 512; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 16; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n      for (int32_t ax11 = 0; ax11 < 8; ++ax11) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused_1 * 40) + (ax3 * 8)) + ax11)] = ph_6[(((ax0_ax1_fused_ax2_fused_1 * 40) + (ax11 * 5)) + ax3)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused = 0; i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused < 160; ++i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused) {\n    float T_batch_matmul_NN[2];\n    for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n      T_batch_matmul_NN[i_inner_init] = 0.000000e+00f;\n    }\n    for (int32_t k_inner = 0; k_inner < 8; ++k_inner) {\n      for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n        T_batch_matmul_NN[i_inner] = (T_batch_matmul_NN[i_inner] + (ph_0[((((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 40) * 128) + (((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 20) / 5) * 32)) + (((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 40) / 20) * 16)) + (i_inner * 8)) + k_inner)] * auto_scheduler_layout_transform[((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 40) * 160) + ((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 20) * 8)) + k_inner)]));\n      }\n    }\n    for (int32_t i1_inner = 0; i1_inner < 2; ++i1_inner) {\n      compute_1[((((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 40) * 80) + (((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 20) / 5) * 20)) + (((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 40) / 20) * 10)) + (i1_inner * 5)) + (i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5))] = asinhf(T_batch_matmul_NN[i1_inner]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 512; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0) {\n  float T_batch_matmul_NN_local[32];\n  __shared__ float compute_shared[576];\n  for (int j_c_outer_inner_init = 0; j_c_outer_inner_init < 4; ++j_c_outer_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 4; ++i_c_inner_init) {\n      for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n        T_batch_matmul_NN_local[(((i_c_inner_init * 8) + (j_c_outer_inner_init * 2)) + j_c_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 32; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 18) + ((int)threadIdx.x))] = __sinf(ph_0[((ax0_ax1_fused_ax2_fused_outer_outer * 18) + ((int)threadIdx.x))]);\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 8; ++k_outer_inner) {\n    for (int j_c_outer_inner = 0; j_c_outer_inner < 4; ++j_c_outer_inner) {\n      for (int i_c_inner = 0; i_c_inner < 4; ++i_c_inner) {\n        for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n          T_batch_matmul_NN_local[(((i_c_inner * 8) + (j_c_outer_inner * 2)) + j_c_inner)] = (T_batch_matmul_NN_local[(((i_c_inner * 8) + (j_c_outer_inner * 2)) + j_c_inner)] + (ph_0[(((((int)threadIdx.x) * 32) + (i_c_inner * 8)) + k_outer_inner)] * compute_shared[(((((((int)threadIdx.x) >> 1) * 64) + (k_outer_inner * 8)) + (j_c_outer_inner * 2)) + j_c_inner)]));\n        }\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 4; ++i_inner) {\n    for (int j_inner = 0; j_inner < 8; ++j_inner) {\n      T_batch_matmul_NN[(((((int)threadIdx.x) * 32) + (i_inner * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ compute) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(T_batch_matmul_NN[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_4(float* __restrict__ T_batch_matmul_NN, float* __restrict__ compute) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(T_batch_matmul_NN[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 4, 8), \"float32\"), ph_6: T.Buffer((16, 8, 5), \"float32\"), T_add: T.Buffer((16, 4, 8), \"float32\"), compute: T.Buffer((16, 4, 8), \"float32\"), compute_1: T.Buffer((16, 4, 5), \"float32\"), compute_2: T.Buffer((16, 4, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([640], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((512,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(512):\n            T_add_1 = T.Buffer((512,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(512):\n            compute_3 = T.Buffer((512,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((640,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(16):\n            for ax3, ax11 in T.grid(5, 8):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 40\n                ph_6_1 = T.Buffer((640,), data=ph_6.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax3 * 8 + ax11] = ph_6_1[cse_var_1 + ax11 * 5 + ax3]\n        for i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(160):\n            T_batch_matmul_NN = T.allocate([2], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((2,), data=T_batch_matmul_NN, align=8)\n            for i_inner_init in range(2):\n                T_batch_matmul_NN_1[i_inner_init] = T.float32(0)\n            for k_inner, i_inner in T.grid(8, 2):\n                cse_var_3: T.int32 = i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 20\n                cse_var_2: T.int32 = i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 40\n                T_batch_matmul_NN_1[i_inner] = T_batch_matmul_NN_1[i_inner] + ph_0_1[cse_var_2 * 128 + cse_var_3 // 5 * 32 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 40 // 20 * 16 + i_inner * 8 + k_inner] * auto_scheduler_layout_transform_1[cse_var_2 * 160 + cse_var_3 * 8 + k_inner]\n            for i1_inner in range(2):\n                compute_3 = T.Buffer((320,), data=compute_1.data)\n                compute_3[i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 40 * 80 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 20 // 5 * 20 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 40 // 20 * 10 + i1_inner * 5 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 5] = T.asinh(T_batch_matmul_NN_1[i1_inner])\n        for i0_i1_fused_i2_fused in T.parallel(512):\n            compute_3 = T.Buffer((512,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "ceil",
                "add",
                "sin",
                "batch_matmul",
                "asinh",
                "abs"
            ]
        ],
        "input_shape": [[16, 4, 8], [16, 8, 5]],
        "output_shape": [[16, 4, 8], [16, 4, 8], [16, 4, 5], [16, 4, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[1254];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1254; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 66; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_2[((i0_i1_fused * 19) + i2)] = expf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 66; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_add[((ax0_ax1_fused * 19) + ax2)] = (ph_0[((ax0_ax1_fused * 19) + ax2)] + asinf(compute_2[((ax0_ax1_fused * 19) + ax2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1254; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + asinf(__expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 11, 19), \"float32\"), compute: T.Buffer((6, 11, 19), \"float32\"), T_add: T.Buffer((6, 11, 19), \"float32\"), compute_1: T.Buffer((6, 11, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([1254], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1254,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1254):\n            compute_3 = T.Buffer((1254,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        compute_3 = T.Buffer((1254,), data=compute_2)\n        for i0_i1_fused in T.parallel(66):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(66):\n            for ax2 in range(19):\n                cse_var_2: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_add_1 = T.Buffer((1254,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] + T.asin(compute_3[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1254):\n            compute_4 = T.Buffer((1254,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "exp",
                "asin",
                "add",
                "acos",
                "sin"
            ]
        ],
        "input_shape": [[6, 11, 19]],
        "output_shape": [[6, 11, 19], [6, 11, 19], [6, 11, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 340; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute_1[(((i0 * 20) + (i1 * 10)) + i2)] = cosf(acoshf(ph_0[(((i0 * 20) + (i1 * 10)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 340; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + sinf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 2, 10), \"float32\"), compute: T.Buffer((17, 2, 10), \"float32\"), compute_1: T.Buffer((17, 2, 10), \"float32\"), T_add: T.Buffer((17, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((340,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(340):\n            compute_2 = T.Buffer((340,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(2, 10):\n                cse_var_1: T.int32 = i0 * 20 + i1 * 10 + i2\n                compute_2 = T.Buffer((340,), data=compute_1.data)\n                compute_2[cse_var_1] = T.cos(T.acosh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(340):\n            T_add_1 = T.Buffer((340,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.sin(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "asinh",
                "acosh",
                "cos",
                "sin",
                "add"
            ]
        ],
        "input_shape": [[17, 2, 10]],
        "output_shape": [[17, 2, 10], [17, 2, 10], [17, 2, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    float compute_2[1];\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        compute_2[0] = expf(sinf(ph_0[(((ax0 * 36) + (ax1 * 12)) + ax2)]));\n        T_subtract[(((ax0 * 36) + (ax1 * 12)) + ax2)] = (ph_0[(((ax0 * 36) + (ax1 * 12)) + ax2)] - compute_2[0]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 576; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acosf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] - __expf(__sinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 3, 12), \"float32\"), compute: T.Buffer((16, 3, 12), \"float32\"), T_subtract: T.Buffer((16, 3, 12), \"float32\"), compute_1: T.Buffer((16, 3, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((576,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_2 = T.Buffer((576,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(16):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(3, 12):\n                cse_var_1: T.int32 = ax0 * 36 + ax1 * 12 + ax2\n                compute_3 = T.Buffer((1,), data=compute_2, align=4)\n                compute_3[0] = T.exp(T.sin(ph_0_1[cse_var_1]))\n                T_subtract_1 = T.Buffer((576,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - compute_3[0]\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_2 = T.Buffer((576,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acos",
                "sin",
                "exp",
                "subtract",
                "sin",
                "sin"
            ]
        ],
        "input_shape": [[16, 3, 12]],
        "output_shape": [[16, 3, 12], [16, 3, 12], [16, 3, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 160; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      T_subtract[((ax0 * 8) + ax1)] = (ph_0[((ax0 * 8) + ax1)] - asinf(ph_0[((ax0 * 8) + ax1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf(ceilf(ph_0[((int)blockIdx.x)]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 8, 1), \"float32\"), compute: T.Buffer((20, 8, 1), \"float32\"), compute_1: T.Buffer((20, 8, 1), \"float32\"), T_subtract: T.Buffer((20, 8, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((160,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            compute_2 = T.Buffer((160,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(160):\n            compute_2 = T.Buffer((160,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(20):\n            for ax1 in range(8):\n                cse_var_1: T.int32 = ax0 * 8 + ax1\n                T_subtract_1 = T.Buffer((160,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "cos",
                "ceil",
                "cos",
                "asin",
                "subtract"
            ]
        ],
        "input_shape": [[20, 8, 1]],
        "output_shape": [[20, 8, 1], [20, 8, 1], [20, 8, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2280; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], asinf(ph_0[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 120) + (i1 * 6)) + i2)] = atanf(ph_0[(((i0 * 120) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2280; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 20, 6), \"float32\"), T_subtract: T.Buffer((19, 20, 6), \"float32\"), compute: T.Buffer((19, 20, 6), \"float32\"), compute_1: T.Buffer((19, 20, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2280,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2280):\n            T_subtract_1 = T.Buffer((2280,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(20, 6):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 6 + i2\n                compute_2 = T.Buffer((2280,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2280):\n            compute_2 = T.Buffer((2280,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asin",
                "mod",
                "subtract",
                "atan",
                "ceil",
                "atanh"
            ]
        ],
        "input_shape": [[19, 20, 6]],
        "output_shape": [[19, 20, 6], [19, 20, 6], [19, 20, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = cosf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2880; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2880; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2880; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 20, 12), \"float32\"), compute: T.Buffer((12, 20, 12), \"float32\"), compute_1: T.Buffer((12, 20, 12), \"float32\"), compute_2: T.Buffer((12, 20, 12), \"float32\"), T_multiply: T.Buffer((12, 20, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((2880,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_3 = T.Buffer((2880,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_3 = T.Buffer((2880,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_multiply_1 = T.Buffer((2880,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "cos",
                "asinh",
                "acosh",
                "acosh",
                "asinh",
                "multiply"
            ]
        ],
        "input_shape": [[12, 20, 12]],
        "output_shape": [[12, 20, 12], [12, 20, 12], [12, 20, 12], [12, 20, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 135; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    compute[i0_i1_fused] = ceilf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 135; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_1] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused_1]) / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      T_mod[((ax0 * 9) + ax1)] = fmodf(atanhf(ph_0[((ax0 * 9) + ax1)]), ph_0[((ax0 * 9) + ax1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 1), \"float32\"), ph_3: T.Buffer((15, 9, 1), \"float32\"), T_add: T.Buffer((15, 9, 1), \"float32\"), compute: T.Buffer((15, 9, 1), \"float32\"), T_divide: T.Buffer((15, 9, 1), \"float32\"), T_mod: T.Buffer((15, 9, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((135,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(135):\n            T_add_1 = T.Buffer((135,), data=T_add.data)\n            ph_3_1 = T.Buffer((135,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(135):\n            compute_1 = T.Buffer((135,), data=compute.data)\n            compute_1[i0_i1_fused] = T.ceil(ph_0_1[i0_i1_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(135):\n            T_divide_1 = T.Buffer((135,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(15):\n            for ax1 in range(9):\n                cse_var_1: T.int32 = ax0 * 9 + ax1\n                T_mod_1 = T.Buffer((135,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.atanh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])",
        "op_args": [
            [
                "add",
                "ceil",
                "atanh",
                "divide",
                "mod"
            ]
        ],
        "input_shape": [[15, 9, 1], [18, 14, 19], [15, 9, 1]],
        "output_shape": [[15, 9, 1], [18, 14, 19], [15, 9, 1], [15, 9, 1], [15, 9, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 100) + (i1 * 20)) + i2)] = expf(ph_0[(((i0 * 100) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 60; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_mod[((ax0_ax1_fused * 20) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 20) + ax2)], acoshf(ceilf(ph_0[((ax0_ax1_fused * 20) + ax2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], acoshf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 5, 20), \"float32\"), compute: T.Buffer((12, 5, 20), \"float32\"), T_mod: T.Buffer((12, 5, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1200,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(5, 20):\n                cse_var_1: T.int32 = i0 * 100 + i1 * 20 + i2\n                compute_1 = T.Buffer((1200,), data=compute.data)\n                compute_1[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(60):\n            for ax2 in range(20):\n                cse_var_2: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_mod_1 = T.Buffer((1200,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], T.acosh(T.ceil(ph_0_1[cse_var_2])))",
        "op_args": [
            [
                "exp",
                "ceil",
                "acosh",
                "mod"
            ]
        ],
        "input_shape": [[12, 5, 20]],
        "output_shape": [[12, 5, 20], [12, 5, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 220; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 220; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 20) + (i1 * 4)) + i2)] = atanhf(ph_0[(((i0 * 20) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 220; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 5, 4), \"float32\"), compute: T.Buffer((11, 5, 4), \"float32\"), T_multiply: T.Buffer((11, 5, 4), \"float32\"), compute_1: T.Buffer((11, 5, 4), \"float32\"), T_add: T.Buffer((11, 5, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((220,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(220):\n            compute_2 = T.Buffer((220,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(220):\n            T_multiply_1 = T.Buffer((220,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(5, 4):\n                cse_var_1: T.int32 = i0 * 20 + i1 * 4 + i2\n                compute_2 = T.Buffer((220,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(220):\n            T_add_1 = T.Buffer((220,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "ceil",
                "acosh",
                "multiply",
                "atanh",
                "atanh",
                "add"
            ]
        ],
        "input_shape": [[11, 5, 4]],
        "output_shape": [[11, 5, 4], [11, 5, 4], [11, 5, 4], [11, 5, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 133; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 19) + ax2)] = (ph_0[((ax0_ax1_fused * 19) + ax2)] - ph_3[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 361) + (i1 * 19)) + i2)] = acoshf(ph_0[(((i0 * 361) + (i1 * 19)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 19, 19), \"float32\"), ph_3: T.Buffer((7, 19, 19), \"float32\"), T_subtract: T.Buffer((7, 19, 19), \"float32\"), compute: T.Buffer((7, 19, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2527,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(133):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_subtract_1 = T.Buffer((2527,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((2527,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(19, 19):\n                cse_var_2: T.int32 = i0 * 361 + i1 * 19 + i2\n                compute_1 = T.Buffer((2527,), data=compute.data)\n                compute_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "subtract",
                "acosh"
            ]
        ],
        "input_shape": [[7, 19, 19], [17, 1, 16], [7, 19, 19]],
        "output_shape": [[7, 19, 19], [17, 1, 16], [7, 19, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 98; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = acosf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 98; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_add[((ax0_ax1_fused * 8) + ax2)] = (ceilf(ph_0[((ax0_ax1_fused * 8) + ax2)]) + ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 7, 8), \"float32\"), compute: T.Buffer((14, 7, 8), \"float32\"), T_add: T.Buffer((14, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((784,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(98):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_1 = T.Buffer((784,), data=compute.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(98):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_add_1 = T.Buffer((784,), data=T_add.data)\n                T_add_1[cse_var_2] = T.ceil(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "acos",
                "ceil",
                "add"
            ]
        ],
        "input_shape": [[14, 7, 8]],
        "output_shape": [[14, 7, 8], [14, 7, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_subtract[(((ax0 * 126) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 126) + (ax1 * 9)) + ax2)] - ph_3[(((ax0 * 126) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 126) + (i1 * 9)) + i2)] = cosf((ph_0[(((i0 * 126) + (i1 * 9)) + i2)] - fmodf(ph_0[(((i0 * 126) + (i1 * 9)) + i2)], ph_3[(((i0 * 126) + (i1 * 9)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 14, 9), \"float32\"), ph_3: T.Buffer((10, 14, 9), \"float32\"), T_subtract: T.Buffer((10, 14, 9), \"float32\"), compute: T.Buffer((10, 14, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1260,), data=ph_3.data)\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(14, 9):\n                cse_var_1: T.int32 = ax0 * 126 + ax1 * 9 + ax2\n                T_subtract_1 = T.Buffer((1260,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(14, 9):\n                cse_var_2: T.int32 = i0 * 126 + i1 * 9 + i2\n                compute_1 = T.Buffer((1260,), data=compute.data)\n                compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2] - T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "subtract",
                "mod",
                "subtract",
                "cos"
            ]
        ],
        "input_shape": [[10, 14, 9], [7, 17, 8], [10, 14, 9]],
        "output_shape": [[10, 14, 9], [7, 17, 8], [10, 14, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 270; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 270; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_divide[((ax0_ax1_fused * 15) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 15) + ax2)]) / ph_0[((ax0_ax1_fused * 15) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 270; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 6, 15), \"float32\"), ph_3: T.Buffer((3, 6, 15), \"float32\"), T_mod: T.Buffer((3, 6, 15), \"float32\"), compute: T.Buffer((3, 6, 15), \"float32\"), T_divide: T.Buffer((3, 6, 15), \"float32\"), compute_1: T.Buffer((3, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((270,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(270):\n            T_mod_1 = T.Buffer((270,), data=T_mod.data)\n            ph_3_1 = T.Buffer((270,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(270):\n            compute_2 = T.Buffer((270,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_divide_1 = T.Buffer((270,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(270):\n            compute_2 = T.Buffer((270,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "sin",
                "asin",
                "divide",
                "asinh"
            ]
        ],
        "input_shape": [[3, 6, 15], [6, 1, 18], [3, 6, 15]],
        "output_shape": [[3, 6, 15], [6, 1, 18], [3, 6, 15], [3, 6, 15], [3, 6, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 108; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] - ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 12, 8), \"float32\"), ph_3: T.Buffer((9, 12, 8), \"float32\"), T_subtract: T.Buffer((9, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused in T.parallel(108):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_subtract_1 = T.Buffer((864,), data=T_subtract.data)\n                ph_0_1 = T.Buffer((864,), data=ph_0.data)\n                ph_3_1 = T.Buffer((864,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]",
        "op_args": [
            [
                "subtract"
            ]
        ],
        "input_shape": [[9, 12, 8], [20, 12, 1], [9, 12, 8]],
        "output_shape": [[9, 12, 8], [20, 12, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 864; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 864; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_1[(((i0 * 48) + (i1 * 16)) + i2)] = asinf(ph_0[(((i0 * 48) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 3, 16), \"float32\"), compute: T.Buffer((18, 3, 16), \"float32\"), T_multiply: T.Buffer((18, 3, 16), \"float32\"), compute_1: T.Buffer((18, 3, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((864,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(864):\n            compute_2 = T.Buffer((864,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(864):\n            T_multiply_1 = T.Buffer((864,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(3, 16):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 16 + i2\n                compute_2 = T.Buffer((864,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acosh",
                "atan",
                "multiply",
                "asin"
            ]
        ],
        "input_shape": [[18, 3, 16]],
        "output_shape": [[18, 3, 16], [18, 3, 16], [18, 3, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 176) + (i1 * 16)) + i2)] = acoshf(ph_0[(((i0 * 176) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n        compute_1[(((i0_1 * 176) + (i1_1 * 16)) + i2_1)] = asinf(atanf(ph_0[(((i0_1 * 176) + (i1_1 * 16)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinf(atanf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 11, 16), \"float32\"), compute: T.Buffer((16, 11, 16), \"float32\"), compute_1: T.Buffer((16, 11, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2816,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(11, 16):\n                cse_var_1: T.int32 = i0 * 176 + i1 * 16 + i2\n                compute_2 = T.Buffer((2816,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(11, 16):\n                cse_var_2: T.int32 = i0 * 176 + i1 * 16 + i2\n                compute_2 = T.Buffer((2816,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acosh",
                "atan",
                "asin"
            ]
        ],
        "input_shape": [[16, 11, 16]],
        "output_shape": [[16, 11, 16], [16, 11, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 360; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 180; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_divide[((ax0_ax1_fused * 2) + ax2)] = (ph_0[((ax0_ax1_fused * 2) + ax2)] / ph_3[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 360; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_2[((i0_i1_fused * 2) + i2)] = asinhf((ph_0[((i0_i1_fused * 2) + i2)] + ph_3[((i0_i1_fused * 2) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 18, 2), \"float32\"), ph_3: T.Buffer((10, 18, 2), \"float32\"), T_divide: T.Buffer((10, 18, 2), \"float32\"), T_multiply: T.Buffer((10, 18, 2), \"float32\"), compute: T.Buffer((10, 18, 2), \"float32\"), compute_1: T.Buffer((10, 18, 2), \"float32\"), compute_2: T.Buffer((10, 18, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((360,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(360):\n            T_multiply_1 = T.Buffer((360,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((360,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(180):\n            for ax2 in range(2):\n                cse_var_1: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_divide_1 = T.Buffer((360,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(360):\n            compute_3 = T.Buffer((360,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((360,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(ph_0_1[cse_var_2] + ph_3_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "divide",
                "atan",
                "multiply",
                "acos",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[10, 18, 2], [1, 6, 15], [10, 18, 2]],
        "output_shape": [[1, 6, 15], [10, 18, 2], [10, 18, 2], [10, 18, 2], [10, 18, 2], [10, 18, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 390; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_multiply[(((ax0 * 65) + (ax1 * 5)) + ax2)] = (acoshf(ph_0[(((ax0 * 65) + (ax1 * 5)) + ax2)]) * ph_0[(((ax0 * 65) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute_1[(((i0 * 65) + (i1 * 5)) + i2)] = acosf(ph_0[(((i0 * 65) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 390; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 13, 5), \"float32\"), compute: T.Buffer((6, 13, 5), \"float32\"), T_multiply: T.Buffer((6, 13, 5), \"float32\"), compute_1: T.Buffer((6, 13, 5), \"float32\"), compute_2: T.Buffer((6, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((390,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(390):\n            compute_3 = T.Buffer((390,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(13, 5):\n                cse_var_1: T.int32 = ax0 * 65 + ax1 * 5 + ax2\n                T_multiply_1 = T.Buffer((390,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(13, 5):\n                cse_var_2: T.int32 = i0 * 65 + i1 * 5 + i2\n                compute_3 = T.Buffer((390,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(390):\n            compute_3 = T.Buffer((390,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "acosh",
                "multiply",
                "acos",
                "acosh"
            ]
        ],
        "input_shape": [[6, 13, 5]],
        "output_shape": [[6, 13, 5], [6, 13, 5], [6, 13, 5], [6, 13, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 54; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = atanf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 54; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 18) + i2_1)] = atanhf(sinf(ph_0[((i0_i1_fused_1 * 18) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 972; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 9, 18), \"float32\"), compute: T.Buffer((6, 9, 18), \"float32\"), compute_1: T.Buffer((6, 9, 18), \"float32\"), compute_2: T.Buffer((6, 9, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((972,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(54):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((972,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(54):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((972,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atanh(T.sin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(972):\n            compute_3 = T.Buffer((972,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "sin",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[6, 9, 18]],
        "output_shape": [[6, 9, 18], [6, 9, 18], [6, 9, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 77; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 77; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_2[((i0_i1_fused * 11) + i2)] = sinf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 77; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 11), \"float32\"), compute: T.Buffer((1, 7, 11), \"float32\"), compute_1: T.Buffer((1, 7, 11), \"float32\"), compute_2: T.Buffer((1, 7, 11), \"float32\"), T_divide: T.Buffer((1, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((77,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(77):\n            compute_3 = T.Buffer((77,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(77):\n            compute_3 = T.Buffer((77,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(7):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((77,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(77):\n            T_divide_1 = T.Buffer((77,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "cos",
                "acosh",
                "ceil",
                "sin",
                "abs",
                "divide"
            ]
        ],
        "input_shape": [[1, 7, 11]],
        "output_shape": [[1, 7, 11], [1, 7, 11], [1, 7, 11], [1, 7, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 16; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf((ph_0[i0_i1_fused_i2_fused] / acosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute_1[((i0 * 2) + i1)] = acosf(ph_0[((i0 * 2) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    compute_2[i0_i1_fused] = atanf(ph_0[i0_i1_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] / acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 2, 1), \"float32\"), compute: T.Buffer((8, 2, 1), \"float32\"), compute_1: T.Buffer((8, 2, 1), \"float32\"), compute_2: T.Buffer((8, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((16,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(16):\n            compute_3 = T.Buffer((16,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] / T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(8):\n            for i1 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i1\n                compute_3 = T.Buffer((16,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(16):\n            compute_3 = T.Buffer((16,), data=compute_2.data)\n            compute_3[i0_i1_fused] = T.atan(ph_0_1[i0_i1_fused])",
        "op_args": [
            [
                "acos",
                "divide",
                "atan",
                "acos",
                "atan"
            ]
        ],
        "input_shape": [[8, 2, 1]],
        "output_shape": [[8, 2, 1], [8, 2, 1], [8, 2, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 42; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = asinf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 714; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n        compute_1[(((i0 * 119) + (i1 * 17)) + i2_1)] = atanf(cosf(ph_0[(((i0 * 119) + (i1 * 17)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 714; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 7, 17), \"float32\"), compute: T.Buffer((6, 7, 17), \"float32\"), T_subtract: T.Buffer((6, 7, 17), \"float32\"), compute_1: T.Buffer((6, 7, 17), \"float32\"), T_multiply: T.Buffer((6, 7, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((714,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(42):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_2 = T.Buffer((714,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(714):\n            T_subtract_1 = T.Buffer((714,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(7, 17):\n                cse_var_2: T.int32 = i0 * 119 + i1 * 17 + i2\n                compute_2 = T.Buffer((714,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(T.cos(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(714):\n            T_multiply_1 = T.Buffer((714,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asin",
                "atanh",
                "multiply",
                "subtract",
                "cos",
                "atan",
                "multiply"
            ]
        ],
        "input_shape": [[6, 7, 17]],
        "output_shape": [[6, 7, 17], [6, 7, 17], [6, 7, 17], [6, 7, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_multiply[((ax0 * 20) + ax2)] = (ph_0[((ax0 * 20) + ax2)] * ph_3[((ax0 * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i0 * 20) + i2)] = asinf(ph_0[((i0 * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 120; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 1, 20), \"float32\"), ph_3: T.Buffer((6, 1, 20), \"float32\"), T_multiply: T.Buffer((6, 1, 20), \"float32\"), compute: T.Buffer((6, 1, 20), \"float32\"), compute_1: T.Buffer((6, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((120,), data=ph_0.data)\n        for ax0 in T.parallel(6):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0 * 20 + ax2\n                T_multiply_1 = T.Buffer((120,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((120,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0 in T.parallel(6):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0 * 20 + i2\n                compute_2 = T.Buffer((120,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(120):\n            compute_2 = T.Buffer((120,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "asin",
                "acos",
                "cos"
            ]
        ],
        "input_shape": [[6, 1, 20], [10, 10, 19], [6, 1, 20]],
        "output_shape": [[6, 1, 20], [10, 10, 19], [6, 1, 20], [6, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2736; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2736; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2736; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 18, 8), \"float32\"), compute: T.Buffer((19, 18, 8), \"float32\"), compute_1: T.Buffer((19, 18, 8), \"float32\"), compute_2: T.Buffer((19, 18, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2736,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2736):\n            compute_3 = T.Buffer((2736,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2736):\n            compute_3 = T.Buffer((2736,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2736):\n            compute_3 = T.Buffer((2736,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "abs",
                "acos",
                "atan"
            ]
        ],
        "input_shape": [[19, 18, 8]],
        "output_shape": [[19, 18, 8], [19, 18, 8], [19, 18, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 280) + (i1 * 14)) + i2)] = acoshf(ph_0[(((i0 * 280) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3080; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 20, 14), \"float32\"), compute: T.Buffer((11, 20, 14), \"float32\"), compute_1: T.Buffer((11, 20, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3080,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(20, 14):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 14 + i2\n                compute_2 = T.Buffer((3080,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3080):\n            compute_2 = T.Buffer((3080,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "asin",
                "asin"
            ]
        ],
        "input_shape": [[11, 20, 14]],
        "output_shape": [[11, 20, 14], [11, 20, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 220; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = acoshf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    float compute_2[209];\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_2[((i1 * 19) + i2_1)] = expf(atanhf(ph_0[(((ax0 * 209) + (i1 * 19)) + i2_1)]));\n      }\n    }\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_subtract[(((ax0 * 209) + (ax1 * 19)) + ax2)] = (ph_0[(((ax0 * 209) + (ax1 * 19)) + ax2)] - compute_2[((ax1 * 19) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4180; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - __expf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 11, 19), \"float32\"), compute: T.Buffer((20, 11, 19), \"float32\"), T_subtract: T.Buffer((20, 11, 19), \"float32\"), compute_1: T.Buffer((20, 11, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4180,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(220):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_2 = T.Buffer((4180,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(20):\n            compute_2 = T.allocate([209], \"float32\", \"global\")\n            compute_3 = T.Buffer((209,), data=compute_2)\n            for i1, i2 in T.grid(11, 19):\n                cse_var_2: T.int32 = i1 * 19\n                compute_3[cse_var_2 + i2] = T.exp(T.atanh(ph_0_1[ax0 * 209 + cse_var_2 + i2]))\n            for ax1, ax2 in T.grid(11, 19):\n                cse_var_4: T.int32 = ax1 * 19\n                cse_var_3: T.int32 = ax0 * 209 + cse_var_4 + ax2\n                T_subtract_1 = T.Buffer((4180,), data=T_subtract.data)\n                T_subtract_1[cse_var_3] = ph_0_1[cse_var_3] - compute_3[cse_var_4 + ax2]\n        for i0_i1_fused_i2_fused in T.parallel(4180):\n            compute_2 = T.Buffer((4180,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "atanh",
                "exp",
                "subtract",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[20, 11, 19]],
        "output_shape": [[20, 11, 19], [20, 11, 19], [20, 11, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 135; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = sinf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1080; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 135; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_divide[((ax0_ax1_fused * 8) + ax2)] = (atanf(ph_0[((ax0_ax1_fused * 8) + ax2)]) / ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1080; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1080; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = acosf((ph_0[i0_i1_fused_i2_fused_2] + ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 9, 8), \"float32\"), ph_3: T.Buffer((15, 9, 8), \"float32\"), compute: T.Buffer((15, 9, 8), \"float32\"), compute_1: T.Buffer((15, 9, 8), \"float32\"), T_divide: T.Buffer((15, 9, 8), \"float32\"), compute_2: T.Buffer((15, 9, 8), \"float32\"), compute_3: T.Buffer((15, 9, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1080,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(135):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((1080,), data=compute.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_4 = T.Buffer((1080,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(135):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((1080,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.atan(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]\n        ph_3_1 = T.Buffer((1080,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_4 = T.Buffer((1080,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1080):\n            compute_4 = T.Buffer((1080,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "sin",
                "atan",
                "acos",
                "divide",
                "abs",
                "acos"
            ]
        ],
        "input_shape": [[15, 9, 8], [20, 3, 5], [15, 9, 8]],
        "output_shape": [[20, 3, 5], [15, 9, 8], [15, 9, 8], [15, 9, 8], [15, 9, 8], [15, 9, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2640; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 132) + (i1 * 12)) + i2)] = asinf(ph_0[(((i0 * 132) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 11, 12), \"float32\"), ph_3: T.Buffer((20, 11, 12), \"float32\"), T_subtract: T.Buffer((20, 11, 12), \"float32\"), compute: T.Buffer((20, 11, 12), \"float32\"), compute_1: T.Buffer((20, 11, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2640,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2640):\n            T_subtract_1 = T.Buffer((2640,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2640,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_2 = T.Buffer((2640,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(11, 12):\n                cse_var_1: T.int32 = i0 * 132 + i1 * 12 + i2\n                compute_2 = T.Buffer((2640,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "subtract",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[20, 11, 12], [13, 19, 1], [20, 11, 12]],
        "output_shape": [[20, 11, 12], [13, 19, 1], [20, 11, 12], [20, 11, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i1 * 8) + i2)] = atanhf(ph_0[((i1 * 8) + i2)]);\n    }\n  }\n  for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i1_1 * 8) + i2_1)] = atanhf(ph_0[((i1_1 * 8) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 12, 8), \"float32\"), compute: T.Buffer((1, 12, 8), \"float32\"), compute_1: T.Buffer((1, 12, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((96,), data=ph_0.data)\n        for i1, i2 in T.grid(12, 8):\n            cse_var_1: T.int32 = i1 * 8 + i2\n            compute_2 = T.Buffer((96,), data=compute.data)\n            compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i1, i2 in T.grid(12, 8):\n            cse_var_2: T.int32 = i1 * 8 + i2\n            compute_2 = T.Buffer((96,), data=compute_1.data)\n            compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atanh",
                "atanh"
            ]
        ],
        "input_shape": [[1, 12, 8]],
        "output_shape": [[1, 12, 8], [1, 12, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_multiply, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 900; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 900; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] + ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n        T_multiply[(((ax0 * 90) + (ax1 * 15)) + ax2)] = (atanf(ph_0[(((ax0 * 90) + (ax1 * 15)) + ax2)]) * ph_0[(((ax0 * 90) + (ax1 * 15)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 6, 15), \"float32\"), ph_3: T.Buffer((10, 6, 15), \"float32\"), T_divide: T.Buffer((10, 6, 15), \"float32\"), T_add: T.Buffer((10, 6, 15), \"float32\"), T_multiply: T.Buffer((10, 6, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        ph_3_1 = T.Buffer((900,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            T_divide_1 = T.Buffer((900,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            T_add_1 = T.Buffer((900,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(6, 15):\n                cse_var_1: T.int32 = ax0 * 90 + ax1 * 15 + ax2\n                T_multiply_1 = T.Buffer((900,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]",
        "op_args": [
            [
                "divide",
                "add",
                "atan",
                "multiply"
            ]
        ],
        "input_shape": [[10, 6, 15], [14, 7, 19], [10, 6, 15]],
        "output_shape": [[10, 6, 15], [14, 7, 19], [10, 6, 15], [10, 6, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* T_subtract_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 48) + (i1 * 4)) + i2)] = atanf(ph_0[(((i0 * 48) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 132; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] * ceilf(atanhf(ph_0[((ax0_ax1_fused * 4) + ax2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 528; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 528; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused_1] = (atanf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ceilf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 12, 4), \"float32\"), compute: T.Buffer((11, 12, 4), \"float32\"), T_multiply: T.Buffer((11, 12, 4), \"float32\"), T_subtract: T.Buffer((11, 12, 4), \"float32\"), T_subtract_1: T.Buffer((11, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((528,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(12, 4):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 4 + i2\n                compute_1 = T.Buffer((528,), data=compute.data)\n                compute_1[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(132):\n            for ax2 in range(4):\n                cse_var_2: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_multiply_1 = T.Buffer((528,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = ph_0_1[cse_var_2] * T.ceil(T.atanh(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(528):\n            T_subtract_2 = T.Buffer((528,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(528):\n            T_subtract_2 = T.Buffer((528,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atan",
                "atanh",
                "ceil",
                "multiply",
                "atan",
                "subtract",
                "subtract"
            ]
        ],
        "input_shape": [[11, 12, 4]],
        "output_shape": [[11, 12, 4], [11, 12, 4], [11, 12, 4], [11, 12, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = asinf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 900; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 900; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 90; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 10) + i2_1)] = ceilf(atanf(ph_0[((i0_i1_fused_1 * 10) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 9, 10), \"float32\"), compute: T.Buffer((10, 9, 10), \"float32\"), T_add: T.Buffer((10, 9, 10), \"float32\"), compute_1: T.Buffer((10, 9, 10), \"float32\"), compute_2: T.Buffer((10, 9, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(90):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_3 = T.Buffer((900,), data=compute.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            T_add_1 = T.Buffer((900,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_3 = T.Buffer((900,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(90):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_3 = T.Buffer((900,), data=compute_2.data)\n                compute_3[cse_var_2] = T.ceil(T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asin",
                "cos",
                "add",
                "ceil",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[10, 9, 10]],
        "output_shape": [[10, 9, 10], [10, 9, 10], [10, 9, 10], [10, 9, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_divide[(((ax0 * 15) + (ax1 * 3)) + ax2)] = (ph_0[(((ax0 * 15) + (ax1 * 3)) + ax2)] / (fmodf(ph_0[(((ax0 * 15) + (ax1 * 3)) + ax2)], atanhf(ph_0[(((ax0 * 15) + (ax1 * 3)) + ax2)])) - ph_0[(((ax0 * 15) + (ax1 * 3)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = cosf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] / (fmodf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 5, 3), \"float32\"), T_divide: T.Buffer((7, 5, 3), \"float32\"), compute: T.Buffer((7, 5, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((105,), data=ph_0.data)\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(5, 3):\n                cse_var_1: T.int32 = ax0 * 15 + ax1 * 3 + ax2\n                T_divide_1 = T.Buffer((105,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / (T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1])) - ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(35):\n            for i2 in range(3):\n                cse_var_2: T.int32 = i0_i1_fused * 3 + i2\n                compute_1 = T.Buffer((105,), data=compute.data)\n                compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atanh",
                "mod",
                "subtract",
                "divide",
                "cos"
            ]
        ],
        "input_shape": [[7, 5, 3]],
        "output_shape": [[7, 5, 3], [7, 5, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1672; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 88) + (i1 * 8)) + i2)] = expf(asinhf(ph_0[(((i0 * 88) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 19; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        compute_2[(((i0_1 * 88) + (i1_1 * 8)) + i2_1)] = expf(ph_0[(((i0_1 * 88) + (i1_1 * 8)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(asinhf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 11, 8), \"float32\"), compute: T.Buffer((19, 11, 8), \"float32\"), compute_1: T.Buffer((19, 11, 8), \"float32\"), compute_2: T.Buffer((19, 11, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1672,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1672):\n            compute_3 = T.Buffer((1672,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(11, 8):\n                cse_var_1: T.int32 = i0 * 88 + i1 * 8 + i2\n                compute_3 = T.Buffer((1672,), data=compute_1.data)\n                compute_3[cse_var_1] = T.exp(T.asinh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(11, 8):\n                cse_var_2: T.int32 = i0 * 88 + i1 * 8 + i2\n                compute_3 = T.Buffer((1672,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asin",
                "asinh",
                "exp",
                "exp"
            ]
        ],
        "input_shape": [[19, 11, 8]],
        "output_shape": [[19, 11, 8], [19, 11, 8], [19, 11, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3420; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_divide[(((ax0 * 171) + (ax1 * 19)) + ax2)] = (ph_0[(((ax0 * 171) + (ax1 * 19)) + ax2)] / ph_3[(((ax0 * 171) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3420; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 19), \"float32\"), ph_3: T.Buffer((20, 9, 19), \"float32\"), T_mod: T.Buffer((20, 9, 19), \"float32\"), T_divide: T.Buffer((20, 9, 19), \"float32\"), compute: T.Buffer((20, 9, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3420,), data=ph_0.data)\n        ph_3_1 = T.Buffer((3420,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3420):\n            T_mod_1 = T.Buffer((3420,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(9, 19):\n                cse_var_1: T.int32 = ax0 * 171 + ax1 * 19 + ax2\n                T_divide_1 = T.Buffer((3420,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(3420):\n            compute_1 = T.Buffer((3420,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "mod",
                "divide",
                "atanh"
            ]
        ],
        "input_shape": [[20, 9, 19], [14, 9, 15], [20, 9, 19]],
        "output_shape": [[20, 9, 19], [14, 9, 15], [20, 9, 19], [20, 9, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 272; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 272; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (atanf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = atanhf(atanf(ph_0[((i0_i1_fused * 17) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 272; ++ax0_ax1_fused_ax2_fused_2) {\n    T_multiply[ax0_ax1_fused_ax2_fused_2] = ((ph_0[ax0_ax1_fused_ax2_fused_2] - ph_3[ax0_ax1_fused_ax2_fused_2]) * ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 4, 17), \"float32\"), ph_3: T.Buffer((4, 4, 17), \"float32\"), T_mod: T.Buffer((4, 4, 17), \"float32\"), T_subtract: T.Buffer((4, 4, 17), \"float32\"), compute: T.Buffer((4, 4, 17), \"float32\"), T_multiply: T.Buffer((4, 4, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((272,), data=ph_0.data)\n        ph_3_1 = T.Buffer((272,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(272):\n            T_mod_1 = T.Buffer((272,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(272):\n            T_subtract_1 = T.Buffer((272,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(16):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_1 = T.Buffer((272,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(T.atan(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(272):\n            T_multiply_1 = T.Buffer((272,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "subtract",
                "mod",
                "atan",
                "subtract",
                "atanh",
                "multiply"
            ]
        ],
        "input_shape": [[4, 4, 17], [5, 19, 2], [4, 4, 17]],
        "output_shape": [[5, 19, 2], [4, 4, 17], [4, 4, 17], [4, 4, 17], [4, 4, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 65; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 13; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_add[((ax0_ax1_fused * 5) + ax2)] = (ph_0[((ax0_ax1_fused * 5) + ax2)] + (asinhf(ph_0[((ax0_ax1_fused * 5) + ax2)]) * ph_0[((ax0_ax1_fused * 5) + ax2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 13; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = ceilf(atanf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + (asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 5), \"float32\"), compute: T.Buffer((13, 1, 5), \"float32\"), T_add: T.Buffer((13, 1, 5), \"float32\"), compute_1: T.Buffer((13, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((65,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(65):\n            compute_2 = T.Buffer((65,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(13):\n            for ax2 in range(5):\n                cse_var_1: T.int32 = ax0_ax1_fused * 5 + ax2\n                T_add_1 = T.Buffer((65,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.asinh(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(13):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((65,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acos",
                "asinh",
                "multiply",
                "add",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[13, 1, 5]],
        "output_shape": [[13, 1, 5], [13, 1, 5], [13, 1, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = atanf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1536; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 96; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 16) + i2_1)] = fabsf(ph_0[((i0_i1_fused_1 * 16) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 8, 16), \"float32\"), compute: T.Buffer((12, 8, 16), \"float32\"), compute_1: T.Buffer((12, 8, 16), \"float32\"), compute_2: T.Buffer((12, 8, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1536,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((1536,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1536):\n            compute_3 = T.Buffer((1536,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((1536,), data=compute_2.data)\n                compute_3[cse_var_2] = T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atan",
                "acosh",
                "cos",
                "abs"
            ]
        ],
        "input_shape": [[12, 8, 16]],
        "output_shape": [[12, 8, 16], [12, 8, 16], [12, 8, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 162; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 162; ++i0_i1_fused) {\n    compute_1[i0_i1_fused] = sinf(sinf(ph_0[i0_i1_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 162; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 162; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinhf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 18, 1), \"float32\"), compute: T.Buffer((9, 18, 1), \"float32\"), compute_1: T.Buffer((9, 18, 1), \"float32\"), compute_2: T.Buffer((9, 18, 1), \"float32\"), compute_3: T.Buffer((9, 18, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((162,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(162):\n            compute_4 = T.Buffer((162,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(162):\n            compute_4 = T.Buffer((162,), data=compute_1.data)\n            compute_4[i0_i1_fused] = T.sin(T.sin(ph_0_1[i0_i1_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(162):\n            compute_4 = T.Buffer((162,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(162):\n            compute_4 = T.Buffer((162,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "sin",
                "sin",
                "abs",
                "asinh"
            ]
        ],
        "input_shape": [[9, 18, 1]],
        "output_shape": [[9, 18, 1], [9, 18, 1], [9, 18, 1], [9, 18, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 260; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf((ph_0[i0_i1_fused_i2_fused] - acoshf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 260; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf((ph_0[i0_i1_fused_i2_fused_1] - acoshf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 13; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_2[((i0_i1_fused * 20) + i2)] = asinhf(fmodf(ph_0[((i0_i1_fused * 20) + i2)], ph_3[((i0_i1_fused * 20) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 260; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf((ph_0[((int)blockIdx.x)] - acoshf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 20), \"float32\"), ph_3: T.Buffer((13, 1, 20), \"float32\"), compute: T.Buffer((13, 1, 20), \"float32\"), compute_1: T.Buffer((13, 1, 20), \"float32\"), compute_2: T.Buffer((13, 1, 20), \"float32\"), compute_3: T.Buffer((13, 1, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((260,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(260):\n            compute_4 = T.Buffer((260,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] - T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(260):\n            compute_4 = T.Buffer((260,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] - T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((260,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(13):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2\n                compute_4 = T.Buffer((260,), data=compute_2.data)\n                compute_4[cse_var_1] = T.asinh(T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(260):\n            compute_4 = T.Buffer((260,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "acosh",
                "subtract",
                "cos",
                "cos",
                "asinh",
                "acosh"
            ]
        ],
        "input_shape": [[13, 1, 20], [8, 16, 7], [13, 1, 20]],
        "output_shape": [[8, 16, 7], [13, 1, 20], [13, 1, 20], [13, 1, 20], [13, 1, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 91) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 91) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 182; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 182; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 13, 7), \"float32\"), compute: T.Buffer((2, 13, 7), \"float32\"), T_divide: T.Buffer((2, 13, 7), \"float32\"), compute_1: T.Buffer((2, 13, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((182,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(13, 7):\n                cse_var_1: T.int32 = i0 * 91 + i1 * 7 + i2\n                compute_2 = T.Buffer((182,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(182):\n            T_divide_1 = T.Buffer((182,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(182):\n            compute_2 = T.Buffer((182,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "atan",
                "divide",
                "exp"
            ]
        ],
        "input_shape": [[2, 13, 7]],
        "output_shape": [[2, 13, 7], [2, 13, 7], [2, 13, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_multiply[(((ax0 * 60) + (ax1 * 5)) + ax2)] = (ph_0[(((ax0 * 60) + (ax1 * 5)) + ax2)] * ph_3[(((ax0 * 60) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = acoshf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 420; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 420; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = fabsf(atanf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 12, 5), \"float32\"), ph_3: T.Buffer((7, 12, 5), \"float32\"), T_multiply: T.Buffer((7, 12, 5), \"float32\"), compute: T.Buffer((7, 12, 5), \"float32\"), compute_1: T.Buffer((7, 12, 5), \"float32\"), compute_2: T.Buffer((7, 12, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((420,), data=ph_0.data)\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(12, 5):\n                cse_var_1: T.int32 = ax0 * 60 + ax1 * 5 + ax2\n                T_multiply_1 = T.Buffer((420,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((420,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((420,), data=compute.data)\n                compute_3[cse_var_2] = T.acosh(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(420):\n            compute_3 = T.Buffer((420,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(420):\n            compute_3 = T.Buffer((420,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "multiply",
                "acosh",
                "atan",
                "sin",
                "abs"
            ]
        ],
        "input_shape": [[7, 12, 5], [8, 8, 3], [7, 12, 5]],
        "output_shape": [[7, 12, 5], [8, 8, 3], [7, 12, 5], [7, 12, 5], [7, 12, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 180) + (i1 * 10)) + i2)] = asinhf(ph_0[(((i0 * 180) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2880; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2880; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_3[((i0_i1_fused * 10) + i2_1)] = acoshf(ph_0[((i0_i1_fused * 10) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 18, 10), \"float32\"), compute: T.Buffer((16, 18, 10), \"float32\"), compute_1: T.Buffer((16, 18, 10), \"float32\"), compute_2: T.Buffer((16, 18, 10), \"float32\"), compute_3: T.Buffer((16, 18, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(18, 10):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 10 + i2\n                compute_4 = T.Buffer((2880,), data=compute.data)\n                compute_4[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_4 = T.Buffer((2880,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_4 = T.Buffer((2880,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_4 = T.Buffer((2880,), data=compute_3.data)\n                compute_4[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "atan",
                "cos",
                "asinh",
                "acosh"
            ]
        ],
        "input_shape": [[16, 18, 10]],
        "output_shape": [[16, 18, 10], [16, 18, 10], [16, 18, 10], [16, 18, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 832; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 104; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_1[((i0_i1_fused * 8) + i2)] = acoshf(asinf(ph_0[((i0_i1_fused * 8) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 104; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 8) + i2_1)] = asinf(fmodf(ph_0[((i0_i1_fused_1 * 8) + i2_1)], fabsf(ph_0[((i0_i1_fused_1 * 8) + i2_1)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2_2 = 0; i2_2 < 8; ++i2_2) {\n        compute_3[(((i0 * 64) + (i1 * 8)) + i2_2)] = atanhf(fmodf(ph_0[(((i0 * 64) + (i1 * 8)) + i2_2)], fabsf(ph_0[(((i0 * 64) + (i1 * 8)) + i2_2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(fmodf(ph_0[((int)blockIdx.x)], fabsf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 8, 8), \"float32\"), compute: T.Buffer((13, 8, 8), \"float32\"), compute_1: T.Buffer((13, 8, 8), \"float32\"), compute_2: T.Buffer((13, 8, 8), \"float32\"), compute_3: T.Buffer((13, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((832,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(832):\n            compute_4 = T.Buffer((832,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(104):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((832,), data=compute_1.data)\n                compute_4[cse_var_1] = T.acosh(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(104):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((832,), data=compute_2.data)\n                compute_4[cse_var_2] = T.asin(T.truncmod(ph_0_1[cse_var_2], T.fabs(ph_0_1[cse_var_2])))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(8, 8):\n                cse_var_3: T.int32 = i0 * 64 + i1 * 8 + i2\n                compute_4 = T.Buffer((832,), data=compute_3.data)\n                compute_4[cse_var_3] = T.atanh(T.truncmod(ph_0_1[cse_var_3], T.fabs(ph_0_1[cse_var_3])))",
        "op_args": [
            [
                "acos",
                "asin",
                "acosh",
                "abs",
                "mod",
                "asin",
                "atanh"
            ]
        ],
        "input_shape": [[13, 8, 8]],
        "output_shape": [[13, 8, 8], [13, 8, 8], [13, 8, 8], [13, 8, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 50; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = sinf((ph_0[((i0_i1_fused * 18) + i2)] - atanf(ph_0[((i0_i1_fused * 18) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 900; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 50; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 18) + i2_1)] = expf(ph_0[((i0_i1_fused_1 * 18) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 5, 18), \"float32\"), compute: T.Buffer((10, 5, 18), \"float32\"), compute_1: T.Buffer((10, 5, 18), \"float32\"), compute_2: T.Buffer((10, 5, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(50):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((900,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1] - T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_3 = T.Buffer((900,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(50):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((900,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atan",
                "subtract",
                "sin",
                "atanh",
                "exp"
            ]
        ],
        "input_shape": [[10, 5, 18]],
        "output_shape": [[10, 5, 18], [10, 5, 18], [10, 5, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_divide_1, float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2560; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2560; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide_1[ax0_ax1_fused_ax2_fused_1] = (acosf(ph_0[ax0_ax1_fused_ax2_fused_1]) / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2560; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 2560; ++ax0_ax1_fused_ax2_fused_2) {\n    T_multiply[ax0_ax1_fused_ax2_fused_2] = ((ph_0[ax0_ax1_fused_ax2_fused_2] / ph_3[ax0_ax1_fused_ax2_fused_2]) * ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 8, 20), \"float32\"), ph_3: T.Buffer((16, 8, 20), \"float32\"), T_divide: T.Buffer((16, 8, 20), \"float32\"), T_divide_1: T.Buffer((16, 8, 20), \"float32\"), compute: T.Buffer((16, 8, 20), \"float32\"), T_multiply: T.Buffer((16, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2560,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2560,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2560):\n            T_divide_2 = T.Buffer((2560,), data=T_divide.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2560):\n            T_divide_2 = T.Buffer((2560,), data=T_divide_1.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2560):\n            compute_1 = T.Buffer((2560,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(2560):\n            T_multiply_1 = T.Buffer((2560,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "divide",
                "acos",
                "divide",
                "abs",
                "multiply"
            ]
        ],
        "input_shape": [[16, 8, 20], [16, 11, 9], [16, 8, 20]],
        "output_shape": [[16, 11, 9], [16, 8, 20], [16, 8, 20], [16, 8, 20], [16, 8, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1872; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1872; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute_1[((i0_i1_fused * 13) + i2)] = acoshf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 12, 13), \"float32\"), ph_3: T.Buffer((12, 12, 13), \"float32\"), T_mod: T.Buffer((12, 12, 13), \"float32\"), compute: T.Buffer((12, 12, 13), \"float32\"), compute_1: T.Buffer((12, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1872,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1872):\n            T_mod_1 = T.Buffer((1872,), data=T_mod.data)\n            ph_3_1 = T.Buffer((1872,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1872):\n            compute_2 = T.Buffer((1872,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(144):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((1872,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "mod",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[12, 12, 13], [5, 9, 12], [12, 12, 13]],
        "output_shape": [[12, 12, 13], [5, 9, 12], [12, 12, 13], [12, 12, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_divide_1, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 45) + (i1 * 9)) + i2)] = sinf(ph_0[(((i0 * 45) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 900; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / fmodf(atanf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 900; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 5; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_divide_1[(((ax0 * 45) + (ax1 * 9)) + ax2)] = (asinhf(ph_0[(((ax0 * 45) + (ax1 * 9)) + ax2)]) / ph_0[(((ax0 * 45) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / fmodf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 5, 9), \"float32\"), compute: T.Buffer((20, 5, 9), \"float32\"), T_divide: T.Buffer((20, 5, 9), \"float32\"), compute_1: T.Buffer((20, 5, 9), \"float32\"), T_divide_1: T.Buffer((20, 5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((900,), data=ph_0.data)\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(5, 9):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 9 + i2\n                compute_2 = T.Buffer((900,), data=compute.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(900):\n            T_divide_2 = T.Buffer((900,), data=T_divide.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.truncmod(T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(900):\n            compute_2 = T.Buffer((900,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(5, 9):\n                cse_var_2: T.int32 = ax0 * 45 + ax1 * 9 + ax2\n                T_divide_2 = T.Buffer((900,), data=T_divide_1.data)\n                T_divide_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]",
        "op_args": [
            [
                "sin",
                "atan",
                "mod",
                "divide",
                "asinh",
                "asin",
                "divide"
            ]
        ],
        "input_shape": [[20, 5, 9]],
        "output_shape": [[20, 5, 9], [20, 5, 9], [20, 5, 9], [20, 5, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = acoshf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 48; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_add[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] + (ph_0[((ax0_ax1_fused * 4) + ax2)] - atanf(atanf(ph_0[((ax0_ax1_fused * 4) + ax2)]))));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - atanf(atanf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(40) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 16, 4), \"float32\"), compute: T.Buffer((3, 16, 4), \"float32\"), T_add: T.Buffer((3, 16, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((192,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_1 = T.Buffer((192,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(48):\n            for ax2 in range(4):\n                cse_var_2: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_add_1 = T.Buffer((192,), data=T_add.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] + (ph_0_1[cse_var_2] - T.atan(T.atan(ph_0_1[cse_var_2])))",
        "op_args": [
            [
                "acosh",
                "atan",
                "atan",
                "subtract",
                "add"
            ]
        ],
        "input_shape": [[3, 16, 4]],
        "output_shape": [[3, 16, 4], [3, 16, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = expf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 126; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 126; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 126; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = cosf(atanhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 6, 7), \"float32\"), compute: T.Buffer((3, 6, 7), \"float32\"), compute_1: T.Buffer((3, 6, 7), \"float32\"), compute_2: T.Buffer((3, 6, 7), \"float32\"), compute_3: T.Buffer((3, 6, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((126,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_4 = T.Buffer((126,), data=compute.data)\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_4 = T.Buffer((126,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_4 = T.Buffer((126,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_4 = T.Buffer((126,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "abs",
                "asin",
                "sin",
                "atanh",
                "cos"
            ]
        ],
        "input_shape": [[3, 6, 7]],
        "output_shape": [[3, 6, 7], [3, 6, 7], [3, 6, 7], [3, 6, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3, float* ph_6) {\n  float auto_scheduler_layout_transform[65];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 208; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 13; ++ax4) {\n      auto_scheduler_layout_transform[((ax0_ax1_fused_ax2_fused * 13) + ax4)] = ph_6[((ax4 * 5) + ax0_ax1_fused_ax2_fused)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_i1_outer_fused_i2_outer_fused = 0; i0_outer_i1_outer_fused_i2_outer_fused < 10; ++i0_outer_i1_outer_fused_i2_outer_fused) {\n    float T_batch_matmul_NN[104];\n    for (int32_t b_outer_outer_inner = 0; b_outer_outer_inner < 13; ++b_outer_outer_inner) {\n      for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 2; ++i_outer_outer_inner) {\n        for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 4; ++i_outer_inner_init) {\n          T_batch_matmul_NN[(((b_outer_outer_inner * 8) + (i_outer_outer_inner * 4)) + i_outer_inner_init)] = 0.000000e+00f;\n        }\n        for (int32_t i_outer_inner = 0; i_outer_inner < 4; ++i_outer_inner) {\n          T_batch_matmul_NN[(((b_outer_outer_inner * 8) + (i_outer_outer_inner * 4)) + i_outer_inner)] = (T_batch_matmul_NN[(((b_outer_outer_inner * 8) + (i_outer_outer_inner * 4)) + i_outer_inner)] + (ph_0[((((b_outer_outer_inner * 16) + ((i0_outer_i1_outer_fused_i2_outer_fused / 5) * 8)) + (i_outer_outer_inner * 4)) + i_outer_inner)] * auto_scheduler_layout_transform[(((i0_outer_i1_outer_fused_i2_outer_fused % 5) * 13) + b_outer_outer_inner)]));\n        }\n      }\n    }\n    for (int32_t i0_inner = 0; i0_inner < 13; ++i0_inner) {\n      for (int32_t i1_inner = 0; i1_inner < 8; ++i1_inner) {\n        compute_1[((((i0_inner * 80) + ((i0_outer_i1_outer_fused_i2_outer_fused / 5) * 40)) + (i1_inner * 5)) + (i0_outer_i1_outer_fused_i2_outer_fused % 5))] = acoshf(T_batch_matmul_NN[((i0_inner * 8) + i1_inner)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 208; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 208; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] / ph_3[ax0_ax1_fused_ax2_fused_1]) / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 208; ++ax0_ax1_fused_ax2_fused_2) {\n    T_subtract[ax0_ax1_fused_ax2_fused_2] = ((ph_0[ax0_ax1_fused_ax2_fused_2] / ph_3[ax0_ax1_fused_ax2_fused_2]) - ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN[4];\n  __shared__ float ph_6_shared[4];\n  for (int i_outer_inner_init = 0; i_outer_inner_init < 4; ++i_outer_inner_init) {\n    T_batch_matmul_NN[i_outer_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    if (((int)threadIdx.x) < 4) {\n      ph_6_shared[((int)threadIdx.x)] = ph_6[(((((((int)blockIdx.x) / 5) * 100) + (((int)threadIdx.x) * 25)) + (k_outer_outer * 5)) + (((int)blockIdx.x) % 5))];\n    }\n    __syncthreads();\n    for (int i_outer_inner = 0; i_outer_inner < 4; ++i_outer_inner) {\n      T_batch_matmul_NN[i_outer_inner] = (T_batch_matmul_NN[i_outer_inner] + (ph_0[(((((((int)blockIdx.x) / 5) * 160) + (((int)threadIdx.x) * 20)) + (i_outer_inner * 5)) + k_outer_outer)] * ph_6_shared[(((int)threadIdx.x) >> 1)]));\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 4; ++i1_inner) {\n    compute[(((((((int)blockIdx.x) / 5) * 160) + (((int)threadIdx.x) * 20)) + (i1_inner * 5)) + (((int)blockIdx.x) % 5))] = acoshf(T_batch_matmul_NN[i1_inner]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 16, 1), \"float32\"), ph_3: T.Buffer((13, 16, 1), \"float32\"), ph_6: T.Buffer((13, 1, 5), \"float32\"), compute: T.Buffer((13, 16, 1), \"float32\"), compute_1: T.Buffer((13, 16, 5), \"float32\"), compute_2: T.Buffer((13, 16, 1), \"float32\"), T_divide: T.Buffer((13, 16, 1), \"float32\"), T_subtract: T.Buffer((13, 16, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([65], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((208,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(208):\n            compute_3 = T.Buffer((208,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((65,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax4 in range(13):\n                ph_6_1 = T.Buffer((65,), data=ph_6.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 13 + ax4] = ph_6_1[ax4 * 5 + ax0_ax1_fused_ax2_fused]\n        for i0_outer_i1_outer_fused_i2_outer_fused in T.parallel(10):\n            T_batch_matmul_NN = T.allocate([104], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((104,), data=T_batch_matmul_NN)\n            for b_outer_outer_inner, i_outer_outer_inner in T.grid(13, 2):\n                for i_outer_inner_init in range(4):\n                    T_batch_matmul_NN_1[b_outer_outer_inner * 8 + i_outer_outer_inner * 4 + i_outer_inner_init] = T.float32(0)\n                for i_outer_inner in range(4):\n                    cse_var_2: T.int32 = i_outer_outer_inner * 4\n                    cse_var_1: T.int32 = b_outer_outer_inner * 8 + cse_var_2 + i_outer_inner\n                    T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_outer_inner * 16 + i0_outer_i1_outer_fused_i2_outer_fused // 5 * 8 + cse_var_2 + i_outer_inner] * auto_scheduler_layout_transform_1[i0_outer_i1_outer_fused_i2_outer_fused % 5 * 13 + b_outer_outer_inner]\n            for i0_inner, i1_inner in T.grid(13, 8):\n                compute_3 = T.Buffer((1040,), data=compute_1.data)\n                compute_3[i0_inner * 80 + i0_outer_i1_outer_fused_i2_outer_fused // 5 * 40 + i1_inner * 5 + i0_outer_i1_outer_fused_i2_outer_fused % 5] = T.acosh(T_batch_matmul_NN_1[i0_inner * 8 + i1_inner])\n        ph_3_1 = T.Buffer((208,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(208):\n            compute_3 = T.Buffer((208,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(208):\n            T_divide_1 = T.Buffer((208,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(208):\n            T_subtract_1 = T.Buffer((208,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "exp",
                "batch_matmul",
                "acosh",
                "cos",
                "divide",
                "subtract"
            ]
        ],
        "input_shape": [[13, 16, 1], [11, 14, 14], [13, 16, 1], [13, 1, 5]],
        "output_shape": [[11, 14, 14], [13, 16, 1], [13, 16, 5], [13, 16, 1], [13, 16, 1], [13, 16, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_mod[(((ax0 * 48) + (ax1 * 6)) + ax2)] = fmodf(ph_0[(((ax0 * 48) + (ax1 * 6)) + ax2)], ph_3[(((ax0 * 48) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 288; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 288; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 288; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 8, 6), \"float32\"), ph_3: T.Buffer((6, 8, 6), \"float32\"), T_mod: T.Buffer((6, 8, 6), \"float32\"), compute: T.Buffer((6, 8, 6), \"float32\"), compute_1: T.Buffer((6, 8, 6), \"float32\"), T_divide: T.Buffer((6, 8, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((288,), data=ph_0.data)\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(8, 6):\n                cse_var_1: T.int32 = ax0 * 48 + ax1 * 6 + ax2\n                T_mod_1 = T.Buffer((288,), data=T_mod.data)\n                ph_3_1 = T.Buffer((288,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(288):\n            compute_2 = T.Buffer((288,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(288):\n            compute_2 = T.Buffer((288,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(288):\n            T_divide_1 = T.Buffer((288,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "exp",
                "atan",
                "atanh",
                "divide"
            ]
        ],
        "input_shape": [[6, 8, 6], [1, 15, 20], [6, 8, 6]],
        "output_shape": [[6, 8, 6], [1, 15, 20], [6, 8, 6], [6, 8, 6], [6, 8, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 21) + (i1 * 7)) + i2)] = acosf(ph_0[(((i0 * 21) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 336; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 336; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 336; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinf((ph_0[i0_i1_fused_i2_fused_2] - ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 3, 7), \"float32\"), ph_3: T.Buffer((16, 3, 7), \"float32\"), compute: T.Buffer((16, 3, 7), \"float32\"), compute_1: T.Buffer((16, 3, 7), \"float32\"), compute_2: T.Buffer((16, 3, 7), \"float32\"), compute_3: T.Buffer((16, 3, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((336,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(3, 7):\n                cse_var_1: T.int32 = i0 * 21 + i1 * 7 + i2\n                compute_4 = T.Buffer((336,), data=compute.data)\n                compute_4[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(336):\n            compute_4 = T.Buffer((336,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(336):\n            compute_4 = T.Buffer((336,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(336):\n            compute_4 = T.Buffer((336,), data=compute_3.data)\n            ph_3_1 = T.Buffer((336,), data=ph_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "acos",
                "atan",
                "asin",
                "exp",
                "asin"
            ]
        ],
        "input_shape": [[16, 3, 7], [18, 11, 14], [16, 3, 7]],
        "output_shape": [[18, 11, 14], [16, 3, 7], [16, 3, 7], [16, 3, 7], [16, 3, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 765; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 85; ++ax0_ax1_fused) {\n    float compute_1[1];\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      compute_1[0] = expf(ph_0[((ax0_ax1_fused * 9) + ax2)]);\n      T_add[((ax0_ax1_fused * 9) + ax2)] = (compute_1[0] + ph_0[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 765; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * fabsf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] * fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 17, 9), \"float32\"), compute: T.Buffer((5, 17, 9), \"float32\"), T_add: T.Buffer((5, 17, 9), \"float32\"), T_multiply: T.Buffer((5, 17, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((765,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(765):\n            compute_1 = T.Buffer((765,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(85):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                compute_2 = T.Buffer((1,), data=compute_1, align=4)\n                compute_2[0] = T.exp(ph_0_1[cse_var_1])\n                T_add_1 = T.Buffer((765,), data=T_add.data)\n                T_add_1[cse_var_1] = compute_2[0] + ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(765):\n            T_multiply_1 = T.Buffer((765,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "abs",
                "exp",
                "add",
                "abs",
                "multiply"
            ]
        ],
        "input_shape": [[5, 17, 9]],
        "output_shape": [[5, 17, 9], [5, 17, 9], [5, 17, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_multiply, float* T_multiply_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 180; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n      T_mod[((ax0_ax1_fused * 20) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 20) + ax2)], ph_3[((ax0_ax1_fused * 20) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3600; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 3600; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 3600; ++ax0_ax1_fused_ax2_fused_2) {\n    T_multiply_1[ax0_ax1_fused_ax2_fused_2] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_2]) * ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] + ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 18, 20), \"float32\"), ph_3: T.Buffer((10, 18, 20), \"float32\"), T_mod: T.Buffer((10, 18, 20), \"float32\"), T_add: T.Buffer((10, 18, 20), \"float32\"), T_multiply: T.Buffer((10, 18, 20), \"float32\"), T_multiply_1: T.Buffer((10, 18, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3600,), data=ph_0.data)\n        ph_3_1 = T.Buffer((3600,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(180):\n            for ax2 in range(20):\n                cse_var_1: T.int32 = ax0_ax1_fused * 20 + ax2\n                T_mod_1 = T.Buffer((3600,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3600):\n            T_add_1 = T.Buffer((3600,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(3600):\n            T_multiply_2 = T.Buffer((3600,), data=T_multiply.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(3600):\n            T_multiply_2 = T.Buffer((3600,), data=T_multiply_1.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "add",
                "cos",
                "multiply",
                "multiply"
            ]
        ],
        "input_shape": [[10, 18, 20], [1, 15, 8], [10, 18, 20]],
        "output_shape": [[10, 18, 20], [1, 15, 8], [10, 18, 20], [10, 18, 20], [10, 18, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 162; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = sinf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2268; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2268; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2268; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanf(acosf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 2268; ++i0_i1_fused_i2_fused_3) {\n    compute_4[i0_i1_fused_i2_fused_3] = fabsf(acosf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 9, 14), \"float32\"), compute: T.Buffer((18, 9, 14), \"float32\"), compute_1: T.Buffer((18, 9, 14), \"float32\"), compute_2: T.Buffer((18, 9, 14), \"float32\"), compute_3: T.Buffer((18, 9, 14), \"float32\"), compute_4: T.Buffer((18, 9, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2268,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(162):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_5 = T.Buffer((2268,), data=compute.data)\n                compute_5[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2268):\n            compute_5 = T.Buffer((2268,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2268):\n            compute_5 = T.Buffer((2268,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2268):\n            compute_5 = T.Buffer((2268,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2268):\n            compute_5 = T.Buffer((2268,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.fabs(T.acos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "asin",
                "exp",
                "asinh",
                "acos",
                "atan",
                "abs"
            ]
        ],
        "input_shape": [[18, 9, 14]],
        "output_shape": [[18, 9, 14], [18, 9, 14], [18, 9, 14], [18, 9, 14], [18, 9, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 140) + (i1 * 20)) + i2)] = fabsf(ph_0[(((i0 * 140) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 133; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_1[((i0_i1_fused * 20) + i2_1)] = fabsf(atanhf(ph_0[((i0_i1_fused * 20) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2660; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + acosf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 7, 20), \"float32\"), compute: T.Buffer((19, 7, 20), \"float32\"), compute_1: T.Buffer((19, 7, 20), \"float32\"), T_add: T.Buffer((19, 7, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2660,), data=ph_0.data)\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(7, 20):\n                cse_var_1: T.int32 = i0 * 140 + i1 * 20 + i2\n                compute_2 = T.Buffer((2660,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(133):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_2 = T.Buffer((2660,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(T.atanh(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(2660):\n            T_add_1 = T.Buffer((2660,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "abs",
                "atanh",
                "abs",
                "acos",
                "add"
            ]
        ],
        "input_shape": [[19, 7, 20]],
        "output_shape": [[19, 7, 20], [19, 7, 20], [19, 7, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0) {\n  float compute[4788];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4788; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 19; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_subtract[(((ax0 * 342) + (ax1 * 18)) + ax2)] = (ph_0[(((ax0 * 342) + (ax1 * 18)) + ax2)] - compute[(((ax0 * 342) + (ax1 * 18)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] - __expf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 19, 18), \"float32\"), T_subtract: T.Buffer((14, 19, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute = T.allocate([4788], \"float32\", \"global\")\n        compute_1 = T.Buffer((4788,), data=compute)\n        ph_0_1 = T.Buffer((4788,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4788):\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(14):\n            for ax1, ax2 in T.grid(19, 18):\n                cse_var_1: T.int32 = ax0 * 342 + ax1 * 18 + ax2\n                T_subtract_1 = T.Buffer((4788,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - compute_1[cse_var_1]",
        "op_args": [
            [
                "exp",
                "subtract"
            ]
        ],
        "input_shape": [[14, 19, 18]],
        "output_shape": [[14, 19, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 12; ++ax0) {\n    float compute[1];\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        compute[0] = expf(ph_0[(((ax0 * 165) + (ax1 * 11)) + ax2)]);\n        T_divide[(((ax0 * 165) + (ax1 * 11)) + ax2)] = (ph_0[(((ax0 * 165) + (ax1 * 11)) + ax2)] / sinf((ph_0[(((ax0 * 165) + (ax1 * 11)) + ax2)] + compute[0])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] / __sinf((ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] + __expf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 15, 11), \"float32\"), T_divide: T.Buffer((12, 15, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0 in T.parallel(12):\n            compute = T.allocate([1], \"float32\", \"global\")\n            for ax1, ax2 in T.grid(15, 11):\n                cse_var_1: T.int32 = ax0 * 165 + ax1 * 11 + ax2\n                compute_1 = T.Buffer((1,), data=compute, align=4)\n                ph_0_1 = T.Buffer((1980,), data=ph_0.data)\n                compute_1[0] = T.exp(ph_0_1[cse_var_1])\n                T_divide_1 = T.Buffer((1980,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.sin(ph_0_1[cse_var_1] + compute_1[0])",
        "op_args": [
            [
                "exp",
                "add",
                "sin",
                "divide"
            ]
        ],
        "input_shape": [[12, 15, 11]],
        "output_shape": [[12, 15, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 39; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ((ph_0[ax0_ax1_fused_ax2_fused] / asinhf(ph_0[ax0_ax1_fused_ax2_fused])) / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0 * 3) + i2)] = ceilf(ph_0[((i0 * 3) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] * ((ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] / asinhf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 3), \"float32\"), T_multiply: T.Buffer((13, 1, 3), \"float32\"), compute: T.Buffer((13, 1, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((39,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(39):\n            T_multiply_1 = T.Buffer((39,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * (ph_0_1[ax0_ax1_fused_ax2_fused] / T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(13):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0 * 3 + i2\n                compute_1 = T.Buffer((39,), data=compute.data)\n                compute_1[cse_var_1] = T.ceil(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asinh",
                "divide",
                "divide",
                "multiply",
                "ceil"
            ]
        ],
        "input_shape": [[13, 1, 3]],
        "output_shape": [[13, 1, 3], [13, 1, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_6) {\n  float auto_scheduler_layout_transform[330];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 990; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 990; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 990; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  for (int32_t ax4 = 0; ax4 < 6; ++ax4) {\n    for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n      for (int32_t ax9 = 0; ax9 < 11; ++ax9) {\n        auto_scheduler_layout_transform[(((ax4 * 55) + (ax8 * 11)) + ax9)] = ph_6[(((ax4 * 55) + (ax9 * 5)) + ax8)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_i1_outer_fused_i2_outer_fused = 0; i0_outer_i1_outer_fused_i2_outer_fused < 5; ++i0_outer_i1_outer_fused_i2_outer_fused) {\n    float T_batch_matmul_NN[90];\n    for (int32_t b_outer_outer_inner = 0; b_outer_outer_inner < 6; ++b_outer_outer_inner) {\n      for (int32_t i_outer_outer_inner = 0; i_outer_outer_inner < 3; ++i_outer_outer_inner) {\n        for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n          T_batch_matmul_NN[(((b_outer_outer_inner * 15) + (i_outer_outer_inner * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n        }\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t k_inner = 0; k_inner < 11; ++k_inner) {\n            T_batch_matmul_NN[(((b_outer_outer_inner * 15) + (i_outer_outer_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_outer_outer_inner * 15) + (i_outer_outer_inner * 5)) + j_outer_inner)] + (ph_0[((((b_outer_outer_inner * 165) + (i0_outer_i1_outer_fused_i2_outer_fused * 33)) + (i_outer_outer_inner * 11)) + k_inner)] * auto_scheduler_layout_transform[(((b_outer_outer_inner * 55) + (j_outer_inner * 11)) + k_inner)]));\n          }\n        }\n      }\n    }\n    for (int32_t i0_inner = 0; i0_inner < 6; ++i0_inner) {\n      for (int32_t i1_inner = 0; i1_inner < 3; ++i1_inner) {\n        for (int32_t i2_inner_s = 0; i2_inner_s < 5; ++i2_inner_s) {\n          compute_3[((((i0_inner * 75) + (i0_outer_i1_outer_fused_i2_outer_fused * 15)) + (i1_inner * 5)) + i2_inner_s)] = asinhf(T_batch_matmul_NN[(((i0_inner * 15) + (i1_inner * 5)) + i2_inner_s)]);\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN[1];\n  __shared__ float ph_6_shared[1];\n  T_batch_matmul_NN[0] = 0.000000e+00f;\n  for (int k_outer_outer = 0; k_outer_outer < 8; ++k_outer_outer) {\n    __syncthreads();\n    ph_6_shared[0] = ph_6[((k_outer_outer * 5) + (((int)blockIdx.x) % 5))];\n    __syncthreads();\n    T_batch_matmul_NN[0] = (T_batch_matmul_NN[0] + (ph_0[(((((int)blockIdx.x) / 5) * 8) + k_outer_outer)] * ph_6_shared[0]));\n  }\n  compute[((int)blockIdx.x)] = asinhf(T_batch_matmul_NN[0]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 15, 11), \"float32\"), ph_6: T.Buffer((6, 11, 5), \"float32\"), compute: T.Buffer((6, 15, 11), \"float32\"), compute_1: T.Buffer((6, 15, 11), \"float32\"), compute_2: T.Buffer((6, 15, 11), \"float32\"), compute_3: T.Buffer((6, 15, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([330], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((990,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(990):\n            compute_4 = T.Buffer((990,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(990):\n            compute_4 = T.Buffer((990,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(990):\n            compute_4 = T.Buffer((990,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((330,), data=auto_scheduler_layout_transform)\n        for ax4, ax8, ax9 in T.grid(6, 5, 11):\n            cse_var_1: T.int32 = ax4 * 55\n            ph_6_1 = T.Buffer((330,), data=ph_6.data)\n            auto_scheduler_layout_transform_1[cse_var_1 + ax8 * 11 + ax9] = ph_6_1[cse_var_1 + ax9 * 5 + ax8]\n        for i0_outer_i1_outer_fused_i2_outer_fused in T.parallel(5):\n            T_batch_matmul_NN = T.allocate([90], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((90,), data=T_batch_matmul_NN)\n            for b_outer_outer_inner, i_outer_outer_inner in T.grid(6, 3):\n                for j_outer_inner_init in range(5):\n                    T_batch_matmul_NN_1[b_outer_outer_inner * 15 + i_outer_outer_inner * 5 + j_outer_inner_init] = T.float32(0)\n                for j_outer_inner, k_inner in T.grid(5, 11):\n                    cse_var_2: T.int32 = b_outer_outer_inner * 15 + i_outer_outer_inner * 5 + j_outer_inner\n                    T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[b_outer_outer_inner * 165 + i0_outer_i1_outer_fused_i2_outer_fused * 33 + i_outer_outer_inner * 11 + k_inner] * auto_scheduler_layout_transform_1[b_outer_outer_inner * 55 + j_outer_inner * 11 + k_inner]\n            for i0_inner, i1_inner, i2_inner_s in T.grid(6, 3, 5):\n                cse_var_3: T.int32 = i1_inner * 5\n                compute_4 = T.Buffer((450,), data=compute_3.data)\n                compute_4[i0_inner * 75 + i0_outer_i1_outer_fused_i2_outer_fused * 15 + cse_var_3 + i2_inner_s] = T.asinh(T_batch_matmul_NN_1[i0_inner * 15 + cse_var_3 + i2_inner_s])",
        "op_args": [
            [
                "acosh",
                "cos",
                "atanh",
                "cos",
                "batch_matmul",
                "asinh"
            ]
        ],
        "input_shape": [[6, 15, 11], [6, 11, 5]],
        "output_shape": [[6, 15, 11], [6, 15, 11], [6, 15, 11], [6, 15, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 245; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 245; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] / ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 35) + (i1 * 5)) + i2)] = fabsf(ph_0[(((i0 * 35) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 7, 5), \"float32\"), ph_3: T.Buffer((7, 7, 5), \"float32\"), T_subtract: T.Buffer((7, 7, 5), \"float32\"), T_divide: T.Buffer((7, 7, 5), \"float32\"), compute: T.Buffer((7, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((245,), data=ph_0.data)\n        ph_3_1 = T.Buffer((245,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(245):\n            T_subtract_1 = T.Buffer((245,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(245):\n            T_divide_1 = T.Buffer((245,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(7, 5):\n                cse_var_1: T.int32 = i0 * 35 + i1 * 5 + i2\n                compute_1 = T.Buffer((245,), data=compute.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "subtract",
                "divide",
                "abs"
            ]
        ],
        "input_shape": [[7, 7, 5], [16, 7, 17], [7, 7, 5]],
        "output_shape": [[7, 7, 5], [16, 7, 17], [7, 7, 5], [7, 7, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* T_subtract_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 104; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - asinhf((ph_0[ax0_ax1_fused_ax2_fused] / atanf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 52) + (i1 * 13)) + i2)] = acoshf(asinhf(ph_0[(((i0 * 52) + (i1 * 13)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 8; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_subtract_1[((ax0_ax1_fused * 13) + ax2)] = (asinhf(ph_0[((ax0_ax1_fused * 13) + ax2)]) - ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - asinhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 4, 13), \"float32\"), T_subtract: T.Buffer((2, 4, 13), \"float32\"), compute: T.Buffer((2, 4, 13), \"float32\"), T_subtract_1: T.Buffer((2, 4, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((104,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(104):\n            T_subtract_2 = T.Buffer((104,), data=T_subtract.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused] / T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(4, 13):\n                cse_var_1: T.int32 = i0 * 52 + i1 * 13 + i2\n                compute_1 = T.Buffer((104,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(T.asinh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused in T.parallel(8):\n            for ax2 in range(13):\n                cse_var_2: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_subtract_2 = T.Buffer((104,), data=T_subtract_1.data)\n                T_subtract_2[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "atan",
                "divide",
                "asinh",
                "subtract",
                "asinh",
                "acosh",
                "subtract"
            ]
        ],
        "input_shape": [[2, 4, 13]],
        "output_shape": [[2, 4, 13], [2, 4, 13], [2, 4, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 21) + (i1 * 7)) + i2)] = atanhf((ph_0[(((i0 * 21) + (i1 * 7)) + i2)] / atanf(ph_0[(((i0 * 21) + (i1 * 7)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 126; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 126; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 3, 7), \"float32\"), compute: T.Buffer((6, 3, 7), \"float32\"), compute_1: T.Buffer((6, 3, 7), \"float32\"), compute_2: T.Buffer((6, 3, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((126,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(3, 7):\n                cse_var_1: T.int32 = i0 * 21 + i1 * 7 + i2\n                compute_3 = T.Buffer((126,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1] / T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_3 = T.Buffer((126,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_3 = T.Buffer((126,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "divide",
                "atanh",
                "ceil",
                "cos"
            ]
        ],
        "input_shape": [[6, 3, 7]],
        "output_shape": [[6, 3, 7], [6, 3, 7], [6, 3, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 28) + (i1 * 2)) + i2)] = ceilf(fmodf(ph_0[(((i0 * 28) + (i1 * 2)) + i2)], fabsf(ph_0[(((i0 * 28) + (i1 * 2)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 14; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_1[(((i0_1 * 28) + (i1_1 * 2)) + i2_1)] = atanf(ph_0[(((i0_1 * 28) + (i1_1 * 2)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 448; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 14, 2), \"float32\"), compute: T.Buffer((16, 14, 2), \"float32\"), compute_1: T.Buffer((16, 14, 2), \"float32\"), compute_2: T.Buffer((16, 14, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((448,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(14, 2):\n                cse_var_1: T.int32 = i0 * 28 + i1 * 2 + i2\n                compute_3 = T.Buffer((448,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(T.truncmod(ph_0_1[cse_var_1], T.fabs(ph_0_1[cse_var_1])))\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(14, 2):\n                cse_var_2: T.int32 = i0 * 28 + i1 * 2 + i2\n                compute_3 = T.Buffer((448,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(448):\n            compute_3 = T.Buffer((448,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "mod",
                "ceil",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[16, 14, 2]],
        "output_shape": [[16, 14, 2], [16, 14, 2], [16, 14, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 192; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 192; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 12) + (i1 * 4)) + i2)] = asinf(atanhf(ph_0[(((i0 * 12) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 3, 4), \"float32\"), ph_3: T.Buffer((16, 3, 4), \"float32\"), T_subtract: T.Buffer((16, 3, 4), \"float32\"), compute: T.Buffer((16, 3, 4), \"float32\"), compute_1: T.Buffer((16, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((192,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(192):\n            T_subtract_1 = T.Buffer((192,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((192,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(192):\n            compute_2 = T.Buffer((192,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(3, 4):\n                cse_var_1: T.int32 = i0 * 12 + i1 * 4 + i2\n                compute_2 = T.Buffer((192,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asin(T.atanh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "subtract",
                "atanh",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[16, 3, 4], [13, 2, 17], [16, 3, 4]],
        "output_shape": [[16, 3, 4], [13, 2, 17], [16, 3, 4], [16, 3, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 12) + (i1 * 4)) + i2)] = ceilf(ph_0[(((i0 * 12) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 18; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_mod[(((ax0 * 12) + (ax1 * 4)) + ax2)] = fmodf(ph_0[(((ax0 * 12) + (ax1 * 4)) + ax2)], cosf(asinf(ph_0[(((ax0 * 12) + (ax1 * 4)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 216; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], __cosf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 3, 4), \"float32\"), compute: T.Buffer((18, 3, 4), \"float32\"), T_mod: T.Buffer((18, 3, 4), \"float32\"), compute_1: T.Buffer((18, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((216,), data=ph_0.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(3, 4):\n                cse_var_1: T.int32 = i0 * 12 + i1 * 4 + i2\n                compute_2 = T.Buffer((216,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(18):\n            for ax1, ax2 in T.grid(3, 4):\n                cse_var_2: T.int32 = ax0 * 12 + ax1 * 4 + ax2\n                T_mod_1 = T.Buffer((216,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], T.cos(T.asin(ph_0_1[cse_var_2])))\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_2 = T.Buffer((216,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "asin",
                "cos",
                "mod",
                "atan"
            ]
        ],
        "input_shape": [[18, 3, 4]],
        "output_shape": [[18, 3, 4], [18, 3, 4], [18, 3, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 13; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      T_add[((ax0_ax1_fused * 6) + ax2)] = (ph_0[((ax0_ax1_fused * 6) + ax2)] + ph_3[((ax0_ax1_fused * 6) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 13; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = cosf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 78; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 6), \"float32\"), ph_3: T.Buffer((13, 1, 6), \"float32\"), T_add: T.Buffer((13, 1, 6), \"float32\"), compute: T.Buffer((13, 1, 6), \"float32\"), compute_1: T.Buffer((13, 1, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((78,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(13):\n            for ax2 in range(6):\n                cse_var_1: T.int32 = ax0_ax1_fused * 6 + ax2\n                T_add_1 = T.Buffer((78,), data=T_add.data)\n                ph_3_1 = T.Buffer((78,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(13):\n            for i2 in range(6):\n                cse_var_2: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((78,), data=compute.data)\n                compute_2[cse_var_2] = T.cos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(78):\n            compute_2 = T.Buffer((78,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "cos",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[13, 1, 6], [3, 6, 19], [13, 1, 6]],
        "output_shape": [[13, 1, 6], [3, 6, 19], [13, 1, 6], [13, 1, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 98) + (i1 * 14)) + i2)] = acoshf(ph_0[(((i0 * 98) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1666; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1666; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1666; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused_1], ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1666; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = fabsf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 7, 14), \"float32\"), ph_3: T.Buffer((17, 7, 14), \"float32\"), compute: T.Buffer((17, 7, 14), \"float32\"), compute_1: T.Buffer((17, 7, 14), \"float32\"), T_divide: T.Buffer((17, 7, 14), \"float32\"), compute_2: T.Buffer((17, 7, 14), \"float32\"), compute_3: T.Buffer((17, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1666,), data=ph_0.data)\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(7, 14):\n                cse_var_1: T.int32 = i0 * 98 + i1 * 14 + i2\n                compute_4 = T.Buffer((1666,), data=compute.data)\n                compute_4[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1666):\n            compute_4 = T.Buffer((1666,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1666):\n            T_divide_1 = T.Buffer((1666,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        ph_3_1 = T.Buffer((1666,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(1666):\n            compute_4 = T.Buffer((1666,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1666):\n            compute_4 = T.Buffer((1666,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "acosh",
                "atanh",
                "abs",
                "divide",
                "acosh",
                "abs"
            ]
        ],
        "input_shape": [[17, 7, 14], [14, 3, 17], [17, 7, 14]],
        "output_shape": [[14, 3, 17], [17, 7, 14], [17, 7, 14], [17, 7, 14], [17, 7, 14], [17, 7, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3840; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 320; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 12) + ax2)] = (acoshf(ph_0[((ax0_ax1_fused * 12) + ax2)]) * ph_0[((ax0_ax1_fused * 12) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 320; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_1[((i0_i1_fused * 12) + i2)] = asinf((ph_0[((i0_i1_fused * 12) + i2)] * cosf(ph_0[((i0_i1_fused * 12) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 16, 12), \"float32\"), compute: T.Buffer((20, 16, 12), \"float32\"), T_multiply: T.Buffer((20, 16, 12), \"float32\"), compute_1: T.Buffer((20, 16, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3840,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3840):\n            compute_2 = T.Buffer((3840,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(320):\n            for ax2 in range(12):\n                cse_var_1: T.int32 = ax0_ax1_fused * 12 + ax2\n                T_multiply_1 = T.Buffer((3840,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(320):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((3840,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2] * T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acos",
                "acosh",
                "multiply",
                "cos",
                "multiply",
                "asin"
            ]
        ],
        "input_shape": [[20, 16, 12]],
        "output_shape": [[20, 16, 12], [20, 16, 12], [20, 16, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 380; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i0_i1_fused * 4) + i2)] = fabsf(ph_0[((i0_i1_fused * 4) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_1[(((i0 * 76) + (i1 * 4)) + i2_1)] = asinf(atanhf(ph_0[(((i0 * 76) + (i1 * 4)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1520; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1520; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 19, 4), \"float32\"), compute: T.Buffer((20, 19, 4), \"float32\"), compute_1: T.Buffer((20, 19, 4), \"float32\"), compute_2: T.Buffer((20, 19, 4), \"float32\"), compute_3: T.Buffer((20, 19, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1520,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(380):\n            for i2 in range(4):\n                cse_var_1: T.int32 = i0_i1_fused * 4 + i2\n                compute_4 = T.Buffer((1520,), data=compute.data)\n                compute_4[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(19, 4):\n                cse_var_2: T.int32 = i0 * 76 + i1 * 4 + i2\n                compute_4 = T.Buffer((1520,), data=compute_1.data)\n                compute_4[cse_var_2] = T.asin(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1520):\n            compute_4 = T.Buffer((1520,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1520):\n            compute_4 = T.Buffer((1520,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "atanh",
                "asin",
                "asinh",
                "sin"
            ]
        ],
        "input_shape": [[20, 19, 4]],
        "output_shape": [[20, 19, 4], [20, 19, 4], [20, 19, 4], [20, 19, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 121; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = expf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 726; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 726; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 11, 6), \"float32\"), compute: T.Buffer((11, 11, 6), \"float32\"), T_multiply: T.Buffer((11, 11, 6), \"float32\"), compute_1: T.Buffer((11, 11, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((726,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(121):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((726,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(726):\n            T_multiply_1 = T.Buffer((726,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(726):\n            compute_2 = T.Buffer((726,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "exp",
                "ceil",
                "multiply",
                "cos"
            ]
        ],
        "input_shape": [[11, 11, 6]],
        "output_shape": [[11, 11, 6], [11, 11, 6], [11, 11, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1008; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 7; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n        T_add[(((ax0 * 144) + (ax1 * 18)) + ax2)] = (ph_0[(((ax0 * 144) + (ax1 * 18)) + ax2)] + ceilf(ceilf(ph_0[(((ax0 * 144) + (ax1 * 18)) + ax2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1008; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ceilf(ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 8, 18), \"float32\"), compute: T.Buffer((7, 8, 18), \"float32\"), T_add: T.Buffer((7, 8, 18), \"float32\"), compute_1: T.Buffer((7, 8, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1008,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_2 = T.Buffer((1008,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(7):\n            for ax1, ax2 in T.grid(8, 18):\n                cse_var_1: T.int32 = ax0 * 144 + ax1 * 18 + ax2\n                T_add_1 = T.Buffer((1008,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.ceil(T.ceil(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1008):\n            compute_2 = T.Buffer((1008,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "ceil",
                "ceil",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[7, 8, 18]],
        "output_shape": [[7, 8, 18], [7, 8, 18], [7, 8, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 260) + (i1 * 20)) + i2)] = asinf(ph_0[(((i0 * 260) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2080; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2080; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2080; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 13, 20), \"float32\"), compute: T.Buffer((8, 13, 20), \"float32\"), T_multiply: T.Buffer((8, 13, 20), \"float32\"), compute_1: T.Buffer((8, 13, 20), \"float32\"), compute_2: T.Buffer((8, 13, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2080,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(13, 20):\n                cse_var_1: T.int32 = i0 * 260 + i1 * 20 + i2\n                compute_3 = T.Buffer((2080,), data=compute.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2080):\n            T_multiply_1 = T.Buffer((2080,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2080):\n            compute_3 = T.Buffer((2080,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2080):\n            compute_3 = T.Buffer((2080,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "asin",
                "cos",
                "multiply",
                "sin",
                "ceil"
            ]
        ],
        "input_shape": [[8, 13, 20]],
        "output_shape": [[8, 13, 20], [8, 13, 20], [8, 13, 20], [8, 13, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 117) + (i1 * 9)) + i2)] = atanhf(ph_0[(((i0 * 117) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 130; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_1[((i0_i1_fused * 9) + i2_1)] = asinf(atanf(ph_0[((i0_i1_fused * 9) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1170; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1170; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(cosf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1170; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinf(atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 13, 9), \"float32\"), compute: T.Buffer((10, 13, 9), \"float32\"), compute_1: T.Buffer((10, 13, 9), \"float32\"), compute_2: T.Buffer((10, 13, 9), \"float32\"), T_mod: T.Buffer((10, 13, 9), \"float32\"), compute_3: T.Buffer((10, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(13, 9):\n                cse_var_1: T.int32 = i0 * 117 + i1 * 9 + i2\n                compute_4 = T.Buffer((1170,), data=compute.data)\n                compute_4[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(130):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_4 = T.Buffer((1170,), data=compute_1.data)\n                compute_4[cse_var_2] = T.asin(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_4 = T.Buffer((1170,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1170):\n            T_mod_1 = T.Buffer((1170,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1170):\n            compute_4 = T.Buffer((1170,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "atan",
                "asin",
                "cos",
                "cos",
                "mod",
                "atan"
            ]
        ],
        "input_shape": [[10, 13, 9]],
        "output_shape": [[10, 13, 9], [10, 13, 9], [10, 13, 9], [10, 13, 9], [10, 13, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1078; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1078; ++ax0_ax1_fused_ax2_fused) {\n    float compute_1[1];\n    compute_1[0] = expf(fabsf(ph_0[ax0_ax1_fused_ax2_fused]));\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / (ph_0[ax0_ax1_fused_ax2_fused] / compute_1[0]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] / (ph_0[((int)blockIdx.x)] / __expf(fabsf(ph_0[((int)blockIdx.x)]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(20) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 7, 14), \"float32\"), compute: T.Buffer((11, 7, 14), \"float32\"), T_divide: T.Buffer((11, 7, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1078,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1078):\n            compute_1 = T.Buffer((1078,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1078):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.exp(T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]))\n            T_divide_1 = T.Buffer((1078,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / (ph_0_1[ax0_ax1_fused_ax2_fused] / compute_2[0])",
        "op_args": [
            [
                "atan",
                "abs",
                "exp",
                "divide",
                "divide"
            ]
        ],
        "input_shape": [[11, 7, 14]],
        "output_shape": [[11, 7, 14], [11, 7, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 816; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 816; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 272; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_1[((i0_i1_fused * 3) + i2)] = fabsf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 17, 3), \"float32\"), ph_3: T.Buffer((16, 17, 3), \"float32\"), T_divide: T.Buffer((16, 17, 3), \"float32\"), compute: T.Buffer((16, 17, 3), \"float32\"), compute_1: T.Buffer((16, 17, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((816,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(816):\n            T_divide_1 = T.Buffer((816,), data=T_divide.data)\n            ph_3_1 = T.Buffer((816,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(816):\n            compute_2 = T.Buffer((816,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(272):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((816,), data=compute_1.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "divide",
                "sin",
                "abs"
            ]
        ],
        "input_shape": [[16, 17, 3], [7, 13, 16], [16, 17, 3]],
        "output_shape": [[16, 17, 3], [7, 13, 16], [16, 17, 3], [16, 17, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 220) + (i1 * 20)) + i2)] = asinf(ph_0[(((i0 * 220) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    float compute_1[220];\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n        compute_1[((i1_1 * 20) + i2_1)] = expf(ph_0[(((ax0 * 220) + (i1_1 * 20)) + i2_1)]);\n      }\n    }\n    for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_mod[(((ax0 * 220) + (ax1 * 20)) + ax2)] = fmodf(compute_1[((ax1 * 20) + ax2)], ph_0[(((ax0 * 220) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 11, 20), \"float32\"), compute: T.Buffer((19, 11, 20), \"float32\"), T_mod: T.Buffer((19, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4180,), data=ph_0.data)\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(11, 20):\n                cse_var_1: T.int32 = i0 * 220 + i1 * 20 + i2\n                compute_1 = T.Buffer((4180,), data=compute.data)\n                compute_1[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(19):\n            compute_1 = T.allocate([220], \"float32\", \"global\")\n            compute_2 = T.Buffer((220,), data=compute_1)\n            for i1, i2 in T.grid(11, 20):\n                cse_var_2: T.int32 = i1 * 20\n                compute_2[cse_var_2 + i2] = T.exp(ph_0_1[ax0 * 220 + cse_var_2 + i2])\n            for ax1, ax2 in T.grid(11, 20):\n                cse_var_4: T.int32 = ax1 * 20\n                cse_var_3: T.int32 = ax0 * 220 + cse_var_4 + ax2\n                T_mod_1 = T.Buffer((4180,), data=T_mod.data)\n                T_mod_1[cse_var_3] = T.truncmod(compute_2[cse_var_4 + ax2], ph_0_1[cse_var_3])",
        "op_args": [
            [
                "asin",
                "exp",
                "mod"
            ]
        ],
        "input_shape": [[19, 11, 20]],
        "output_shape": [[19, 11, 20], [19, 11, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3, float* ph_9) {\n  float auto_scheduler_layout_transform[30];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 102; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 102; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 102; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(atanf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n      for (int32_t ax12 = 0; ax12 < 3; ++ax12) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused * 6) + (ax8 * 3)) + ax12)] = ph_3[(((ax12 * 10) + (ax8 * 5)) + ax0_ax1_fused_ax2_fused)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused = 0; ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused < 5; ++ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused) {\n    float T_batch_matmul_NN[51];\n    for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 17; ++i_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 3; ++b_inner_init) {\n        T_batch_matmul_NN[((b_inner_init * 17) + i_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 17; ++i_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n          T_batch_matmul_NN[((b_inner * 17) + i_outer_inner)] = (T_batch_matmul_NN[((b_inner * 17) + i_outer_inner)] + (ph_0[(((b_inner * 34) + (i_outer_inner * 2)) + k_outer)] * auto_scheduler_layout_transform[(((ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused * 6) + (k_outer * 3)) + b_inner)]));\n        }\n      }\n    }\n    for (int32_t ax0_inner = 0; ax0_inner < 3; ++ax0_inner) {\n      for (int32_t ax1_inner = 0; ax1_inner < 17; ++ax1_inner) {\n        T_subtract[(((ax0_inner * 85) + (ax1_inner * 5)) + ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused)] = (T_batch_matmul_NN[((ax0_inner * 17) + ax1_inner)] - ph_9[(((ax0_inner * 85) + (ax1_inner * 5)) + ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[8];\n  __shared__ float ph_3_shared[40];\n  for (int i_inner_init = 0; i_inner_init < 4; ++i_inner_init) {\n    T_batch_matmul_NN[i_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN[(i_inner_init + 4)] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 5; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 8) + ((int)threadIdx.x))] = ph_3[(((ax0_ax1_fused_ax2_fused_outer_outer * 40) + (((int)threadIdx.x) * 5)) + ((int)blockIdx.x))];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int i_inner = 0; i_inner < 4; ++i_inner) {\n      T_batch_matmul_NN[i_inner] = (T_batch_matmul_NN[i_inner] + (ph_0[(((((int)threadIdx.x) * 40) + (i_inner * 5)) + k_outer_inner)] * ph_3_shared[((((int)threadIdx.x) * 5) + k_outer_inner)]));\n      T_batch_matmul_NN[(i_inner + 4)] = (T_batch_matmul_NN[(i_inner + 4)] + (ph_0[((((((int)threadIdx.x) * 40) + (i_inner * 5)) + k_outer_inner) + 20)] * ph_3_shared[((((int)threadIdx.x) * 5) + k_outer_inner)]));\n    }\n  }\n  for (int ax1_inner = 0; ax1_inner < 4; ++ax1_inner) {\n    T_subtract[(((((int)threadIdx.x) * 40) + (ax1_inner * 5)) + ((int)blockIdx.x))] = (T_batch_matmul_NN[ax1_inner] - ph_0[(((((int)threadIdx.x) * 40) + (ax1_inner * 5)) + ((int)blockIdx.x))]);\n    T_subtract[((((((int)threadIdx.x) * 40) + (ax1_inner * 5)) + ((int)blockIdx.x)) + 20)] = (T_batch_matmul_NN[(ax1_inner + 4)] - ph_0[((((((int)threadIdx.x) * 40) + (ax1_inner * 5)) + ((int)blockIdx.x)) + 20)]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 17, 2), \"float32\"), ph_3: T.Buffer((3, 2, 5), \"float32\"), ph_9: T.Buffer((3, 17, 5), \"float32\"), compute: T.Buffer((3, 17, 2), \"float32\"), compute_1: T.Buffer((3, 17, 2), \"float32\"), compute_2: T.Buffer((3, 17, 2), \"float32\"), T_subtract: T.Buffer((3, 17, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([30], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((102,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_3 = T.Buffer((102,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_3 = T.Buffer((102,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(102):\n            compute_3 = T.Buffer((102,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((30,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax8, ax12 in T.grid(2, 3):\n                ph_3_1 = T.Buffer((30,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[ax0_ax1_fused_ax2_fused * 6 + ax8 * 3 + ax12] = ph_3_1[ax12 * 10 + ax8 * 5 + ax0_ax1_fused_ax2_fused]\n        for ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused in T.parallel(5):\n            T_batch_matmul_NN = T.allocate([51], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((51,), data=T_batch_matmul_NN)\n            for i_outer_inner_init, b_inner_init in T.grid(17, 3):\n                T_batch_matmul_NN_1[b_inner_init * 17 + i_outer_inner_init] = T.float32(0)\n            for k_outer, i_outer_inner, b_inner in T.grid(2, 17, 3):\n                cse_var_1: T.int32 = b_inner * 17 + i_outer_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 34 + i_outer_inner * 2 + k_outer] * auto_scheduler_layout_transform_1[ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused * 6 + k_outer * 3 + b_inner]\n            for ax0_inner, ax1_inner in T.grid(3, 17):\n                cse_var_2: T.int32 = ax0_inner * 85 + ax1_inner * 5 + ax0_outer_outer_ax1_outer_outer_fused_ax2_outer_outer_fused_ax0_outer_inner_fused_ax1_outer_inner_fused_ax2_outer_inner_fused\n                T_subtract_1 = T.Buffer((255,), data=T_subtract.data)\n                ph_9_1 = T.Buffer((255,), data=ph_9.data)\n                T_subtract_1[cse_var_2] = T_batch_matmul_NN_1[ax0_inner * 17 + ax1_inner] - ph_9_1[cse_var_2]",
        "op_args": [
            [
                "batch_matmul",
                "acosh",
                "atan",
                "atanh",
                "exp",
                "subtract"
            ]
        ],
        "input_shape": [[3, 17, 2], [16, 20, 12], [3, 2, 5], [3, 17, 5]],
        "output_shape": [[16, 20, 12], [3, 17, 2], [3, 17, 2], [3, 17, 2], [3, 17, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_subtract, float* ph_0, float* ph_3, float* ph_5) {\n  float auto_scheduler_layout_transform[180];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 396; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n    for (int32_t ax7 = 0; ax7 < 4; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 9; ++ax8) {\n        auto_scheduler_layout_transform[(((ax6 * 36) + (ax7 * 9)) + ax8)] = ph_5[(((ax8 * 20) + (ax7 * 5)) + ax6)];\n      }\n    }\n  }\n  for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 11; ++i_outer_inner_init) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 9; ++b_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 55) + (i_outer_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t i_outer_inner = 0; i_outer_inner < 11; ++i_outer_inner) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 4; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 9; ++b_inner) {\n          T_batch_matmul_NN[(((b_inner * 55) + (i_outer_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_inner * 55) + (i_outer_inner * 5)) + j_outer_inner)] + (ph_0[(((b_inner * 44) + (i_outer_inner * 4)) + k_inner)] * auto_scheduler_layout_transform[(((j_outer_inner * 36) + (k_inner * 9)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_5_shared[32];\n  T_batch_matmul_NN_local[0] = 0.000000e+00f;\n  T_batch_matmul_NN_local[1] = 0.000000e+00f;\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) >> 2)) < 8) {\n      ph_5_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 20) + ((int)threadIdx.x))] = ph_5[(((ax0_ax1_fused_ax2_fused_outer_outer * 100) + (((int)threadIdx.x) * 5)) + ((int)blockIdx.x))];\n    }\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 4; ++k_inner) {\n    T_batch_matmul_NN_local[0] = (T_batch_matmul_NN_local[0] + (ph_0[((((int)threadIdx.x) * 4) + k_inner)] * ph_5_shared[(((((int)threadIdx.x) / 5) * 4) + k_inner)]));\n    T_batch_matmul_NN_local[1] = (T_batch_matmul_NN_local[1] + (ph_0[(((((int)threadIdx.x) * 4) + k_inner) + 80)] * ph_5_shared[((((((int)threadIdx.x) / 5) * 4) + k_inner) + 16)]));\n  }\n  T_batch_matmul_NN[((((int)threadIdx.x) * 5) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[0];\n  T_batch_matmul_NN[(((((int)threadIdx.x) * 5) + ((int)blockIdx.x)) + 100)] = T_batch_matmul_NN_local[1];\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 11, 4), \"float32\"), ph_3: T.Buffer((9, 11, 4), \"float32\"), ph_5: T.Buffer((9, 4, 5), \"float32\"), T_subtract: T.Buffer((9, 11, 4), \"float32\"), T_batch_matmul_NN: T.Buffer((9, 11, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([180], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((396,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(396):\n            T_subtract_1 = T.Buffer((396,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((396,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((180,), data=auto_scheduler_layout_transform)\n        for ax6, ax7, ax8 in T.grid(5, 4, 9):\n            ph_5_1 = T.Buffer((180,), data=ph_5.data)\n            auto_scheduler_layout_transform_1[ax6 * 36 + ax7 * 9 + ax8] = ph_5_1[ax8 * 20 + ax7 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((495,), data=T_batch_matmul_NN.data)\n        for i_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(11, 5, 9):\n            T_batch_matmul_NN_1[b_inner_init * 55 + i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n        for i_outer_inner, j_outer_inner, k_inner, b_inner in T.grid(11, 5, 4, 9):\n            cse_var_1: T.int32 = b_inner * 55 + i_outer_inner * 5 + j_outer_inner\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 44 + i_outer_inner * 4 + k_inner] * auto_scheduler_layout_transform_1[j_outer_inner * 36 + k_inner * 9 + b_inner]",
        "op_args": [
            [
                "subtract",
                "batch_matmul"
            ]
        ],
        "input_shape": [[9, 11, 4], [13, 9, 10], [9, 11, 4], [9, 4, 5]],
        "output_shape": [[9, 11, 4], [13, 9, 10], [9, 11, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = ceilf((ph_0[((i0_i1_fused * 18) + i2)] / cosf(ph_0[((i0_i1_fused * 18) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 108; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] / cosf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 6; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 18) + i2_1)] = expf((ph_0[((i0_i1_fused_1 * 18) + i2_1)] / ph_3[((i0_i1_fused_1 * 18) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((int)blockIdx.x)] = __expf((ph_0[((int)blockIdx.x)] / ph_3[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 3, 18), \"float32\"), ph_3: T.Buffer((2, 3, 18), \"float32\"), compute: T.Buffer((2, 3, 18), \"float32\"), T_mod: T.Buffer((2, 3, 18), \"float32\"), compute_1: T.Buffer((2, 3, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((108,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((108,), data=compute.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1] / T.cos(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(108):\n            T_mod_1 = T.Buffer((108,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] / T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(18):\n                cse_var_2: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((108,), data=compute_1.data)\n                ph_3_1 = T.Buffer((108,), data=ph_3.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2] / ph_3_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "cos",
                "divide",
                "ceil",
                "mod",
                "exp"
            ]
        ],
        "input_shape": [[2, 3, 18], [8, 18, 15], [2, 3, 18]],
        "output_shape": [[8, 18, 15], [2, 3, 18], [2, 3, 18], [2, 3, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 462; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 462; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 462; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = cosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanhf(asinhf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 11, 14), \"float32\"), compute: T.Buffer((3, 11, 14), \"float32\"), compute_1: T.Buffer((3, 11, 14), \"float32\"), compute_2: T.Buffer((3, 11, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((462,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_3 = T.Buffer((462,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_3 = T.Buffer((462,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(462):\n            compute_3 = T.Buffer((462,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "asinh",
                "atanh",
                "cos"
            ]
        ],
        "input_shape": [[3, 11, 14]],
        "output_shape": [[3, 11, 14], [3, 11, 14], [3, 11, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1859; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_1[(((i0 * 143) + (i1 * 11)) + i2)] = sinf(sinf(ph_0[(((i0 * 143) + (i1 * 11)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 169; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n      compute_2[((i0_i1_fused * 11) + i2_1)] = atanf(sinf(ph_0[((i0_i1_fused * 11) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1859; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(__sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 13, 11), \"float32\"), ph_3: T.Buffer((13, 13, 11), \"float32\"), compute: T.Buffer((13, 13, 11), \"float32\"), compute_1: T.Buffer((13, 13, 11), \"float32\"), compute_2: T.Buffer((13, 13, 11), \"float32\"), T_multiply: T.Buffer((13, 13, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1859,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1859):\n            compute_3 = T.Buffer((1859,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(13, 11):\n                cse_var_1: T.int32 = i0 * 143 + i1 * 11 + i2\n                compute_3 = T.Buffer((1859,), data=compute_1.data)\n                compute_3[cse_var_1] = T.sin(T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(169):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((1859,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(T.sin(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1859):\n            T_multiply_1 = T.Buffer((1859,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((1859,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "acosh",
                "sin",
                "sin",
                "atan",
                "multiply"
            ]
        ],
        "input_shape": [[13, 13, 11], [12, 5, 8], [13, 13, 11]],
        "output_shape": [[12, 5, 8], [13, 13, 11], [13, 13, 11], [13, 13, 11], [13, 13, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2720; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2720; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_1[((i0_i1_fused * 17) + i2)] = sinf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 20, 17), \"float32\"), ph_3: T.Buffer((8, 20, 17), \"float32\"), T_add: T.Buffer((8, 20, 17), \"float32\"), compute: T.Buffer((8, 20, 17), \"float32\"), compute_1: T.Buffer((8, 20, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2720,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2720):\n            T_add_1 = T.Buffer((2720,), data=T_add.data)\n            ph_3_1 = T.Buffer((2720,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2720):\n            compute_2 = T.Buffer((2720,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_2 = T.Buffer((2720,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "add",
                "acosh",
                "sin"
            ]
        ],
        "input_shape": [[8, 20, 17], [7, 15, 17], [8, 20, 17]],
        "output_shape": [[8, 20, 17], [7, 15, 17], [8, 20, 17], [8, 20, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 272; ++i0_i1_fused_i2_fused) {\n    float compute_2[1];\n    compute_2[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute[i0_i1_fused_i2_fused] = asinhf((ph_0[i0_i1_fused_i2_fused] / compute_2[0]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 17; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = asinhf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 17, 16), \"float32\"), compute: T.Buffer((1, 17, 16), \"float32\"), compute_1: T.Buffer((1, 17, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((272,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(272):\n            compute_2 = T.allocate([1], \"float32\", \"global\")\n            compute_3 = T.Buffer((1,), data=compute_2, align=4)\n            compute_3[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_4 = T.Buffer((272,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] / compute_3[0])\n        for i0_i1_fused in T.parallel(17):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((272,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "exp",
                "divide",
                "asinh",
                "asinh"
            ]
        ],
        "input_shape": [[1, 17, 16]],
        "output_shape": [[1, 17, 16], [1, 17, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 252) + (i1 * 18)) + i2)] = acoshf(acoshf(ph_0[(((i0 * 252) + (i1 * 18)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 18; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 14; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_1[(((i0_1 * 252) + (i1_1 * 18)) + i2_1)] = expf(acoshf(ph_0[(((i0_1 * 252) + (i1_1 * 18)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4536; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4536; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = acosf((ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 14, 18), \"float32\"), ph_3: T.Buffer((18, 14, 18), \"float32\"), T_subtract: T.Buffer((18, 14, 18), \"float32\"), compute: T.Buffer((18, 14, 18), \"float32\"), compute_1: T.Buffer((18, 14, 18), \"float32\"), compute_2: T.Buffer((18, 14, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4536,), data=ph_0.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(14, 18):\n                cse_var_1: T.int32 = i0 * 252 + i1 * 18 + i2\n                compute_3 = T.Buffer((4536,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(T.acosh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(14, 18):\n                cse_var_2: T.int32 = i0 * 252 + i1 * 18 + i2\n                compute_3 = T.Buffer((4536,), data=compute_1.data)\n                compute_3[cse_var_2] = T.exp(T.acosh(ph_0_1[cse_var_2]))\n        ph_3_1 = T.Buffer((4536,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4536):\n            T_subtract_1 = T.Buffer((4536,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(4536):\n            compute_3 = T.Buffer((4536,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "subtract",
                "acosh",
                "acosh",
                "exp",
                "acos"
            ]
        ],
        "input_shape": [[18, 14, 18], [10, 4, 7], [18, 14, 18]],
        "output_shape": [[10, 4, 7], [18, 14, 18], [18, 14, 18], [18, 14, 18], [18, 14, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute[(((i0 * 176) + (i1 * 11)) + i2)] = asinf(ph_0[(((i0 * 176) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2112; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], sinf(acosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2112; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 192; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n      compute_2[((i0_i1_fused * 11) + i2_1)] = acoshf(asinf(ph_0[((i0_i1_fused * 11) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __sinf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 16, 11), \"float32\"), compute: T.Buffer((12, 16, 11), \"float32\"), T_mod: T.Buffer((12, 16, 11), \"float32\"), compute_1: T.Buffer((12, 16, 11), \"float32\"), compute_2: T.Buffer((12, 16, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2112,), data=ph_0.data)\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(16, 11):\n                cse_var_1: T.int32 = i0 * 176 + i1 * 11 + i2\n                compute_3 = T.Buffer((2112,), data=compute.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2112):\n            T_mod_1 = T.Buffer((2112,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.sin(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(2112):\n            compute_3 = T.Buffer((2112,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(192):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((2112,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acosh(T.asin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "asin",
                "acos",
                "sin",
                "mod",
                "asin",
                "exp",
                "acosh"
            ]
        ],
        "input_shape": [[12, 16, 11]],
        "output_shape": [[12, 16, 11], [12, 16, 11], [12, 16, 11], [12, 16, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 25; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 5; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = ceilf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 5, 5), \"float32\"), compute: T.Buffer((1, 5, 5), \"float32\"), compute_1: T.Buffer((1, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((25,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(25):\n            compute_2 = T.Buffer((25,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(5):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((25,), data=compute_1.data)\n                compute_2[cse_var_1] = T.ceil(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asin",
                "ceil"
            ]
        ],
        "input_shape": [[1, 5, 5]],
        "output_shape": [[1, 5, 5], [1, 5, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 616; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 77; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_1[((i0_i1_fused * 8) + i2)] = fabsf(asinf(ph_0[((i0_i1_fused * 8) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 616; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 77; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_3[((i0_i1_fused_1 * 8) + i2_1)] = atanf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 8), \"float32\"), compute: T.Buffer((7, 11, 8), \"float32\"), compute_1: T.Buffer((7, 11, 8), \"float32\"), compute_2: T.Buffer((7, 11, 8), \"float32\"), compute_3: T.Buffer((7, 11, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((616,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(616):\n            compute_4 = T.Buffer((616,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(77):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((616,), data=compute_1.data)\n                compute_4[cse_var_1] = T.fabs(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(616):\n            compute_4 = T.Buffer((616,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(77):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((616,), data=compute_3.data)\n                compute_4[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "asin",
                "abs",
                "asin",
                "atan"
            ]
        ],
        "input_shape": [[7, 11, 8]],
        "output_shape": [[7, 11, 8], [7, 11, 8], [7, 11, 8], [7, 11, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2772; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2772; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 252; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_1[((i0_i1_fused * 11) + i2)] = ceilf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2772; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 18, 11), \"float32\"), compute: T.Buffer((14, 18, 11), \"float32\"), T_multiply: T.Buffer((14, 18, 11), \"float32\"), compute_1: T.Buffer((14, 18, 11), \"float32\"), compute_2: T.Buffer((14, 18, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2772,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2772):\n            compute_3 = T.Buffer((2772,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2772):\n            T_multiply_1 = T.Buffer((2772,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(252):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((2772,), data=compute_1.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2772):\n            compute_3 = T.Buffer((2772,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "abs",
                "multiply",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[14, 18, 11]],
        "output_shape": [[14, 18, 11], [14, 18, 11], [14, 18, 11], [14, 18, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 72; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = atanf(fmodf(ph_0[((i0_i1_fused * 14) + i2)], (ph_0[((i0_i1_fused * 14) + i2)] / ph_3[((i0_i1_fused * 14) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        compute_1[(((i0 * 168) + (i1 * 14)) + i2_1)] = ceilf(fmodf(ph_0[(((i0 * 168) + (i1 * 14)) + i2_1)], (ph_0[(((i0 * 168) + (i1 * 14)) + i2_1)] / ph_3[(((i0 * 168) + (i1 * 14)) + i2_1)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1008; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = atanf(fmodf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 12, 14), \"float32\"), ph_3: T.Buffer((6, 12, 14), \"float32\"), compute: T.Buffer((6, 12, 14), \"float32\"), compute_1: T.Buffer((6, 12, 14), \"float32\"), T_add: T.Buffer((6, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1008,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1008,), data=ph_3.data)\n        for i0_i1_fused in T.parallel(72):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_2 = T.Buffer((1008,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(T.truncmod(ph_0_1[cse_var_1], ph_0_1[cse_var_1] / ph_3_1[cse_var_1]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(12, 14):\n                cse_var_2: T.int32 = i0 * 168 + i1 * 14 + i2\n                compute_2 = T.Buffer((1008,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.truncmod(ph_0_1[cse_var_2], ph_0_1[cse_var_2] / ph_3_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1008):\n            T_add_1 = T.Buffer((1008,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "subtract",
                "divide",
                "mod",
                "atan",
                "ceil",
                "add"
            ]
        ],
        "input_shape": [[6, 12, 14], [2, 15, 8], [6, 12, 14]],
        "output_shape": [[2, 15, 8], [6, 12, 14], [6, 12, 14], [6, 12, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 216; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n        T_subtract[(((ax0 * 72) + (ax1 * 8)) + ax2)] = (asinf(ph_0[(((ax0 * 72) + (ax1 * 8)) + ax2)]) - ph_0[(((ax0 * 72) + (ax1 * 8)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 216; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 27; ++ax0_ax1_fused) {\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      T_multiply[((ax0_ax1_fused * 8) + ax2_1)] = (asinhf(ph_0[((ax0_ax1_fused * 8) + ax2_1)]) * ph_0[((ax0_ax1_fused * 8) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 216; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf(asinhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 9, 8), \"float32\"), compute: T.Buffer((3, 9, 8), \"float32\"), T_subtract: T.Buffer((3, 9, 8), \"float32\"), compute_1: T.Buffer((3, 9, 8), \"float32\"), T_multiply: T.Buffer((3, 9, 8), \"float32\"), compute_2: T.Buffer((3, 9, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((216,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_3 = T.Buffer((216,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(9, 8):\n                cse_var_1: T.int32 = ax0 * 72 + ax1 * 8 + ax2\n                T_subtract_1 = T.Buffer((216,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_3 = T.Buffer((216,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(27):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_multiply_1 = T.Buffer((216,), data=T_multiply.data)\n                T_multiply_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(216):\n            compute_3 = T.Buffer((216,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acos",
                "asin",
                "subtract",
                "asinh",
                "asinh",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[3, 9, 8]],
        "output_shape": [[3, 9, 8], [3, 9, 8], [3, 9, 8], [3, 9, 8], [3, 9, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 48; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 48; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 6, 4), \"float32\"), ph_3: T.Buffer((2, 6, 4), \"float32\"), T_add: T.Buffer((2, 6, 4), \"float32\"), T_mod: T.Buffer((2, 6, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((48,), data=ph_0.data)\n        ph_3_1 = T.Buffer((48,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_add_1 = T.Buffer((48,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(48):\n            T_mod_1 = T.Buffer((48,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "add",
                "mod"
            ]
        ],
        "input_shape": [[2, 6, 4], [15, 6, 5], [2, 6, 4]],
        "output_shape": [[2, 6, 4], [15, 6, 5], [2, 6, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_divide[(((ax0 * 60) + (ax1 * 6)) + ax2)] = ((ph_0[(((ax0 * 60) + (ax1 * 6)) + ax2)] - fabsf(ph_0[(((ax0 * 60) + (ax1 * 6)) + ax2)])) / ph_0[(((ax0 * 60) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] - fabsf(ph_0[ax0_ax1_fused_ax2_fused])) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 10, 6), \"float32\"), ph_3: T.Buffer((4, 10, 6), \"float32\"), T_divide: T.Buffer((4, 10, 6), \"float32\"), T_add: T.Buffer((4, 10, 6), \"float32\"), compute: T.Buffer((4, 10, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for ax0 in T.parallel(4):\n            for ax1, ax2 in T.grid(10, 6):\n                cse_var_1: T.int32 = ax0 * 60 + ax1 * 6 + ax2\n                T_divide_1 = T.Buffer((240,), data=T_divide.data)\n                T_divide_1[cse_var_1] = (ph_0_1[cse_var_1] - T.fabs(ph_0_1[cse_var_1])) / ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_add_1 = T.Buffer((240,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_1 = T.Buffer((240,), data=compute.data)\n            ph_3_1 = T.Buffer((240,), data=ph_3.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "abs",
                "subtract",
                "divide",
                "add",
                "abs"
            ]
        ],
        "input_shape": [[4, 10, 6], [17, 9, 6], [4, 10, 6]],
        "output_shape": [[17, 9, 6], [4, 10, 6], [4, 10, 6], [4, 10, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 19; ++i1) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i1 * 20) + i2)] = atanhf(ph_0[((i1 * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 19; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_1[((i0_i1_fused * 20) + i2_1)] = asinhf(atanhf(ph_0[((i0_i1_fused * 20) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 380; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 19, 20), \"float32\"), compute: T.Buffer((1, 19, 20), \"float32\"), compute_1: T.Buffer((1, 19, 20), \"float32\"), compute_2: T.Buffer((1, 19, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((380,), data=ph_0.data)\n        for i1, i2 in T.grid(19, 20):\n            cse_var_1: T.int32 = i1 * 20 + i2\n            compute_3 = T.Buffer((380,), data=compute.data)\n            compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(19):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((380,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asinh(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(380):\n            compute_3 = T.Buffer((380,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "atanh",
                "asinh",
                "atan"
            ]
        ],
        "input_shape": [[1, 19, 20]],
        "output_shape": [[1, 19, 20], [1, 19, 20], [1, 19, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2754; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2754; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 153; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0_i1_fused * 18) + i2)] = atanhf(asinf(ph_0[((i0_i1_fused * 18) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2754; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 17, 18), \"float32\"), ph_3: T.Buffer((9, 17, 18), \"float32\"), T_subtract: T.Buffer((9, 17, 18), \"float32\"), compute: T.Buffer((9, 17, 18), \"float32\"), compute_1: T.Buffer((9, 17, 18), \"float32\"), compute_2: T.Buffer((9, 17, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2754,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2754):\n            T_subtract_1 = T.Buffer((2754,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((2754,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2754):\n            compute_3 = T.Buffer((2754,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(153):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_3 = T.Buffer((2754,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atanh(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2754):\n            compute_3 = T.Buffer((2754,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "asinh",
                "asin",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[9, 17, 18], [11, 18, 6], [9, 17, 18]],
        "output_shape": [[9, 17, 18], [11, 18, 6], [9, 17, 18], [9, 17, 18], [9, 17, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1260; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1260; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], fabsf(sinf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_1[((i0_i1_fused * 15) + i2)] = fabsf(atanhf(ph_0[((i0_i1_fused * 15) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], fabsf(__sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 7, 15), \"float32\"), compute: T.Buffer((12, 7, 15), \"float32\"), T_mod: T.Buffer((12, 7, 15), \"float32\"), compute_1: T.Buffer((12, 7, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1260):\n            compute_2 = T.Buffer((1260,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1260):\n            T_mod_1 = T.Buffer((1260,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.fabs(T.sin(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((1260,), data=compute_1.data)\n                compute_2[cse_var_1] = T.fabs(T.atanh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asinh",
                "sin",
                "abs",
                "mod",
                "atanh",
                "abs"
            ]
        ],
        "input_shape": [[12, 7, 15]],
        "output_shape": [[12, 7, 15], [12, 7, 15], [12, 7, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3648; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3648; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3648; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3648; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute_3[(((i0 * 192) + (i1 * 16)) + i2)] = asinf(asinf(ph_0[(((i0 * 192) + (i1 * 16)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 12, 16), \"float32\"), compute: T.Buffer((19, 12, 16), \"float32\"), T_add: T.Buffer((19, 12, 16), \"float32\"), compute_1: T.Buffer((19, 12, 16), \"float32\"), compute_2: T.Buffer((19, 12, 16), \"float32\"), compute_3: T.Buffer((19, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3648,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3648):\n            compute_4 = T.Buffer((3648,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3648):\n            T_add_1 = T.Buffer((3648,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3648):\n            compute_4 = T.Buffer((3648,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(3648):\n            compute_4 = T.Buffer((3648,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_1: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_4 = T.Buffer((3648,), data=compute_3.data)\n                compute_4[cse_var_1] = T.asin(T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asin",
                "sin",
                "add",
                "cos",
                "asin",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[19, 12, 16]],
        "output_shape": [[19, 12, 16], [19, 12, 16], [19, 12, 16], [19, 12, 16], [19, 12, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3, float* ph_9) {\n  float auto_scheduler_layout_transform[595];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1309; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1309; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1309; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = ceilf(acosf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n    for (int32_t ax7 = 0; ax7 < 7; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 17; ++ax8) {\n        auto_scheduler_layout_transform[(((ax6 * 119) + (ax7 * 17)) + ax8)] = ph_9[(((ax8 * 35) + (ax7 * 5)) + ax6)];\n      }\n    }\n  }\n  for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 11; ++i_outer_inner_init) {\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 17; ++b_inner_init) {\n        T_batch_matmul_NN[(((b_inner_init * 55) + (i_outer_inner_init * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int32_t i_outer_inner = 0; i_outer_inner < 11; ++i_outer_inner) {\n    for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 7; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 17; ++b_inner) {\n          T_batch_matmul_NN[(((b_inner * 55) + (i_outer_inner * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_inner * 55) + (i_outer_inner * 5)) + j_outer_inner)] + ((ph_0[(((b_inner * 77) + (i_outer_inner * 7)) + k_inner)] / ph_3[(((b_inner * 77) + (i_outer_inner * 7)) + k_inner)]) * auto_scheduler_layout_transform[(((j_outer_inner * 119) + (k_inner * 17)) + b_inner)]));\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(25) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3, float* __restrict__ ph_9) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float T_divide_shared[160];\n  __shared__ float ph_9_shared[160];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 4; ++b_c_outer_inner_init) {\n    T_batch_matmul_NN_local[b_c_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_outer_inner_init + 4)] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 7; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 5) + (((int)threadIdx.x) / 5)) < 32) {\n      T_divide_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 25) + ((int)threadIdx.x))] = (ph_0[((ax0_ax1_fused_ax2_fused_outer_outer * 25) + ((int)threadIdx.x))] / ph_3[((ax0_ax1_fused_ax2_fused_outer_outer * 25) + ((int)threadIdx.x))]);\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 7; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer_1 * 5) + (((int)threadIdx.x) / 5)) < 32) {\n      ph_9_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 25) + ((int)threadIdx.x))] = ph_9[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 25) + ((int)threadIdx.x))];\n    }\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 4; ++k_outer_inner) {\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 4; ++b_c_outer_inner) {\n      T_batch_matmul_NN_local[b_c_outer_inner] = (T_batch_matmul_NN_local[b_c_outer_inner] + (T_divide_shared[(((b_c_outer_inner * 20) + ((((int)threadIdx.x) / 5) * 4)) + k_outer_inner)] * ph_9_shared[(((b_c_outer_inner * 20) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5))]));\n      T_batch_matmul_NN_local[(b_c_outer_inner + 4)] = (T_batch_matmul_NN_local[(b_c_outer_inner + 4)] + (T_divide_shared[((((b_c_outer_inner * 20) + ((((int)threadIdx.x) / 5) * 4)) + k_outer_inner) + 80)] * ph_9_shared[((((b_c_outer_inner * 20) + (k_outer_inner * 5)) + (((int)threadIdx.x) % 5)) + 80)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    T_batch_matmul_NN[((b_inner * 25) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[(((b_inner * 25) + ((int)threadIdx.x)) + 100)] = T_batch_matmul_NN_local[(b_inner + 4)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 11, 7), \"float32\"), ph_3: T.Buffer((17, 11, 7), \"float32\"), ph_9: T.Buffer((17, 7, 5), \"float32\"), compute: T.Buffer((17, 11, 7), \"float32\"), compute_1: T.Buffer((17, 11, 7), \"float32\"), compute_2: T.Buffer((17, 11, 7), \"float32\"), T_batch_matmul_NN: T.Buffer((17, 11, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([595], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1309,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1309):\n            compute_3 = T.Buffer((1309,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1309):\n            compute_3 = T.Buffer((1309,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1309):\n            compute_3 = T.Buffer((1309,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((595,), data=auto_scheduler_layout_transform)\n        for ax6, ax7, ax8 in T.grid(5, 7, 17):\n            ph_9_1 = T.Buffer((595,), data=ph_9.data)\n            auto_scheduler_layout_transform_1[ax6 * 119 + ax7 * 17 + ax8] = ph_9_1[ax8 * 35 + ax7 * 5 + ax6]\n        T_batch_matmul_NN_1 = T.Buffer((935,), data=T_batch_matmul_NN.data)\n        for i_outer_inner_init, j_outer_inner_init, b_inner_init in T.grid(11, 5, 17):\n            T_batch_matmul_NN_1[b_inner_init * 55 + i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n        for i_outer_inner, j_outer_inner, k_inner, b_inner in T.grid(11, 5, 7, 17):\n            cse_var_2: T.int32 = b_inner * 77 + i_outer_inner * 7 + k_inner\n            cse_var_1: T.int32 = b_inner * 55 + i_outer_inner * 5 + j_outer_inner\n            ph_3_1 = T.Buffer((1309,), data=ph_3.data)\n            T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[cse_var_2] / ph_3_1[cse_var_2] * auto_scheduler_layout_transform_1[j_outer_inner * 119 + k_inner * 17 + b_inner]",
        "op_args": [
            [
                "divide",
                "abs",
                "acos",
                "acos",
                "ceil",
                "batch_matmul"
            ]
        ],
        "input_shape": [[17, 11, 7], [7, 20, 4], [17, 11, 7], [17, 7, 5]],
        "output_shape": [[7, 20, 4], [17, 11, 7], [17, 11, 7], [17, 11, 7], [17, 11, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 14; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_add[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] + ph_3[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 14; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i1 * 4) + i2)] = atanf(ph_0[((i1 * 4) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 14, 4), \"float32\"), ph_3: T.Buffer((1, 14, 4), \"float32\"), T_add: T.Buffer((1, 14, 4), \"float32\"), compute: T.Buffer((1, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((56,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(14):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_add_1 = T.Buffer((56,), data=T_add.data)\n                ph_3_1 = T.Buffer((56,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i1, i2 in T.grid(14, 4):\n            cse_var_2: T.int32 = i1 * 4 + i2\n            compute_1 = T.Buffer((56,), data=compute.data)\n            compute_1[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "atan"
            ]
        ],
        "input_shape": [[1, 14, 4], [14, 17, 6], [1, 14, 4]],
        "output_shape": [[1, 14, 4], [14, 17, 6], [1, 14, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[2640];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 8; ++ax4) {\n    for (int32_t ax5 = 0; ax5 < 11; ++ax5) {\n      for (int32_t ax6 = 0; ax6 < 5; ++ax6) {\n        for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n          auto_scheduler_layout_transform[((((ax4 * 110) + (ax5 * 10)) + (ax6 * 2)) + ax7)] = ph_3[((((ax5 * 80) + (ax4 * 10)) + (ax7 * 5)) + ax6)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 15; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 11; ++b_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        T_batch_matmul_NN[(((b_outer_inner_init * 75) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 8; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 11; ++b_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n            T_batch_matmul_NN[(((b_outer_inner * 75) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner)] = (T_batch_matmul_NN[(((b_outer_inner * 75) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5)) + j_outer_inner)] + (ph_0[((((b_outer_inner * 240) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 16)) + (k_outer * 2)) + k_inner)] * auto_scheduler_layout_transform[((((k_outer * 110) + (b_outer_inner * 10)) + (j_outer_inner * 2)) + k_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2640; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2640; ++i0_i1_fused_i2_fused_2) {\n    auto_scheduler_layout_transform[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2640; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (auto_scheduler_layout_transform[ax0_ax1_fused_ax2_fused] * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 2640; ++i0_i1_fused_i2_fused_3) {\n    compute_2[i0_i1_fused_i2_fused_3] = expf(auto_scheduler_layout_transform[i0_i1_fused_i2_fused_3]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[9];\n  __shared__ float ph_3_shared[4];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 3; ++i_c_outer_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 3; ++i_c_inner_init) {\n      T_batch_matmul_NN_local[((i_c_outer_inner_init * 3) + i_c_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      ph_3_shared[ax0_ax1_fused_ax2_fused_outer_outer] = ph_3[(((k_outer_outer * 20) + (ax0_ax1_fused_ax2_fused_outer_outer * 5)) + ((int)blockIdx.x))];\n    }\n    __syncthreads();\n    for (int k_outer_inner = 0; k_outer_inner < 4; ++k_outer_inner) {\n      for (int i_c_outer_inner = 0; i_c_outer_inner < 3; ++i_c_outer_inner) {\n        for (int i_c_inner = 0; i_c_inner < 3; ++i_c_inner) {\n          T_batch_matmul_NN_local[((i_c_outer_inner * 3) + i_c_inner)] = (T_batch_matmul_NN_local[((i_c_outer_inner * 3) + i_c_inner)] + (ph_0[((((i_c_outer_inner * 24) + (i_c_inner * 8)) + (k_outer_outer * 4)) + k_outer_inner)] * ph_3_shared[k_outer_inner]));\n        }\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 9; ++i_inner) {\n    T_batch_matmul_NN[((i_inner * 5) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[i_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 15, 16), \"float32\"), ph_3: T.Buffer((11, 16, 5), \"float32\"), compute: T.Buffer((11, 15, 16), \"float32\"), T_batch_matmul_NN: T.Buffer((11, 15, 5), \"float32\"), compute_1: T.Buffer((11, 15, 16), \"float32\"), T_multiply: T.Buffer((11, 15, 16), \"float32\"), compute_2: T.Buffer((11, 15, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([2640], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((2640,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_3 = T.Buffer((2640,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((880,), data=auto_scheduler_layout_transform)\n        for ax4, ax5, ax6, ax7 in T.grid(8, 11, 5, 2):\n            ph_3_1 = T.Buffer((880,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax4 * 110 + ax5 * 10 + ax6 * 2 + ax7] = ph_3_1[ax5 * 80 + ax4 * 10 + ax7 * 5 + ax6]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(15):\n            T_batch_matmul_NN_1 = T.Buffer((825,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, j_outer_inner_init in T.grid(11, 5):\n                T_batch_matmul_NN_1[b_outer_inner_init * 75 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, j_outer_inner, k_inner in T.grid(8, 11, 5, 2):\n                cse_var_1: T.int32 = b_outer_inner * 75 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_outer_inner * 240 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 16 + k_outer * 2 + k_inner] * auto_scheduler_layout_transform_1[k_outer * 110 + b_outer_inner * 10 + j_outer_inner * 2 + k_inner]\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_3 = T.Buffer((2640,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_2 = T.Buffer((2640,), data=auto_scheduler_layout_transform)\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            auto_scheduler_layout_transform_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2640):\n            T_multiply_1 = T.Buffer((2640,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = auto_scheduler_layout_transform_2[ax0_ax1_fused_ax2_fused] * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_3 = T.Buffer((2640,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(auto_scheduler_layout_transform_2[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "batch_matmul",
                "acos",
                "exp",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[11, 15, 16], [11, 16, 5]],
        "output_shape": [[11, 15, 16], [11, 15, 5], [11, 15, 16], [11, 15, 16], [11, 15, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* T_subtract_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 6; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 9) + ax2)] = (ph_0[((ax0_ax1_fused * 9) + ax2)] - ph_3[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 54; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract_1[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 54; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 2, 9), \"float32\"), ph_3: T.Buffer((3, 2, 9), \"float32\"), T_subtract: T.Buffer((3, 2, 9), \"float32\"), T_subtract_1: T.Buffer((3, 2, 9), \"float32\"), compute: T.Buffer((3, 2, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((54,), data=ph_0.data)\n        ph_3_1 = T.Buffer((54,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(6):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_subtract_2 = T.Buffer((54,), data=T_subtract.data)\n                T_subtract_2[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(54):\n            T_subtract_2 = T.Buffer((54,), data=T_subtract_1.data)\n            T_subtract_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(54):\n            compute_1 = T.Buffer((54,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "subtract",
                "sin"
            ]
        ],
        "input_shape": [[3, 2, 9], [14, 14, 18], [3, 2, 9]],
        "output_shape": [[3, 2, 9], [14, 14, 18], [3, 2, 9], [3, 2, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 750; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 750; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_2[(((i0 * 75) + (i1 * 15)) + i2)] = atanf(ph_0[(((i0 * 75) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 750; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 5, 15), \"float32\"), compute: T.Buffer((10, 5, 15), \"float32\"), compute_1: T.Buffer((10, 5, 15), \"float32\"), compute_2: T.Buffer((10, 5, 15), \"float32\"), compute_3: T.Buffer((10, 5, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((750,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_4 = T.Buffer((750,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_4 = T.Buffer((750,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(5, 15):\n                cse_var_1: T.int32 = i0 * 75 + i1 * 15 + i2\n                compute_4 = T.Buffer((750,), data=compute_2.data)\n                compute_4[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_4 = T.Buffer((750,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "atan",
                "cos",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[10, 5, 15]],
        "output_shape": [[10, 5, 15], [10, 5, 15], [10, 5, 15], [10, 5, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1862; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1862; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1862; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 19, 14), \"float32\"), ph_3: T.Buffer((7, 19, 14), \"float32\"), T_add: T.Buffer((7, 19, 14), \"float32\"), compute: T.Buffer((7, 19, 14), \"float32\"), compute_1: T.Buffer((7, 19, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1862,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1862):\n            T_add_1 = T.Buffer((1862,), data=T_add.data)\n            ph_3_1 = T.Buffer((1862,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1862):\n            compute_2 = T.Buffer((1862,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1862):\n            compute_2 = T.Buffer((1862,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "sin",
                "asin"
            ]
        ],
        "input_shape": [[7, 19, 14], [4, 8, 13], [7, 19, 14]],
        "output_shape": [[7, 19, 14], [4, 8, 13], [7, 19, 14], [7, 19, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 90; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 90; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_2[((i0_i1_fused * 9) + i2)] = acoshf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 5, 9), \"float32\"), compute: T.Buffer((2, 5, 9), \"float32\"), compute_1: T.Buffer((2, 5, 9), \"float32\"), compute_2: T.Buffer((2, 5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((90,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(90):\n            compute_3 = T.Buffer((90,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(90):\n            compute_3 = T.Buffer((90,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(10):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((90,), data=compute_2.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asinh",
                "acosh",
                "atanh",
                "acosh"
            ]
        ],
        "input_shape": [[2, 5, 9]],
        "output_shape": [[2, 5, 9], [2, 5, 9], [2, 5, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 198; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = cosf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3168; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3168; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 18, 16), \"float32\"), compute: T.Buffer((11, 18, 16), \"float32\"), T_add: T.Buffer((11, 18, 16), \"float32\"), compute_1: T.Buffer((11, 18, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3168,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(198):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((3168,), data=compute.data)\n                compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(3168):\n            T_add_1 = T.Buffer((3168,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3168):\n            compute_2 = T.Buffer((3168,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "atanh",
                "add",
                "exp"
            ]
        ],
        "input_shape": [[11, 18, 16]],
        "output_shape": [[11, 18, 16], [11, 18, 16], [11, 18, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 5; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = cosf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 25; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 25; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = atanhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 5; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_3[((i0_i1_fused_1 * 5) + i2_1)] = acoshf(cosf(ph_0[((i0_i1_fused_1 * 5) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 1, 5), \"float32\"), compute: T.Buffer((5, 1, 5), \"float32\"), compute_1: T.Buffer((5, 1, 5), \"float32\"), compute_2: T.Buffer((5, 1, 5), \"float32\"), compute_3: T.Buffer((5, 1, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((25,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(5):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_4 = T.Buffer((25,), data=compute.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(25):\n            compute_4 = T.Buffer((25,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(25):\n            compute_4 = T.Buffer((25,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(5):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_4 = T.Buffer((25,), data=compute_3.data)\n                compute_4[cse_var_2] = T.acosh(T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "cos",
                "ceil",
                "atanh",
                "atanh",
                "cos",
                "acosh"
            ]
        ],
        "input_shape": [[5, 1, 5]],
        "output_shape": [[5, 1, 5], [5, 1, 5], [5, 1, 5], [5, 1, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 70) + (i1 * 14)) + i2)] = sinf(ph_0[(((i0 * 70) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 8; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        compute_1[(((i0_1 * 70) + (i1_1 * 14)) + i2_1)] = atanf(atanhf(ph_0[(((i0_1 * 70) + (i1_1 * 14)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 560; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 5, 14), \"float32\"), compute: T.Buffer((8, 5, 14), \"float32\"), compute_1: T.Buffer((8, 5, 14), \"float32\"), compute_2: T.Buffer((8, 5, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((560,), data=ph_0.data)\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(5, 14):\n                cse_var_1: T.int32 = i0 * 70 + i1 * 14 + i2\n                compute_3 = T.Buffer((560,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(5, 14):\n                cse_var_2: T.int32 = i0 * 70 + i1 * 14 + i2\n                compute_3 = T.Buffer((560,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(560):\n            compute_3 = T.Buffer((560,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "atanh",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[8, 5, 14]],
        "output_shape": [[8, 5, 14], [8, 5, 14], [8, 5, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 880; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf((ph_0[ax0_ax1_fused_ax2_fused] + atanf(ph_0[ax0_ax1_fused_ax2_fused])), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 880; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_1[(((i0 * 80) + (i1 * 20)) + i2)] = ceilf(atanf(ph_0[(((i0 * 80) + (i1 * 20)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf((ph_0[((int)blockIdx.x)] + atanf(ph_0[((int)blockIdx.x)])), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 4, 20), \"float32\"), T_mod: T.Buffer((11, 4, 20), \"float32\"), compute: T.Buffer((11, 4, 20), \"float32\"), compute_1: T.Buffer((11, 4, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((880,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(880):\n            T_mod_1 = T.Buffer((880,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] + T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(880):\n            compute_2 = T.Buffer((880,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(4, 20):\n                cse_var_1: T.int32 = i0 * 80 + i1 * 20 + i2\n                compute_2 = T.Buffer((880,), data=compute_1.data)\n                compute_2[cse_var_1] = T.ceil(T.atan(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "atan",
                "add",
                "mod",
                "sin",
                "atan",
                "ceil"
            ]
        ],
        "input_shape": [[11, 4, 20]],
        "output_shape": [[11, 4, 20], [11, 4, 20], [11, 4, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 4; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_divide[(((ax0 * 66) + (ax1 * 11)) + ax2)] = (ph_0[(((ax0 * 66) + (ax1 * 11)) + ax2)] / ph_3[(((ax0 * 66) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 264; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf((ph_0[i0_i1_fused_i2_fused] * atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_1[(((i0 * 66) + (i1 * 11)) + i2)] = atanf((ph_0[(((i0 * 66) + (i1 * 11)) + i2)] * atanf(ph_0[(((i0 * 66) + (i1 * 11)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 6, 11), \"float32\"), ph_3: T.Buffer((4, 6, 11), \"float32\"), T_divide: T.Buffer((4, 6, 11), \"float32\"), compute: T.Buffer((4, 6, 11), \"float32\"), compute_1: T.Buffer((4, 6, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((264,), data=ph_0.data)\n        for ax0 in T.parallel(4):\n            for ax1, ax2 in T.grid(6, 11):\n                cse_var_1: T.int32 = ax0 * 66 + ax1 * 11 + ax2\n                T_divide_1 = T.Buffer((264,), data=T_divide.data)\n                ph_3_1 = T.Buffer((264,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(264):\n            compute_2 = T.Buffer((264,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] * T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(6, 11):\n                cse_var_2: T.int32 = i0 * 66 + i1 * 11 + i2\n                compute_2 = T.Buffer((264,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atan(ph_0_1[cse_var_2] * T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "atan",
                "multiply",
                "asin",
                "atan"
            ]
        ],
        "input_shape": [[4, 6, 11], [19, 19, 6], [4, 6, 11]],
        "output_shape": [[4, 6, 11], [19, 19, 6], [4, 6, 11], [4, 6, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 18; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 156) + (i1 * 12)) + i2)] = acoshf(fmodf(ph_0[(((i0 * 156) + (i1 * 12)) + i2)], ceilf(ph_0[(((i0 * 156) + (i1 * 12)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2808; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 234; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n      compute_2[((i0_i1_fused * 12) + i2_1)] = acoshf(asinf(ph_0[((i0_i1_fused * 12) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = fabsf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 13, 12), \"float32\"), compute: T.Buffer((18, 13, 12), \"float32\"), compute_1: T.Buffer((18, 13, 12), \"float32\"), compute_2: T.Buffer((18, 13, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2808,), data=ph_0.data)\n        for i0 in T.parallel(18):\n            for i1, i2 in T.grid(13, 12):\n                cse_var_1: T.int32 = i0 * 156 + i1 * 12 + i2\n                compute_3 = T.Buffer((2808,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(T.truncmod(ph_0_1[cse_var_1], T.ceil(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(2808):\n            compute_3 = T.Buffer((2808,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(234):\n            for i2 in range(12):\n                cse_var_2: T.int32 = i0_i1_fused * 12 + i2\n                compute_3 = T.Buffer((2808,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acosh(T.asin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "ceil",
                "mod",
                "acosh",
                "abs",
                "asin",
                "acosh"
            ]
        ],
        "input_shape": [[18, 13, 12]],
        "output_shape": [[18, 13, 12], [18, 13, 12], [18, 13, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3, float* ph_7) {\n  float auto_scheduler_layout_transform[70];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n    for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n      for (int32_t ax8 = 0; ax8 < 7; ++ax8) {\n        auto_scheduler_layout_transform[(((ax4 * 14) + (ax7 * 7)) + ax8)] = ph_7[(((ax8 * 10) + (ax4 * 2)) + ax7)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 20; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_inner_init = 0; b_inner_init < 7; ++b_inner_init) {\n      T_batch_matmul_NN[((b_inner_init * 20) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 5; ++k_outer) {\n      for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 7; ++b_inner) {\n          T_batch_matmul_NN[((b_inner * 20) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[((b_inner * 20) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (atanhf(ph_0[((((b_inner * 200) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10)) + (k_outer * 2)) + k_inner)]) * auto_scheduler_layout_transform[(((k_outer * 14) + (k_inner * 7)) + b_inner)]));\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1400; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1400; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_7) {\n  float T_batch_matmul_NN_local[32];\n  __shared__ float compute_shared[160];\n  __shared__ float ph_7_shared[40];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 2; ++b_c_outer_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 4; ++i_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_outer_inner_init * 4) + i_c_inner_init)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_outer_inner_init * 4) + i_c_inner_init) + 8)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_outer_inner_init * 4) + i_c_inner_init) + 16)] = 0.000000e+00f;\n      T_batch_matmul_NN_local[(((b_c_outer_inner_init * 4) + i_c_inner_init) + 24)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 160; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[ax0_ax1_fused_ax2_fused_outer_outer] = atanhf(ph_0[((((ax0_ax1_fused_ax2_fused_outer_outer / 20) * 40) + (((int)blockIdx.x) * 20)) + (ax0_ax1_fused_ax2_fused_outer_outer % 20))]);\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 40; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_7_shared[ax0_ax1_fused_ax2_fused_outer_outer_1] = ph_7[ax0_ax1_fused_ax2_fused_outer_outer_1];\n  }\n  __syncthreads();\n  for (int k_outer_inner = 0; k_outer_inner < 5; ++k_outer_inner) {\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 2; ++b_c_outer_inner) {\n      for (int i_c_inner = 0; i_c_inner < 4; ++i_c_inner) {\n        T_batch_matmul_NN_local[((b_c_outer_inner * 4) + i_c_inner)] = (T_batch_matmul_NN_local[((b_c_outer_inner * 4) + i_c_inner)] + (compute_shared[(((b_c_outer_inner * 20) + (i_c_inner * 5)) + k_outer_inner)] * ph_7_shared[((b_c_outer_inner * 5) + k_outer_inner)]));\n        T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + i_c_inner) + 8)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + i_c_inner) + 8)] + (compute_shared[((((b_c_outer_inner * 20) + (i_c_inner * 5)) + k_outer_inner) + 40)] * ph_7_shared[(((b_c_outer_inner * 5) + k_outer_inner) + 10)]));\n        T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + i_c_inner) + 16)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + i_c_inner) + 16)] + (compute_shared[((((b_c_outer_inner * 20) + (i_c_inner * 5)) + k_outer_inner) + 80)] * ph_7_shared[(((b_c_outer_inner * 5) + k_outer_inner) + 20)]));\n        T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + i_c_inner) + 24)] = (T_batch_matmul_NN_local[(((b_c_outer_inner * 4) + i_c_inner) + 24)] + (compute_shared[((((b_c_outer_inner * 20) + (i_c_inner * 5)) + k_outer_inner) + 120)] * ph_7_shared[(((b_c_outer_inner * 5) + k_outer_inner) + 30)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    for (int i_inner = 0; i_inner < 4; ++i_inner) {\n      T_batch_matmul_NN[(((b_inner * 8) + (((int)blockIdx.x) * 4)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 4) + i_inner)];\n      T_batch_matmul_NN[((((b_inner * 8) + (((int)blockIdx.x) * 4)) + i_inner) + 16)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 8)];\n      T_batch_matmul_NN[((((b_inner * 8) + (((int)blockIdx.x) * 4)) + i_inner) + 32)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 16)];\n      T_batch_matmul_NN[((((b_inner * 8) + (((int)blockIdx.x) * 4)) + i_inner) + 48)] = T_batch_matmul_NN_local[(((b_inner * 4) + i_inner) + 24)];\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 20, 10), \"float32\"), ph_3: T.Buffer((7, 20, 10), \"float32\"), ph_7: T.Buffer((7, 10, 1), \"float32\"), compute: T.Buffer((7, 20, 10), \"float32\"), T_batch_matmul_NN: T.Buffer((7, 20, 1), \"float32\"), compute_1: T.Buffer((7, 20, 10), \"float32\"), T_multiply: T.Buffer((7, 20, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([70], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1400,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1400):\n            compute_2 = T.Buffer((1400,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((70,), data=auto_scheduler_layout_transform)\n        for ax4, ax7, ax8 in T.grid(5, 2, 7):\n            ph_7_1 = T.Buffer((70,), data=ph_7.data)\n            auto_scheduler_layout_transform_1[ax4 * 14 + ax7 * 7 + ax8] = ph_7_1[ax8 * 10 + ax4 * 2 + ax7]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(20):\n            T_batch_matmul_NN_1 = T.Buffer((140,), data=T_batch_matmul_NN.data)\n            for b_inner_init in range(7):\n                T_batch_matmul_NN_1[b_inner_init * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for k_outer, k_inner, b_inner in T.grid(5, 2, 7):\n                cse_var_1: T.int32 = b_inner * 20 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + T.atanh(ph_0_1[b_inner * 200 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 10 + k_outer * 2 + k_inner]) * auto_scheduler_layout_transform_1[k_outer * 14 + k_inner * 7 + b_inner]\n        ph_3_1 = T.Buffer((1400,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(1400):\n            compute_2 = T.Buffer((1400,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1400):\n            T_multiply_1 = T.Buffer((1400,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "asinh",
                "atanh",
                "batch_matmul",
                "asinh",
                "multiply"
            ]
        ],
        "input_shape": [[7, 20, 10], [18, 16, 15], [7, 20, 10], [7, 10, 1]],
        "output_shape": [[18, 16, 15], [7, 20, 10], [7, 20, 1], [7, 20, 10], [7, 20, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 748; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 748; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_1[(((i0 * 44) + (i1 * 11)) + i2)] = atanhf(ph_0[(((i0 * 44) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 4, 11), \"float32\"), compute: T.Buffer((17, 4, 11), \"float32\"), T_add: T.Buffer((17, 4, 11), \"float32\"), compute_1: T.Buffer((17, 4, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((748,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(748):\n            compute_2 = T.Buffer((748,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(748):\n            T_add_1 = T.Buffer((748,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(4, 11):\n                cse_var_1: T.int32 = i0 * 44 + i1 * 11 + i2\n                compute_2 = T.Buffer((748,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "ceil",
                "add",
                "atanh"
            ]
        ],
        "input_shape": [[17, 4, 11]],
        "output_shape": [[17, 4, 11], [17, 4, 11], [17, 4, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 798; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 798; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 133; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 6) + ax2)] = (ph_0[((ax0_ax1_fused * 6) + ax2)] - acosf(ph_0[((ax0_ax1_fused * 6) + ax2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 19, 6), \"float32\"), compute: T.Buffer((7, 19, 6), \"float32\"), compute_1: T.Buffer((7, 19, 6), \"float32\"), T_subtract: T.Buffer((7, 19, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((798,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(798):\n            compute_2 = T.Buffer((798,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(798):\n            compute_2 = T.Buffer((798,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(133):\n            for ax2 in range(6):\n                cse_var_1: T.int32 = ax0_ax1_fused * 6 + ax2\n                T_subtract_1 = T.Buffer((798,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acos",
                "abs",
                "exp",
                "acos",
                "subtract"
            ]
        ],
        "input_shape": [[7, 19, 6]],
        "output_shape": [[7, 19, 6], [7, 19, 6], [7, 19, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = atanf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 96; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinhf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 96; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 16, 2), \"float32\"), compute: T.Buffer((3, 16, 2), \"float32\"), compute_1: T.Buffer((3, 16, 2), \"float32\"), compute_2: T.Buffer((3, 16, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((96,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((96,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(96):\n            compute_3 = T.Buffer((96,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(96):\n            compute_3 = T.Buffer((96,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "acosh",
                "asinh",
                "sin"
            ]
        ],
        "input_shape": [[3, 16, 2]],
        "output_shape": [[3, 16, 2], [3, 16, 2], [3, 16, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 64; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] - cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 32) + (i1 * 4)) + i2)] = atanf(ph_0[(((i0 * 32) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 8; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        compute_2[(((i0_1 * 32) + (i1_1 * 4)) + i2_1)] = acosf(sinf(ph_0[(((i0_1 * 32) + (i1_1 * 4)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 8, 4), \"float32\"), compute: T.Buffer((2, 8, 4), \"float32\"), compute_1: T.Buffer((2, 8, 4), \"float32\"), compute_2: T.Buffer((2, 8, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((64,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(64):\n            compute_3 = T.Buffer((64,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] - T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(8, 4):\n                cse_var_1: T.int32 = i0 * 32 + i1 * 4 + i2\n                compute_3 = T.Buffer((64,), data=compute_1.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(8, 4):\n                cse_var_2: T.int32 = i0 * 32 + i1 * 4 + i2\n                compute_3 = T.Buffer((64,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acos(T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "cos",
                "subtract",
                "atanh",
                "atan",
                "sin",
                "acos"
            ]
        ],
        "input_shape": [[2, 8, 4]],
        "output_shape": [[2, 8, 4], [2, 8, 4], [2, 8, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 420; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 420; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute_1[(((i0 * 30) + (i1 * 3)) + i2)] = sinf((ph_0[(((i0 * 30) + (i1 * 3)) + i2)] + fabsf(ph_0[(((i0 * 30) + (i1 * 3)) + i2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 10, 3), \"float32\"), compute: T.Buffer((14, 10, 3), \"float32\"), T_divide: T.Buffer((14, 10, 3), \"float32\"), compute_1: T.Buffer((14, 10, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((420,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(420):\n            compute_2 = T.Buffer((420,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(420):\n            T_divide_1 = T.Buffer((420,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(10, 3):\n                cse_var_1: T.int32 = i0 * 30 + i1 * 3 + i2\n                compute_2 = T.Buffer((420,), data=compute_1.data)\n                compute_2[cse_var_1] = T.sin(ph_0_1[cse_var_1] + T.fabs(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asinh",
                "asin",
                "divide",
                "abs",
                "add",
                "sin"
            ]
        ],
        "input_shape": [[14, 10, 3]],
        "output_shape": [[14, 10, 3], [14, 10, 3], [14, 10, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  float compute_1[663];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 663; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 39; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_1[((i0_i1_fused * 17) + i2)] = expf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 39; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_mod[((ax0_ax1_fused * 17) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 17) + ax2)], (ph_0[((ax0_ax1_fused * 17) + ax2)] * atanf(compute_1[((ax0_ax1_fused * 17) + ax2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf(ph_0[((int)blockIdx.x)], (ph_0[((int)blockIdx.x)] * atanf(__expf(ph_0[((int)blockIdx.x)]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(10) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 10) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 13, 17), \"float32\"), compute: T.Buffer((3, 13, 17), \"float32\"), T_mod: T.Buffer((3, 13, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([663], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((663,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(663):\n            compute_2 = T.Buffer((663,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_2 = T.Buffer((663,), data=compute_1)\n        for i0_i1_fused in T.parallel(39):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(39):\n            for ax2 in range(17):\n                cse_var_2: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_mod_1 = T.Buffer((663,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], ph_0_1[cse_var_2] * T.atan(compute_2[cse_var_2]))",
        "op_args": [
            [
                "asin",
                "exp",
                "atan",
                "multiply",
                "mod"
            ]
        ],
        "input_shape": [[3, 13, 17]],
        "output_shape": [[3, 13, 17], [3, 13, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_batch_matmul_NN, float* T_multiply, float* compute, float* ph_0, float* ph_3, float* ph_7) {\n  float auto_scheduler_layout_transform[60];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 10; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused * 20) + (ax4 * 2)) + ax5)] = ph_7[(((ax0_ax1_fused_ax2_fused * 20) + (ax5 * 10)) + ax4)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 24; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 16) + (b_outer_inner_init * 8)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7))] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 10; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 16) + (b_outer_inner * 8)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7))] = (T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 16) + (b_outer_inner * 8)) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7))] + (acosf(ph_0[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 160) + (b_outer_inner * 80)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused & 7) * 10)) + k_outer)]) * auto_scheduler_layout_transform[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused >> 3) * 20) + (k_outer * 2)) + b_outer_inner)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 480; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_2 = 0; ax0_ax1_fused_ax2_fused_2 < 480; ++ax0_ax1_fused_ax2_fused_2) {\n    T_add[ax0_ax1_fused_ax2_fused_2] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused_2], ph_3[ax0_ax1_fused_ax2_fused_2]) + ph_0[ax0_ax1_fused_ax2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((int)blockIdx.x)] = (fmodf(ph_0[((int)blockIdx.x)], ph_3[((int)blockIdx.x)]) * ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_7) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float compute_shared[40];\n  __shared__ float ph_7_shared[40];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    compute_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x)) / 5) * 40) + (((int)blockIdx.x) * 5)) + (((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x)) % 5))]);\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer_1 = 0; ax0_ax1_fused_ax2_fused_outer_outer_1 < 10; ++ax0_ax1_fused_ax2_fused_outer_outer_1) {\n    ph_7_shared[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 4) + ((int)threadIdx.x))] = ph_7[((ax0_ax1_fused_ax2_fused_outer_outer_1 * 4) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 5; ++k_inner) {\n    for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (compute_shared[(((((int)threadIdx.x) * 10) + (b_c_inner * 5)) + k_inner)] * ph_7_shared[(((((int)threadIdx.x) * 10) + (b_c_inner * 5)) + k_inner)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[(((((int)threadIdx.x) * 16) + (b_inner * 8)) + ((int)blockIdx.x))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 8, 10), \"float32\"), ph_3: T.Buffer((6, 8, 10), \"float32\"), ph_7: T.Buffer((6, 10, 1), \"float32\"), compute: T.Buffer((6, 8, 10), \"float32\"), T_batch_matmul_NN: T.Buffer((6, 8, 1), \"float32\"), T_multiply: T.Buffer((6, 8, 10), \"float32\"), T_add: T.Buffer((6, 8, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([60], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((60,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3):\n            for ax4, ax5 in T.grid(10, 2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 20\n                ph_7_1 = T.Buffer((60,), data=ph_7.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 2 + ax5] = ph_7_1[cse_var_1 + ax5 * 10 + ax4]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(24):\n            T_batch_matmul_NN_1 = T.Buffer((48,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init in range(2):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 8 * 16 + b_outer_inner_init * 8 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 8] = T.float32(0)\n            for k_outer, b_outer_inner in T.grid(10, 2):\n                cse_var_4: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 8\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 8\n                cse_var_2: T.int32 = cse_var_3 * 16 + b_outer_inner * 8 + cse_var_4\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.acos(ph_0_1[cse_var_3 * 160 + b_outer_inner * 80 + cse_var_4 * 10 + k_outer]) * auto_scheduler_layout_transform_1[cse_var_3 * 20 + k_outer * 2 + b_outer_inner]\n        ph_3_1 = T.Buffer((480,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_multiply_1 = T.Buffer((480,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_add_1 = T.Buffer((480,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "exp",
                "acos",
                "batch_matmul",
                "multiply",
                "add"
            ]
        ],
        "input_shape": [[6, 8, 10], [15, 3, 17], [6, 8, 10], [6, 10, 1]],
        "output_shape": [[15, 3, 17], [6, 8, 10], [6, 8, 1], [6, 8, 10], [6, 8, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 144; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 18; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = acoshf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 18; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 8) + i2_1)] = acosf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 6, 8), \"float32\"), ph_3: T.Buffer((3, 6, 8), \"float32\"), T_add: T.Buffer((3, 6, 8), \"float32\"), compute: T.Buffer((3, 6, 8), \"float32\"), compute_1: T.Buffer((3, 6, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((144,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            T_add_1 = T.Buffer((144,), data=T_add.data)\n            ph_3_1 = T.Buffer((144,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((144,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(18):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((144,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acos(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "acosh",
                "acos"
            ]
        ],
        "input_shape": [[3, 6, 8], [4, 15, 4], [3, 6, 8]],
        "output_shape": [[3, 6, 8], [4, 15, 4], [3, 6, 8], [3, 6, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 72; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 8) + ax2)] = (atanhf(ph_0[((ax0_ax1_fused * 8) + ax2)]) * ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 72) + (i1 * 8)) + i2)] = acoshf(ph_0[(((i0 * 72) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 9, 8), \"float32\"), compute: T.Buffer((8, 9, 8), \"float32\"), T_multiply: T.Buffer((8, 9, 8), \"float32\"), compute_1: T.Buffer((8, 9, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((576,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_2 = T.Buffer((576,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(72):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_multiply_1 = T.Buffer((576,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(9, 8):\n                cse_var_2: T.int32 = i0 * 72 + i1 * 8 + i2\n                compute_2 = T.Buffer((576,), data=compute_1.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "atanh",
                "multiply",
                "acosh"
            ]
        ],
        "input_shape": [[8, 9, 8]],
        "output_shape": [[8, 9, 8], [8, 9, 8], [8, 9, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1360; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_divide[(((ax0 * 80) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 80) + (ax1 * 4)) + ax2)] / (ceilf(ph_0[(((ax0 * 80) + (ax1 * 4)) + ax2)]) - ph_0[(((ax0 * 80) + (ax1 * 4)) + ax2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1360; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] / (ceilf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 4), \"float32\"), compute: T.Buffer((17, 20, 4), \"float32\"), T_divide: T.Buffer((17, 20, 4), \"float32\"), compute_1: T.Buffer((17, 20, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1360,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1360):\n            compute_2 = T.Buffer((1360,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(17):\n            for ax1, ax2 in T.grid(20, 4):\n                cse_var_1: T.int32 = ax0 * 80 + ax1 * 4 + ax2\n                T_divide_1 = T.Buffer((1360,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / (T.ceil(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(1360):\n            compute_2 = T.Buffer((1360,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acosh",
                "ceil",
                "subtract",
                "divide",
                "atan",
                "atan"
            ]
        ],
        "input_shape": [[17, 20, 4]],
        "output_shape": [[17, 20, 4], [17, 20, 4], [17, 20, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 12; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    float compute_4[1];\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_4[0] = expf(ph_0[((i0 * 2) + i2)]);\n      compute_1[((i0 * 2) + i2)] = acoshf(compute_4[0]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 6; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_2[((i0_i1_fused * 2) + i2_1)] = expf(fmodf(ph_0[((i0_i1_fused * 2) + i2_1)], acoshf(ph_0[((i0_i1_fused * 2) + i2_1)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 12; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused_1], acoshf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = acoshf(__expf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 1, 2), \"float32\"), compute: T.Buffer((6, 1, 2), \"float32\"), compute_1: T.Buffer((6, 1, 2), \"float32\"), compute_2: T.Buffer((6, 1, 2), \"float32\"), compute_3: T.Buffer((6, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((12,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(12):\n            compute_4 = T.Buffer((12,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(6):\n            compute_4 = T.allocate([1], \"float32\", \"global\")\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i2\n                compute_5 = T.Buffer((1,), data=compute_4, align=4)\n                compute_5[0] = T.exp(ph_0_1[cse_var_1])\n                compute_6 = T.Buffer((12,), data=compute_1.data)\n                compute_6[cse_var_1] = T.acosh(compute_5[0])\n        for i0_i1_fused in T.parallel(6):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_4 = T.Buffer((12,), data=compute_2.data)\n                compute_4[cse_var_2] = T.exp(T.truncmod(ph_0_1[cse_var_2], T.acosh(ph_0_1[cse_var_2])))\n        for i0_i1_fused_i2_fused in T.parallel(12):\n            compute_4 = T.Buffer((12,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.acosh(ph_0_1[i0_i1_fused_i2_fused])))",
        "op_args": [
            [
                "sin",
                "exp",
                "acosh",
                "acosh",
                "mod",
                "exp",
                "atanh"
            ]
        ],
        "input_shape": [[6, 1, 2]],
        "output_shape": [[6, 1, 2], [6, 1, 2], [6, 1, 2], [6, 1, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_add[(((ax0 * 224) + (ax1 * 16)) + ax2)] = (ph_0[(((ax0 * 224) + (ax1 * 16)) + ax2)] + ph_3[(((ax0 * 224) + (ax1 * 16)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1344; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = cosf(cosf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 14, 16), \"float32\"), ph_3: T.Buffer((6, 14, 16), \"float32\"), T_add: T.Buffer((6, 14, 16), \"float32\"), compute: T.Buffer((6, 14, 16), \"float32\"), compute_1: T.Buffer((6, 14, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1344,), data=ph_0.data)\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(14, 16):\n                cse_var_1: T.int32 = ax0 * 224 + ax1 * 16 + ax2\n                T_add_1 = T.Buffer((1344,), data=T_add.data)\n                ph_3_1 = T.Buffer((1344,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1344):\n            compute_2 = T.Buffer((1344,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_2 = T.Buffer((1344,), data=compute_1.data)\n                compute_2[cse_var_2] = T.cos(T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "add",
                "acosh",
                "cos",
                "cos"
            ]
        ],
        "input_shape": [[6, 14, 16], [16, 12, 8], [6, 14, 16]],
        "output_shape": [[6, 14, 16], [16, 12, 8], [6, 14, 16], [6, 14, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 210) + (i1 * 14)) + i2)] = fabsf(ph_0[(((i0 * 210) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_subtract[(((ax0 * 210) + (ax1 * 14)) + ax2)] = (ph_0[(((ax0 * 210) + (ax1 * 14)) + ax2)] - fmodf(ph_0[(((ax0 * 210) + (ax1 * 14)) + ax2)], (sinf(ph_0[(((ax0 * 210) + (ax1 * 14)) + ax2)]) * ph_0[(((ax0 * 210) + (ax1 * 14)) + ax2)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))] - fmodf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))], (__sinf(ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 20) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 15, 14), \"float32\"), compute: T.Buffer((10, 15, 14), \"float32\"), T_subtract: T.Buffer((10, 15, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2100,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(15, 14):\n                cse_var_1: T.int32 = i0 * 210 + i1 * 14 + i2\n                compute_1 = T.Buffer((2100,), data=compute.data)\n                compute_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(15, 14):\n                cse_var_2: T.int32 = ax0 * 210 + ax1 * 14 + ax2\n                T_subtract_1 = T.Buffer((2100,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] - T.truncmod(ph_0_1[cse_var_2], T.sin(ph_0_1[cse_var_2]) * ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "sin",
                "multiply",
                "mod",
                "subtract"
            ]
        ],
        "input_shape": [[10, 15, 14]],
        "output_shape": [[10, 15, 14], [10, 15, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute[(((i0 * 182) + (i1 * 14)) + i2)] = ceilf(ph_0[(((i0 * 182) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_subtract[(((ax0 * 182) + (ax1 * 14)) + ax2)] = (cosf(ph_0[(((ax0 * 182) + (ax1 * 14)) + ax2)]) - ph_0[(((ax0 * 182) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 546; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 546; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((int)blockIdx.x)] = (__cosf(ph_0[((int)blockIdx.x)]) - ph_0[((int)blockIdx.x)]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 13, 14), \"float32\"), compute: T.Buffer((3, 13, 14), \"float32\"), T_subtract: T.Buffer((3, 13, 14), \"float32\"), compute_1: T.Buffer((3, 13, 14), \"float32\"), compute_2: T.Buffer((3, 13, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((546,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(13, 14):\n                cse_var_1: T.int32 = i0 * 182 + i1 * 14 + i2\n                compute_3 = T.Buffer((546,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(13, 14):\n                cse_var_2: T.int32 = ax0 * 182 + ax1 * 14 + ax2\n                T_subtract_1 = T.Buffer((546,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.cos(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(546):\n            compute_3 = T.Buffer((546,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(546):\n            compute_3 = T.Buffer((546,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "cos",
                "subtract",
                "cos",
                "sin",
                "ceil"
            ]
        ],
        "input_shape": [[3, 13, 14]],
        "output_shape": [[3, 13, 14], [3, 13, 14], [3, 13, 14], [3, 13, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 19; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      T_subtract[((ax0 * 7) + ax1)] = (ph_0[((ax0 * 7) + ax1)] - ph_3[((ax0 * 7) + ax1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 133; ++i0_i1_fused) {\n    compute[i0_i1_fused] = acosf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 133; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 7, 1), \"float32\"), ph_3: T.Buffer((19, 7, 1), \"float32\"), T_subtract: T.Buffer((19, 7, 1), \"float32\"), compute: T.Buffer((19, 7, 1), \"float32\"), compute_1: T.Buffer((19, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((133,), data=ph_0.data)\n        for ax0 in T.parallel(19):\n            for ax1 in range(7):\n                cse_var_1: T.int32 = ax0 * 7 + ax1\n                T_subtract_1 = T.Buffer((133,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((133,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(133):\n            compute_2 = T.Buffer((133,), data=compute.data)\n            compute_2[i0_i1_fused] = T.acos(ph_0_1[i0_i1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(133):\n            compute_2 = T.Buffer((133,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "acos",
                "abs"
            ]
        ],
        "input_shape": [[19, 7, 1], [5, 17, 11], [19, 7, 1]],
        "output_shape": [[19, 7, 1], [5, 17, 11], [19, 7, 1], [19, 7, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1287; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 99; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = acoshf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 99; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 13) + i2_1)] = asinhf(atanhf(ph_0[((i0_i1_fused_1 * 13) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(atanhf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 11, 13), \"float32\"), ph_3: T.Buffer((9, 11, 13), \"float32\"), T_add: T.Buffer((9, 11, 13), \"float32\"), compute: T.Buffer((9, 11, 13), \"float32\"), compute_1: T.Buffer((9, 11, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1287,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1287):\n            T_add_1 = T.Buffer((1287,), data=T_add.data)\n            ph_3_1 = T.Buffer((1287,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(99):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((1287,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(99):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_2 = T.Buffer((1287,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asinh(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "add",
                "acosh",
                "atanh",
                "asinh"
            ]
        ],
        "input_shape": [[9, 11, 13], [16, 11, 3], [9, 11, 13]],
        "output_shape": [[9, 11, 13], [16, 11, 3], [9, 11, 13], [9, 11, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3168; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 176; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 18) + ax2)] = (atanf(ph_0[((ax0_ax1_fused * 18) + ax2)]) - ph_0[((ax0_ax1_fused * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3168; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3168; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3168; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinf((ph_0[i0_i1_fused_i2_fused_2] + ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 16, 18), \"float32\"), ph_3: T.Buffer((11, 16, 18), \"float32\"), compute: T.Buffer((11, 16, 18), \"float32\"), T_subtract: T.Buffer((11, 16, 18), \"float32\"), compute_1: T.Buffer((11, 16, 18), \"float32\"), T_divide: T.Buffer((11, 16, 18), \"float32\"), compute_2: T.Buffer((11, 16, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3168,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3168):\n            compute_3 = T.Buffer((3168,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(176):\n            for ax2 in range(18):\n                cse_var_1: T.int32 = ax0_ax1_fused * 18 + ax2\n                T_subtract_1 = T.Buffer((3168,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(3168):\n            compute_3 = T.Buffer((3168,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((3168,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3168):\n            T_divide_1 = T.Buffer((3168,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(3168):\n            compute_3 = T.Buffer((3168,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "acosh",
                "atan",
                "subtract",
                "atan",
                "divide",
                "asin"
            ]
        ],
        "input_shape": [[11, 16, 18], [18, 16, 16], [11, 16, 18]],
        "output_shape": [[18, 16, 16], [11, 16, 18], [11, 16, 18], [11, 16, 18], [11, 16, 18], [11, 16, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2340; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf((ph_0[i0_i1_fused_i2_fused] * acosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_1[(((i0 * 180) + (i1 * 15)) + i2)] = asinhf(ph_0[(((i0 * 180) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2340; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 12, 15), \"float32\"), compute: T.Buffer((13, 12, 15), \"float32\"), compute_1: T.Buffer((13, 12, 15), \"float32\"), compute_2: T.Buffer((13, 12, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2340,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_3 = T.Buffer((2340,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] * T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(12, 15):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 15 + i2\n                compute_3 = T.Buffer((2340,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_3 = T.Buffer((2340,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "multiply",
                "acosh",
                "asinh",
                "asin"
            ]
        ],
        "input_shape": [[13, 12, 15]],
        "output_shape": [[13, 12, 15], [13, 12, 15], [13, 12, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1960; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1960; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - fmodf(asinhf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_1[(((i0 * 280) + (i1 * 20)) + i2)] = atanf(ph_0[(((i0 * 280) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - fmodf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 14, 20), \"float32\"), compute: T.Buffer((7, 14, 20), \"float32\"), T_subtract: T.Buffer((7, 14, 20), \"float32\"), compute_1: T.Buffer((7, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1960,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1960):\n            compute_2 = T.Buffer((1960,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1960):\n            T_subtract_1 = T.Buffer((1960,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.truncmod(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(14, 20):\n                cse_var_1: T.int32 = i0 * 280 + i1 * 20 + i2\n                compute_2 = T.Buffer((1960,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atan",
                "asinh",
                "mod",
                "subtract",
                "atan"
            ]
        ],
        "input_shape": [[7, 14, 20]],
        "output_shape": [[7, 14, 20], [7, 14, 20], [7, 14, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 234; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute[((i0_i1_fused * 12) + i2)] = asinf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2808; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2808; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(acosf(ph_0[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2808; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] + ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 13, 12), \"float32\"), ph_3: T.Buffer((18, 13, 12), \"float32\"), compute: T.Buffer((18, 13, 12), \"float32\"), T_divide: T.Buffer((18, 13, 12), \"float32\"), T_mod: T.Buffer((18, 13, 12), \"float32\"), compute_1: T.Buffer((18, 13, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2808,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(234):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_2 = T.Buffer((2808,), data=compute.data)\n                compute_2[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2808):\n            T_divide_1 = T.Buffer((2808,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2808):\n            T_mod_1 = T.Buffer((2808,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2808):\n            compute_2 = T.Buffer((2808,), data=compute_1.data)\n            ph_3_1 = T.Buffer((2808,), data=ph_3.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "asin",
                "acos",
                "divide",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[18, 13, 12], [15, 12, 2], [18, 13, 12]],
        "output_shape": [[15, 12, 2], [18, 13, 12], [18, 13, 12], [18, 13, 12], [18, 13, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 300; ++ax0_ax1_fused) {\n    T_divide[ax0_ax1_fused] = (ph_0[ax0_ax1_fused] / ph_3[ax0_ax1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 300; ++ax0_ax1_fused_1) {\n    T_mod[ax0_ax1_fused_1] = fmodf(ph_0[ax0_ax1_fused_1], ph_3[ax0_ax1_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 300; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 20, 1), \"float32\"), ph_3: T.Buffer((15, 20, 1), \"float32\"), T_divide: T.Buffer((15, 20, 1), \"float32\"), T_mod: T.Buffer((15, 20, 1), \"float32\"), compute: T.Buffer((15, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((300,), data=ph_0.data)\n        ph_3_1 = T.Buffer((300,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(300):\n            T_divide_1 = T.Buffer((300,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused] = ph_0_1[ax0_ax1_fused] / ph_3_1[ax0_ax1_fused]\n        for ax0_ax1_fused in T.parallel(300):\n            T_mod_1 = T.Buffer((300,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused] = T.truncmod(ph_0_1[ax0_ax1_fused], ph_3_1[ax0_ax1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(300):\n            compute_1 = T.Buffer((300,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "mod",
                "atan",
                "atanh"
            ]
        ],
        "input_shape": [[15, 20, 1], [5, 10, 18], [15, 20, 1]],
        "output_shape": [[15, 20, 1], [5, 10, 18], [15, 20, 1], [15, 20, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 630; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_1[((i0_i1_fused * 7) + i2)] = acoshf(asinf(ph_0[((i0_i1_fused * 7) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 630; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 630; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinhf(asinf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 18, 7), \"float32\"), compute: T.Buffer((5, 18, 7), \"float32\"), compute_1: T.Buffer((5, 18, 7), \"float32\"), compute_2: T.Buffer((5, 18, 7), \"float32\"), compute_3: T.Buffer((5, 18, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((630,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(630):\n            compute_4 = T.Buffer((630,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(90):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_4 = T.Buffer((630,), data=compute_1.data)\n                compute_4[cse_var_1] = T.acosh(T.asin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(630):\n            compute_4 = T.Buffer((630,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(630):\n            compute_4 = T.Buffer((630,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "asin",
                "acosh",
                "acosh",
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[5, 18, 7]],
        "output_shape": [[5, 18, 7], [5, 18, 7], [5, 18, 7], [5, 18, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 204) + (i1 * 12)) + i2)] = atanhf(ph_0[(((i0 * 204) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1020; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1020; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __cosf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 17, 12), \"float32\"), compute: T.Buffer((5, 17, 12), \"float32\"), T_subtract: T.Buffer((5, 17, 12), \"float32\"), compute_1: T.Buffer((5, 17, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1020,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(17, 12):\n                cse_var_1: T.int32 = i0 * 204 + i1 * 12 + i2\n                compute_2 = T.Buffer((1020,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1020):\n            T_subtract_1 = T.Buffer((1020,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1020):\n            compute_2 = T.Buffer((1020,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "abs",
                "subtract",
                "cos"
            ]
        ],
        "input_shape": [[5, 17, 12]],
        "output_shape": [[5, 17, 12], [5, 17, 12], [5, 17, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      compute[((i0 * 10) + i1)] = acosf(ph_0[((i0 * 10) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 110; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 110; ++i0_i1_fused) {\n    compute_1[i0_i1_fused] = asinhf(ph_0[i0_i1_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 10, 1), \"float32\"), compute: T.Buffer((11, 10, 1), \"float32\"), T_add: T.Buffer((11, 10, 1), \"float32\"), compute_1: T.Buffer((11, 10, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((110,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1 in range(10):\n                cse_var_1: T.int32 = i0 * 10 + i1\n                compute_2 = T.Buffer((110,), data=compute.data)\n                compute_2[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(110):\n            T_add_1 = T.Buffer((110,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(110):\n            compute_2 = T.Buffer((110,), data=compute_1.data)\n            compute_2[i0_i1_fused] = T.asinh(ph_0_1[i0_i1_fused])",
        "op_args": [
            [
                "acos",
                "asinh",
                "add",
                "asinh"
            ]
        ],
        "input_shape": [[11, 10, 1]],
        "output_shape": [[11, 10, 1], [11, 10, 1], [11, 10, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1536; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1536; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_2[((i0_i1_fused * 16) + i2)] = fabsf(sinf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1536; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 12, 16), \"float32\"), ph_3: T.Buffer((8, 12, 16), \"float32\"), compute: T.Buffer((8, 12, 16), \"float32\"), compute_1: T.Buffer((8, 12, 16), \"float32\"), compute_2: T.Buffer((8, 12, 16), \"float32\"), T_multiply: T.Buffer((8, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1536,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1536):\n            compute_3 = T.Buffer((1536,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1536):\n            compute_3 = T.Buffer((1536,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((1536,), data=compute_2.data)\n                compute_3[cse_var_1] = T.fabs(T.sin(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1536):\n            T_multiply_1 = T.Buffer((1536,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((1536,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "acos",
                "sin",
                "cos",
                "abs",
                "multiply"
            ]
        ],
        "input_shape": [[8, 12, 16], [2, 9, 7], [8, 12, 16]],
        "output_shape": [[2, 9, 7], [8, 12, 16], [8, 12, 16], [8, 12, 16], [8, 12, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 180; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = atanhf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 180; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_mod[((ax0_ax1_fused * 3) + ax2)] = fmodf(fabsf(ph_0[((ax0_ax1_fused * 3) + ax2)]), ph_0[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 540; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], atanhf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 9, 3), \"float32\"), compute: T.Buffer((20, 9, 3), \"float32\"), T_mod: T.Buffer((20, 9, 3), \"float32\"), T_multiply: T.Buffer((20, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((540,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(180):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_1 = T.Buffer((540,), data=compute.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(180):\n            for ax2 in range(3):\n                cse_var_2: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_mod_1 = T.Buffer((540,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.fabs(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(540):\n            T_multiply_1 = T.Buffer((540,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "abs",
                "mod",
                "atanh",
                "mod",
                "multiply"
            ]
        ],
        "input_shape": [[20, 9, 3]],
        "output_shape": [[20, 9, 3], [20, 9, 3], [20, 9, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 45) + (i1 * 9)) + i2)] = ceilf(ph_0[(((i0 * 45) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 765; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 765; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 765; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 5, 9), \"float32\"), compute: T.Buffer((17, 5, 9), \"float32\"), T_divide: T.Buffer((17, 5, 9), \"float32\"), compute_1: T.Buffer((17, 5, 9), \"float32\"), compute_2: T.Buffer((17, 5, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((765,), data=ph_0.data)\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(5, 9):\n                cse_var_1: T.int32 = i0 * 45 + i1 * 9 + i2\n                compute_3 = T.Buffer((765,), data=compute.data)\n                compute_3[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(765):\n            T_divide_1 = T.Buffer((765,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(765):\n            compute_3 = T.Buffer((765,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(765):\n            compute_3 = T.Buffer((765,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "asin",
                "divide",
                "atan",
                "sin"
            ]
        ],
        "input_shape": [[17, 5, 9]],
        "output_shape": [[17, 5, 9], [17, 5, 9], [17, 5, 9], [17, 5, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1260; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - asinf((ph_0[ax0_ax1_fused_ax2_fused] + sinf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 7, 12), \"float32\"), T_subtract: T.Buffer((15, 7, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(1260):\n            T_subtract_1 = T.Buffer((1260,), data=T_subtract.data)\n            ph_0_1 = T.Buffer((1260,), data=ph_0.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.asin(ph_0_1[ax0_ax1_fused_ax2_fused] + T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]))",
        "op_args": [
            [
                "sin",
                "add",
                "asin",
                "subtract"
            ]
        ],
        "input_shape": [[15, 7, 12]],
        "output_shape": [[15, 7, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_divide_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 48) + (i1 * 6)) + i2)] = atanf(ph_0[(((i0 * 48) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 192; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 192; ++ax0_ax1_fused_ax2_fused_1) {\n    float compute_1[1];\n    compute_1[0] = expf(ph_0[ax0_ax1_fused_ax2_fused_1]);\n    T_divide_1[ax0_ax1_fused_ax2_fused_1] = (ph_0[ax0_ax1_fused_ax2_fused_1] / compute_1[0]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(5) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 5) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 8, 6), \"float32\"), compute: T.Buffer((4, 8, 6), \"float32\"), T_divide: T.Buffer((4, 8, 6), \"float32\"), T_divide_1: T.Buffer((4, 8, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((192,), data=ph_0.data)\n        for i0 in T.parallel(4):\n            for i1, i2 in T.grid(8, 6):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 6 + i2\n                compute_1 = T.Buffer((192,), data=compute.data)\n                compute_1[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(192):\n            T_divide_2 = T.Buffer((192,), data=T_divide.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(192):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.exp(ph_0_1[ax0_ax1_fused_ax2_fused])\n            T_divide_2 = T.Buffer((192,), data=T_divide_1.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / compute_2[0]",
        "op_args": [
            [
                "atan",
                "abs",
                "divide",
                "exp",
                "divide"
            ]
        ],
        "input_shape": [[4, 8, 6]],
        "output_shape": [[4, 8, 6], [4, 8, 6], [4, 8, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2160; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2160; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + acosf(acosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2160; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2160; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf(sinf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(__sinf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + acosf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 12, 15), \"float32\"), compute: T.Buffer((12, 12, 15), \"float32\"), T_add: T.Buffer((12, 12, 15), \"float32\"), compute_1: T.Buffer((12, 12, 15), \"float32\"), compute_2: T.Buffer((12, 12, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2160,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_3 = T.Buffer((2160,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2160):\n            T_add_1 = T.Buffer((2160,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.acos(T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_3 = T.Buffer((2160,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2160):\n            compute_3 = T.Buffer((2160,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "acos",
                "acos",
                "add",
                "sin",
                "asinh",
                "asinh"
            ]
        ],
        "input_shape": [[12, 12, 15]],
        "output_shape": [[12, 12, 15], [12, 12, 15], [12, 12, 15], [12, 12, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2340; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_subtract[(((ax0 * 180) + (ax1 * 9)) + ax2)] = (sinf(ph_0[(((ax0 * 180) + (ax1 * 9)) + ax2)]) - ph_0[(((ax0 * 180) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2340; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2340; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 20, 9), \"float32\"), ph_3: T.Buffer((13, 20, 9), \"float32\"), compute: T.Buffer((13, 20, 9), \"float32\"), T_subtract: T.Buffer((13, 20, 9), \"float32\"), compute_1: T.Buffer((13, 20, 9), \"float32\"), T_mod: T.Buffer((13, 20, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2340,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_2 = T.Buffer((2340,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(20, 9):\n                cse_var_1: T.int32 = ax0 * 180 + ax1 * 9 + ax2\n                T_subtract_1 = T.Buffer((2340,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.sin(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_2 = T.Buffer((2340,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(2340):\n            T_mod_1 = T.Buffer((2340,), data=T_mod.data)\n            ph_3_1 = T.Buffer((2340,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "mod",
                "exp",
                "sin",
                "subtract",
                "abs",
                "mod"
            ]
        ],
        "input_shape": [[13, 20, 9], [17, 6, 3], [13, 20, 9]],
        "output_shape": [[17, 6, 3], [13, 20, 9], [13, 20, 9], [13, 20, 9], [13, 20, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[378];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 378; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / asinhf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 126; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = acoshf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 378; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 378; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(compute_2[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 378; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide[ax0_ax1_fused_ax2_fused_1] = (compute_2[ax0_ax1_fused_ax2_fused_1] / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf(__expf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 18, 3), \"float32\"), T_multiply: T.Buffer((7, 18, 3), \"float32\"), compute: T.Buffer((7, 18, 3), \"float32\"), compute_1: T.Buffer((7, 18, 3), \"float32\"), T_divide: T.Buffer((7, 18, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([378], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((378,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(378):\n            T_multiply_1 = T.Buffer((378,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(126):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((378,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        compute_3 = T.Buffer((378,), data=compute_2)\n        for i0_i1_fused_i2_fused in T.parallel(378):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(378):\n            compute_4 = T.Buffer((378,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(compute_3[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(378):\n            T_divide_1 = T.Buffer((378,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = compute_3[ax0_ax1_fused_ax2_fused] / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asinh",
                "divide",
                "multiply",
                "acosh",
                "exp",
                "exp",
                "divide"
            ]
        ],
        "input_shape": [[7, 18, 3]],
        "output_shape": [[7, 18, 3], [7, 18, 3], [7, 18, 3], [7, 18, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1020; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1020; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1020; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1020; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 5, 12), \"float32\"), ph_3: T.Buffer((17, 5, 12), \"float32\"), T_add: T.Buffer((17, 5, 12), \"float32\"), compute: T.Buffer((17, 5, 12), \"float32\"), T_multiply: T.Buffer((17, 5, 12), \"float32\"), compute_1: T.Buffer((17, 5, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1020,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1020):\n            T_add_1 = T.Buffer((1020,), data=T_add.data)\n            ph_3_1 = T.Buffer((1020,), data=ph_3.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1020):\n            compute_2 = T.Buffer((1020,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1020):\n            T_multiply_1 = T.Buffer((1020,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1020):\n            compute_2 = T.Buffer((1020,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "add",
                "acosh",
                "atanh",
                "multiply",
                "ceil"
            ]
        ],
        "input_shape": [[17, 5, 12], [13, 18, 10], [17, 5, 12]],
        "output_shape": [[17, 5, 12], [13, 18, 10], [17, 5, 12], [17, 5, 12], [17, 5, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1666; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1666; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 7; ++ax2) {\n        T_multiply[(((ax0 * 98) + (ax1 * 7)) + ax2)] = (ph_0[(((ax0 * 98) + (ax1 * 7)) + ax2)] * ph_3[(((ax0 * 98) + (ax1 * 7)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1666; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = expf((ph_0[i0_i1_fused_i2_fused_2] / ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanf(__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __cosf(__cosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 14, 7), \"float32\"), ph_3: T.Buffer((17, 14, 7), \"float32\"), T_multiply: T.Buffer((17, 14, 7), \"float32\"), compute: T.Buffer((17, 14, 7), \"float32\"), compute_1: T.Buffer((17, 14, 7), \"float32\"), compute_2: T.Buffer((17, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1666,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1666):\n            compute_3 = T.Buffer((1666,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1666):\n            compute_3 = T.Buffer((1666,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((1666,), data=ph_3.data)\n        for ax0 in T.parallel(17):\n            for ax1, ax2 in T.grid(14, 7):\n                cse_var_1: T.int32 = ax0 * 98 + ax1 * 7 + ax2\n                T_multiply_1 = T.Buffer((1666,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1666):\n            compute_3 = T.Buffer((1666,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "multiply",
                "cos",
                "atan",
                "cos",
                "exp"
            ]
        ],
        "input_shape": [[17, 14, 7], [20, 17, 6], [17, 14, 7]],
        "output_shape": [[20, 17, 6], [17, 14, 7], [17, 14, 7], [17, 14, 7], [17, 14, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 65; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 65; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 65; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n      T_divide[((ax1 * 5) + ax2)] = (fabsf(ph_0[((ax1 * 5) + ax2)]) / ph_0[((ax1 * 5) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = asinhf(fabsf(ph_0[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 13, 5), \"float32\"), ph_3: T.Buffer((1, 13, 5), \"float32\"), T_mod: T.Buffer((1, 13, 5), \"float32\"), compute: T.Buffer((1, 13, 5), \"float32\"), compute_1: T.Buffer((1, 13, 5), \"float32\"), T_divide: T.Buffer((1, 13, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((65,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(65):\n            T_mod_1 = T.Buffer((65,), data=T_mod.data)\n            ph_3_1 = T.Buffer((65,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(65):\n            compute_2 = T.Buffer((65,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(65):\n            compute_2 = T.Buffer((65,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax1, ax2 in T.grid(13, 5):\n            cse_var_1: T.int32 = ax1 * 5 + ax2\n            T_divide_1 = T.Buffer((65,), data=T_divide.data)\n            T_divide_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]",
        "op_args": [
            [
                "mod",
                "acosh",
                "abs",
                "asinh",
                "divide"
            ]
        ],
        "input_shape": [[1, 13, 5], [18, 3, 14], [1, 13, 5]],
        "output_shape": [[1, 13, 5], [18, 3, 14], [1, 13, 5], [1, 13, 5], [1, 13, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 20; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = atanf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    float compute_3[34];\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n        compute_3[((i1 * 17) + i2_1)] = expf(fabsf(ph_0[(((ax0 * 34) + (i1 * 17)) + i2_1)]));\n      }\n    }\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_subtract[(((ax0 * 34) + (ax1 * 17)) + ax2)] = (ph_0[(((ax0 * 34) + (ax1 * 17)) + ax2)] - compute_3[((ax1 * 17) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 340; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 20; ++i0_i1_fused_1) {\n    for (int32_t i2_2 = 0; i2_2 < 17; ++i2_2) {\n      compute_2[((i0_i1_fused_1 * 17) + i2_2)] = fabsf(asinf(ph_0[((i0_i1_fused_1 * 17) + i2_2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __expf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 2, 17), \"float32\"), compute: T.Buffer((10, 2, 17), \"float32\"), T_subtract: T.Buffer((10, 2, 17), \"float32\"), compute_1: T.Buffer((10, 2, 17), \"float32\"), compute_2: T.Buffer((10, 2, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((340,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(20):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_3 = T.Buffer((340,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(10):\n            compute_3 = T.allocate([34], \"float32\", \"global\")\n            compute_4 = T.Buffer((34,), data=compute_3)\n            for i1, i2 in T.grid(2, 17):\n                cse_var_2: T.int32 = i1 * 17\n                compute_4[cse_var_2 + i2] = T.exp(T.fabs(ph_0_1[ax0 * 34 + cse_var_2 + i2]))\n            for ax1, ax2 in T.grid(2, 17):\n                cse_var_4: T.int32 = ax1 * 17\n                cse_var_3: T.int32 = ax0 * 34 + cse_var_4 + ax2\n                T_subtract_1 = T.Buffer((340,), data=T_subtract.data)\n                T_subtract_1[cse_var_3] = ph_0_1[cse_var_3] - compute_4[cse_var_4 + ax2]\n        for i0_i1_fused_i2_fused in T.parallel(340):\n            compute_3 = T.Buffer((340,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(20):\n            for i2 in range(17):\n                cse_var_5: T.int32 = i0_i1_fused * 17 + i2\n                compute_3 = T.Buffer((340,), data=compute_2.data)\n                compute_3[cse_var_5] = T.fabs(T.asin(ph_0_1[cse_var_5]))",
        "op_args": [
            [
                "atan",
                "abs",
                "exp",
                "subtract",
                "asin",
                "atanh",
                "abs"
            ]
        ],
        "input_shape": [[10, 2, 17]],
        "output_shape": [[10, 2, 17], [10, 2, 17], [10, 2, 17], [10, 2, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1904; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1904; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute_1[(((i0 * 238) + (i1 * 17)) + i2)] = acoshf(ph_0[(((i0 * 238) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 14, 17), \"float32\"), compute: T.Buffer((8, 14, 17), \"float32\"), T_add: T.Buffer((8, 14, 17), \"float32\"), compute_1: T.Buffer((8, 14, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1904,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1904):\n            compute_2 = T.Buffer((1904,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1904):\n            T_add_1 = T.Buffer((1904,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(14, 17):\n                cse_var_1: T.int32 = i0 * 238 + i1 * 17 + i2\n                compute_2 = T.Buffer((1904,), data=compute_1.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "sin",
                "ceil",
                "add",
                "acosh"
            ]
        ],
        "input_shape": [[8, 14, 17]],
        "output_shape": [[8, 14, 17], [8, 14, 17], [8, 14, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2376; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 216; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_mod[((ax0_ax1_fused * 11) + ax2)] = fmodf(fabsf(ph_0[((ax0_ax1_fused * 11) + ax2)]), ph_0[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 216; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 11; ++ax2_1) {\n      T_divide[((ax0_ax1_fused_1 * 11) + ax2_1)] = ((ph_0[((ax0_ax1_fused_1 * 11) + ax2_1)] - acosf(ph_0[((ax0_ax1_fused_1 * 11) + ax2_1)])) / ph_0[((ax0_ax1_fused_1 * 11) + ax2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 18, 11), \"float32\"), compute: T.Buffer((12, 18, 11), \"float32\"), T_mod: T.Buffer((12, 18, 11), \"float32\"), T_divide: T.Buffer((12, 18, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2376,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2376):\n            compute_1 = T.Buffer((2376,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(216):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_mod_1 = T.Buffer((2376,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.fabs(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(216):\n            for ax2 in range(11):\n                cse_var_2: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_divide_1 = T.Buffer((2376,), data=T_divide.data)\n                T_divide_1[cse_var_2] = (ph_0_1[cse_var_2] - T.acos(ph_0_1[cse_var_2])) / ph_0_1[cse_var_2]",
        "op_args": [
            [
                "acosh",
                "abs",
                "mod",
                "acos",
                "subtract",
                "divide"
            ]
        ],
        "input_shape": [[12, 18, 11]],
        "output_shape": [[12, 18, 11], [12, 18, 11], [12, 18, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 156; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n      T_divide[((ax0_ax1_fused * 11) + ax2)] = (ph_0[((ax0_ax1_fused * 11) + ax2)] / ph_3[((ax0_ax1_fused * 11) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1716; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_1[(((i0 * 132) + (i1 * 11)) + i2)] = fabsf(ph_0[(((i0 * 132) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 12, 11), \"float32\"), ph_3: T.Buffer((13, 12, 11), \"float32\"), T_divide: T.Buffer((13, 12, 11), \"float32\"), compute: T.Buffer((13, 12, 11), \"float32\"), compute_1: T.Buffer((13, 12, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1716,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(156):\n            for ax2 in range(11):\n                cse_var_1: T.int32 = ax0_ax1_fused * 11 + ax2\n                T_divide_1 = T.Buffer((1716,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1716,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1716):\n            compute_2 = T.Buffer((1716,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(12, 11):\n                cse_var_2: T.int32 = i0 * 132 + i1 * 11 + i2\n                compute_2 = T.Buffer((1716,), data=compute_1.data)\n                compute_2[cse_var_2] = T.fabs(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "divide",
                "sin",
                "abs"
            ]
        ],
        "input_shape": [[13, 12, 11], [18, 10, 11], [13, 12, 11]],
        "output_shape": [[13, 12, 11], [18, 10, 11], [13, 12, 11], [13, 12, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = acoshf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 84; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 16) + ax2)] = (fabsf(ph_0[((ax0_ax1_fused * 16) + ax2)]) - ph_0[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1344; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], sinf(ph_0[ax0_ax1_fused_ax2_fused])) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], __sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 6, 16), \"float32\"), compute: T.Buffer((14, 6, 16), \"float32\"), T_subtract: T.Buffer((14, 6, 16), \"float32\"), T_divide: T.Buffer((14, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1344,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_1 = T.Buffer((1344,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(84):\n            for ax2 in range(16):\n                cse_var_2: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_subtract_1 = T.Buffer((1344,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.fabs(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1344):\n            T_divide_1 = T.Buffer((1344,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.sin(ph_0_1[ax0_ax1_fused_ax2_fused])) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acosh",
                "abs",
                "subtract",
                "sin",
                "mod",
                "divide"
            ]
        ],
        "input_shape": [[14, 6, 16]],
        "output_shape": [[14, 6, 16], [14, 6, 16], [14, 6, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3, float* ph_8) {\n  float auto_scheduler_layout_transform[57];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 399; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 399; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 399; ++ax0_ax1_fused_ax2_fused_1) {\n    T_multiply[ax0_ax1_fused_ax2_fused_1] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused_1]) * ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  for (int32_t ax4 = 0; ax4 < 19; ++ax4) {\n    for (int32_t ax8 = 0; ax8 < 3; ++ax8) {\n      auto_scheduler_layout_transform[((ax4 * 3) + ax8)] = ph_8[((ax8 * 19) + ax4)];\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 7; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_inner_init = 0; b_inner_init < 3; ++b_inner_init) {\n      T_batch_matmul_NN[((b_inner_init * 7) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 19; ++k_outer) {\n      for (int32_t b_inner = 0; b_inner < 3; ++b_inner) {\n        T_batch_matmul_NN[((b_inner * 7) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[((b_inner * 7) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (atanhf(ph_0[(((b_inner * 133) + (b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 19)) + k_outer)]) * auto_scheduler_layout_transform[((k_outer * 3) + b_inner)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_8) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float ph_8_shared[40];\n  for (int b_c_inner_init = 0; b_c_inner_init < 4; ++b_c_inner_init) {\n    for (int i_c_inner_init = 0; i_c_inner_init < 2; ++i_c_inner_init) {\n      T_batch_matmul_NN_local[((b_c_inner_init * 2) + i_c_inner_init)] = 0.000000e+00f;\n    }\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 10; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    ph_8_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))] = ph_8[((ax0_ax1_fused_ax2_fused_outer_outer * 4) + ((int)threadIdx.x))];\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 5; ++k_inner) {\n    for (int b_c_inner = 0; b_c_inner < 4; ++b_c_inner) {\n      for (int i_c_inner = 0; i_c_inner < 2; ++i_c_inner) {\n        T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_inner)] = (T_batch_matmul_NN_local[((b_c_inner * 2) + i_c_inner)] + (atanhf(ph_0[(((((((((int)threadIdx.x) >> 1) * 160) + (b_c_inner * 40)) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) & 1) * 10)) + (i_c_inner * 5)) + k_inner)]) * ph_8_shared[((((((int)threadIdx.x) >> 1) * 20) + (b_c_inner * 5)) + k_inner)]));\n      }\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    for (int i_inner = 0; i_inner < 2; ++i_inner) {\n      T_batch_matmul_NN[((((((((int)threadIdx.x) >> 1) * 32) + (b_inner * 8)) + (((int)blockIdx.x) * 4)) + ((((int)threadIdx.x) & 1) * 2)) + i_inner)] = T_batch_matmul_NN_local[((b_inner * 2) + i_inner)];\n    }\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 7, 19), \"float32\"), ph_3: T.Buffer((3, 7, 19), \"float32\"), ph_8: T.Buffer((3, 19, 1), \"float32\"), T_subtract: T.Buffer((3, 7, 19), \"float32\"), compute: T.Buffer((3, 7, 19), \"float32\"), T_multiply: T.Buffer((3, 7, 19), \"float32\"), T_batch_matmul_NN: T.Buffer((3, 7, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([57], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((399,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(399):\n            T_subtract_1 = T.Buffer((399,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((399,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(399):\n            compute_1 = T.Buffer((399,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(399):\n            T_multiply_1 = T.Buffer((399,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        auto_scheduler_layout_transform_1 = T.Buffer((57,), data=auto_scheduler_layout_transform)\n        for ax4, ax8 in T.grid(19, 3):\n            ph_8_1 = T.Buffer((57,), data=ph_8.data)\n            auto_scheduler_layout_transform_1[ax4 * 3 + ax8] = ph_8_1[ax8 * 19 + ax4]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(7):\n            T_batch_matmul_NN_1 = T.Buffer((21,), data=T_batch_matmul_NN.data)\n            for b_inner_init in range(3):\n                T_batch_matmul_NN_1[b_inner_init * 7 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for k_outer, b_inner in T.grid(19, 3):\n                cse_var_1: T.int32 = b_inner * 7 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + T.atanh(ph_0_1[b_inner * 133 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 19 + k_outer]) * auto_scheduler_layout_transform_1[k_outer * 3 + b_inner]",
        "op_args": [
            [
                "subtract",
                "atan",
                "atanh",
                "multiply",
                "batch_matmul"
            ]
        ],
        "input_shape": [[3, 7, 19], [1, 14, 8], [3, 7, 19], [3, 19, 1]],
        "output_shape": [[3, 7, 19], [1, 14, 8], [3, 7, 19], [3, 7, 19], [3, 7, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 190; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i0_i1_fused * 13) + i2)] = expf(ph_0[((i0_i1_fused * 13) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2470; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2470; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = cosf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2470; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 19, 13), \"float32\"), compute: T.Buffer((10, 19, 13), \"float32\"), compute_1: T.Buffer((10, 19, 13), \"float32\"), compute_2: T.Buffer((10, 19, 13), \"float32\"), T_subtract: T.Buffer((10, 19, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2470,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(190):\n            for i2 in range(13):\n                cse_var_1: T.int32 = i0_i1_fused * 13 + i2\n                compute_3 = T.Buffer((2470,), data=compute.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2470):\n            compute_3 = T.Buffer((2470,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2470):\n            compute_3 = T.Buffer((2470,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(2470):\n            T_subtract_1 = T.Buffer((2470,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "exp",
                "acosh",
                "acos",
                "cos",
                "acosh",
                "subtract"
            ]
        ],
        "input_shape": [[10, 19, 13]],
        "output_shape": [[10, 19, 13], [10, 19, 13], [10, 19, 13], [10, 19, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4760; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4760; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 340; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_2[((i0_i1_fused * 14) + i2)] = acoshf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 20, 14), \"float32\"), compute: T.Buffer((17, 20, 14), \"float32\"), compute_1: T.Buffer((17, 20, 14), \"float32\"), compute_2: T.Buffer((17, 20, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4760,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_3 = T.Buffer((4760,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4760):\n            compute_3 = T.Buffer((4760,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(340):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((4760,), data=compute_2.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acos",
                "asin",
                "atan",
                "acosh"
            ]
        ],
        "input_shape": [[17, 20, 14]],
        "output_shape": [[17, 20, 14], [17, 20, 14], [17, 20, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  float compute_2[14];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 14; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 14; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    compute_1[i0] = asinf(compute_2[i0]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 14; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / asinf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 1, 1), \"float32\"), compute: T.Buffer((14, 1, 1), \"float32\"), compute_1: T.Buffer((14, 1, 1), \"float32\"), T_divide: T.Buffer((14, 1, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_2 = T.allocate([14], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((14,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(14):\n            compute_3 = T.Buffer((14,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_3 = T.Buffer((14,), data=compute_2, align=32)\n        for i0_i1_fused_i2_fused in T.parallel(14):\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(14):\n            compute_4 = T.Buffer((14,), data=compute_1.data)\n            compute_4[i0] = T.asin(compute_3[i0])\n        for ax0_ax1_fused_ax2_fused in T.parallel(14):\n            T_divide_1 = T.Buffer((14,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "asin",
                "exp",
                "asin",
                "asin",
                "divide"
            ]
        ],
        "input_shape": [[14, 1, 1]],
        "output_shape": [[14, 1, 1], [14, 1, 1], [14, 1, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1632; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1632; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 102; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n      T_divide[((ax0_ax1_fused * 16) + ax2)] = (ph_0[((ax0_ax1_fused * 16) + ax2)] / ph_3[((ax0_ax1_fused * 16) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1632; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1632; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 6, 16), \"float32\"), ph_3: T.Buffer((17, 6, 16), \"float32\"), T_divide: T.Buffer((17, 6, 16), \"float32\"), compute: T.Buffer((17, 6, 16), \"float32\"), compute_1: T.Buffer((17, 6, 16), \"float32\"), T_add: T.Buffer((17, 6, 16), \"float32\"), compute_2: T.Buffer((17, 6, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1632,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1632):\n            compute_3 = T.Buffer((1632,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1632):\n            compute_3 = T.Buffer((1632,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((1632,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(102):\n            for ax2 in range(16):\n                cse_var_1: T.int32 = ax0_ax1_fused * 16 + ax2\n                T_divide_1 = T.Buffer((1632,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(1632):\n            T_add_1 = T.Buffer((1632,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(1632):\n            compute_3 = T.Buffer((1632,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "divide",
                "cos",
                "asinh",
                "exp",
                "add",
                "sin"
            ]
        ],
        "input_shape": [[17, 6, 16], [8, 15, 5], [17, 6, 16]],
        "output_shape": [[8, 15, 5], [17, 6, 16], [17, 6, 16], [17, 6, 16], [17, 6, 16], [17, 6, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 140; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = asinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 980; ++i0_i1_fused_i2_fused) {\n    float compute_4[1];\n    compute_4[0] = expf(ph_0[i0_i1_fused_i2_fused]);\n    compute_1[i0_i1_fused_i2_fused] = sinf(compute_4[0]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 980; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 980; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = ceilf(asinhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 14, 7), \"float32\"), compute: T.Buffer((10, 14, 7), \"float32\"), compute_1: T.Buffer((10, 14, 7), \"float32\"), compute_2: T.Buffer((10, 14, 7), \"float32\"), compute_3: T.Buffer((10, 14, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((980,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(140):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_4 = T.Buffer((980,), data=compute.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(980):\n            compute_4 = T.allocate([1], \"float32\", \"global\")\n            compute_5 = T.Buffer((1,), data=compute_4, align=4)\n            compute_5[0] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n            compute_6 = T.Buffer((980,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.sin(compute_5[0])\n        for i0_i1_fused_i2_fused in T.parallel(980):\n            compute_4 = T.Buffer((980,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(980):\n            compute_4 = T.Buffer((980,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asin",
                "exp",
                "sin",
                "acosh",
                "asinh",
                "ceil"
            ]
        ],
        "input_shape": [[10, 14, 7]],
        "output_shape": [[10, 14, 7], [10, 14, 7], [10, 14, 7], [10, 14, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 12; ++ax2) {\n        T_divide[(((ax0 * 84) + (ax1 * 12)) + ax2)] = (ph_0[(((ax0 * 84) + (ax1 * 12)) + ax2)] / ph_3[(((ax0 * 84) + (ax1 * 12)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 756; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 84) + (i1 * 12)) + i2)] = asinf(asinhf(ph_0[(((i0 * 84) + (i1 * 12)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 7, 12), \"float32\"), ph_3: T.Buffer((9, 7, 12), \"float32\"), T_divide: T.Buffer((9, 7, 12), \"float32\"), compute: T.Buffer((9, 7, 12), \"float32\"), compute_1: T.Buffer((9, 7, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((756,), data=ph_0.data)\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(7, 12):\n                cse_var_1: T.int32 = ax0 * 84 + ax1 * 12 + ax2\n                T_divide_1 = T.Buffer((756,), data=T_divide.data)\n                ph_3_1 = T.Buffer((756,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(756):\n            compute_2 = T.Buffer((756,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(7, 12):\n                cse_var_2: T.int32 = i0 * 84 + i1 * 12 + i2\n                compute_2 = T.Buffer((756,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "atan",
                "asinh",
                "asin"
            ]
        ],
        "input_shape": [[9, 7, 12], [4, 18, 16], [9, 7, 12]],
        "output_shape": [[9, 7, 12], [4, 18, 16], [9, 7, 12], [9, 7, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 120) + (i1 * 12)) + i2)] = atanf((ph_0[(((i0 * 120) + (i1 * 12)) + i2)] + cosf(ph_0[(((i0 * 120) + (i1 * 12)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 10; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n        compute_1[(((i0_1 * 120) + (i1_1 * 12)) + i2_1)] = acosf(ph_0[(((i0_1 * 120) + (i1_1 * 12)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1920; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 10, 12), \"float32\"), compute: T.Buffer((16, 10, 12), \"float32\"), compute_1: T.Buffer((16, 10, 12), \"float32\"), compute_2: T.Buffer((16, 10, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1920,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(10, 12):\n                cse_var_1: T.int32 = i0 * 120 + i1 * 12 + i2\n                compute_3 = T.Buffer((1920,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1] + T.cos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(10, 12):\n                cse_var_2: T.int32 = i0 * 120 + i1 * 12 + i2\n                compute_3 = T.Buffer((1920,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1920):\n            compute_3 = T.Buffer((1920,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "cos",
                "add",
                "atan",
                "acos",
                "exp"
            ]
        ],
        "input_shape": [[16, 10, 12]],
        "output_shape": [[16, 10, 12], [16, 10, 12], [16, 10, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2700; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2700; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_2[(((i0 * 180) + (i1 * 18)) + i2)] = ceilf(ph_0[(((i0 * 180) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2700; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 10, 18), \"float32\"), compute: T.Buffer((15, 10, 18), \"float32\"), compute_1: T.Buffer((15, 10, 18), \"float32\"), compute_2: T.Buffer((15, 10, 18), \"float32\"), compute_3: T.Buffer((15, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2700,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2700):\n            compute_4 = T.Buffer((2700,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2700):\n            compute_4 = T.Buffer((2700,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(10, 18):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 18 + i2\n                compute_4 = T.Buffer((2700,), data=compute_2.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2700):\n            compute_4 = T.Buffer((2700,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "ceil",
                "acosh",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[15, 10, 18]],
        "output_shape": [[15, 10, 18], [15, 10, 18], [15, 10, 18], [15, 10, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 66; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = atanf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 528; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / (asinf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 66; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 8) + i2_1)] = asinf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / (asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 11, 8), \"float32\"), compute: T.Buffer((6, 11, 8), \"float32\"), T_divide: T.Buffer((6, 11, 8), \"float32\"), compute_1: T.Buffer((6, 11, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((528,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(66):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((528,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(528):\n            T_divide_1 = T.Buffer((528,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / (T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(66):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((528,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atan",
                "asin",
                "divide",
                "divide",
                "asin"
            ]
        ],
        "input_shape": [[6, 11, 8]],
        "output_shape": [[6, 11, 8], [6, 11, 8], [6, 11, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3978; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 221; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = acoshf((ph_0[((i0_i1_fused * 18) + i2)] / acosf(ph_0[((i0_i1_fused * 18) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 3978; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_1] = fmodf((ph_0[ax0_ax1_fused_ax2_fused_1] / acosf(ph_0[ax0_ax1_fused_ax2_fused_1])), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 13, 18), \"float32\"), ph_3: T.Buffer((17, 13, 18), \"float32\"), T_mod: T.Buffer((17, 13, 18), \"float32\"), compute: T.Buffer((17, 13, 18), \"float32\"), T_mod_1: T.Buffer((17, 13, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3978,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3978):\n            T_mod_2 = T.Buffer((3978,), data=T_mod.data)\n            ph_3_1 = T.Buffer((3978,), data=ph_3.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(221):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_1 = T.Buffer((3978,), data=compute.data)\n                compute_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1] / T.acos(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(3978):\n            T_mod_2 = T.Buffer((3978,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused] / T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "mod",
                "acos",
                "divide",
                "acosh",
                "mod"
            ]
        ],
        "input_shape": [[17, 13, 18], [5, 17, 5], [17, 13, 18]],
        "output_shape": [[17, 13, 18], [5, 17, 5], [17, 13, 18], [17, 13, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3040; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute_1[(((i0 * 160) + (i1 * 20)) + i2)] = fabsf(asinhf(ph_0[(((i0 * 160) + (i1 * 20)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 152; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_2[((i0_i1_fused * 20) + i2_1)] = asinf(ph_0[((i0_i1_fused * 20) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 8, 20), \"float32\"), compute: T.Buffer((19, 8, 20), \"float32\"), compute_1: T.Buffer((19, 8, 20), \"float32\"), compute_2: T.Buffer((19, 8, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3040,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3040):\n            compute_3 = T.Buffer((3040,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(8, 20):\n                cse_var_1: T.int32 = i0 * 160 + i1 * 20 + i2\n                compute_3 = T.Buffer((3040,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(152):\n            for i2 in range(20):\n                cse_var_2: T.int32 = i0_i1_fused * 20 + i2\n                compute_3 = T.Buffer((3040,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "cos",
                "asinh",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[19, 8, 20]],
        "output_shape": [[19, 8, 20], [19, 8, 20], [19, 8, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 3840; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + (fmodf(ph_0[ax0_ax1_fused_ax2_fused], atanf(ph_0[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 12, 16), \"float32\"), T_add: T.Buffer((20, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for ax0_ax1_fused_ax2_fused in T.parallel(3840):\n            T_add_1 = T.Buffer((3840,), data=T_add.data)\n            ph_0_1 = T.Buffer((3840,), data=ph_0.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + (T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.atan(ph_0_1[ax0_ax1_fused_ax2_fused])) - ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "atan",
                "mod",
                "subtract",
                "add"
            ]
        ],
        "input_shape": [[20, 12, 16]],
        "output_shape": [[20, 12, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2880; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / cosf((ph_0[ax0_ax1_fused_ax2_fused] * asinf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2880; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2880; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / __cosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 10, 18), \"float32\"), T_divide: T.Buffer((16, 10, 18), \"float32\"), compute: T.Buffer((16, 10, 18), \"float32\"), T_add: T.Buffer((16, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_divide_1 = T.Buffer((2880,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.cos(ph_0_1[ax0_ax1_fused_ax2_fused] * T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_1 = T.Buffer((2880,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.exp(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(2880):\n            T_add_1 = T.Buffer((2880,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "asin",
                "multiply",
                "cos",
                "divide",
                "acosh",
                "exp",
                "add"
            ]
        ],
        "input_shape": [[16, 10, 18]],
        "output_shape": [[16, 10, 18], [16, 10, 18], [16, 10, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 272; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n      T_mod[((ax0_ax1_fused * 19) + ax2)] = fmodf(ph_0[((ax0_ax1_fused * 19) + ax2)], ph_3[((ax0_ax1_fused * 19) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5168; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5168; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(fabsf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute_2[(((i0 * 323) + (i1 * 19)) + i2)] = acosf(fabsf(ph_0[(((i0 * 323) + (i1 * 19)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 17, 19), \"float32\"), ph_3: T.Buffer((16, 17, 19), \"float32\"), T_mod: T.Buffer((16, 17, 19), \"float32\"), compute: T.Buffer((16, 17, 19), \"float32\"), compute_1: T.Buffer((16, 17, 19), \"float32\"), compute_2: T.Buffer((16, 17, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5168,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(272):\n            for ax2 in range(19):\n                cse_var_1: T.int32 = ax0_ax1_fused * 19 + ax2\n                T_mod_1 = T.Buffer((5168,), data=T_mod.data)\n                ph_3_1 = T.Buffer((5168,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(5168):\n            compute_3 = T.Buffer((5168,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(5168):\n            compute_3 = T.Buffer((5168,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(17, 19):\n                cse_var_2: T.int32 = i0 * 323 + i1 * 19 + i2\n                compute_3 = T.Buffer((5168,), data=compute_2.data)\n                compute_3[cse_var_2] = T.acos(T.fabs(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "abs",
                "abs",
                "ceil",
                "acos"
            ]
        ],
        "input_shape": [[16, 17, 19], [5, 19, 20], [16, 17, 19]],
        "output_shape": [[16, 17, 19], [5, 19, 20], [16, 17, 19], [16, 17, 19], [16, 17, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_multiply, float* compute, float* ph_0, float* ph_3, float* ph_5) {\n  float auto_scheduler_layout_transform[80];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 16; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t ax3 = 0; ax3 < 5; ++ax3) {\n    for (int32_t ax4 = 0; ax4 < 2; ++ax4) {\n      for (int32_t ax8 = 0; ax8 < 8; ++ax8) {\n        auto_scheduler_layout_transform[(((ax3 * 16) + (ax4 * 8)) + ax8)] = ph_3[(((ax8 * 10) + (ax4 * 5)) + ax3)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 5; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_inner_init = 0; b_inner_init < 8; ++b_inner_init) {\n      T_batch_matmul_NN[((b_inner_init * 5) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = 0.000000e+00f;\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t b_inner = 0; b_inner < 8; ++b_inner) {\n        T_batch_matmul_NN[((b_inner * 5) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] = (T_batch_matmul_NN[((b_inner * 5) + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused)] + (ph_0[((b_inner * 2) + k_outer)] * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 16) + (k_outer * 8)) + b_inner)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 16; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_5[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(20) default_function_kernel_1(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN_local[8];\n  __shared__ float ph_3_shared[40];\n  for (int b_c_outer_inner_init = 0; b_c_outer_inner_init < 4; ++b_c_outer_inner_init) {\n    T_batch_matmul_NN_local[b_c_outer_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN_local[(b_c_outer_inner_init + 4)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 5; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 20) + ((int)threadIdx.x))] = ph_3[((((ax0_ax1_fused_ax2_fused_outer_outer * 100) + ((((int)threadIdx.x) / 5) * 25)) + (k_outer_outer * 5)) + (((int)threadIdx.x) % 5))];\n    }\n    __syncthreads();\n    for (int b_c_outer_inner = 0; b_c_outer_inner < 4; ++b_c_outer_inner) {\n      T_batch_matmul_NN_local[b_c_outer_inner] = (T_batch_matmul_NN_local[b_c_outer_inner] + (ph_0[((((b_c_outer_inner * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) / 5) * 5)) + k_outer_outer)] * ph_3_shared[((b_c_outer_inner * 5) + (((int)threadIdx.x) % 5))]));\n      T_batch_matmul_NN_local[(b_c_outer_inner + 4)] = (T_batch_matmul_NN_local[(b_c_outer_inner + 4)] + (ph_0[(((((b_c_outer_inner * 40) + (((int)blockIdx.x) * 20)) + ((((int)threadIdx.x) / 5) * 5)) + k_outer_outer) + 160)] * ph_3_shared[(((b_c_outer_inner * 5) + (((int)threadIdx.x) % 5)) + 20)]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 4; ++b_inner) {\n    T_batch_matmul_NN[(((b_inner * 40) + (((int)blockIdx.x) * 20)) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[b_inner];\n    T_batch_matmul_NN[((((b_inner * 40) + (((int)blockIdx.x) * 20)) + ((int)threadIdx.x)) + 160)] = T_batch_matmul_NN_local[(b_inner + 4)];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_5) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_5[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 1, 2), \"float32\"), ph_3: T.Buffer((8, 2, 5), \"float32\"), ph_5: T.Buffer((8, 1, 2), \"float32\"), compute: T.Buffer((8, 1, 2), \"float32\"), T_batch_matmul_NN: T.Buffer((8, 1, 5), \"float32\"), T_multiply: T.Buffer((8, 1, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([80], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((16,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(16):\n            compute_1 = T.Buffer((16,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        auto_scheduler_layout_transform_1 = T.Buffer((80,), data=auto_scheduler_layout_transform)\n        for ax3, ax4, ax8 in T.grid(5, 2, 8):\n            ph_3_1 = T.Buffer((80,), data=ph_3.data)\n            auto_scheduler_layout_transform_1[ax3 * 16 + ax4 * 8 + ax8] = ph_3_1[ax8 * 10 + ax4 * 5 + ax3]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(5):\n            T_batch_matmul_NN_1 = T.Buffer((40,), data=T_batch_matmul_NN.data)\n            for b_inner_init in range(8):\n                T_batch_matmul_NN_1[b_inner_init * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused] = T.float32(0)\n            for k_outer, b_inner in T.grid(2, 8):\n                cse_var_1: T.int32 = b_inner * 5 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused\n                T_batch_matmul_NN_1[cse_var_1] = T_batch_matmul_NN_1[cse_var_1] + ph_0_1[b_inner * 2 + k_outer] * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 16 + k_outer * 8 + b_inner]\n        for ax0_ax1_fused_ax2_fused in T.parallel(16):\n            T_multiply_1 = T.Buffer((16,), data=T_multiply.data)\n            ph_5_1 = T.Buffer((16,), data=ph_5.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_5_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "acosh",
                "batch_matmul",
                "multiply"
            ]
        ],
        "input_shape": [[8, 1, 2], [8, 2, 5], [8, 1, 2]],
        "output_shape": [[8, 1, 2], [8, 1, 5], [8, 1, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1320; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_subtract[(((ax0 * 88) + (ax1 * 11)) + ax2)] = (fabsf(ph_0[(((ax0 * 88) + (ax1 * 11)) + ax2)]) - ph_0[(((ax0 * 88) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 120; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_1[((i0_i1_fused * 11) + i2)] = expf(fabsf(ph_0[((i0_i1_fused * 11) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1320; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 8, 11), \"float32\"), ph_3: T.Buffer((15, 8, 11), \"float32\"), compute: T.Buffer((15, 8, 11), \"float32\"), T_subtract: T.Buffer((15, 8, 11), \"float32\"), compute_1: T.Buffer((15, 8, 11), \"float32\"), T_multiply: T.Buffer((15, 8, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1320,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1320):\n            compute_2 = T.Buffer((1320,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(8, 11):\n                cse_var_1: T.int32 = ax0 * 88 + ax1 * 11 + ax2\n                T_subtract_1 = T.Buffer((1320,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = T.fabs(ph_0_1[cse_var_1]) - ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(120):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_2 = T.Buffer((1320,), data=compute_1.data)\n                compute_2[cse_var_2] = T.exp(T.fabs(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1320):\n            T_multiply_1 = T.Buffer((1320,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((1320,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused] * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "asin",
                "abs",
                "subtract",
                "exp",
                "multiply"
            ]
        ],
        "input_shape": [[15, 8, 11], [12, 3, 4], [15, 8, 11]],
        "output_shape": [[12, 3, 4], [15, 8, 11], [15, 8, 11], [15, 8, 11], [15, 8, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 672; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (atanhf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_1[(((i0 * 84) + (i1 * 14)) + i2)] = asinf(ph_0[(((i0 * 84) + (i1 * 14)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 672; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n        T_subtract[(((ax0 * 84) + (ax1 * 14)) + ax2)] = (asinf(ph_0[(((ax0 * 84) + (ax1 * 14)) + ax2)]) - ph_0[(((ax0 * 84) + (ax1 * 14)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 14), \"float32\"), compute: T.Buffer((8, 6, 14), \"float32\"), T_add: T.Buffer((8, 6, 14), \"float32\"), compute_1: T.Buffer((8, 6, 14), \"float32\"), compute_2: T.Buffer((8, 6, 14), \"float32\"), T_subtract: T.Buffer((8, 6, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((672,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_3 = T.Buffer((672,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            T_add_1 = T.Buffer((672,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(6, 14):\n                cse_var_1: T.int32 = i0 * 84 + i1 * 14 + i2\n                compute_3 = T.Buffer((672,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_3 = T.Buffer((672,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(6, 14):\n                cse_var_2: T.int32 = ax0 * 84 + ax1 * 14 + ax2\n                T_subtract_1 = T.Buffer((672,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.asin(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "asin",
                "atanh",
                "add",
                "asin",
                "asin",
                "abs",
                "subtract"
            ]
        ],
        "input_shape": [[8, 6, 14]],
        "output_shape": [[8, 6, 14], [8, 6, 14], [8, 6, 14], [8, 6, 14], [8, 6, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 300; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = acoshf((ph_0[((i0_i1_fused * 15) + i2)] / ceilf(ph_0[((i0_i1_fused * 15) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4500; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 15, 15), \"float32\"), compute: T.Buffer((20, 15, 15), \"float32\"), compute_1: T.Buffer((20, 15, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4500,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(300):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((4500,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1] / T.ceil(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(4500):\n            compute_2 = T.Buffer((4500,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "divide",
                "acosh",
                "acosh"
            ]
        ],
        "input_shape": [[20, 15, 15]],
        "output_shape": [[20, 15, 15], [20, 15, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      T_divide[((ax0 * 6) + ax1)] = (ph_0[((ax0 * 6) + ax1)] / ph_3[((ax0 * 6) + ax1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 36; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf((ph_0[i0_i1_fused_i2_fused] + (ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 6, 1), \"float32\"), ph_3: T.Buffer((6, 6, 1), \"float32\"), T_divide: T.Buffer((6, 6, 1), \"float32\"), compute: T.Buffer((6, 6, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((36,), data=ph_0.data)\n        ph_3_1 = T.Buffer((36,), data=ph_3.data)\n        for ax0 in T.parallel(6):\n            for ax1 in range(6):\n                cse_var_1: T.int32 = ax0 * 6 + ax1\n                T_divide_1 = T.Buffer((36,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_1 = T.Buffer((36,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] + ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "multiply",
                "add",
                "asin"
            ]
        ],
        "input_shape": [[6, 6, 1], [6, 5, 9], [6, 6, 1]],
        "output_shape": [[6, 6, 1], [6, 5, 9], [6, 6, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3, float* ph_8) {\n  float auto_scheduler_layout_transform[50];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 750; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 750; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n      for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused * 25) + (ax4 * 5)) + ax8)] = ph_8[(((ax0_ax1_fused_ax2_fused * 25) + (ax8 * 5)) + ax4)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 3; ++i_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75) + (b_inner_init * 15)) + (i_outer_inner_init * 5)) + i_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 5; ++k_outer) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 3; ++i_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n          for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n            T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75) + (b_inner * 15)) + (i_outer_inner * 5)) + i_inner)] = (T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75) + (b_inner * 15)) + (i_outer_inner * 5)) + i_inner)] + (acoshf(ph_0[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 375) + (b_inner * 75)) + (i_outer_inner * 25)) + (i_inner * 5)) + k_outer)]) * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 25) + (k_outer * 5)) + b_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 750; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 750; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((int)blockIdx.x)] = (fmodf(ph_0[((int)blockIdx.x)], ph_3[((int)blockIdx.x)]) - ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0) {\n  float T_batch_matmul_NN_local[32];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 4; ++i_c_outer_inner_init) {\n    for (int j_c_outer_inner_init = 0; j_c_outer_inner_init < 4; ++j_c_outer_inner_init) {\n      for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n        T_batch_matmul_NN_local[(((i_c_outer_inner_init * 8) + (j_c_outer_inner_init * 2)) + j_c_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 4; ++i_c_outer_inner) {\n      for (int j_c_outer_inner = 0; j_c_outer_inner < 4; ++j_c_outer_inner) {\n        for (int k_inner = 0; k_inner < 4; ++k_inner) {\n          for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n            T_batch_matmul_NN_local[(((i_c_outer_inner * 8) + (j_c_outer_inner * 2)) + j_c_inner)] = (T_batch_matmul_NN_local[(((i_c_outer_inner * 8) + (j_c_outer_inner * 2)) + j_c_inner)] + (acoshf(ph_0[((((((int)threadIdx.x) * 32) + (i_c_outer_inner * 8)) + (k_outer_outer * 4)) + k_inner)]) * ph_0[((((((((int)threadIdx.x) >> 1) * 64) + (k_outer_outer * 32)) + (k_inner * 8)) + (j_c_outer_inner * 2)) + j_c_inner)]));\n          }\n        }\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 4; ++i_inner) {\n    for (int j_inner = 0; j_inner < 8; ++j_inner) {\n      T_batch_matmul_NN[(((((int)threadIdx.x) * 32) + (i_inner * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 15, 5), \"float32\"), ph_3: T.Buffer((10, 15, 5), \"float32\"), ph_8: T.Buffer((10, 5, 1), \"float32\"), compute: T.Buffer((10, 15, 5), \"float32\"), compute_1: T.Buffer((10, 15, 5), \"float32\"), T_batch_matmul_NN: T.Buffer((10, 15, 1), \"float32\"), T_subtract: T.Buffer((10, 15, 5), \"float32\"), compute_2: T.Buffer((10, 15, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([50], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((750,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_3 = T.Buffer((750,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_3 = T.Buffer((750,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((50,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2):\n            for ax4, ax8 in T.grid(5, 5):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 25\n                ph_8_1 = T.Buffer((50,), data=ph_8.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 5 + ax8] = ph_8_1[cse_var_1 + ax8 * 5 + ax4]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((150,), data=T_batch_matmul_NN.data)\n            for i_outer_inner_init, b_inner_init, i_inner_init in T.grid(3, 5, 5):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75 + b_inner_init * 15 + i_outer_inner_init * 5 + i_inner_init] = T.float32(0)\n            for k_outer, i_outer_inner, b_inner, i_inner in T.grid(5, 3, 5, 5):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75 + b_inner * 15 + i_outer_inner * 5 + i_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.acosh(ph_0_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 375 + b_inner * 75 + i_outer_inner * 25 + i_inner * 5 + k_outer]) * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 25 + k_outer * 5 + b_inner]\n        ph_3_1 = T.Buffer((750,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(750):\n            T_subtract_1 = T.Buffer((750,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_3 = T.Buffer((750,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "acosh",
                "acosh",
                "ceil",
                "batch_matmul",
                "subtract",
                "acos"
            ]
        ],
        "input_shape": [[10, 15, 5], [10, 20, 3], [10, 15, 5], [10, 5, 1]],
        "output_shape": [[10, 20, 3], [10, 15, 5], [10, 15, 5], [10, 15, 1], [10, 15, 5], [10, 15, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 26; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (acosf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 26; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 26; ++ax0_ax1_fused) {\n    T_multiply[ax0_ax1_fused] = (ph_0[ax0_ax1_fused] * ph_3[ax0_ax1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 26; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf((ph_0[i0_i1_fused_i2_fused_1] * ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 2, 1), \"float32\"), ph_3: T.Buffer((13, 2, 1), \"float32\"), T_multiply: T.Buffer((13, 2, 1), \"float32\"), T_divide: T.Buffer((13, 2, 1), \"float32\"), compute: T.Buffer((13, 2, 1), \"float32\"), compute_1: T.Buffer((13, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((26,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(26):\n            T_divide_1 = T.Buffer((26,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.acos(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(26):\n            compute_2 = T.Buffer((26,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((26,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(26):\n            T_multiply_1 = T.Buffer((26,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused] = ph_0_1[ax0_ax1_fused] * ph_3_1[ax0_ax1_fused]\n        for i0_i1_fused_i2_fused in T.parallel(26):\n            compute_2 = T.Buffer((26,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "multiply",
                "acos",
                "divide",
                "asin",
                "exp"
            ]
        ],
        "input_shape": [[13, 2, 1], [2, 16, 2], [13, 2, 1]],
        "output_shape": [[2, 16, 2], [13, 2, 1], [13, 2, 1], [13, 2, 1], [13, 2, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 5; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 70) + (i1 * 10)) + i2)] = sinf(ph_0[(((i0 * 70) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_1[((i0_i1_fused * 10) + i2_1)] = ceilf(acoshf(ph_0[((i0_i1_fused * 10) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 350; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 7, 10), \"float32\"), compute: T.Buffer((5, 7, 10), \"float32\"), compute_1: T.Buffer((5, 7, 10), \"float32\"), compute_2: T.Buffer((5, 7, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((350,), data=ph_0.data)\n        for i0 in T.parallel(5):\n            for i1, i2 in T.grid(7, 10):\n                cse_var_1: T.int32 = i0 * 70 + i1 * 10 + i2\n                compute_3 = T.Buffer((350,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(35):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_3 = T.Buffer((350,), data=compute_1.data)\n                compute_3[cse_var_2] = T.ceil(T.acosh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(350):\n            compute_3 = T.Buffer((350,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "acosh",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[5, 7, 10]],
        "output_shape": [[5, 7, 10], [5, 7, 10], [5, 7, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 243; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 243; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] / (ph_0[i0_i1_fused_i2_fused_1] - fmodf(asinf(ph_0[i0_i1_fused_i2_fused_1]), ph_0[i0_i1_fused_i2_fused_1]))));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 81; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n      T_mod[((ax0_ax1_fused * 3) + ax2)] = fmodf((ph_0[((ax0_ax1_fused * 3) + ax2)] / (ph_0[((ax0_ax1_fused * 3) + ax2)] - fmodf(asinf(ph_0[((ax0_ax1_fused * 3) + ax2)]), ph_0[((ax0_ax1_fused * 3) + ax2)]))), ph_0[((ax0_ax1_fused * 3) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((int)blockIdx.x)] = fmodf((ph_0[((int)blockIdx.x)] / (ph_0[((int)blockIdx.x)] - fmodf(asinf(ph_0[((int)blockIdx.x)]), ph_0[((int)blockIdx.x)]))), ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - fmodf(asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 9, 3), \"float32\"), compute: T.Buffer((9, 9, 3), \"float32\"), compute_1: T.Buffer((9, 9, 3), \"float32\"), T_mod: T.Buffer((9, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((243,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(243):\n            compute_2 = T.Buffer((243,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(243):\n            compute_2 = T.Buffer((243,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] / (ph_0_1[i0_i1_fused_i2_fused] - T.truncmod(T.asin(ph_0_1[i0_i1_fused_i2_fused]), ph_0_1[i0_i1_fused_i2_fused])))\n        for ax0_ax1_fused in T.parallel(81):\n            for ax2 in range(3):\n                cse_var_1: T.int32 = ax0_ax1_fused * 3 + ax2\n                T_mod_1 = T.Buffer((243,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1] / (ph_0_1[cse_var_1] - T.truncmod(T.asin(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])), ph_0_1[cse_var_1])",
        "op_args": [
            [
                "abs",
                "asin",
                "mod",
                "subtract",
                "divide",
                "sin",
                "mod"
            ]
        ],
        "input_shape": [[9, 9, 3]],
        "output_shape": [[9, 9, 3], [9, 9, 3], [9, 9, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf((ph_0[i0_i1_fused_i2_fused] + cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 64; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_add[((ax0_ax1_fused * 10) + ax2)] = ((ph_0[((ax0_ax1_fused * 10) + ax2)] + cosf(ph_0[((ax0_ax1_fused * 10) + ax2)])) + ph_0[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 640; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 8, 10), \"float32\"), ph_3: T.Buffer((8, 8, 10), \"float32\"), compute: T.Buffer((8, 8, 10), \"float32\"), T_add: T.Buffer((8, 8, 10), \"float32\"), compute_1: T.Buffer((8, 8, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((640,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            compute_2 = T.Buffer((640,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] + T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(64):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_add_1 = T.Buffer((640,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.cos(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            compute_2 = T.Buffer((640,), data=compute_1.data)\n            ph_3_1 = T.Buffer((640,), data=ph_3.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "cos",
                "add",
                "asin",
                "add",
                "acos"
            ]
        ],
        "input_shape": [[8, 8, 10], [14, 5, 14], [8, 8, 10]],
        "output_shape": [[14, 5, 14], [8, 8, 10], [8, 8, 10], [8, 8, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* T_divide_1, float* compute, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_add[((ax1 * 13) + ax2)] = (ph_0[((ax1 * 13) + ax2)] + ph_3[((ax1 * 13) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 156; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 156; ++ax0_ax1_fused_ax2_fused_1) {\n    T_divide_1[ax0_ax1_fused_ax2_fused_1] = (sinf(ph_0[ax0_ax1_fused_ax2_fused_1]) / ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  for (int32_t i1 = 0; i1 < 12; ++i1) {\n    for (int32_t i2 = 0; i2 < 13; ++i2) {\n      compute[((i1 * 13) + i2)] = expf(sinf(ph_0[((i1 * 13) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 12, 13), \"float32\"), ph_3: T.Buffer((1, 12, 13), \"float32\"), T_add: T.Buffer((1, 12, 13), \"float32\"), T_divide: T.Buffer((1, 12, 13), \"float32\"), T_divide_1: T.Buffer((1, 12, 13), \"float32\"), compute: T.Buffer((1, 12, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((156,), data=ph_0.data)\n        ph_3_1 = T.Buffer((156,), data=ph_3.data)\n        for ax1, ax2 in T.grid(12, 13):\n            cse_var_1: T.int32 = ax1 * 13 + ax2\n            T_add_1 = T.Buffer((156,), data=T_add.data)\n            T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(156):\n            T_divide_2 = T.Buffer((156,), data=T_divide.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(156):\n            T_divide_2 = T.Buffer((156,), data=T_divide_1.data)\n            T_divide_2[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i1, i2 in T.grid(12, 13):\n            cse_var_2: T.int32 = i1 * 13 + i2\n            compute_1 = T.Buffer((156,), data=compute.data)\n            compute_1[cse_var_2] = T.exp(T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "add",
                "divide",
                "sin",
                "divide",
                "exp"
            ]
        ],
        "input_shape": [[1, 12, 13], [1, 3, 13], [1, 12, 13]],
        "output_shape": [[1, 12, 13], [1, 3, 13], [1, 12, 13], [1, 12, 13], [1, 12, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused], atanhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 180; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 180; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 20, 1), \"float32\"), compute: T.Buffer((9, 20, 1), \"float32\"), compute_1: T.Buffer((9, 20, 1), \"float32\"), compute_2: T.Buffer((9, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.atanh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_3 = T.Buffer((180,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "mod",
                "acosh",
                "abs",
                "cos",
                "asinh"
            ]
        ],
        "input_shape": [[9, 20, 1]],
        "output_shape": [[9, 20, 1], [9, 20, 1], [9, 20, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 336; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused], ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 48) + (i1 * 4)) + i2)] = expf(ph_0[(((i0 * 48) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 84; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n      compute_2[((i0_i1_fused * 4) + i2_1)] = expf(ph_0[((i0_i1_fused * 4) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 12, 4), \"float32\"), compute: T.Buffer((7, 12, 4), \"float32\"), compute_1: T.Buffer((7, 12, 4), \"float32\"), compute_2: T.Buffer((7, 12, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((336,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(336):\n            compute_3 = T.Buffer((336,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.ceil(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(12, 4):\n                cse_var_1: T.int32 = i0 * 48 + i1 * 4 + i2\n                compute_3 = T.Buffer((336,), data=compute_1.data)\n                compute_3[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(84):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_3 = T.Buffer((336,), data=compute_2.data)\n                compute_3[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "ceil",
                "mod",
                "atanh",
                "exp",
                "exp"
            ]
        ],
        "input_shape": [[7, 12, 4]],
        "output_shape": [[7, 12, 4], [7, 12, 4], [7, 12, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 468; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 468; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acoshf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 36) + (i1 * 4)) + i2)] = fabsf(ph_0[(((i0 * 36) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_add[(((ax0 * 36) + (ax1 * 4)) + ax2)] = (acosf(ph_0[(((ax0 * 36) + (ax1 * 4)) + ax2)]) + ph_0[(((ax0 * 36) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 4), \"float32\"), compute: T.Buffer((13, 9, 4), \"float32\"), T_mod: T.Buffer((13, 9, 4), \"float32\"), compute_1: T.Buffer((13, 9, 4), \"float32\"), T_add: T.Buffer((13, 9, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((468,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(468):\n            compute_2 = T.Buffer((468,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(468):\n            T_mod_1 = T.Buffer((468,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(9, 4):\n                cse_var_1: T.int32 = i0 * 36 + i1 * 4 + i2\n                compute_2 = T.Buffer((468,), data=compute_1.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(9, 4):\n                cse_var_2: T.int32 = ax0 * 36 + ax1 * 4 + ax2\n                T_add_1 = T.Buffer((468,), data=T_add.data)\n                T_add_1[cse_var_2] = T.acos(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "asin",
                "acosh",
                "mod",
                "abs",
                "acos",
                "add"
            ]
        ],
        "input_shape": [[13, 9, 4]],
        "output_shape": [[13, 9, 4], [13, 9, 4], [13, 9, 4], [13, 9, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 117; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_divide[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] / ph_3[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 117; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = atanf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1170; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 13, 10), \"float32\"), ph_3: T.Buffer((9, 13, 10), \"float32\"), T_divide: T.Buffer((9, 13, 10), \"float32\"), compute: T.Buffer((9, 13, 10), \"float32\"), T_subtract: T.Buffer((9, 13, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(117):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_divide_1 = T.Buffer((1170,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1170,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_1 = T.Buffer((1170,), data=compute.data)\n                compute_1[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1170):\n            T_subtract_1 = T.Buffer((1170,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "atan",
                "cos",
                "subtract"
            ]
        ],
        "input_shape": [[9, 13, 10], [17, 7, 20], [9, 13, 10]],
        "output_shape": [[9, 13, 10], [17, 7, 20], [9, 13, 10], [9, 13, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1248; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 96; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n      T_add[((ax0_ax1_fused * 13) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 13) + ax2)]) + ph_0[((ax0_ax1_fused * 13) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1248; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 16, 13), \"float32\"), compute: T.Buffer((6, 16, 13), \"float32\"), T_add: T.Buffer((6, 16, 13), \"float32\"), compute_1: T.Buffer((6, 16, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1248,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1248):\n            compute_2 = T.Buffer((1248,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(96):\n            for ax2 in range(13):\n                cse_var_1: T.int32 = ax0_ax1_fused * 13 + ax2\n                T_add_1 = T.Buffer((1248,), data=T_add.data)\n                T_add_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1248):\n            compute_2 = T.Buffer((1248,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil",
                "asin",
                "add",
                "acosh"
            ]
        ],
        "input_shape": [[6, 16, 13]],
        "output_shape": [[6, 16, 13], [6, 16, 13], [6, 16, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(atanhf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_1[((i0_i1_fused * 8) + i2)] = acoshf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 160; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 8) + i2_1)] = asinhf(sinf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 10, 8), \"float32\"), compute: T.Buffer((16, 10, 8), \"float32\"), T_mod: T.Buffer((16, 10, 8), \"float32\"), compute_1: T.Buffer((16, 10, 8), \"float32\"), compute_2: T.Buffer((16, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1280):\n            compute_3 = T.Buffer((1280,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1280):\n            T_mod_1 = T.Buffer((1280,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((1280,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((1280,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "sin",
                "atanh",
                "mod",
                "acosh",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[16, 10, 8]],
        "output_shape": [[16, 10, 8], [16, 10, 8], [16, 10, 8], [16, 10, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[36];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 36; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 36; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 36; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute_1[((i0 * 18) + i2)] = atanhf(compute_3[((i0 * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 36; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acoshf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 1, 18), \"float32\"), ph_3: T.Buffer((2, 1, 18), \"float32\"), T_multiply: T.Buffer((2, 1, 18), \"float32\"), compute: T.Buffer((2, 1, 18), \"float32\"), compute_1: T.Buffer((2, 1, 18), \"float32\"), compute_2: T.Buffer((2, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([36], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((36,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(36):\n            T_multiply_1 = T.Buffer((36,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((36,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_4 = T.Buffer((36,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((36,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(2):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0 * 18 + i2\n                compute_5 = T.Buffer((36,), data=compute_1.data)\n                compute_5[cse_var_1] = T.atanh(compute_4[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(36):\n            compute_5 = T.Buffer((36,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acosh(compute_4[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "multiply",
                "asinh",
                "exp",
                "atanh",
                "acosh"
            ]
        ],
        "input_shape": [[2, 1, 18], [19, 11, 4], [2, 1, 18]],
        "output_shape": [[2, 1, 18], [19, 11, 4], [2, 1, 18], [2, 1, 18], [2, 1, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 360) + (i1 * 18)) + i2)] = ceilf(ph_0[(((i0 * 360) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3600; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(acosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 3600; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf((ph_0[i0_i1_fused_i2_fused_1] / acoshf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 3600; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = expf((ph_0[i0_i1_fused_i2_fused_2] / acoshf(ph_0[i0_i1_fused_i2_fused_2])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 20, 18), \"float32\"), compute: T.Buffer((10, 20, 18), \"float32\"), compute_1: T.Buffer((10, 20, 18), \"float32\"), compute_2: T.Buffer((10, 20, 18), \"float32\"), compute_3: T.Buffer((10, 20, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3600,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(20, 18):\n                cse_var_1: T.int32 = i0 * 360 + i1 * 18 + i2\n                compute_4 = T.Buffer((3600,), data=compute.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(3600):\n            compute_4 = T.Buffer((3600,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(3600):\n            compute_4 = T.Buffer((3600,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused] / T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(3600):\n            compute_4 = T.Buffer((3600,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] / T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "acos",
                "exp",
                "acosh",
                "divide",
                "asin",
                "exp"
            ]
        ],
        "input_shape": [[10, 20, 18]],
        "output_shape": [[10, 20, 18], [10, 20, 18], [10, 20, 18], [10, 20, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 104) + (i1 * 8)) + i2)] = expf(ph_0[(((i0 * 104) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 182; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_1[((i0_i1_fused * 8) + i2_1)] = ceilf(acosf(ph_0[((i0_i1_fused * 8) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1456; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * acoshf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 13, 8), \"float32\"), compute: T.Buffer((14, 13, 8), \"float32\"), compute_1: T.Buffer((14, 13, 8), \"float32\"), T_multiply: T.Buffer((14, 13, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1456,), data=ph_0.data)\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(13, 8):\n                cse_var_1: T.int32 = i0 * 104 + i1 * 8 + i2\n                compute_2 = T.Buffer((1456,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(182):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_2 = T.Buffer((1456,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.acos(ph_0_1[cse_var_2]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1456):\n            T_multiply_1 = T.Buffer((1456,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "exp",
                "acos",
                "ceil",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[14, 13, 8]],
        "output_shape": [[14, 13, 8], [14, 13, 8], [14, 13, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1080; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 60; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0_i1_fused * 18) + i2)] = atanf(ph_0[((i0_i1_fused * 18) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_1[(((i0 * 360) + (i1 * 18)) + i2_1)] = sinf(cosf(ph_0[(((i0 * 360) + (i1 * 18)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(__cosf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 20, 18), \"float32\"), ph_3: T.Buffer((3, 20, 18), \"float32\"), T_divide: T.Buffer((3, 20, 18), \"float32\"), compute: T.Buffer((3, 20, 18), \"float32\"), compute_1: T.Buffer((3, 20, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1080,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1080):\n            T_divide_1 = T.Buffer((1080,), data=T_divide.data)\n            ph_3_1 = T.Buffer((1080,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(60):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0_i1_fused * 18 + i2\n                compute_2 = T.Buffer((1080,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0 in T.parallel(3):\n            for i1, i2 in T.grid(20, 18):\n                cse_var_2: T.int32 = i0 * 360 + i1 * 18 + i2\n                compute_2 = T.Buffer((1080,), data=compute_1.data)\n                compute_2[cse_var_2] = T.sin(T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "divide",
                "atan",
                "cos",
                "sin"
            ]
        ],
        "input_shape": [[3, 20, 18], [7, 3, 1], [3, 20, 18]],
        "output_shape": [[3, 20, 18], [7, 3, 1], [3, 20, 18], [3, 20, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 130; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 130; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (sinf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute_1[((i0 * 10) + i2)] = asinf(ph_0[((i0 * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 130; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 1, 10), \"float32\"), compute: T.Buffer((13, 1, 10), \"float32\"), T_subtract: T.Buffer((13, 1, 10), \"float32\"), compute_1: T.Buffer((13, 1, 10), \"float32\"), compute_2: T.Buffer((13, 1, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((130,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(130):\n            compute_3 = T.Buffer((130,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(130):\n            T_subtract_1 = T.Buffer((130,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.sin(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(13):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0 * 10 + i2\n                compute_3 = T.Buffer((130,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(130):\n            compute_3 = T.Buffer((130,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asinh",
                "sin",
                "subtract",
                "asin",
                "cos",
                "acosh"
            ]
        ],
        "input_shape": [[13, 1, 10]],
        "output_shape": [[13, 1, 10], [13, 1, 10], [13, 1, 10], [13, 1, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 8; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 16; ++ax2) {\n        T_add[(((ax0 * 128) + (ax1 * 16)) + ax2)] = (ph_0[(((ax0 * 128) + (ax1 * 16)) + ax2)] + ph_3[(((ax0 * 128) + (ax1 * 16)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute[((i0_i1_fused * 16) + i2)] = asinhf((ph_0[((i0_i1_fused * 16) + i2)] / (ph_0[((i0_i1_fused * 16) + i2)] - ph_3[((i0_i1_fused * 16) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 8, 16), \"float32\"), ph_3: T.Buffer((20, 8, 16), \"float32\"), T_add: T.Buffer((20, 8, 16), \"float32\"), compute: T.Buffer((20, 8, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2560,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2560,), data=ph_3.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(8, 16):\n                cse_var_1: T.int32 = ax0 * 128 + ax1 * 16 + ax2\n                T_add_1 = T.Buffer((2560,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_1 = T.Buffer((2560,), data=compute.data)\n                compute_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2] / (ph_0_1[cse_var_2] - ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "add",
                "subtract",
                "divide",
                "asinh"
            ]
        ],
        "input_shape": [[20, 8, 16], [7, 13, 4], [20, 8, 16]],
        "output_shape": [[20, 8, 16], [7, 13, 4], [20, 8, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 342; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = atanhf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2736; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2736; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] * ceilf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 19, 8), \"float32\"), compute: T.Buffer((18, 19, 8), \"float32\"), compute_1: T.Buffer((18, 19, 8), \"float32\"), compute_2: T.Buffer((18, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2736,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(342):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((2736,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2736):\n            compute_3 = T.Buffer((2736,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2736):\n            compute_3 = T.Buffer((2736,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] * T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "acosh",
                "atan",
                "ceil",
                "multiply",
                "sin"
            ]
        ],
        "input_shape": [[18, 19, 8]],
        "output_shape": [[18, 19, 8], [18, 19, 8], [18, 19, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 117; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = asinhf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 234; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 117; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 2) + i2_1)] = asinhf(ph_0[((i0_i1_fused_1 * 2) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __expf(fabsf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 2), \"float32\"), compute: T.Buffer((13, 9, 2), \"float32\"), compute_1: T.Buffer((13, 9, 2), \"float32\"), compute_2: T.Buffer((13, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((234,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((234,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(234):\n            compute_3 = T.Buffer((234,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_3 = T.Buffer((234,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "abs",
                "exp",
                "asinh"
            ]
        ],
        "input_shape": [[13, 9, 2]],
        "output_shape": [[13, 9, 2], [13, 9, 2], [13, 9, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 576; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 576; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 12; ++i2) {\n      compute_2[((i0_i1_fused * 12) + i2)] = asinhf(ph_0[((i0_i1_fused * 12) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 576; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = asinhf(acoshf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 576; ++i0_i1_fused_i2_fused_3) {\n    compute_4[i0_i1_fused_i2_fused_3] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 4, 12), \"float32\"), compute: T.Buffer((12, 4, 12), \"float32\"), compute_1: T.Buffer((12, 4, 12), \"float32\"), compute_2: T.Buffer((12, 4, 12), \"float32\"), compute_3: T.Buffer((12, 4, 12), \"float32\"), compute_4: T.Buffer((12, 4, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((576,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_5 = T.Buffer((576,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_5 = T.Buffer((576,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acosh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(12):\n                cse_var_1: T.int32 = i0_i1_fused * 12 + i2\n                compute_5 = T.Buffer((576,), data=compute_2.data)\n                compute_5[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_5 = T.Buffer((576,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asinh(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(576):\n            compute_5 = T.Buffer((576,), data=compute_4.data)\n            compute_5[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "atan",
                "acosh",
                "asinh",
                "acosh",
                "asinh",
                "ceil"
            ]
        ],
        "input_shape": [[12, 4, 12]],
        "output_shape": [[12, 4, 12], [12, 4, 12], [12, 4, 12], [12, 4, 12], [12, 4, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 3; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] - ph_3[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 3; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 4; ++ax2_1) {\n      T_mod[((ax0_ax1_fused_1 * 4) + ax2_1)] = fmodf(ph_0[((ax0_ax1_fused_1 * 4) + ax2_1)], ph_3[((ax0_ax1_fused_1 * 4) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 12; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(__sinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 3, 4), \"float32\"), ph_3: T.Buffer((1, 3, 4), \"float32\"), T_subtract: T.Buffer((1, 3, 4), \"float32\"), T_mod: T.Buffer((1, 3, 4), \"float32\"), compute: T.Buffer((1, 3, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((12,), data=ph_0.data)\n        ph_3_1 = T.Buffer((12,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_subtract_1 = T.Buffer((12,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(3):\n            for ax2 in range(4):\n                cse_var_2: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_mod_1 = T.Buffer((12,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(12):\n            compute_1 = T.Buffer((12,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "mod",
                "sin",
                "ceil"
            ]
        ],
        "input_shape": [[1, 3, 4], [18, 5, 4], [1, 3, 4]],
        "output_shape": [[1, 3, 4], [18, 5, 4], [1, 3, 4], [1, 3, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 16; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute[((i0_i1_fused * 6) + i2)] = cosf(ph_0[((i0_i1_fused * 6) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 96; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 96; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 96; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = expf((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 4, 6), \"float32\"), ph_3: T.Buffer((4, 4, 6), \"float32\"), compute: T.Buffer((4, 4, 6), \"float32\"), T_divide: T.Buffer((4, 4, 6), \"float32\"), compute_1: T.Buffer((4, 4, 6), \"float32\"), compute_2: T.Buffer((4, 4, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((96,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(16):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_3 = T.Buffer((96,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(96):\n            T_divide_1 = T.Buffer((96,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(96):\n            compute_3 = T.Buffer((96,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(96):\n            compute_3 = T.Buffer((96,), data=compute_2.data)\n            ph_3_1 = T.Buffer((96,), data=ph_3.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "cos",
                "ceil",
                "divide",
                "abs",
                "exp"
            ]
        ],
        "input_shape": [[4, 4, 6], [7, 7, 15], [4, 4, 6]],
        "output_shape": [[7, 7, 15], [4, 4, 6], [4, 4, 6], [4, 4, 6], [4, 4, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0, float* ph_3) {\n  for (int32_t ax1 = 0; ax1 < 11; ++ax1) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_multiply[((ax1 * 10) + ax2)] = (ph_0[((ax1 * 10) + ax2)] * ph_3[((ax1 * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = cosf((ph_0[((i0_i1_fused * 10) + i2)] + acoshf(ph_0[((i0_i1_fused * 10) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 10), \"float32\"), ph_3: T.Buffer((1, 11, 10), \"float32\"), T_multiply: T.Buffer((1, 11, 10), \"float32\"), compute: T.Buffer((1, 11, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((110,), data=ph_0.data)\n        for ax1, ax2 in T.grid(11, 10):\n            cse_var_1: T.int32 = ax1 * 10 + ax2\n            T_multiply_1 = T.Buffer((110,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((110,), data=ph_3.data)\n            T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(11):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_1 = T.Buffer((110,), data=compute.data)\n                compute_1[cse_var_2] = T.cos(ph_0_1[cse_var_2] + T.acosh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "multiply",
                "acosh",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[1, 11, 10], [8, 8, 3], [1, 11, 10]],
        "output_shape": [[1, 11, 10], [8, 8, 3], [1, 11, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2016; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 144; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute_1[((i0_i1_fused * 14) + i2)] = cosf(acosf(ph_0[((i0_i1_fused * 14) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2016; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf((ph_0[i0_i1_fused_i2_fused_1] * atanf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n        compute_3[(((i0 * 252) + (i1 * 14)) + i2_1)] = ceilf((ph_0[(((i0 * 252) + (i1 * 14)) + i2_1)] * atanf(ph_0[(((i0 * 252) + (i1 * 14)) + i2_1)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 18, 14), \"float32\"), compute: T.Buffer((8, 18, 14), \"float32\"), compute_1: T.Buffer((8, 18, 14), \"float32\"), compute_2: T.Buffer((8, 18, 14), \"float32\"), compute_3: T.Buffer((8, 18, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2016,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2016):\n            compute_4 = T.Buffer((2016,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(144):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_4 = T.Buffer((2016,), data=compute_1.data)\n                compute_4[cse_var_1] = T.cos(T.acos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2016):\n            compute_4 = T.Buffer((2016,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] * T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(8):\n            for i1, i2 in T.grid(18, 14):\n                cse_var_2: T.int32 = i0 * 252 + i1 * 14 + i2\n                compute_4 = T.Buffer((2016,), data=compute_3.data)\n                compute_4[cse_var_2] = T.ceil(ph_0_1[cse_var_2] * T.atan(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "atan",
                "acos",
                "cos",
                "atan",
                "multiply",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[8, 18, 14]],
        "output_shape": [[8, 18, 14], [8, 18, 14], [8, 18, 14], [8, 18, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1900; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 100; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute_1[((i0_i1_fused * 19) + i2)] = fabsf(asinhf(ph_0[((i0_i1_fused * 19) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_2[(((i0 * 190) + (i1 * 19)) + i2_1)] = cosf((ph_0[(((i0 * 190) + (i1 * 19)) + i2_1)] / sinf(ph_0[(((i0 * 190) + (i1 * 19)) + i2_1)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 10, 19), \"float32\"), compute: T.Buffer((10, 10, 19), \"float32\"), compute_1: T.Buffer((10, 10, 19), \"float32\"), compute_2: T.Buffer((10, 10, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1900,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1900):\n            compute_3 = T.Buffer((1900,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(100):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((1900,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(T.asinh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(10, 19):\n                cse_var_2: T.int32 = i0 * 190 + i1 * 19 + i2\n                compute_3 = T.Buffer((1900,), data=compute_2.data)\n                compute_3[cse_var_2] = T.cos(ph_0_1[cse_var_2] / T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "asinh",
                "abs",
                "sin",
                "divide",
                "cos"
            ]
        ],
        "input_shape": [[10, 10, 19]],
        "output_shape": [[10, 10, 19], [10, 10, 19], [10, 10, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 40) + (i1 * 20)) + i2)] = atanhf(ph_0[(((i0 * 40) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 640; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 640; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 2, 20), \"float32\"), compute: T.Buffer((16, 2, 20), \"float32\"), compute_1: T.Buffer((16, 2, 20), \"float32\"), compute_2: T.Buffer((16, 2, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((640,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(2, 20):\n                cse_var_1: T.int32 = i0 * 40 + i1 * 20 + i2\n                compute_3 = T.Buffer((640,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            compute_3 = T.Buffer((640,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(640):\n            compute_3 = T.Buffer((640,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "sin",
                "cos",
                "asin"
            ]
        ],
        "input_shape": [[16, 2, 20]],
        "output_shape": [[16, 2, 20], [16, 2, 20], [16, 2, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 49; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] - ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 392; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 49; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      T_divide[((ax0_ax1_fused_1 * 8) + ax2_1)] = (acosf(ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)]) / ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 7, 8), \"float32\"), ph_3: T.Buffer((7, 7, 8), \"float32\"), T_subtract: T.Buffer((7, 7, 8), \"float32\"), compute: T.Buffer((7, 7, 8), \"float32\"), T_divide: T.Buffer((7, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((392,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(49):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_subtract_1 = T.Buffer((392,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((392,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(392):\n            compute_1 = T.Buffer((392,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(49):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_divide_1 = T.Buffer((392,), data=T_divide.data)\n                T_divide_1[cse_var_2] = T.acos(ph_0_1[cse_var_2]) / ph_0_1[cse_var_2]",
        "op_args": [
            [
                "subtract",
                "acos",
                "acos",
                "divide"
            ]
        ],
        "input_shape": [[7, 7, 8], [2, 1, 15], [7, 7, 8]],
        "output_shape": [[7, 7, 8], [2, 1, 15], [7, 7, 8], [7, 7, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 220; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * acoshf(fmodf(ph_0[ax0_ax1_fused_ax2_fused], sinf(ph_0[ax0_ax1_fused_ax2_fused]))));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute[((i0_i1_fused * 20) + i2)] = atanf(ceilf(ph_0[((i0_i1_fused * 20) + i2)]));\n    }\n  }\n  for (int32_t i1 = 0; i1 < 11; ++i1) {\n    for (int32_t i2_1 = 0; i2_1 < 20; ++i2_1) {\n      compute_1[((i1 * 20) + i2_1)] = acosf(ceilf(ph_0[((i1 * 20) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * acoshf(fmodf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))], __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]))));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 20), \"float32\"), T_multiply: T.Buffer((1, 11, 20), \"float32\"), compute: T.Buffer((1, 11, 20), \"float32\"), compute_1: T.Buffer((1, 11, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((220,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(220):\n            T_multiply_1 = T.Buffer((220,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.acosh(T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.sin(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused in T.parallel(11):\n            for i2 in range(20):\n                cse_var_1: T.int32 = i0_i1_fused * 20 + i2\n                compute_2 = T.Buffer((220,), data=compute.data)\n                compute_2[cse_var_1] = T.atan(T.ceil(ph_0_1[cse_var_1]))\n        for i1, i2 in T.grid(11, 20):\n            cse_var_2: T.int32 = i1 * 20 + i2\n            compute_2 = T.Buffer((220,), data=compute_1.data)\n            compute_2[cse_var_2] = T.acos(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "sin",
                "mod",
                "acosh",
                "multiply",
                "ceil",
                "atan",
                "acos"
            ]
        ],
        "input_shape": [[1, 11, 20]],
        "output_shape": [[1, 11, 20], [1, 11, 20], [1, 11, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[2340];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2340; ++i0_i1_fused_i2_fused) {\n    compute_3[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2340; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = acoshf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2340; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = atanhf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_add[(((ax0 * 117) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)] + ph_3[(((ax0 * 117) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 2340; ++i0_i1_fused_i2_fused_3) {\n    compute_2[i0_i1_fused_i2_fused_3] = atanf((ph_0[i0_i1_fused_i2_fused_3] / ph_3[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 13, 9), \"float32\"), ph_3: T.Buffer((20, 13, 9), \"float32\"), T_add: T.Buffer((20, 13, 9), \"float32\"), compute: T.Buffer((20, 13, 9), \"float32\"), compute_1: T.Buffer((20, 13, 9), \"float32\"), compute_2: T.Buffer((20, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([2340], \"float32\", \"global\")\n        compute_4 = T.Buffer((2340,), data=compute_3)\n        ph_0_1 = T.Buffer((2340,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_5 = T.Buffer((2340,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acosh(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_5 = T.Buffer((2340,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(compute_4[i0_i1_fused_i2_fused])\n        ph_3_1 = T.Buffer((2340,), data=ph_3.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(13, 9):\n                cse_var_1: T.int32 = ax0 * 117 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((2340,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_5 = T.Buffer((2340,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "add",
                "exp",
                "acosh",
                "atanh",
                "atan"
            ]
        ],
        "input_shape": [[20, 13, 9], [16, 4, 9], [20, 13, 9]],
        "output_shape": [[16, 4, 9], [20, 13, 9], [20, 13, 9], [20, 13, 9], [20, 13, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1188; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf((ph_0[i0_i1_fused_i2_fused] + (ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1188; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf((ph_0[i0_i1_fused_i2_fused_1] + (ph_0[i0_i1_fused_i2_fused_1] - ph_3[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 132; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_2[((i0_i1_fused * 9) + i2)] = acoshf((ph_0[((i0_i1_fused * 9) + i2)] / ph_3[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 11, 9), \"float32\"), ph_3: T.Buffer((12, 11, 9), \"float32\"), compute: T.Buffer((12, 11, 9), \"float32\"), compute_1: T.Buffer((12, 11, 9), \"float32\"), compute_2: T.Buffer((12, 11, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1188,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1188,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            compute_3 = T.Buffer((1188,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] + (ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(1188):\n            compute_3 = T.Buffer((1188,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] + (ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(132):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((1188,), data=compute_2.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1] / ph_3_1[cse_var_1])",
        "op_args": [
            [
                "divide",
                "subtract",
                "add",
                "sin",
                "sin",
                "acosh"
            ]
        ],
        "input_shape": [[12, 11, 9], [3, 1, 1], [12, 11, 9]],
        "output_shape": [[3, 1, 1], [12, 11, 9], [12, 11, 9], [12, 11, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 6; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 24) + (i1 * 4)) + i2)] = fabsf(fmodf(ph_0[(((i0 * 24) + (i1 * 4)) + i2)], fabsf(ph_0[(((i0 * 24) + (i1 * 4)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 264; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 66; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n      compute_2[((i0_i1_fused * 4) + i2_1)] = atanf(asinhf(ph_0[((i0_i1_fused * 4) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], fabsf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 6, 4), \"float32\"), compute: T.Buffer((11, 6, 4), \"float32\"), compute_1: T.Buffer((11, 6, 4), \"float32\"), compute_2: T.Buffer((11, 6, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((264,), data=ph_0.data)\n        for i0 in T.parallel(11):\n            for i1, i2 in T.grid(6, 4):\n                cse_var_1: T.int32 = i0 * 24 + i1 * 4 + i2\n                compute_3 = T.Buffer((264,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(T.truncmod(ph_0_1[cse_var_1], T.fabs(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(264):\n            compute_3 = T.Buffer((264,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(66):\n            for i2 in range(4):\n                cse_var_2: T.int32 = i0_i1_fused * 4 + i2\n                compute_3 = T.Buffer((264,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(T.asinh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "mod",
                "abs",
                "acos",
                "asinh",
                "atan"
            ]
        ],
        "input_shape": [[11, 6, 4]],
        "output_shape": [[11, 6, 4], [11, 6, 4], [11, 6, 4]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 384; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 64; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 6; ++i2) {\n      compute_1[((i0_i1_fused * 6) + i2)] = asinhf(asinf(ph_0[((i0_i1_fused * 6) + i2)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 8, 6), \"float32\"), ph_3: T.Buffer((8, 8, 6), \"float32\"), T_subtract: T.Buffer((8, 8, 6), \"float32\"), compute: T.Buffer((8, 8, 6), \"float32\"), compute_1: T.Buffer((8, 8, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((384,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(384):\n            T_subtract_1 = T.Buffer((384,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((384,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_2 = T.Buffer((384,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(64):\n            for i2 in range(6):\n                cse_var_1: T.int32 = i0_i1_fused * 6 + i2\n                compute_2 = T.Buffer((384,), data=compute_1.data)\n                compute_2[cse_var_1] = T.asinh(T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "subtract",
                "acosh",
                "asin",
                "asinh"
            ]
        ],
        "input_shape": [[8, 8, 6], [5, 8, 6], [8, 8, 6]],
        "output_shape": [[8, 8, 6], [5, 8, 6], [8, 8, 6], [8, 8, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 630; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 630; ++ax0_ax1_fused_ax2_fused) {\n    float compute_1[1];\n    compute_1[0] = expf(ph_0[ax0_ax1_fused_ax2_fused]);\n    T_multiply[ax0_ax1_fused_ax2_fused] = (compute_1[0] * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 7; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_add[(((ax0 * 63) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 63) + (ax1 * 9)) + ax2)] + acosf(ph_0[(((ax0 * 63) + (ax1 * 9)) + ax2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 7, 9), \"float32\"), compute: T.Buffer((10, 7, 9), \"float32\"), T_multiply: T.Buffer((10, 7, 9), \"float32\"), T_add: T.Buffer((10, 7, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((630,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(630):\n            compute_1 = T.Buffer((630,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(630):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.exp(ph_0_1[ax0_ax1_fused_ax2_fused])\n            T_multiply_1 = T.Buffer((630,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = compute_2[0] * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(7, 9):\n                cse_var_1: T.int32 = ax0 * 63 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((630,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + T.acos(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acos",
                "exp",
                "multiply",
                "acos",
                "add"
            ]
        ],
        "input_shape": [[10, 7, 9]],
        "output_shape": [[10, 7, 9], [10, 7, 9], [10, 7, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 60; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 14) + ax2)] = (ph_0[((ax0_ax1_fused * 14) + ax2)] * ph_3[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 840; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 14; ++i2) {\n        compute_1[(((i0 * 140) + (i1 * 14)) + i2)] = atanhf(sinf(ph_0[(((i0 * 140) + (i1 * 14)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 840; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acoshf(fmodf(ph_0[i0_i1_fused_i2_fused_1], ph_3[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 840; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(__sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 10, 14), \"float32\"), ph_3: T.Buffer((6, 10, 14), \"float32\"), T_multiply: T.Buffer((6, 10, 14), \"float32\"), compute: T.Buffer((6, 10, 14), \"float32\"), compute_1: T.Buffer((6, 10, 14), \"float32\"), compute_2: T.Buffer((6, 10, 14), \"float32\"), T_subtract: T.Buffer((6, 10, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((840,), data=ph_0.data)\n        ph_3_1 = T.Buffer((840,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(60):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_multiply_1 = T.Buffer((840,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] * ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            compute_3 = T.Buffer((840,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(10, 14):\n                cse_var_2: T.int32 = i0 * 140 + i1 * 14 + i2\n                compute_3 = T.Buffer((840,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atanh(T.sin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(840):\n            compute_3 = T.Buffer((840,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(840):\n            T_subtract_1 = T.Buffer((840,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "mod",
                "multiply",
                "sin",
                "atanh",
                "atanh",
                "acosh",
                "subtract"
            ]
        ],
        "input_shape": [[6, 10, 14], [1, 10, 4], [6, 10, 14]],
        "output_shape": [[1, 10, 4], [6, 10, 14], [6, 10, 14], [6, 10, 14], [6, 10, 14], [6, 10, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  for (int32_t i1 = 0; i1 < 7; ++i1) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i1 * 7) + i2)] = sinf(ph_0[((i1 * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_1[((i0_i1_fused * 7) + i2_1)] = acoshf(ceilf(ph_0[((i0_i1_fused * 7) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 49; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 49; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = acosf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 7), \"float32\"), compute: T.Buffer((1, 7, 7), \"float32\"), compute_1: T.Buffer((1, 7, 7), \"float32\"), compute_2: T.Buffer((1, 7, 7), \"float32\"), compute_3: T.Buffer((1, 7, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((49,), data=ph_0.data)\n        for i1, i2 in T.grid(7, 7):\n            cse_var_1: T.int32 = i1 * 7 + i2\n            compute_4 = T.Buffer((49,), data=compute.data)\n            compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(7):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_4 = T.Buffer((49,), data=compute_1.data)\n                compute_4[cse_var_2] = T.acosh(T.ceil(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(49):\n            compute_4 = T.Buffer((49,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(49):\n            compute_4 = T.Buffer((49,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "ceil",
                "acosh",
                "exp",
                "atanh",
                "acos"
            ]
        ],
        "input_shape": [[1, 7, 7]],
        "output_shape": [[1, 7, 7], [1, 7, 7], [1, 7, 7], [1, 7, 7]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 55) + (i1 * 5)) + i2)] = acosf(ph_0[(((i0 * 55) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 385; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 7; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n        compute_1[(((i0_1 * 55) + (i1_1 * 5)) + i2_1)] = acosf(ph_0[(((i0_1 * 55) + (i1_1 * 5)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 385; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 11, 5), \"float32\"), compute: T.Buffer((7, 11, 5), \"float32\"), T_multiply: T.Buffer((7, 11, 5), \"float32\"), compute_1: T.Buffer((7, 11, 5), \"float32\"), compute_2: T.Buffer((7, 11, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((385,), data=ph_0.data)\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(11, 5):\n                cse_var_1: T.int32 = i0 * 55 + i1 * 5 + i2\n                compute_3 = T.Buffer((385,), data=compute.data)\n                compute_3[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(385):\n            T_multiply_1 = T.Buffer((385,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(7):\n            for i1, i2 in T.grid(11, 5):\n                cse_var_2: T.int32 = i0 * 55 + i1 * 5 + i2\n                compute_3 = T.Buffer((385,), data=compute_1.data)\n                compute_3[cse_var_2] = T.acos(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(385):\n            compute_3 = T.Buffer((385,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acos",
                "asinh",
                "multiply",
                "acos",
                "atanh"
            ]
        ],
        "input_shape": [[7, 11, 5]],
        "output_shape": [[7, 11, 5], [7, 11, 5], [7, 11, 5], [7, 11, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 133; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = atanhf((ph_0[((i0_i1_fused * 5) + i2)] + acoshf(ph_0[((i0_i1_fused * 5) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 665; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + acoshf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 133; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 5) + i2_1)] = acoshf((ph_0[((i0_i1_fused_1 * 5) + i2_1)] - ph_3[((i0_i1_fused_1 * 5) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] + acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((int)blockIdx.x)] = acoshf((ph_0[((int)blockIdx.x)] - ph_3[((int)blockIdx.x)]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 7, 5), \"float32\"), ph_3: T.Buffer((19, 7, 5), \"float32\"), compute: T.Buffer((19, 7, 5), \"float32\"), T_multiply: T.Buffer((19, 7, 5), \"float32\"), compute_1: T.Buffer((19, 7, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((665,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(133):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((665,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1] + T.acosh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(665):\n            T_multiply_1 = T.Buffer((665,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(133):\n            for i2 in range(5):\n                cse_var_2: T.int32 = i0_i1_fused * 5 + i2\n                compute_2 = T.Buffer((665,), data=compute_1.data)\n                ph_3_1 = T.Buffer((665,), data=ph_3.data)\n                compute_2[cse_var_2] = T.acosh(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])",
        "op_args": [
            [
                "subtract",
                "acosh",
                "add",
                "atanh",
                "multiply",
                "acosh"
            ]
        ],
        "input_shape": [[19, 7, 5], [9, 2, 7], [19, 7, 5]],
        "output_shape": [[9, 2, 7], [19, 7, 5], [19, 7, 5], [19, 7, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0 * 18) + i2)] = sinf((ph_0[((i0 * 18) + i2)] + sinf(ph_0[((i0 * 18) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 54; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax2 = 0; ax2 < 18; ++ax2) {\n      T_mod[((ax0 * 18) + ax2)] = fmodf(atanhf(ph_0[((ax0 * 18) + ax2)]), ph_0[((ax0 * 18) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 54; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] + __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 1, 18), \"float32\"), compute: T.Buffer((3, 1, 18), \"float32\"), compute_1: T.Buffer((3, 1, 18), \"float32\"), T_mod: T.Buffer((3, 1, 18), \"float32\"), compute_2: T.Buffer((3, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((54,), data=ph_0.data)\n        for i0 in T.parallel(3):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0 * 18 + i2\n                compute_3 = T.Buffer((54,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1] + T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(54):\n            compute_3 = T.Buffer((54,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(3):\n            for ax2 in range(18):\n                cse_var_2: T.int32 = ax0 * 18 + ax2\n                T_mod_1 = T.Buffer((54,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.atanh(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(54):\n            compute_3 = T.Buffer((54,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "add",
                "sin",
                "acos",
                "atanh",
                "mod",
                "acos"
            ]
        ],
        "input_shape": [[3, 1, 18]],
        "output_shape": [[3, 1, 18], [3, 1, 18], [3, 1, 18], [3, 1, 18]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_divide[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] / ph_3[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] - asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 180; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf((ph_0[i0_i1_fused_i2_fused_1] - asinhf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf((ph_0[((int)blockIdx.x)] - asinhf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 1, 10), \"float32\"), ph_3: T.Buffer((18, 1, 10), \"float32\"), T_divide: T.Buffer((18, 1, 10), \"float32\"), compute: T.Buffer((18, 1, 10), \"float32\"), compute_1: T.Buffer((18, 1, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_divide_1 = T.Buffer((180,), data=T_divide.data)\n                ph_3_1 = T.Buffer((180,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_2 = T.Buffer((180,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_2 = T.Buffer((180,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "asinh",
                "subtract",
                "abs",
                "exp"
            ]
        ],
        "input_shape": [[18, 1, 10], [16, 1, 3], [18, 1, 10]],
        "output_shape": [[18, 1, 10], [16, 1, 3], [18, 1, 10], [18, 1, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 504; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute_1[(((i0 * 84) + (i1 * 12)) + i2)] = fabsf(acoshf(ph_0[(((i0 * 84) + (i1 * 12)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 6; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 12; ++i2_1) {\n        compute_2[(((i0_1 * 84) + (i1_1 * 12)) + i2_1)] = fabsf(fmodf(ph_0[(((i0_1 * 84) + (i1_1 * 12)) + i2_1)], asinf(ph_0[(((i0_1 * 84) + (i1_1 * 12)) + i2_1)])));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 7, 12), \"float32\"), compute: T.Buffer((6, 7, 12), \"float32\"), compute_1: T.Buffer((6, 7, 12), \"float32\"), compute_2: T.Buffer((6, 7, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((504,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(504):\n            compute_3 = T.Buffer((504,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(7, 12):\n                cse_var_1: T.int32 = i0 * 84 + i1 * 12 + i2\n                compute_3 = T.Buffer((504,), data=compute_1.data)\n                compute_3[cse_var_1] = T.fabs(T.acosh(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(7, 12):\n                cse_var_2: T.int32 = i0 * 84 + i1 * 12 + i2\n                compute_3 = T.Buffer((504,), data=compute_2.data)\n                compute_3[cse_var_2] = T.fabs(T.truncmod(ph_0_1[cse_var_2], T.asin(ph_0_1[cse_var_2])))",
        "op_args": [
            [
                "atan",
                "acosh",
                "abs",
                "asin",
                "mod",
                "abs"
            ]
        ],
        "input_shape": [[6, 7, 12]],
        "output_shape": [[6, 7, 12], [6, 7, 12], [6, 7, 12]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* T_multiply_1, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 459; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] * asinf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 459; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = ((ph_0[ax0_ax1_fused_ax2_fused_1] * asinf(ph_0[ax0_ax1_fused_ax2_fused_1])) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 459; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(fmodf(ph_0[i0_i1_fused_i2_fused], ph_3[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 17; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_multiply_1[(((ax0 * 27) + (ax1 * 9)) + ax2)] = (fmodf(ph_0[(((ax0 * 27) + (ax1 * 9)) + ax2)], ph_3[(((ax0 * 27) + (ax1 * 9)) + ax2)]) * ph_0[(((ax0 * 27) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinf(fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 3, 9), \"float32\"), ph_3: T.Buffer((17, 3, 9), \"float32\"), T_multiply: T.Buffer((17, 3, 9), \"float32\"), T_add: T.Buffer((17, 3, 9), \"float32\"), compute: T.Buffer((17, 3, 9), \"float32\"), T_multiply_1: T.Buffer((17, 3, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((459,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(459):\n            T_multiply_2 = T.Buffer((459,), data=T_multiply.data)\n            T_multiply_2[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(459):\n            T_add_1 = T.Buffer((459,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        ph_3_1 = T.Buffer((459,), data=ph_3.data)\n        for i0_i1_fused_i2_fused in T.parallel(459):\n            compute_1 = T.Buffer((459,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asin(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))\n        for ax0 in T.parallel(17):\n            for ax1, ax2 in T.grid(3, 9):\n                cse_var_1: T.int32 = ax0 * 27 + ax1 * 9 + ax2\n                T_multiply_2 = T.Buffer((459,), data=T_multiply_1.data)\n                T_multiply_2[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1]) * ph_0_1[cse_var_1]",
        "op_args": [
            [
                "mod",
                "asin",
                "multiply",
                "multiply",
                "add",
                "asin",
                "multiply"
            ]
        ],
        "input_shape": [[17, 3, 9], [4, 19, 16], [17, 3, 9]],
        "output_shape": [[4, 19, 16], [17, 3, 9], [17, 3, 9], [17, 3, 9], [17, 3, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 340) + (i1 * 17)) + i2)] = cosf(ph_0[(((i0 * 340) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 20; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_divide[(((ax0 * 340) + (ax1 * 17)) + ax2)] = ((ph_0[(((ax0 * 340) + (ax1 * 17)) + ax2)] * (ph_0[(((ax0 * 340) + (ax1 * 17)) + ax2)] / asinf(atanf(ph_0[(((ax0 * 340) + (ax1 * 17)) + ax2)])))) / ph_0[(((ax0 * 340) + (ax1 * 17)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] / asinf(atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))])))) / ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 20, 17), \"float32\"), compute: T.Buffer((15, 20, 17), \"float32\"), T_divide: T.Buffer((15, 20, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5100,), data=ph_0.data)\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(20, 17):\n                cse_var_1: T.int32 = i0 * 340 + i1 * 17 + i2\n                compute_1 = T.Buffer((5100,), data=compute.data)\n                compute_1[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(20, 17):\n                cse_var_2: T.int32 = ax0 * 340 + ax1 * 17 + ax2\n                T_divide_1 = T.Buffer((5100,), data=T_divide.data)\n                T_divide_1[cse_var_2] = ph_0_1[cse_var_2] * (ph_0_1[cse_var_2] / T.asin(T.atan(ph_0_1[cse_var_2]))) / ph_0_1[cse_var_2]",
        "op_args": [
            [
                "cos",
                "atan",
                "asin",
                "divide",
                "multiply",
                "divide"
            ]
        ],
        "input_shape": [[15, 20, 17]],
        "output_shape": [[15, 20, 17], [15, 20, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 240; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (ceilf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 240; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 4, 3), \"float32\"), ph_3: T.Buffer((20, 4, 3), \"float32\"), T_subtract: T.Buffer((20, 4, 3), \"float32\"), compute: T.Buffer((20, 4, 3), \"float32\"), T_add: T.Buffer((20, 4, 3), \"float32\"), compute_1: T.Buffer((20, 4, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_subtract_1 = T.Buffer((240,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((240,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_2 = T.Buffer((240,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_add_1 = T.Buffer((240,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_2 = T.Buffer((240,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "acosh",
                "ceil",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[20, 4, 3], [2, 1, 8], [20, 4, 3]],
        "output_shape": [[20, 4, 3], [2, 1, 8], [20, 4, 3], [20, 4, 3], [20, 4, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2640; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] + cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_1[(((i0 * 165) + (i1 * 11)) + i2)] = sinf(ph_0[(((i0 * 165) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 240; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 11; ++i2_1) {\n      compute_2[((i0_i1_fused * 11) + i2_1)] = asinhf(atanhf(ph_0[((i0_i1_fused * 11) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 15, 11), \"float32\"), compute: T.Buffer((16, 15, 11), \"float32\"), compute_1: T.Buffer((16, 15, 11), \"float32\"), compute_2: T.Buffer((16, 15, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2640,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2640):\n            compute_3 = T.Buffer((2640,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] + T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(15, 11):\n                cse_var_1: T.int32 = i0 * 165 + i1 * 11 + i2\n                compute_3 = T.Buffer((2640,), data=compute_1.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(240):\n            for i2 in range(11):\n                cse_var_2: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((2640,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "cos",
                "add",
                "atanh",
                "sin",
                "atanh",
                "asinh"
            ]
        ],
        "input_shape": [[16, 15, 11]],
        "output_shape": [[16, 15, 11], [16, 15, 11], [16, 15, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  for (int32_t i1 = 0; i1 < 10; ++i1) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i1 * 8) + i2)] = cosf(ph_0[((i1 * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 80; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (acoshf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 80; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 10; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_add[((ax0_ax1_fused * 8) + ax2)] = ((ph_0[((ax0_ax1_fused * 8) + ax2)] * ph_3[((ax0_ax1_fused * 8) + ax2)]) + ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 10, 8), \"float32\"), ph_3: T.Buffer((1, 10, 8), \"float32\"), compute: T.Buffer((1, 10, 8), \"float32\"), T_subtract: T.Buffer((1, 10, 8), \"float32\"), compute_1: T.Buffer((1, 10, 8), \"float32\"), T_add: T.Buffer((1, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((80,), data=ph_0.data)\n        for i1, i2 in T.grid(10, 8):\n            cse_var_1: T.int32 = i1 * 8 + i2\n            compute_2 = T.Buffer((80,), data=compute.data)\n            compute_2[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(80):\n            T_subtract_1 = T.Buffer((80,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(80):\n            compute_2 = T.Buffer((80,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(10):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_add_1 = T.Buffer((80,), data=T_add.data)\n                ph_3_1 = T.Buffer((80,), data=ph_3.data)\n                T_add_1[cse_var_2] = ph_0_1[cse_var_2] * ph_3_1[cse_var_2] + ph_0_1[cse_var_2]",
        "op_args": [
            [
                "multiply",
                "cos",
                "acosh",
                "subtract",
                "acos",
                "add"
            ]
        ],
        "input_shape": [[1, 10, 8], [14, 15, 2], [1, 10, 8]],
        "output_shape": [[14, 15, 2], [1, 10, 8], [1, 10, 8], [1, 10, 8], [1, 10, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[510];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 510; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 510; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 510; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (compute_3[ax0_ax1_fused_ax2_fused] + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 510; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = cosf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 3; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax10 = 0; ax10 < 5; ++ax10) {\n      for (int32_t ax11 = 0; ax11 < 2; ++ax11) {\n        for (int32_t ax12 = 0; ax12 < 5; ++ax12) {\n          compute_3[((((ax0_ax1_fused_ax2_fused_1 * 50) + (ax10 * 10)) + (ax11 * 5)) + ax12)] = ph_3[((((ax0_ax1_fused_ax2_fused_1 * 50) + (ax12 * 10)) + (ax11 * 5)) + ax10)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused = 0; i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused < 51; ++i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused) {\n    float T_batch_matmul_NN[25];\n    for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n        T_batch_matmul_NN[((b_inner_init * 5) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n      for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n        for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n          T_batch_matmul_NN[((b_inner * 5) + j_outer_inner)] = (T_batch_matmul_NN[((b_inner * 5) + j_outer_inner)] + (ph_0[(((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 17) * 170) + (b_inner * 34)) + ((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 17) * 2)) + k_inner)] * compute_3[(((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 17) * 50) + (j_outer_inner * 10)) + (k_inner * 5)) + b_inner)]));\n        }\n      }\n    }\n    for (int32_t i0_inner = 0; i0_inner < 5; ++i0_inner) {\n      for (int32_t i2_inner_s = 0; i2_inner_s < 5; ++i2_inner_s) {\n        compute_2[(((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused / 17) * 425) + (i0_inner * 85)) + ((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 17) * 5)) + i2_inner_s)] = asinf(T_batch_matmul_NN[((i0_inner * 5) + i2_inner_s)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[8];\n  __shared__ float ph_3_shared[180];\n  for (int i_inner_init = 0; i_inner_init < 8; ++i_inner_init) {\n    T_batch_matmul_NN[i_inner_init] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 4; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 45) + ((int)threadIdx.x))] = ph_3[(((((((ax0_ax1_fused_ax2_fused_outer_outer * 9) + (((int)threadIdx.x) / 5)) >> 2) * 40) + (k_outer_outer * 20)) + ((((((int)threadIdx.x) / 5) + ax0_ax1_fused_ax2_fused_outer_outer) & 3) * 5)) + (((int)threadIdx.x) % 5))];\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 4; ++k_inner) {\n      for (int i_inner = 0; i_inner < 8; ++i_inner) {\n        T_batch_matmul_NN[i_inner] = (T_batch_matmul_NN[i_inner] + (ph_0[(((((((int)threadIdx.x) / 5) * 64) + (i_inner * 8)) + (k_outer_outer * 4)) + k_inner)] * ph_3_shared[((((((int)threadIdx.x) / 5) * 20) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n      }\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 8; ++i1_inner) {\n    compute[((((((int)threadIdx.x) / 5) * 40) + (i1_inner * 5)) + (((int)threadIdx.x) % 5))] = asinf(T_batch_matmul_NN[i1_inner]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 17, 2), \"float32\"), ph_3: T.Buffer((15, 2, 5), \"float32\"), compute: T.Buffer((15, 17, 2), \"float32\"), T_add: T.Buffer((15, 17, 2), \"float32\"), compute_1: T.Buffer((15, 17, 2), \"float32\"), compute_2: T.Buffer((15, 17, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([510], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((510,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(510):\n            compute_4 = T.Buffer((510,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((510,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(510):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(510):\n            T_add_1 = T.Buffer((510,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = compute_4[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(510):\n            compute_5 = T.Buffer((510,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.cos(compute_4[i0_i1_fused_i2_fused])\n        compute_5 = T.Buffer((150,), data=compute_3)\n        for ax0_ax1_fused_ax2_fused in T.parallel(3):\n            for ax10, ax11, ax12 in T.grid(5, 2, 5):\n                cse_var_2: T.int32 = ax0_ax1_fused_ax2_fused * 50\n                cse_var_1: T.int32 = ax11 * 5\n                ph_3_1 = T.Buffer((150,), data=ph_3.data)\n                compute_5[cse_var_2 + ax10 * 10 + cse_var_1 + ax12] = ph_3_1[cse_var_2 + ax12 * 10 + cse_var_1 + ax10]\n        for i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(51):\n            T_batch_matmul_NN = T.allocate([25], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((25,), data=T_batch_matmul_NN)\n            for j_outer_inner_init, b_inner_init in T.grid(5, 5):\n                T_batch_matmul_NN_1[b_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n            for j_outer_inner, k_inner, b_inner in T.grid(5, 2, 5):\n                cse_var_4: T.int32 = i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 17\n                cse_var_3: T.int32 = b_inner * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_3] = T_batch_matmul_NN_1[cse_var_3] + ph_0_1[cse_var_4 * 170 + b_inner * 34 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 17 * 2 + k_inner] * compute_5[cse_var_4 * 50 + j_outer_inner * 10 + k_inner * 5 + b_inner]\n            for i0_inner, i2_inner_s in T.grid(5, 5):\n                compute_6 = T.Buffer((1275,), data=compute_2.data)\n                compute_6[i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused // 17 * 425 + i0_inner * 85 + i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused % 17 * 5 + i2_inner_s] = T.asin(T_batch_matmul_NN_1[i0_inner * 5 + i2_inner_s])",
        "op_args": [
            [
                "batch_matmul",
                "sin",
                "exp",
                "add",
                "cos",
                "asin"
            ]
        ],
        "input_shape": [[15, 17, 2], [20, 4, 13], [15, 2, 5]],
        "output_shape": [[20, 4, 13], [15, 17, 2], [15, 17, 2], [15, 17, 2], [15, 17, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 38; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = cosf(sinf(ph_0[((i0_i1_fused * 10) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 38; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 10) + i2_1)] = expf(sinf(ph_0[((i0_i1_fused_1 * 10) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 380; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 380; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 2, 10), \"float32\"), ph_3: T.Buffer((19, 2, 10), \"float32\"), T_mod: T.Buffer((19, 2, 10), \"float32\"), compute: T.Buffer((19, 2, 10), \"float32\"), compute_1: T.Buffer((19, 2, 10), \"float32\"), compute_2: T.Buffer((19, 2, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((380,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(38):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_3 = T.Buffer((380,), data=compute.data)\n                compute_3[cse_var_1] = T.cos(T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(38):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_3 = T.Buffer((380,), data=compute_1.data)\n                compute_3[cse_var_2] = T.exp(T.sin(ph_0_1[cse_var_2]))\n        ph_3_1 = T.Buffer((380,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(380):\n            T_mod_1 = T.Buffer((380,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(380):\n            compute_3 = T.Buffer((380,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "subtract",
                "mod",
                "sin",
                "cos",
                "exp",
                "abs"
            ]
        ],
        "input_shape": [[19, 2, 10], [20, 7, 2], [19, 2, 10]],
        "output_shape": [[20, 7, 2], [19, 2, 10], [19, 2, 10], [19, 2, 10], [19, 2, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 672; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 12; ++i0) {\n    for (int32_t i1 = 0; i1 < 7; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute_1[(((i0 * 56) + (i1 * 8)) + i2)] = acoshf(asinhf(ph_0[(((i0 * 56) + (i1 * 8)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 672; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (asinhf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 12; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        compute_2[(((i0_1 * 56) + (i1_1 * 8)) + i2_1)] = asinhf(fmodf(ph_0[(((i0_1 * 56) + (i1_1 * 8)) + i2_1)], ph_3[(((i0_1 * 56) + (i1_1 * 8)) + i2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((12, 7, 8), \"float32\"), ph_3: T.Buffer((12, 7, 8), \"float32\"), compute: T.Buffer((12, 7, 8), \"float32\"), compute_1: T.Buffer((12, 7, 8), \"float32\"), T_multiply: T.Buffer((12, 7, 8), \"float32\"), compute_2: T.Buffer((12, 7, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((672,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(672):\n            compute_3 = T.Buffer((672,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(7, 8):\n                cse_var_1: T.int32 = i0 * 56 + i1 * 8 + i2\n                compute_3 = T.Buffer((672,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acosh(T.asinh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(672):\n            T_multiply_1 = T.Buffer((672,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(12):\n            for i1, i2 in T.grid(7, 8):\n                cse_var_2: T.int32 = i0 * 56 + i1 * 8 + i2\n                compute_3 = T.Buffer((672,), data=compute_2.data)\n                ph_3_1 = T.Buffer((672,), data=ph_3.data)\n                compute_3[cse_var_2] = T.asinh(T.truncmod(ph_0_1[cse_var_2], ph_3_1[cse_var_2]))",
        "op_args": [
            [
                "mod",
                "atanh",
                "asinh",
                "acosh",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[12, 7, 8], [5, 17, 11], [12, 7, 8]],
        "output_shape": [[5, 17, 11], [12, 7, 8], [12, 7, 8], [12, 7, 8], [12, 7, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 450; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 450; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_2[((i0_i1_fused * 5) + i2)] = asinhf((ph_0[((i0_i1_fused * 5) + i2)] * asinf(ph_0[((i0_i1_fused * 5) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 10, 5), \"float32\"), compute: T.Buffer((9, 10, 5), \"float32\"), compute_1: T.Buffer((9, 10, 5), \"float32\"), compute_2: T.Buffer((9, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((450,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(450):\n            compute_3 = T.Buffer((450,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(450):\n            compute_3 = T.Buffer((450,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(90):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((450,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1] * T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "ceil",
                "asin",
                "abs",
                "asin",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[9, 10, 5]],
        "output_shape": [[9, 10, 5], [9, 10, 5], [9, 10, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1287; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1287; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 11; ++i2) {\n        compute_2[(((i0 * 99) + (i1 * 11)) + i2)] = asinf(ph_0[(((i0 * 99) + (i1 * 11)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 13; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 11; ++ax2) {\n        T_subtract[(((ax0 * 99) + (ax1 * 11)) + ax2)] = (asinhf(ph_0[(((ax0 * 99) + (ax1 * 11)) + ax2)]) - ph_0[(((ax0 * 99) + (ax1 * 11)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 1287; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = sinf(asinhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 9, 11), \"float32\"), compute: T.Buffer((13, 9, 11), \"float32\"), compute_1: T.Buffer((13, 9, 11), \"float32\"), compute_2: T.Buffer((13, 9, 11), \"float32\"), T_subtract: T.Buffer((13, 9, 11), \"float32\"), compute_3: T.Buffer((13, 9, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1287,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1287):\n            compute_4 = T.Buffer((1287,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1287):\n            compute_4 = T.Buffer((1287,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(9, 11):\n                cse_var_1: T.int32 = i0 * 99 + i1 * 11 + i2\n                compute_4 = T.Buffer((1287,), data=compute_2.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(13):\n            for ax1, ax2 in T.grid(9, 11):\n                cse_var_2: T.int32 = ax0 * 99 + ax1 * 11 + ax2\n                T_subtract_1 = T.Buffer((1287,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2]) - ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(1287):\n            compute_4 = T.Buffer((1287,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "asin",
                "acosh",
                "ceil",
                "asin",
                "asinh",
                "subtract",
                "sin"
            ]
        ],
        "input_shape": [[13, 9, 11]],
        "output_shape": [[13, 9, 11], [13, 9, 11], [13, 9, 11], [13, 9, 11], [13, 9, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1326; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] / (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused])) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 6; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n        T_multiply[(((ax0 * 221) + (ax1 * 17)) + ax2)] = ((ph_0[(((ax0 * 221) + (ax1 * 17)) + ax2)] / (ph_0[(((ax0 * 221) + (ax1 * 17)) + ax2)] - ph_3[(((ax0 * 221) + (ax1 * 17)) + ax2)])) * ph_0[(((ax0 * 221) + (ax1 * 17)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1326; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(fmodf(ph_0[i0_i1_fused_i2_fused], ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))])) - ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 13, 17), \"float32\"), ph_3: T.Buffer((6, 13, 17), \"float32\"), T_subtract: T.Buffer((6, 13, 17), \"float32\"), T_multiply: T.Buffer((6, 13, 17), \"float32\"), compute: T.Buffer((6, 13, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1326,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1326,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1326):\n            T_subtract_1 = T.Buffer((1326,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / (ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(6):\n            for ax1, ax2 in T.grid(13, 17):\n                cse_var_1: T.int32 = ax0 * 221 + ax1 * 17 + ax2\n                T_multiply_1 = T.Buffer((1326,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] / (ph_0_1[cse_var_1] - ph_3_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1326):\n            compute_1 = T.Buffer((1326,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.fabs(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "subtract",
                "divide",
                "subtract",
                "multiply",
                "abs"
            ]
        ],
        "input_shape": [[6, 13, 17], [15, 15, 3], [6, 13, 17]],
        "output_shape": [[15, 15, 3], [6, 13, 17], [6, 13, 17], [6, 13, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 168; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 168; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 168; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanhf(atanhf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 14; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 14) + ax2)] = ((ph_0[((ax0_ax1_fused * 14) + ax2)] / ph_3[((ax0_ax1_fused * 14) + ax2)]) * ph_0[((ax0_ax1_fused * 14) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 12, 14), \"float32\"), ph_3: T.Buffer((1, 12, 14), \"float32\"), compute: T.Buffer((1, 12, 14), \"float32\"), compute_1: T.Buffer((1, 12, 14), \"float32\"), compute_2: T.Buffer((1, 12, 14), \"float32\"), T_multiply: T.Buffer((1, 12, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((168,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(168):\n            compute_3 = T.Buffer((168,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(168):\n            compute_3 = T.Buffer((168,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(168):\n            compute_3 = T.Buffer((168,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(14):\n                cse_var_1: T.int32 = ax0_ax1_fused * 14 + ax2\n                T_multiply_1 = T.Buffer((168,), data=T_multiply.data)\n                ph_3_1 = T.Buffer((168,), data=ph_3.data)\n                T_multiply_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1] * ph_0_1[cse_var_1]",
        "op_args": [
            [
                "divide",
                "abs",
                "atanh",
                "acos",
                "atanh",
                "multiply"
            ]
        ],
        "input_shape": [[1, 12, 14], [9, 3, 15], [1, 12, 14]],
        "output_shape": [[9, 3, 15], [1, 12, 14], [1, 12, 14], [1, 12, 14], [1, 12, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 40; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 40; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acosf(acosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      compute_2[((i0 * 20) + i1)] = sinf(ph_0[((i0 * 20) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 40; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 40; ++i0_i1_fused) {\n    compute_3[i0_i1_fused] = asinhf(cosf(ph_0[i0_i1_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __sinf(ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 20, 1), \"float32\"), compute: T.Buffer((2, 20, 1), \"float32\"), compute_1: T.Buffer((2, 20, 1), \"float32\"), compute_2: T.Buffer((2, 20, 1), \"float32\"), T_subtract: T.Buffer((2, 20, 1), \"float32\"), compute_3: T.Buffer((2, 20, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((40,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(40):\n            compute_4 = T.Buffer((40,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(40):\n            compute_4 = T.Buffer((40,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(T.acos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(2):\n            for i1 in range(20):\n                cse_var_1: T.int32 = i0 * 20 + i1\n                compute_4 = T.Buffer((40,), data=compute_2.data)\n                compute_4[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(40):\n            T_subtract_1 = T.Buffer((40,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(40):\n            compute_4 = T.Buffer((40,), data=compute_3.data)\n            compute_4[i0_i1_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused]))",
        "op_args": [
            [
                "asinh",
                "acos",
                "acos",
                "sin",
                "cos",
                "subtract",
                "asinh"
            ]
        ],
        "input_shape": [[2, 20, 1]],
        "output_shape": [[2, 20, 1], [2, 20, 1], [2, 20, 1], [2, 20, 1], [2, 20, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2873; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 169; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_divide[((ax0_ax1_fused * 17) + ax2)] = (ph_0[((ax0_ax1_fused * 17) + ax2)] / fmodf(acoshf(ph_0[((ax0_ax1_fused * 17) + ax2)]), ph_0[((ax0_ax1_fused * 17) + ax2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 169; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute_1[((i0_i1_fused * 17) + i2)] = expf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / fmodf(acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 13, 17), \"float32\"), compute: T.Buffer((13, 13, 17), \"float32\"), T_divide: T.Buffer((13, 13, 17), \"float32\"), compute_1: T.Buffer((13, 13, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2873,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2873):\n            compute_2 = T.Buffer((2873,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(169):\n            for ax2 in range(17):\n                cse_var_1: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_divide_1 = T.Buffer((2873,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.truncmod(T.acosh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(169):\n            for i2 in range(17):\n                cse_var_2: T.int32 = i0_i1_fused * 17 + i2\n                compute_2 = T.Buffer((2873,), data=compute_1.data)\n                compute_2[cse_var_2] = T.exp(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "abs",
                "acosh",
                "mod",
                "divide",
                "exp"
            ]
        ],
        "input_shape": [[13, 13, 17]],
        "output_shape": [[13, 13, 17], [13, 13, 17], [13, 13, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_multiply, float* T_subtract, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 144; ++ax0_ax1_fused_ax2_fused) {\n    float compute_1[1];\n    compute_1[0] = expf(asinhf(ph_0[ax0_ax1_fused_ax2_fused]));\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + compute_1[0]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 16; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 9) + ax2)] = (asinf(ph_0[((ax0_ax1_fused * 9) + ax2)]) * ph_0[((ax0_ax1_fused * 9) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 144; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (asinf(ph_0[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + __expf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 1, 9), \"float32\"), compute: T.Buffer((16, 1, 9), \"float32\"), T_add: T.Buffer((16, 1, 9), \"float32\"), T_multiply: T.Buffer((16, 1, 9), \"float32\"), T_subtract: T.Buffer((16, 1, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((144,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(144):\n            compute_1 = T.Buffer((144,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            compute_1 = T.allocate([1], \"float32\", \"global\")\n            compute_2 = T.Buffer((1,), data=compute_1, align=4)\n            compute_2[0] = T.exp(T.asinh(ph_0_1[ax0_ax1_fused_ax2_fused]))\n            T_add_1 = T.Buffer((144,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + compute_2[0]\n        for ax0_ax1_fused in T.parallel(16):\n            for ax2 in range(9):\n                cse_var_1: T.int32 = ax0_ax1_fused * 9 + ax2\n                T_multiply_1 = T.Buffer((144,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = T.asin(ph_0_1[cse_var_1]) * ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(144):\n            T_subtract_1 = T.Buffer((144,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.asin(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "asinh",
                "exp",
                "add",
                "asin",
                "multiply",
                "subtract"
            ]
        ],
        "input_shape": [[16, 1, 9]],
        "output_shape": [[16, 1, 9], [16, 1, 9], [16, 1, 9], [16, 1, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 10; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 6; ++ax2) {\n        T_mod[(((ax0 * 60) + (ax1 * 6)) + ax2)] = fmodf((ph_0[(((ax0 * 60) + (ax1 * 6)) + ax2)] * atanhf(ph_0[(((ax0 * 60) + (ax1 * 6)) + ax2)])), ph_0[(((ax0 * 60) + (ax1 * 6)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 60) + (i1 * 6)) + i2)] = asinf((ph_0[(((i0 * 60) + (i1 * 6)) + i2)] * atanhf(ph_0[(((i0 * 60) + (i1 * 6)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 120; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] + ph_3[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] * atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 10, 6), \"float32\"), ph_3: T.Buffer((2, 10, 6), \"float32\"), T_mod: T.Buffer((2, 10, 6), \"float32\"), compute: T.Buffer((2, 10, 6), \"float32\"), compute_1: T.Buffer((2, 10, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((120,), data=ph_0.data)\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(10, 6):\n                cse_var_1: T.int32 = ax0 * 60 + ax1 * 6 + ax2\n                T_mod_1 = T.Buffer((120,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1] * T.atanh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(10, 6):\n                cse_var_2: T.int32 = i0 * 60 + i1 * 6 + i2\n                compute_2 = T.Buffer((120,), data=compute.data)\n                compute_2[cse_var_2] = T.asin(ph_0_1[cse_var_2] * T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(120):\n            compute_2 = T.Buffer((120,), data=compute_1.data)\n            ph_3_1 = T.Buffer((120,), data=ph_3.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] + ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "add",
                "atanh",
                "multiply",
                "mod",
                "asin",
                "atanh"
            ]
        ],
        "input_shape": [[2, 10, 6], [17, 11, 18], [2, 10, 6]],
        "output_shape": [[17, 11, 18], [2, 10, 6], [2, 10, 6], [2, 10, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  float compute_1[175];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 35; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_1[((i0_i1_fused * 5) + i2)] = expf(ph_0[((i0_i1_fused * 5) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 175; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((0.000000e+00f - ((ph_0[ax0_ax1_fused_ax2_fused] / compute_1[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 175; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf((0.000000e+00f - ((ph_0[i0_i1_fused_i2_fused] / compute_1[i0_i1_fused_i2_fused]) + ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __expf((0.000000e+00f - ((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = ((0.000000e+00f - ((ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] / __expf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) + ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 5, 5), \"float32\"), T_multiply: T.Buffer((7, 5, 5), \"float32\"), compute: T.Buffer((7, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([175], \"float32\", \"global\")\n        compute_2 = T.Buffer((175,), data=compute_1)\n        ph_0_1 = T.Buffer((175,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(35):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(175):\n            T_multiply_1 = T.Buffer((175,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (T.float32(0) - (ph_0_1[ax0_ax1_fused_ax2_fused] / compute_2[ax0_ax1_fused_ax2_fused] + ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(175):\n            compute_3 = T.Buffer((175,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(T.float32(0) - (ph_0_1[i0_i1_fused_i2_fused] / compute_2[i0_i1_fused_i2_fused] + ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "divide",
                "add",
                "add",
                "subtract",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[7, 5, 5]],
        "output_shape": [[7, 5, 5], [7, 5, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 363; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 363; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 121; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute_2[((i0_i1_fused * 3) + i2)] = asinhf((ph_0[((i0_i1_fused * 3) + i2)] * cosf(ph_0[((i0_i1_fused * 3) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 11, 3), \"float32\"), compute: T.Buffer((11, 11, 3), \"float32\"), compute_1: T.Buffer((11, 11, 3), \"float32\"), compute_2: T.Buffer((11, 11, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((363,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(363):\n            compute_3 = T.Buffer((363,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(363):\n            compute_3 = T.Buffer((363,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(121):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_3 = T.Buffer((363,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1] * T.cos(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "atanh",
                "cos",
                "asinh",
                "cos",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[11, 11, 3]],
        "output_shape": [[11, 11, 3], [11, 11, 3], [11, 11, 3]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_divide, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 15; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n        T_divide[(((ax0 * 30) + (ax1 * 2)) + ax2)] = ((ph_0[(((ax0 * 30) + (ax1 * 2)) + ax2)] / cosf(ph_0[(((ax0 * 30) + (ax1 * 2)) + ax2)])) / ph_0[(((ax0 * 30) + (ax1 * 2)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 90; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + atanhf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))] + atanhf(ph_0[((((int)blockIdx.x) * 40) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])) / ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 15, 2), \"float32\"), T_divide: T.Buffer((3, 15, 2), \"float32\"), T_add: T.Buffer((3, 15, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((90,), data=ph_0.data)\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(15, 2):\n                cse_var_1: T.int32 = ax0 * 30 + ax1 * 2 + ax2\n                T_divide_1 = T.Buffer((90,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.cos(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for ax0_ax1_fused_ax2_fused in T.parallel(90):\n            T_add_1 = T.Buffer((90,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] + T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "cos",
                "divide",
                "divide",
                "atanh",
                "add"
            ]
        ],
        "input_shape": [[3, 15, 2]],
        "output_shape": [[3, 15, 2], [3, 15, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[480];\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 480; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 48; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute_3[((i0_i1_fused * 10) + i2)] = expf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(compute_3[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 480; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 480; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf((ph_0[i0_i1_fused_i2_fused_2] * ph_3[i0_i1_fused_i2_fused_2]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 8; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 6; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n        T_subtract[(((ax0 * 60) + (ax1 * 10)) + ax2)] = ((ph_0[(((ax0 * 60) + (ax1 * 10)) + ax2)] * ph_3[(((ax0 * 60) + (ax1 * 10)) + ax2)]) - ph_0[(((ax0 * 60) + (ax1 * 10)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 10), \"float32\"), ph_3: T.Buffer((8, 6, 10), \"float32\"), T_mod: T.Buffer((8, 6, 10), \"float32\"), compute: T.Buffer((8, 6, 10), \"float32\"), compute_1: T.Buffer((8, 6, 10), \"float32\"), compute_2: T.Buffer((8, 6, 10), \"float32\"), T_subtract: T.Buffer((8, 6, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([480], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        ph_3_1 = T.Buffer((480,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(480):\n            T_mod_1 = T.Buffer((480,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        compute_4 = T.Buffer((480,), data=compute_3)\n        for i0_i1_fused in T.parallel(48):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_5 = T.Buffer((480,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.asin(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_5 = T.Buffer((480,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.fabs(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_5 = T.Buffer((480,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(8):\n            for ax1, ax2 in T.grid(6, 10):\n                cse_var_2: T.int32 = ax0 * 60 + ax1 * 10 + ax2\n                T_subtract_1 = T.Buffer((480,), data=T_subtract.data)\n                T_subtract_1[cse_var_2] = ph_0_1[cse_var_2] * ph_3_1[cse_var_2] - ph_0_1[cse_var_2]",
        "op_args": [
            [
                "multiply",
                "mod",
                "exp",
                "asin",
                "abs",
                "acos",
                "subtract"
            ]
        ],
        "input_shape": [[8, 6, 10], [13, 9, 18], [8, 6, 10]],
        "output_shape": [[13, 9, 18], [8, 6, 10], [8, 6, 10], [8, 6, 10], [8, 6, 10], [8, 6, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 476; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 34) + (i1 * 17)) + i2)] = atanf(ph_0[(((i0 * 34) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 28; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n      compute_1[((i0_i1_fused * 17) + i2_1)] = expf(asinhf(ph_0[((i0_i1_fused * 17) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 476; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = acoshf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 2, 17), \"float32\"), ph_3: T.Buffer((14, 2, 17), \"float32\"), T_mod: T.Buffer((14, 2, 17), \"float32\"), compute: T.Buffer((14, 2, 17), \"float32\"), compute_1: T.Buffer((14, 2, 17), \"float32\"), compute_2: T.Buffer((14, 2, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((476,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(476):\n            T_mod_1 = T.Buffer((476,), data=T_mod.data)\n            ph_3_1 = T.Buffer((476,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(2, 17):\n                cse_var_1: T.int32 = i0 * 34 + i1 * 17 + i2\n                compute_3 = T.Buffer((476,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(28):\n            for i2 in range(17):\n                cse_var_2: T.int32 = i0_i1_fused * 17 + i2\n                compute_3 = T.Buffer((476,), data=compute_1.data)\n                compute_3[cse_var_2] = T.exp(T.asinh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(476):\n            compute_3 = T.Buffer((476,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "atan",
                "asinh",
                "exp",
                "acosh"
            ]
        ],
        "input_shape": [[14, 2, 17], [19, 2, 18], [14, 2, 17]],
        "output_shape": [[14, 2, 17], [19, 2, 18], [14, 2, 17], [14, 2, 17], [14, 2, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 16; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 272) + (i1 * 17)) + i2)] = atanhf(ph_0[(((i0 * 272) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5168; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], atanhf(ceilf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 5168; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = asinf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 5168; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], atanhf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(atanf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinf(atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 16, 17), \"float32\"), compute: T.Buffer((19, 16, 17), \"float32\"), T_mod: T.Buffer((19, 16, 17), \"float32\"), compute_1: T.Buffer((19, 16, 17), \"float32\"), compute_2: T.Buffer((19, 16, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((5168,), data=ph_0.data)\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(16, 17):\n                cse_var_1: T.int32 = i0 * 272 + i1 * 17 + i2\n                compute_3 = T.Buffer((5168,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(5168):\n            T_mod_1 = T.Buffer((5168,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.atanh(T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused])))\n        for i0_i1_fused_i2_fused in T.parallel(5168):\n            compute_3 = T.Buffer((5168,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(5168):\n            compute_3 = T.Buffer((5168,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atanh",
                "ceil",
                "atanh",
                "mod",
                "atan",
                "asin",
                "acos"
            ]
        ],
        "input_shape": [[19, 16, 17]],
        "output_shape": [[19, 16, 17], [19, 16, 17], [19, 16, 17], [19, 16, 17]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 204; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 17; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_1[(((i0 * 12) + (i1 * 6)) + i2)] = ceilf(ceilf(ph_0[(((i0 * 12) + (i1 * 6)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 204; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / asinf(ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((17, 2, 6), \"float32\"), compute: T.Buffer((17, 2, 6), \"float32\"), compute_1: T.Buffer((17, 2, 6), \"float32\"), T_divide: T.Buffer((17, 2, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((204,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(204):\n            compute_2 = T.Buffer((204,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(17):\n            for i1, i2 in T.grid(2, 6):\n                cse_var_1: T.int32 = i0 * 12 + i1 * 6 + i2\n                compute_2 = T.Buffer((204,), data=compute_1.data)\n                compute_2[cse_var_1] = T.ceil(T.ceil(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(204):\n            T_divide_1 = T.Buffer((204,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "abs",
                "ceil",
                "ceil",
                "asin",
                "divide"
            ]
        ],
        "input_shape": [[17, 2, 6]],
        "output_shape": [[17, 2, 6], [17, 2, 6], [17, 2, 6]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 119; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_divide[((ax0_ax1_fused * 15) + ax2)] = (ph_0[((ax0_ax1_fused * 15) + ax2)] / asinf(fmodf(ph_0[((ax0_ax1_fused * 15) + ax2)], acoshf(ph_0[((ax0_ax1_fused * 15) + ax2)]))));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 119; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute[((i0_i1_fused * 15) + i2)] = asinhf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] / asinf(fmodf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))], acoshf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((7, 17, 15), \"float32\"), T_divide: T.Buffer((7, 17, 15), \"float32\"), compute: T.Buffer((7, 17, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1785,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(119):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_divide_1 = T.Buffer((1785,), data=T_divide.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / T.asin(T.truncmod(ph_0_1[cse_var_1], T.acosh(ph_0_1[cse_var_1])))\n        for i0_i1_fused in T.parallel(119):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_1 = T.Buffer((1785,), data=compute.data)\n                compute_1[cse_var_2] = T.asinh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "acosh",
                "mod",
                "asin",
                "divide",
                "asinh"
            ]
        ],
        "input_shape": [[7, 17, 15]],
        "output_shape": [[7, 17, 15], [7, 17, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4800; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4800; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf((ph_0[i0_i1_fused_i2_fused_1] + fmodf(ph_0[i0_i1_fused_i2_fused_1], cosf(ceilf(ph_0[i0_i1_fused_i2_fused_1])))));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4800; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = fabsf((ph_0[i0_i1_fused_i2_fused_2] + fmodf(ph_0[i0_i1_fused_i2_fused_2], cosf(ceilf(ph_0[i0_i1_fused_i2_fused_2])))));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __cosf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])))));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + fmodf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))], __cosf(ceilf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 16, 20), \"float32\"), compute: T.Buffer((15, 16, 20), \"float32\"), compute_1: T.Buffer((15, 16, 20), \"float32\"), compute_2: T.Buffer((15, 16, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4800,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] + T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))))\n        for i0_i1_fused_i2_fused in T.parallel(4800):\n            compute_3 = T.Buffer((4800,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] + T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))))",
        "op_args": [
            [
                "exp",
                "ceil",
                "cos",
                "mod",
                "add",
                "exp",
                "abs"
            ]
        ],
        "input_shape": [[15, 16, 20]],
        "output_shape": [[15, 16, 20], [15, 16, 20], [15, 16, 20]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 15; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 15; ++ax0_ax1_fused) {\n    T_multiply[ax0_ax1_fused] = (atanhf(ph_0[ax0_ax1_fused]) * ph_0[ax0_ax1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 15; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = sinf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      compute_2[((i0 * 5) + i1)] = atanf((ph_0[((i0 * 5) + i1)] + ph_3[((i0 * 5) + i1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 5, 1), \"float32\"), ph_3: T.Buffer((3, 5, 1), \"float32\"), compute: T.Buffer((3, 5, 1), \"float32\"), T_multiply: T.Buffer((3, 5, 1), \"float32\"), compute_1: T.Buffer((3, 5, 1), \"float32\"), compute_2: T.Buffer((3, 5, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((15,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(15):\n            compute_3 = T.Buffer((15,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(15):\n            T_multiply_1 = T.Buffer((15,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused] = T.atanh(ph_0_1[ax0_ax1_fused]) * ph_0_1[ax0_ax1_fused]\n        for i0_i1_fused_i2_fused in T.parallel(15):\n            compute_3 = T.Buffer((15,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(3):\n            for i1 in range(5):\n                cse_var_1: T.int32 = i0 * 5 + i1\n                compute_3 = T.Buffer((15,), data=compute_2.data)\n                ph_3_1 = T.Buffer((15,), data=ph_3.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1] + ph_3_1[cse_var_1])",
        "op_args": [
            [
                "add",
                "cos",
                "atanh",
                "multiply",
                "sin",
                "atan"
            ]
        ],
        "input_shape": [[3, 5, 1], [19, 13, 1], [3, 5, 1]],
        "output_shape": [[19, 13, 1], [3, 5, 1], [3, 5, 1], [3, 5, 1], [3, 5, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3000; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 200; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 15; ++ax2) {\n      T_divide[((ax0_ax1_fused * 15) + ax2)] = (acoshf(ph_0[((ax0_ax1_fused * 15) + ax2)]) / ph_0[((ax0_ax1_fused * 15) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 200; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_1[((i0_i1_fused * 15) + i2)] = ceilf(fmodf(ph_0[((i0_i1_fused * 15) + i2)], atanf(ph_0[((i0_i1_fused * 15) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = ceilf(fmodf(ph_0[((int)blockIdx.x)], atanf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 10, 15), \"float32\"), compute: T.Buffer((20, 10, 15), \"float32\"), T_divide: T.Buffer((20, 10, 15), \"float32\"), compute_1: T.Buffer((20, 10, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((3000,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(3000):\n            compute_2 = T.Buffer((3000,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused in T.parallel(200):\n            for ax2 in range(15):\n                cse_var_1: T.int32 = ax0_ax1_fused * 15 + ax2\n                T_divide_1 = T.Buffer((3000,), data=T_divide.data)\n                T_divide_1[cse_var_1] = T.acosh(ph_0_1[cse_var_1]) / ph_0_1[cse_var_1]\n        for i0_i1_fused in T.parallel(200):\n            for i2 in range(15):\n                cse_var_2: T.int32 = i0_i1_fused * 15 + i2\n                compute_2 = T.Buffer((3000,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.truncmod(ph_0_1[cse_var_2], T.atan(ph_0_1[cse_var_2])))",
        "op_args": [
            [
                "asin",
                "acosh",
                "divide",
                "atan",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[20, 10, 15]],
        "output_shape": [[20, 10, 15], [20, 10, 15], [20, 10, 15]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 14; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int32_t i1 = 0; i1 < 7; ++i1) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i1 * 2) + i2)] = acoshf(acoshf(ph_0[((i1 * 2) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 14; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(acoshf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  for (int32_t i1_1 = 0; i1_1 < 7; ++i1_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_2[((i1_1 * 2) + i2_1)] = acoshf((ph_0[((i1_1 * 2) + i2_1)] - ph_3[((i1_1 * 2) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = fmodf(acoshf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 2), \"float32\"), ph_3: T.Buffer((1, 7, 2), \"float32\"), compute: T.Buffer((1, 7, 2), \"float32\"), compute_1: T.Buffer((1, 7, 2), \"float32\"), T_mod: T.Buffer((1, 7, 2), \"float32\"), compute_2: T.Buffer((1, 7, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((14,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(14):\n            compute_3 = T.Buffer((14,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i1, i2 in T.grid(7, 2):\n            cse_var_1: T.int32 = i1 * 2 + i2\n            compute_3 = T.Buffer((14,), data=compute_1.data)\n            compute_3[cse_var_1] = T.acosh(T.acosh(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(14):\n            T_mod_1 = T.Buffer((14,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.acosh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i1, i2 in T.grid(7, 2):\n            cse_var_2: T.int32 = i1 * 2 + i2\n            compute_3 = T.Buffer((14,), data=compute_2.data)\n            ph_3_1 = T.Buffer((14,), data=ph_3.data)\n            compute_3[cse_var_2] = T.acosh(ph_0_1[cse_var_2] - ph_3_1[cse_var_2])",
        "op_args": [
            [
                "subtract",
                "exp",
                "acosh",
                "acosh",
                "mod",
                "acosh"
            ]
        ],
        "input_shape": [[1, 7, 2], [16, 12, 12], [1, 7, 2]],
        "output_shape": [[16, 12, 12], [1, 7, 2], [1, 7, 2], [1, 7, 2], [1, 7, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 247) + (i1 * 13)) + i2)] = expf(ph_0[(((i0 * 247) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2223; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ceilf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2223; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = fabsf((ph_0[i0_i1_fused_i2_fused_1] / ceilf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2223; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = sinf((ph_0[i0_i1_fused_i2_fused_2] / ceilf(ph_0[i0_i1_fused_i2_fused_2])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = __cosf(ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 19, 13), \"float32\"), compute: T.Buffer((9, 19, 13), \"float32\"), compute_1: T.Buffer((9, 19, 13), \"float32\"), compute_2: T.Buffer((9, 19, 13), \"float32\"), compute_3: T.Buffer((9, 19, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2223,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(19, 13):\n                cse_var_1: T.int32 = i0 * 247 + i1 * 13 + i2\n                compute_4 = T.Buffer((2223,), data=compute.data)\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2223):\n            compute_4 = T.Buffer((2223,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2223):\n            compute_4 = T.Buffer((2223,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] / T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2223):\n            compute_4 = T.Buffer((2223,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused] / T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "exp",
                "ceil",
                "cos",
                "ceil",
                "divide",
                "abs",
                "sin"
            ]
        ],
        "input_shape": [[9, 19, 13]],
        "output_shape": [[9, 19, 13], [9, 19, 13], [9, 19, 13], [9, 19, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 520; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(fmodf(ph_0[i0_i1_fused_i2_fused], cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 4; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_1[(((i0 * 52) + (i1 * 13)) + i2)] = asinhf(ph_0[(((i0 * 52) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 520; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = ceilf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 4, 13), \"float32\"), compute: T.Buffer((10, 4, 13), \"float32\"), compute_1: T.Buffer((10, 4, 13), \"float32\"), compute_2: T.Buffer((10, 4, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((520,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(520):\n            compute_3 = T.Buffer((520,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.cos(ph_0_1[i0_i1_fused_i2_fused])))\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(4, 13):\n                cse_var_1: T.int32 = i0 * 52 + i1 * 13 + i2\n                compute_3 = T.Buffer((520,), data=compute_1.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(520):\n            compute_3 = T.Buffer((520,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(T.atan(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "cos",
                "mod",
                "ceil",
                "asinh",
                "atan",
                "sin"
            ]
        ],
        "input_shape": [[10, 4, 13]],
        "output_shape": [[10, 4, 13], [10, 4, 13], [10, 4, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_mod, float* T_subtract, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2280; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - (fmodf(ph_0[ax0_ax1_fused_ax2_fused], asinf(ph_0[ax0_ax1_fused_ax2_fused])) * ph_0[ax0_ax1_fused_ax2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2280; ++ax0_ax1_fused_ax2_fused_1) {\n    T_add[ax0_ax1_fused_ax2_fused_1] = (cosf(ph_0[ax0_ax1_fused_ax2_fused_1]) + ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 10; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 19; ++ax2) {\n        T_mod[(((ax0 * 228) + (ax1 * 19)) + ax2)] = fmodf(cosf(ph_0[(((ax0 * 228) + (ax1 * 19)) + ax2)]), ph_0[(((ax0 * 228) + (ax1 * 19)) + ax2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(__cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - (fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))])) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 12, 19), \"float32\"), T_subtract: T.Buffer((10, 12, 19), \"float32\"), T_add: T.Buffer((10, 12, 19), \"float32\"), T_mod: T.Buffer((10, 12, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2280,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2280):\n            T_subtract_1 = T.Buffer((2280,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.asin(ph_0_1[ax0_ax1_fused_ax2_fused])) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(2280):\n            T_add_1 = T.Buffer((2280,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0 in T.parallel(10):\n            for ax1, ax2 in T.grid(12, 19):\n                cse_var_1: T.int32 = ax0 * 228 + ax1 * 19 + ax2\n                T_mod_1 = T.Buffer((2280,), data=T_mod.data)\n                T_mod_1[cse_var_1] = T.truncmod(T.cos(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asin",
                "mod",
                "multiply",
                "subtract",
                "cos",
                "add",
                "mod"
            ]
        ],
        "input_shape": [[10, 12, 19]],
        "output_shape": [[10, 12, 19], [10, 12, 19], [10, 12, 19]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1530; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1530; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 153; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute_1[((i0_i1_fused * 10) + i2)] = atanhf(sinf(ph_0[((i0_i1_fused * 10) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 1530; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod_1[ax0_ax1_fused_ax2_fused_1] = fmodf(fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 17, 10), \"float32\"), ph_3: T.Buffer((9, 17, 10), \"float32\"), T_mod: T.Buffer((9, 17, 10), \"float32\"), compute: T.Buffer((9, 17, 10), \"float32\"), compute_1: T.Buffer((9, 17, 10), \"float32\"), T_mod_1: T.Buffer((9, 17, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1530,), data=ph_0.data)\n        ph_3_1 = T.Buffer((1530,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(1530):\n            T_mod_2 = T.Buffer((1530,), data=T_mod.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1530):\n            compute_2 = T.Buffer((1530,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(153):\n            for i2 in range(10):\n                cse_var_1: T.int32 = i0_i1_fused * 10 + i2\n                compute_2 = T.Buffer((1530,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(T.sin(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1530):\n            T_mod_2 = T.Buffer((1530,), data=T_mod_1.data)\n            T_mod_2[ax0_ax1_fused_ax2_fused] = T.truncmod(T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])",
        "op_args": [
            [
                "mod",
                "mod",
                "sin",
                "cos",
                "atanh",
                "mod"
            ]
        ],
        "input_shape": [[9, 17, 10], [11, 7, 12], [9, 17, 10]],
        "output_shape": [[11, 7, 12], [9, 17, 10], [9, 17, 10], [9, 17, 10], [9, 17, 10]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  float compute_4[108];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 108; ++i0_i1_fused) {\n    compute[i0_i1_fused] = atanhf(ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 108; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = sinf(asinhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 108; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 108; ++i0_i1_fused_i2_fused_2) {\n    compute_4[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 108; ++i0_i1_fused_1) {\n    compute_3[i0_i1_fused_1] = expf(compute_4[i0_i1_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 108; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (compute_4[ax0_ax1_fused_ax2_fused] - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 18, 1), \"float32\"), compute: T.Buffer((6, 18, 1), \"float32\"), compute_1: T.Buffer((6, 18, 1), \"float32\"), compute_2: T.Buffer((6, 18, 1), \"float32\"), compute_3: T.Buffer((6, 18, 1), \"float32\"), T_subtract: T.Buffer((6, 18, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_4 = T.allocate([108], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((108,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(108):\n            compute_5 = T.Buffer((108,), data=compute.data)\n            compute_5[i0_i1_fused] = T.atanh(ph_0_1[i0_i1_fused])\n        for i0_i1_fused_i2_fused in T.parallel(108):\n            compute_5 = T.Buffer((108,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(108):\n            compute_5 = T.Buffer((108,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        compute_5 = T.Buffer((108,), data=compute_4)\n        for i0_i1_fused_i2_fused in T.parallel(108):\n            compute_5[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(108):\n            compute_6 = T.Buffer((108,), data=compute_3.data)\n            compute_6[i0_i1_fused] = T.exp(compute_5[i0_i1_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(108):\n            T_subtract_1 = T.Buffer((108,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = compute_5[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "atanh",
                "asinh",
                "sin",
                "sin",
                "exp",
                "exp",
                "subtract"
            ]
        ],
        "input_shape": [[6, 18, 1]],
        "output_shape": [[6, 18, 1], [6, 18, 1], [6, 18, 1], [6, 18, 1], [6, 18, 1]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 121; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 11; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute[((i0_i1_fused * 11) + i2)] = acoshf(fmodf(ph_0[((i0_i1_fused * 11) + i2)], atanf(ph_0[((i0_i1_fused * 11) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 121; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf(fmodf(ph_0[i0_i1_fused_i2_fused], atanf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((int)blockIdx.x)] = (ph_0[((int)blockIdx.x)] / ph_3[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = atanhf(fmodf(ph_0[((int)blockIdx.x)], atanf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 11), \"float32\"), ph_3: T.Buffer((1, 11, 11), \"float32\"), T_divide: T.Buffer((1, 11, 11), \"float32\"), compute: T.Buffer((1, 11, 11), \"float32\"), compute_1: T.Buffer((1, 11, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((121,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(121):\n            T_divide_1 = T.Buffer((121,), data=T_divide.data)\n            ph_3_1 = T.Buffer((121,), data=ph_3.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(11):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_2 = T.Buffer((121,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(T.truncmod(ph_0_1[cse_var_1], T.atan(ph_0_1[cse_var_1])))\n        for i0_i1_fused_i2_fused in T.parallel(121):\n            compute_2 = T.Buffer((121,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.atan(ph_0_1[i0_i1_fused_i2_fused])))",
        "op_args": [
            [
                "divide",
                "atan",
                "mod",
                "acosh",
                "atanh"
            ]
        ],
        "input_shape": [[1, 11, 11], [19, 6, 18], [1, 11, 11]],
        "output_shape": [[1, 11, 11], [19, 6, 18], [1, 11, 11], [1, 11, 11]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 270; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ph_0[ax0_ax1_fused_ax2_fused], ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = asinhf(ph_0[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 30; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 9) + i2_1)] = asinhf(acoshf(ph_0[((i0_i1_fused_1 * 9) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 270; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = acosf(acoshf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acosf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 3, 9), \"float32\"), ph_3: T.Buffer((10, 3, 9), \"float32\"), T_mod: T.Buffer((10, 3, 9), \"float32\"), compute: T.Buffer((10, 3, 9), \"float32\"), compute_1: T.Buffer((10, 3, 9), \"float32\"), compute_2: T.Buffer((10, 3, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((270,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(270):\n            T_mod_1 = T.Buffer((270,), data=T_mod.data)\n            ph_3_1 = T.Buffer((270,), data=ph_3.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((270,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(9):\n                cse_var_2: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((270,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asinh(T.acosh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(270):\n            compute_3 = T.Buffer((270,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "asinh",
                "acosh",
                "asinh",
                "acos"
            ]
        ],
        "input_shape": [[10, 3, 9], [11, 19, 20], [10, 3, 9]],
        "output_shape": [[10, 3, 9], [11, 19, 20], [10, 3, 9], [10, 3, 9], [10, 3, 9]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 320; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = acosf(ph_0[((i0_i1_fused * 14) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4480; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(fabsf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4480; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 4480; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) + ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4480; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = atanhf(fabsf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __cosf(fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 16, 14), \"float32\"), compute: T.Buffer((20, 16, 14), \"float32\"), compute_1: T.Buffer((20, 16, 14), \"float32\"), compute_2: T.Buffer((20, 16, 14), \"float32\"), T_add: T.Buffer((20, 16, 14), \"float32\"), compute_3: T.Buffer((20, 16, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4480,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(320):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_4 = T.Buffer((4480,), data=compute.data)\n                compute_4[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_4 = T.Buffer((4480,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_4 = T.Buffer((4480,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(4480):\n            T_add_1 = T.Buffer((4480,), data=T_add.data)\n            T_add_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) + ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(4480):\n            compute_4 = T.Buffer((4480,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.fabs(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "acos",
                "abs",
                "cos",
                "ceil",
                "abs",
                "add",
                "atanh"
            ]
        ],
        "input_shape": [[20, 16, 14]],
        "output_shape": [[20, 16, 14], [20, 16, 14], [20, 16, 14], [20, 16, 14], [20, 16, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 117; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 14; ++i2) {\n      compute[((i0_i1_fused * 14) + i2)] = fabsf((ph_0[((i0_i1_fused * 14) + i2)] - sinf(ph_0[((i0_i1_fused * 14) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 117; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 14; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 14) + i2_1)] = ceilf(ph_0[((i0_i1_fused_1 * 14) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1638; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = cosf(sinf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 13, 14), \"float32\"), compute: T.Buffer((9, 13, 14), \"float32\"), compute_1: T.Buffer((9, 13, 14), \"float32\"), compute_2: T.Buffer((9, 13, 14), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1638,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(14):\n                cse_var_1: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((1638,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1] - T.sin(ph_0_1[cse_var_1]))\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(14):\n                cse_var_2: T.int32 = i0_i1_fused * 14 + i2\n                compute_3 = T.Buffer((1638,), data=compute_1.data)\n                compute_3[cse_var_2] = T.ceil(ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(1638):\n            compute_3 = T.Buffer((1638,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "sin",
                "subtract",
                "abs",
                "ceil",
                "sin",
                "cos"
            ]
        ],
        "input_shape": [[9, 13, 14]],
        "output_shape": [[9, 13, 14], [9, 13, 14], [9, 13, 14]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float auto_scheduler_layout_transform[240];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 240; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 240; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(atanf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 240; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(atanf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 4; ++ax0_ax1_fused_ax2_fused_1) {\n    for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n      for (int32_t ax10 = 0; ax10 < 5; ++ax10) {\n        for (int32_t ax11 = 0; ax11 < 6; ++ax11) {\n          auto_scheduler_layout_transform[((((ax0_ax1_fused_ax2_fused_1 * 60) + (ax8 * 30)) + (ax10 * 6)) + ax11)] = ph_3[((((ax0_ax1_fused_ax2_fused_1 * 60) + (ax8 * 30)) + (ax11 * 5)) + ax10)];\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused = 0; i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused < 4; ++i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused) {\n    float T_batch_matmul_NN[25];\n    for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 5; ++i_outer_inner_init) {\n      for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 5; ++j_outer_inner_init) {\n        T_batch_matmul_NN[((i_outer_inner_init * 5) + j_outer_inner_init)] = 0.000000e+00f;\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 2; ++k_outer) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 5; ++i_outer_inner) {\n        for (int32_t j_outer_inner = 0; j_outer_inner < 5; ++j_outer_inner) {\n          for (int32_t k_inner = 0; k_inner < 6; ++k_inner) {\n            T_batch_matmul_NN[((i_outer_inner * 5) + j_outer_inner)] = (T_batch_matmul_NN[((i_outer_inner * 5) + j_outer_inner)] + (ph_0[((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 60) + (i_outer_inner * 12)) + (k_outer * 6)) + k_inner)] * auto_scheduler_layout_transform[((((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 60) + (k_outer * 30)) + (j_outer_inner * 6)) + k_inner)]));\n          }\n        }\n      }\n    }\n    for (int32_t i1_inner = 0; i1_inner < 5; ++i1_inner) {\n      for (int32_t i2_inner_s = 0; i2_inner_s < 5; ++i2_inner_s) {\n        compute_2[(((i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 25) + (i1_inner * 5)) + i2_inner_s)] = atanhf(T_batch_matmul_NN[((i1_inner * 5) + i2_inner_s)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(45) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  float T_batch_matmul_NN[8];\n  __shared__ float ph_3_shared[90];\n  for (int i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n    T_batch_matmul_NN[i_inner_init] = 0.000000e+00f;\n    T_batch_matmul_NN[(i_inner_init + 2)] = 0.000000e+00f;\n    T_batch_matmul_NN[(i_inner_init + 4)] = 0.000000e+00f;\n    T_batch_matmul_NN[(i_inner_init + 6)] = 0.000000e+00f;\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 4; ++k_outer_outer) {\n    __syncthreads();\n    for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n      ph_3_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 45) + ((int)threadIdx.x))] = ph_3[(((((((ax0_ax1_fused_ax2_fused_outer_outer * 9) + (((int)threadIdx.x) / 5)) >> 1) * 40) + (k_outer_outer * 10)) + ((((((int)threadIdx.x) / 5) + ax0_ax1_fused_ax2_fused_outer_outer) & 1) * 5)) + (((int)threadIdx.x) % 5))];\n    }\n    __syncthreads();\n    for (int k_inner = 0; k_inner < 2; ++k_inner) {\n      for (int i_inner = 0; i_inner < 2; ++i_inner) {\n        T_batch_matmul_NN[i_inner] = (T_batch_matmul_NN[i_inner] + (ph_0[(((((((int)threadIdx.x) / 5) * 64) + (i_inner * 8)) + (k_outer_outer * 2)) + k_inner)] * ph_3_shared[((((((int)threadIdx.x) / 5) * 10) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN[(i_inner + 2)] = (T_batch_matmul_NN[(i_inner + 2)] + (ph_0[((((((((int)threadIdx.x) / 5) * 64) + (i_inner * 8)) + (k_outer_outer * 2)) + k_inner) + 16)] * ph_3_shared[((((((int)threadIdx.x) / 5) * 10) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN[(i_inner + 4)] = (T_batch_matmul_NN[(i_inner + 4)] + (ph_0[((((((((int)threadIdx.x) / 5) * 64) + (i_inner * 8)) + (k_outer_outer * 2)) + k_inner) + 32)] * ph_3_shared[((((((int)threadIdx.x) / 5) * 10) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n        T_batch_matmul_NN[(i_inner + 6)] = (T_batch_matmul_NN[(i_inner + 6)] + (ph_0[((((((((int)threadIdx.x) / 5) * 64) + (i_inner * 8)) + (k_outer_outer * 2)) + k_inner) + 48)] * ph_3_shared[((((((int)threadIdx.x) / 5) * 10) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n      }\n    }\n  }\n  for (int i1_inner = 0; i1_inner < 2; ++i1_inner) {\n    compute[((((((int)threadIdx.x) / 5) * 40) + (i1_inner * 5)) + (((int)threadIdx.x) % 5))] = atanhf(T_batch_matmul_NN[i1_inner]);\n    compute[(((((((int)threadIdx.x) / 5) * 40) + (i1_inner * 5)) + (((int)threadIdx.x) % 5)) + 10)] = atanhf(T_batch_matmul_NN[(i1_inner + 2)]);\n    compute[(((((((int)threadIdx.x) / 5) * 40) + (i1_inner * 5)) + (((int)threadIdx.x) % 5)) + 20)] = atanhf(T_batch_matmul_NN[(i1_inner + 4)]);\n    compute[(((((((int)threadIdx.x) / 5) * 40) + (i1_inner * 5)) + (((int)threadIdx.x) % 5)) + 30)] = atanhf(T_batch_matmul_NN[(i1_inner + 6)]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 5, 12), \"float32\"), ph_3: T.Buffer((4, 12, 5), \"float32\"), compute: T.Buffer((4, 5, 12), \"float32\"), T_mod: T.Buffer((4, 5, 12), \"float32\"), compute_1: T.Buffer((4, 5, 12), \"float32\"), compute_2: T.Buffer((4, 5, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([240], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((240,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(240):\n            T_mod_1 = T.Buffer((240,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(240):\n            compute_3 = T.Buffer((240,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((240,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(4):\n            for ax8, ax10, ax11 in T.grid(2, 5, 6):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 60 + ax8 * 30\n                ph_3_1 = T.Buffer((240,), data=ph_3.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax10 * 6 + ax11] = ph_3_1[cse_var_1 + ax11 * 5 + ax10]\n        for i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused in T.parallel(4):\n            T_batch_matmul_NN = T.allocate([25], \"float32\", \"global\")\n            T_batch_matmul_NN_1 = T.Buffer((25,), data=T_batch_matmul_NN)\n            for i_outer_inner_init, j_outer_inner_init in T.grid(5, 5):\n                T_batch_matmul_NN_1[i_outer_inner_init * 5 + j_outer_inner_init] = T.float32(0)\n            for k_outer, i_outer_inner, j_outer_inner, k_inner in T.grid(2, 5, 5, 6):\n                cse_var_3: T.int32 = i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 60\n                cse_var_2: T.int32 = i_outer_inner * 5 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_3 + i_outer_inner * 12 + k_outer * 6 + k_inner] * auto_scheduler_layout_transform_1[cse_var_3 + k_outer * 30 + j_outer_inner * 6 + k_inner]\n            for i1_inner, i2_inner_s in T.grid(5, 5):\n                cse_var_4: T.int32 = i1_inner * 5\n                compute_3 = T.Buffer((100,), data=compute_2.data)\n                compute_3[i0_outer_outer_i1_outer_outer_fused_i2_outer_outer_fused_i0_outer_inner_fused_i1_outer_inner_fused_i2_outer_inner_fused * 25 + cse_var_4 + i2_inner_s] = T.atanh(T_batch_matmul_NN_1[cse_var_4 + i2_inner_s])",
        "op_args": [
            [
                "batch_matmul",
                "abs",
                "atan",
                "mod",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[4, 5, 12], [10, 2, 18], [4, 12, 5]],
        "output_shape": [[10, 2, 18], [4, 5, 12], [4, 5, 12], [4, 5, 12], [4, 5, 5]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute[(((i0 * 247) + (i1 * 13)) + i2)] = acoshf(ph_0[(((i0 * 247) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 38; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 13; ++i2_1) {\n      compute_1[((i0_i1_fused * 13) + i2_1)] = asinf(asinf(ph_0[((i0_i1_fused * 13) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 494; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 19, 13), \"float32\"), compute: T.Buffer((2, 19, 13), \"float32\"), compute_1: T.Buffer((2, 19, 13), \"float32\"), compute_2: T.Buffer((2, 19, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((494,), data=ph_0.data)\n        for i0 in T.parallel(2):\n            for i1, i2 in T.grid(19, 13):\n                cse_var_1: T.int32 = i0 * 247 + i1 * 13 + i2\n                compute_3 = T.Buffer((494,), data=compute.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(38):\n            for i2 in range(13):\n                cse_var_2: T.int32 = i0_i1_fused * 13 + i2\n                compute_3 = T.Buffer((494,), data=compute_1.data)\n                compute_3[cse_var_2] = T.asin(T.asin(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(494):\n            compute_3 = T.Buffer((494,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "asin",
                "asin",
                "abs"
            ]
        ],
        "input_shape": [[2, 19, 13]],
        "output_shape": [[2, 19, 13], [2, 19, 13], [2, 19, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = expf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 60; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(atanhf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 60; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 30; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_3[((i0_i1_fused_1 * 2) + i2_1)] = acoshf(ceilf(ph_0[((i0_i1_fused_1 * 2) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((5, 6, 2), \"float32\"), compute: T.Buffer((5, 6, 2), \"float32\"), compute_1: T.Buffer((5, 6, 2), \"float32\"), compute_2: T.Buffer((5, 6, 2), \"float32\"), compute_3: T.Buffer((5, 6, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((60,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_4 = T.Buffer((60,), data=compute.data)\n                compute_4[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_4 = T.Buffer((60,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(60):\n            compute_4 = T.Buffer((60,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_4 = T.Buffer((60,), data=compute_3.data)\n                compute_4[cse_var_2] = T.acosh(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "exp",
                "atanh",
                "acosh",
                "sin",
                "ceil",
                "acosh"
            ]
        ],
        "input_shape": [[5, 6, 2]],
        "output_shape": [[5, 6, 2], [5, 6, 2], [5, 6, 2], [5, 6, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 22; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 22; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(fmodf(ph_0[i0_i1_fused_i2_fused], atanhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  for (int32_t i1 = 0; i1 < 11; ++i1) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i1 * 2) + i2)] = atanhf(fmodf(ph_0[((i1 * 2) + i2)], atanhf(ph_0[((i1 * 2) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 11, 2), \"float32\"), ph_3: T.Buffer((1, 11, 2), \"float32\"), T_subtract: T.Buffer((1, 11, 2), \"float32\"), compute: T.Buffer((1, 11, 2), \"float32\"), compute_1: T.Buffer((1, 11, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((22,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(22):\n            T_subtract_1 = T.Buffer((22,), data=T_subtract.data)\n            ph_3_1 = T.Buffer((22,), data=ph_3.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(22):\n            compute_2 = T.Buffer((22,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.atanh(ph_0_1[i0_i1_fused_i2_fused])))\n        for i1, i2 in T.grid(11, 2):\n            cse_var_1: T.int32 = i1 * 2 + i2\n            compute_2 = T.Buffer((22,), data=compute_1.data)\n            compute_2[cse_var_1] = T.atanh(T.truncmod(ph_0_1[cse_var_1], T.atanh(ph_0_1[cse_var_1])))",
        "op_args": [
            [
                "subtract",
                "atanh",
                "mod",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[1, 11, 2], [11, 2, 5], [1, 11, 2]],
        "output_shape": [[1, 11, 2], [11, 2, 5], [1, 11, 2], [1, 11, 2]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 26; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute[((i0_i1_fused * 8) + i2)] = asinf((ph_0[((i0_i1_fused * 8) + i2)] / atanf(ph_0[((i0_i1_fused * 8) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 208; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n        compute_2[(((i0 * 16) + (i1 * 8)) + i2_1)] = acoshf(atanhf(ph_0[(((i0 * 16) + (i1 * 8)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 208; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = atanf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] / atanf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 2, 8), \"float32\"), compute: T.Buffer((13, 2, 8), \"float32\"), compute_1: T.Buffer((13, 2, 8), \"float32\"), compute_2: T.Buffer((13, 2, 8), \"float32\"), compute_3: T.Buffer((13, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((208,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(26):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_4 = T.Buffer((208,), data=compute.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1] / T.atan(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(208):\n            compute_4 = T.Buffer((208,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(13):\n            for i1, i2 in T.grid(2, 8):\n                cse_var_2: T.int32 = i0 * 16 + i1 * 8 + i2\n                compute_4 = T.Buffer((208,), data=compute_2.data)\n                compute_4[cse_var_2] = T.acosh(T.atanh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(208):\n            compute_4 = T.Buffer((208,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atan(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "atan",
                "divide",
                "asin",
                "exp",
                "atanh",
                "acosh",
                "atan"
            ]
        ],
        "input_shape": [[13, 2, 8]],
        "output_shape": [[13, 2, 8], [13, 2, 8], [13, 2, 8], [13, 2, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf((ph_0[i0_i1_fused_i2_fused] * ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 30; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_1[((i0_i1_fused * 16) + i2)] = cosf(ph_0[((i0_i1_fused * 16) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 30; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 16) + i2_1)] = fabsf(ceilf(ph_0[((i0_i1_fused_1 * 16) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 480; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = expf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 10, 16), \"float32\"), compute: T.Buffer((3, 10, 16), \"float32\"), compute_1: T.Buffer((3, 10, 16), \"float32\"), compute_2: T.Buffer((3, 10, 16), \"float32\"), compute_3: T.Buffer((3, 10, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_4 = T.Buffer((480,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] * T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((480,), data=compute_1.data)\n                compute_4[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(30):\n            for i2 in range(16):\n                cse_var_2: T.int32 = i0_i1_fused * 16 + i2\n                compute_4 = T.Buffer((480,), data=compute_2.data)\n                compute_4[cse_var_2] = T.fabs(T.ceil(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_4 = T.Buffer((480,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.exp(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "ceil",
                "multiply",
                "asinh",
                "cos",
                "ceil",
                "abs",
                "exp"
            ]
        ],
        "input_shape": [[3, 10, 16]],
        "output_shape": [[3, 10, 16], [3, 10, 16], [3, 10, 16], [3, 10, 16]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  float compute_3[351];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 351; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 9; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 3; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 13; ++ax2) {\n        T_add[(((ax0 * 39) + (ax1 * 13)) + ax2)] = (atanf(ph_0[(((ax0 * 39) + (ax1 * 13)) + ax2)]) + ph_0[(((ax0 * 39) + (ax1 * 13)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 351; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 351; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 351; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (compute_3[ax0_ax1_fused_ax2_fused] - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 13; ++i2) {\n        compute_2[(((i0 * 39) + (i1 * 13)) + i2)] = cosf(compute_3[(((i0 * 39) + (i1 * 13)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((int)blockIdx.x)] = (atanf(ph_0[((int)blockIdx.x)]) + ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 3, 13), \"float32\"), compute: T.Buffer((9, 3, 13), \"float32\"), T_add: T.Buffer((9, 3, 13), \"float32\"), compute_1: T.Buffer((9, 3, 13), \"float32\"), T_subtract: T.Buffer((9, 3, 13), \"float32\"), compute_2: T.Buffer((9, 3, 13), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([351], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((351,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(351):\n            compute_4 = T.Buffer((351,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(9):\n            for ax1, ax2 in T.grid(3, 13):\n                cse_var_1: T.int32 = ax0 * 39 + ax1 * 13 + ax2\n                T_add_1 = T.Buffer((351,), data=T_add.data)\n                T_add_1[cse_var_1] = T.atan(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(351):\n            compute_4 = T.Buffer((351,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        compute_4 = T.Buffer((351,), data=compute_3)\n        for i0_i1_fused_i2_fused in T.parallel(351):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(351):\n            T_subtract_1 = T.Buffer((351,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = compute_4[ax0_ax1_fused_ax2_fused] - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(3, 13):\n                cse_var_2: T.int32 = i0 * 39 + i1 * 13 + i2\n                compute_5 = T.Buffer((351,), data=compute_2.data)\n                compute_5[cse_var_2] = T.cos(compute_4[cse_var_2])",
        "op_args": [
            [
                "sin",
                "atan",
                "add",
                "abs",
                "exp",
                "subtract",
                "cos"
            ]
        ],
        "input_shape": [[9, 3, 13]],
        "output_shape": [[9, 3, 13], [9, 3, 13], [9, 3, 13], [9, 3, 13], [9, 3, 13]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* ph_0) {\n  float compute_1[384];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 384; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf((ph_0[i0_i1_fused_i2_fused] / asinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 384; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = acoshf((ph_0[i0_i1_fused_i2_fused_1] - (ph_0[i0_i1_fused_i2_fused_1] - compute_1[i0_i1_fused_i2_fused_1])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 48; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_multiply[((ax0_ax1_fused * 8) + ax2)] = ((ph_0[((ax0_ax1_fused * 8) + ax2)] - (ph_0[((ax0_ax1_fused * 8) + ax2)] - compute_1[((ax0_ax1_fused * 8) + ax2)])) * ph_0[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] - (ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] - __expf((ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]))))) * ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - __expf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] / asinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]))))));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 6, 8), \"float32\"), compute: T.Buffer((8, 6, 8), \"float32\"), T_multiply: T.Buffer((8, 6, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_1 = T.allocate([384], \"float32\", \"global\")\n        compute_2 = T.Buffer((384,), data=compute_1)\n        ph_0_1 = T.Buffer((384,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] / T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(384):\n            compute_3 = T.Buffer((384,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused] - (ph_0_1[i0_i1_fused_i2_fused] - compute_2[i0_i1_fused_i2_fused]))\n        for ax0_ax1_fused in T.parallel(48):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_multiply_1 = T.Buffer((384,), data=T_multiply.data)\n                T_multiply_1[cse_var_1] = (ph_0_1[cse_var_1] - (ph_0_1[cse_var_1] - compute_2[cse_var_1])) * ph_0_1[cse_var_1]",
        "op_args": [
            [
                "asin",
                "divide",
                "exp",
                "subtract",
                "subtract",
                "acosh",
                "multiply"
            ]
        ],
        "input_shape": [[8, 6, 8]],
        "output_shape": [[8, 6, 8], [8, 6, 8]]
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  float compute_5[280];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 280; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(atanhf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  for (int32_t i1 = 0; i1 < 14; ++i1) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      compute_2[((i1 * 20) + i2)] = cosf(ph_0[((i1 * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 280; ++i0_i1_fused_i2_fused_2) {\n    compute_5[i0_i1_fused_i2_fused_2] = expf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 280; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = atanhf(compute_5[i0_i1_fused_i2_fused_3]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_4 = 0; i0_i1_fused_i2_fused_4 < 280; ++i0_i1_fused_i2_fused_4) {\n    compute_4[i0_i1_fused_i2_fused_4] = asinhf(compute_5[i0_i1_fused_i2_fused_4]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 14, 20), \"float32\"), compute: T.Buffer((1, 14, 20), \"float32\"), compute_1: T.Buffer((1, 14, 20), \"float32\"), compute_2: T.Buffer((1, 14, 20), \"float32\"), compute_3: T.Buffer((1, 14, 20), \"float32\"), compute_4: T.Buffer((1, 14, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_5 = T.allocate([280], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_6 = T.Buffer((280,), data=compute.data)\n            compute_6[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_6 = T.Buffer((280,), data=compute_1.data)\n            compute_6[i0_i1_fused_i2_fused] = T.fabs(T.atanh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i1, i2 in T.grid(14, 20):\n            cse_var_1: T.int32 = i1 * 20 + i2\n            compute_6 = T.Buffer((280,), data=compute_2.data)\n            compute_6[cse_var_1] = T.cos(ph_0_1[cse_var_1])\n        compute_6 = T.Buffer((280,), data=compute_5)\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_6[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_7 = T.Buffer((280,), data=compute_3.data)\n            compute_7[i0_i1_fused_i2_fused] = T.atanh(compute_6[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(280):\n            compute_7 = T.Buffer((280,), data=compute_4.data)\n            compute_7[i0_i1_fused_i2_fused] = T.asinh(compute_6[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "atanh",
                "abs",
                "cos",
                "exp",
                "atanh",
                "asinh"
            ]
        ],
        "input_shape": [[1, 14, 20]],
        "output_shape": [[1, 14, 20], [1, 14, 20], [1, 14, 20], [1, 14, 20], [1, 14, 20]]
    }
]