[
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 18; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 9; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute_1[((i0_i1_fused * 2) + i2)] = expf(asinhf(ph_0[((i0_i1_fused * 2) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 18; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 18; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __expf(asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 9, 2), \"float32\"), compute: T.Buffer((1, 9, 2), \"float32\"), compute_1: T.Buffer((1, 9, 2), \"float32\"), compute_2: T.Buffer((1, 9, 2), \"float32\"), compute_3: T.Buffer((1, 9, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((18,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_4 = T.Buffer((18,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(9):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_4 = T.Buffer((18,), data=compute_1.data)\n                compute_4[cse_var_1] = T.exp(T.asinh(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_4 = T.Buffer((18,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(18):\n            compute_4 = T.Buffer((18,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "abs",
                "asinh",
                "exp",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[1, 9, 2]],
        "output_shape": [[1, 9, 2], [1, 9, 2], [1, 9, 2], [1, 9, 2]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  for (int i0 = 0; i0 < 1; ++i0) {\n    for (int i1 = 0; i1 < 9; ++i1) {\n      for (int i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 18) + (i1 * 2)) + i2)] = fabsf(ph_0[(((i0 * 18) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  for (int i0_1 = 0; i0_1 < 1; ++i0_1) {\n    for (int i1_1 = 0; i1_1 < 9; ++i1_1) {\n      for (int i2_1 = 0; i2_1 < 2; ++i2_1) {\n        compute_1[(((i0_1 * 18) + (i1_1 * 2)) + i2_1)] = expf(asinhf(ph_0[(((i0_1 * 18) + (i1_1 * 2)) + i2_1)]));\n      }\n    }\n  }\n  for (int i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 18; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  for (int i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 18; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 221; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute[((i0_i1_fused * 7) + i2)] = expf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 221; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 7) + i2_1)] = atanhf(ph_0[((i0_i1_fused_1 * 7) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((13, 17, 7), \"float32\"), compute: T.Buffer((13, 17, 7), \"float32\"), compute_1: T.Buffer((13, 17, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1547,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(221):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1547,), data=compute.data)\n                compute_2[cse_var_1] = T.exp(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(221):\n            for i2 in range(7):\n                cse_var_2: T.int32 = i0_i1_fused * 7 + i2\n                compute_2 = T.Buffer((1547,), data=compute_1.data)\n                compute_2[cse_var_2] = T.atanh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "exp",
                "atanh"
            ]
        ],
        "input_shape": [[13, 17, 7]],
        "output_shape": [[13, 17, 7], [13, 17, 7]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 13; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 119) + (i1 * 7)) + i2)] = expf(ph_0[(((i0 * 119) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 13; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 119) + (i1_1 * 7)) + i2_1)] = atanhf(ph_0[(((i0_1 * 119) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 6840; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 6840; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_1[(((i0 * 360) + (i1 * 18)) + i2)] = atanhf(ph_0[(((i0 * 360) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 20, 18), \"float32\"), ph_3: T.Buffer((19, 20, 18), \"float32\"), T_multiply: T.Buffer((19, 20, 18), \"float32\"), compute: T.Buffer((19, 20, 18), \"float32\"), compute_1: T.Buffer((19, 20, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((6840,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(6840):\n            T_multiply_1 = T.Buffer((6840,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((6840,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(6840):\n            compute_2 = T.Buffer((6840,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(20, 18):\n                cse_var_1: T.int32 = i0 * 360 + i1 * 18 + i2\n                compute_2 = T.Buffer((6840,), data=compute_1.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "multiply",
                "abs",
                "atanh"
            ]
        ],
        "input_shape": [[19, 20, 18], [9, 19, 17], [19, 20, 18]],
        "output_shape": [[19, 20, 18], [9, 19, 17], [19, 20, 18], [19, 20, 18]],
        "deepseek_c": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 6840; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 20; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 360) + (i1 * 18)) + i2)] = fabsf(ph_0[(((i0 * 360) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 19; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 20; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_1[(((i0_1 * 360) + (i1_1 * 18)) + i2_1)] = atanhf(ph_0[(((i0_1 * 360) + (i1_1 * 18)) + i2_1)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 6; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 21) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 21) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 126; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = fabsf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 6; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_2[(((i0_1 * 21) + (i1_1 * 7)) + i2_1)] = atanf(ph_0[(((i0_1 * 21) + (i1_1 * 7)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((6, 3, 7), \"float32\"), compute: T.Buffer((6, 3, 7), \"float32\"), compute_1: T.Buffer((6, 3, 7), \"float32\"), compute_2: T.Buffer((6, 3, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((126,), data=ph_0.data)\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(3, 7):\n                cse_var_1: T.int32 = i0 * 21 + i1 * 7 + i2\n                compute_3 = T.Buffer((126,), data=compute.data)\n                compute_3[cse_var_1] = T.atan(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(126):\n            compute_3 = T.Buffer((126,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(6):\n            for i1, i2 in T.grid(3, 7):\n                cse_var_2: T.int32 = i0 * 21 + i1 * 7 + i2\n                compute_3 = T.Buffer((126,), data=compute_2.data)\n                compute_3[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "atan",
                "atan",
                "abs",
                "atan"
            ]
        ],
        "input_shape": [[6, 3, 7]],
        "output_shape": [[6, 3, 7], [6, 3, 7], [6, 3, 7]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  for (int i0 = 0; i0 < 6; ++i0) {\n    for (int i1 = 0; i1 < 3; ++i1) {\n      for (int i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 21) + (i1 * 7)) + i2)] = atanf(ph_0[(((i0 * 21) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  for (int i0_1 = 0; i0_1 < 6; ++i0_1) {\n    for (int i1_1 = 0; i1_1 < 3; ++i1_1) {\n      for (int i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 21) + (i1_1 * 7)) + i2_1)] = fabsf(atanf(ph_0[(((i0_1 * 21) + (i1_1 * 7)) + i2_1)]));\n      }\n    }\n  }\n  for (int i0_2 = 0; i0_2 < 6; ++i0_2) {\n    for (int i1_2 = 0; i1_2 < 3; ++i1_2) {\n      for (int i2_2 = 0; i2_2 < 7; ++i2_2) {\n        compute_2[(((i0_2 * 21) + (i1_2 * 7)) + i2_2)] = atanf(ph_0[(((i0_2 * 21) + (i1_2 * 7)) + i2_2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  float compute_3[2340];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2340; ++i0_i1_fused_i2_fused) {\n    compute_3[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2340; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = acoshf(compute_3[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2340; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = atanhf(compute_3[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 13; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 9; ++ax2) {\n        T_add[(((ax0 * 117) + (ax1 * 9)) + ax2)] = (ph_0[(((ax0 * 117) + (ax1 * 9)) + ax2)] + ph_3[(((ax0 * 117) + (ax1 * 9)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 2340; ++i0_i1_fused_i2_fused_3) {\n    compute_2[i0_i1_fused_i2_fused_3] = atanf((ph_0[i0_i1_fused_i2_fused_3] / ph_3[i0_i1_fused_i2_fused_3]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = atanf((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(__expf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(__expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 13, 9), \"float32\"), ph_3: T.Buffer((20, 13, 9), \"float32\"), T_add: T.Buffer((20, 13, 9), \"float32\"), compute: T.Buffer((20, 13, 9), \"float32\"), compute_1: T.Buffer((20, 13, 9), \"float32\"), compute_2: T.Buffer((20, 13, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        compute_3 = T.allocate([2340], \"float32\", \"global\")\n        compute_4 = T.Buffer((2340,), data=compute_3)\n        ph_0_1 = T.Buffer((2340,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_4[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_5 = T.Buffer((2340,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acosh(compute_4[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_5 = T.Buffer((2340,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(compute_4[i0_i1_fused_i2_fused])\n        ph_3_1 = T.Buffer((2340,), data=ph_3.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(13, 9):\n                cse_var_1: T.int32 = ax0 * 117 + ax1 * 9 + ax2\n                T_add_1 = T.Buffer((2340,), data=T_add.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(2340):\n            compute_5 = T.Buffer((2340,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "add",
                "exp",
                "acosh",
                "atanh",
                "atan"
            ]
        ],
        "input_shape": [[20, 13, 9], [16, 4, 9], [20, 13, 9]],
        "output_shape": [[16, 4, 9], [20, 13, 9], [20, 13, 9], [20, 13, 9], [20, 13, 9]],
        "deepseek_c": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2340; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2340; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(expf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 260; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      T_add[((i0_i1_fused * 9) + i2)] = (ph_0[((i0_i1_fused * 9) + i2)] + ph_3[((i0_i1_fused * 9) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2340; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = atanf((ph_0[i0_i1_fused_i2_fused_2] / ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1280; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(atanhf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 8; ++i2) {\n      compute_1[((i0_i1_fused * 8) + i2)] = acoshf(ph_0[((i0_i1_fused * 8) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 160; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      compute_2[((i0_i1_fused_1 * 8) + i2_1)] = asinhf(sinf(ph_0[((i0_i1_fused_1 * 8) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(__sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 10, 8), \"float32\"), compute: T.Buffer((16, 10, 8), \"float32\"), T_mod: T.Buffer((16, 10, 8), \"float32\"), compute_1: T.Buffer((16, 10, 8), \"float32\"), compute_2: T.Buffer((16, 10, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1280):\n            compute_3 = T.Buffer((1280,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1280):\n            T_mod_1 = T.Buffer((1280,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.atanh(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(8):\n                cse_var_1: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((1280,), data=compute_1.data)\n                compute_3[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for i0_i1_fused in T.parallel(160):\n            for i2 in range(8):\n                cse_var_2: T.int32 = i0_i1_fused * 8 + i2\n                compute_3 = T.Buffer((1280,), data=compute_2.data)\n                compute_3[cse_var_2] = T.asinh(T.sin(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "sin",
                "atanh",
                "mod",
                "acosh",
                "sin",
                "asinh"
            ]
        ],
        "input_shape": [[16, 10, 8]],
        "output_shape": [[16, 10, 8], [16, 10, 8], [16, 10, 8], [16, 10, 8]],
        "deepseek_c": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 80) + (i1 * 8)) + i2)] = sinf(ph_0[(((i0 * 80) + (i1 * 8)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 160; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 8; ++i2_1) {\n      T_mod[((i0_i1_fused * 8) + i2_1)] = fmodf(atanhf(ph_0[((i0_i1_fused * 8) + i2_1)]), ph_0[((i0_i1_fused * 8) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1280; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 160; ++i0_i1_fused_1) {\n    for (int32_t i2_2 = 0; i2_2 < 8; ++i2_2) {\n      compute_2[(((i0_i1_fused_1 * 8) + i2_2) + 0)] = asinhf(sinf(ph_0[(((i0_i1_fused_1 * 8) + i2_2) + 0)]));\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2700; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2700; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = acoshf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute_2[(((i0 * 180) + (i1 * 18)) + i2)] = ceilf(ph_0[(((i0 * 180) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 2700; ++i0_i1_fused_i2_fused_2) {\n    compute_3[i0_i1_fused_i2_fused_2] = ceilf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acoshf(ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 10, 18), \"float32\"), compute: T.Buffer((15, 10, 18), \"float32\"), compute_1: T.Buffer((15, 10, 18), \"float32\"), compute_2: T.Buffer((15, 10, 18), \"float32\"), compute_3: T.Buffer((15, 10, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2700,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2700):\n            compute_4 = T.Buffer((2700,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(2700):\n            compute_4 = T.Buffer((2700,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acosh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(15):\n            for i1, i2 in T.grid(10, 18):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 18 + i2\n                compute_4 = T.Buffer((2700,), data=compute_2.data)\n                compute_4[cse_var_1] = T.ceil(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2700):\n            compute_4 = T.Buffer((2700,), data=compute_3.data)\n            compute_4[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "ceil",
                "acosh",
                "ceil",
                "ceil"
            ]
        ],
        "input_shape": [[15, 10, 18]],
        "output_shape": [[15, 10, 18], [15, 10, 18], [15, 10, 18], [15, 10, 18]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 18; ++i2) {\n        compute[(((i0 * 180) + (i1 * 18)) + i2)] = atanhf(ph_0[(((i0 * 180) + (i1 * 18)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 15; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 10; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 18; ++i2_1) {\n        compute_1[(((i0_1 * 180) + (i1_1 * 18)) + i2_1)] = acoshf(ceilf(ph_0[(((i0_1 * 180) + (i1_1 * 18)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2700; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2700; ++i0_i1_fused_i2_fused_1) {\n    compute_3[i0_i1_fused_i2_fused_1] = ceilf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 154; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 154; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 22; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 7; ++i2) {\n      compute_2[((i0_i1_fused * 7) + i2)] = sinf(ph_0[((i0_i1_fused * 7) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 11, 7), \"float32\"), compute: T.Buffer((2, 11, 7), \"float32\"), compute_1: T.Buffer((2, 11, 7), \"float32\"), compute_2: T.Buffer((2, 11, 7), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((154,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(154):\n            compute_3 = T.Buffer((154,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(154):\n            compute_3 = T.Buffer((154,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(22):\n            for i2 in range(7):\n                cse_var_1: T.int32 = i0_i1_fused * 7 + i2\n                compute_3 = T.Buffer((154,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "asinh",
                "asin",
                "atanh",
                "sin"
            ]
        ],
        "input_shape": [[2, 11, 7]],
        "output_shape": [[2, 11, 7], [2, 11, 7], [2, 11, 7]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  for (int i0 = 0; i0 < 2; ++i0) {\n    for (int i1 = 0; i1 < 11; ++i1) {\n      for (int i2 = 0; i2 < 7; ++i2) {\n        compute[(((i0 * 77) + (i1 * 7)) + i2)] = asinhf(ph_0[(((i0 * 77) + (i1 * 7)) + i2)]);\n      }\n    }\n  }\n  for (int i0_1 = 0; i0_1 < 2; ++i0_1) {\n    for (int i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int i2_1 = 0; i2_1 < 7; ++i2_1) {\n        compute_1[(((i0_1 * 77) + (i1_1 * 7)) + i2_1)] = atanhf(asinf(ph_0[(((i0_1 * 77) + (i1_1 * 7)) + i2_1)]));\n      }\n    }\n  }\n  for (int i0_2 = 0; i0_2 < 2; ++i0_2) {\n    for (int i1_2 = 0; i1_2 < 11; ++i1_2) {\n      for (int i2_2 = 0; i2_2 < 7; ++i2_2) {\n        compute_2[(((i0_2 * 77) + (i1_2 * 7)) + i2_2)] = sinf(ph_0[(((i0_2 * 77) + (i1_2 * 7)) + i2_2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 594; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 594; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 594; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_3 = 0; i0_i1_fused_i2_fused_3 < 594; ++i0_i1_fused_i2_fused_3) {\n    compute_3[i0_i1_fused_i2_fused_3] = expf(sinf(ph_0[i0_i1_fused_i2_fused_3]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      for (int32_t i2 = 0; i2 < 6; ++i2) {\n        compute_4[(((i0 * 66) + (i1 * 6)) + i2)] = asinf(sinf(ph_0[(((i0 * 66) + (i1 * 6)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __expf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 11, 6), \"float32\"), compute: T.Buffer((9, 11, 6), \"float32\"), compute_1: T.Buffer((9, 11, 6), \"float32\"), compute_2: T.Buffer((9, 11, 6), \"float32\"), compute_3: T.Buffer((9, 11, 6), \"float32\"), compute_4: T.Buffer((9, 11, 6), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((594,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute.data)\n            compute_5[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute_1.data)\n            compute_5[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute_2.data)\n            compute_5[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(594):\n            compute_5 = T.Buffer((594,), data=compute_3.data)\n            compute_5[i0_i1_fused_i2_fused] = T.exp(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(11, 6):\n                cse_var_1: T.int32 = i0 * 66 + i1 * 6 + i2\n                compute_5 = T.Buffer((594,), data=compute_4.data)\n                compute_5[cse_var_1] = T.asin(T.sin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "ceil",
                "asin",
                "atanh",
                "acos",
                "sin",
                "exp",
                "asin"
            ]
        ],
        "input_shape": [[9, 11, 6]],
        "output_shape": [[9, 11, 6], [9, 11, 6], [9, 11, 6], [9, 11, 6], [9, 11, 6]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* compute_4, float* ph_0) {\n  for (int i0 = 0; i0 < 9; ++i0) {\n    for (int i1 = 0; i1 < 11; ++i1) {\n      for (int i2 = 0; i2 < 6; ++i2) {\n        compute[(((i0 * 66) + (i1 * 6)) + i2)] = ceilf(ph_0[(((i0 * 66) + (i1 * 6)) + i2)]);\n      }\n    }\n  }\n  for (int i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int i1_1 = 0; i1_1 < 11; ++i1_1) {\n      for (int i2_1 = 0; i2_1 < 6; ++i2_1) {\n        compute_1[(((i0_1 * 66) + (i1_1 * 6)) + i2_1)] = atanhf(asinf(ph_0[(((i0_1 * 66) + (i1_1 * 6)) + i2_1)]));\n      }\n    }\n  }\n  for (int i0_2 = 0; i0_2 < 9; ++i0_2) {\n    for (int i1_2 = 0; i1_2 < 11; ++i1_2) {\n      for (int i2_2 = 0; i2_2 < 6; ++i2_2) {\n        compute_2[(((i0_2 * 66) + (i1_2 * 6)) + i2_2)] = acosf(ph_0[(((i0_2 * 66) + (i1_2 * 6)) + i2_2)]);\n      }\n    }\n  }\n  for (int i0_3 = 0; i0_3 < 9; ++i0_3) {\n    for (int i1_3 = 0; i1_3 < 11; ++i1_3) {\n      for (int i2_3 = 0; i2_3 < 6; ++i2_3) {\n        compute_3[(((i0_3 * 66) + (i1_3 * 6)) + i2_3)] = expf(sinf(ph_0[(((i0_3 * 66) + (i1_3 * 6)) + i2_3)]));\n      }\n    }\n  }\n  for (int i0_4 = 0; i0_4 < 9; ++i0_4) {\n    for (int i1_4 = 0; i1_4 < 11; ++i1_4) {\n      for (int i2_4 = 0; i2_4 < 6; ++i2_4) {\n        compute_4[(((i0_4 * 66) + (i1_4 * 6)) + i2_4)] = asinf(sinf(ph_0[(((i0_4 * 66) + (i1_4 * 6)) + i2_4)]));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 150; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 150; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = atanhf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 150; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = fabsf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 10; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 15; ++i2) {\n      compute_3[((i0_i1_fused * 15) + i2)] = asinf(ph_0[((i0_i1_fused * 15) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(3) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))] = atanhf(asinf(ph_0[((((int)blockIdx.x) * 3) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 5, 15), \"float32\"), compute: T.Buffer((2, 5, 15), \"float32\"), compute_1: T.Buffer((2, 5, 15), \"float32\"), compute_2: T.Buffer((2, 5, 15), \"float32\"), compute_3: T.Buffer((2, 5, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((150,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_4 = T.Buffer((150,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_4 = T.Buffer((150,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.atanh(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(150):\n            compute_4 = T.Buffer((150,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(10):\n            for i2 in range(15):\n                cse_var_1: T.int32 = i0_i1_fused * 15 + i2\n                compute_4 = T.Buffer((150,), data=compute_3.data)\n                compute_4[cse_var_1] = T.asin(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "atanh",
                "asin",
                "atanh",
                "abs",
                "asin"
            ]
        ],
        "input_shape": [[2, 5, 15]],
        "output_shape": [[2, 5, 15], [2, 5, 15], [2, 5, 15], [2, 5, 15]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 75) + (i1 * 15)) + i2)] = atanhf(ph_0[(((i0 * 75) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 2; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 5; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 15; ++i2_1) {\n        compute_1[(((i0_1 * 75) + (i1_1 * 15)) + i2_1)] = atanhf(asinf(ph_0[(((i0_1 * 75) + (i1_1 * 15)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 2; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 5; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 15; ++i2_2) {\n        compute_2[(((i0_2 * 75) + (i1_2 * 15)) + i2_2)] = fabsf(ph_0[(((i0_2 * 75) + (i1_2 * 15)) + i2_2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_3 = 0; i0_3 < 2; ++i0_3) {\n    for (int32_t i1_3 = 0; i1_3 < 5; ++i1_3) {\n      for (int32_t i2_3 = 0; i2_3 < 15; ++i2_3) {\n        compute_3[(((i0_3 * 75) + (i1_3 * 15)) + i2_3)] = asinf(ph_0[(((i0_3 * 75) + (i1_3 * 15)) + i2_3)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1088; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 16; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 17; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_add[(((ax0 * 68) + (ax1 * 4)) + ax2)] = (cosf(ph_0[(((ax0 * 68) + (ax1 * 4)) + ax2)]) + ph_0[(((ax0 * 68) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1088; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 17, 4), \"float32\"), compute: T.Buffer((16, 17, 4), \"float32\"), T_add: T.Buffer((16, 17, 4), \"float32\"), compute_1: T.Buffer((16, 17, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1088,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_2 = T.Buffer((1088,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(16):\n            for ax1, ax2 in T.grid(17, 4):\n                cse_var_1: T.int32 = ax0 * 68 + ax1 * 4 + ax2\n                T_add_1 = T.Buffer((1088,), data=T_add.data)\n                T_add_1[cse_var_1] = T.cos(ph_0_1[cse_var_1]) + ph_0_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1088):\n            compute_2 = T.Buffer((1088,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atan",
                "cos",
                "add",
                "exp"
            ]
        ],
        "input_shape": [[16, 17, 4]],
        "output_shape": [[16, 17, 4], [16, 17, 4], [16, 17, 4]],
        "deepseek_c": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 68) + (i1 * 4)) + i2)] = atanf(ph_0[(((i0 * 68) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 272; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n      T_add[((i0_i1_fused * 4) + i2_1)] = (cosf(ph_0[((i0_i1_fused * 4) + i2_1)]) + ph_0[((i0_i1_fused * 4) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1088; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* ph_0) {\n  float auto_scheduler_layout_transform[1280];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1280; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(fmodf(ph_0[i0_i1_fused_i2_fused], ceilf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 5; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 4; ++ax4) {\n      for (int32_t ax5 = 0; ax5 < 2; ++ax5) {\n        for (int32_t ax6 = 0; ax6 < 8; ++ax6) {\n          for (int32_t ax7 = 0; ax7 < 2; ++ax7) {\n            for (int32_t ax8 = 0; ax8 < 2; ++ax8) {\n              auto_scheduler_layout_transform[((((((ax0_ax1_fused_ax2_fused * 256) + (ax4 * 64)) + (ax5 * 32)) + (ax6 * 4)) + (ax7 * 2)) + ax8)] = atanf(ph_0[((((((ax0_ax1_fused_ax2_fused * 256) + (ax5 * 128)) + (ax8 * 64)) + (ax4 * 16)) + (ax7 * 8)) + ax6)]);\n            }\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 10; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t b_outer_inner_init = 0; b_outer_inner_init < 2; ++b_outer_inner_init) {\n      for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 2; ++i_outer_inner_init) {\n        for (int32_t j_outer_inner_init = 0; j_outer_inner_init < 8; ++j_outer_inner_init) {\n          for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n            for (int32_t i_inner_init = 0; i_inner_init < 2; ++i_inner_init) {\n              T_batch_matmul_NN[((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner_init * 128)) + (b_inner_init * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner_init * 16)) + (i_inner_init * 8)) + j_outer_inner_init)] = 0.000000e+00f;\n            }\n          }\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 4; ++k_outer) {\n      for (int32_t b_outer_inner = 0; b_outer_inner < 2; ++b_outer_inner) {\n        for (int32_t i_outer_inner = 0; i_outer_inner < 2; ++i_outer_inner) {\n          for (int32_t j_outer_inner = 0; j_outer_inner < 8; ++j_outer_inner) {\n            for (int32_t k_inner = 0; k_inner < 2; ++k_inner) {\n              for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n                for (int32_t i_inner = 0; i_inner < 2; ++i_inner) {\n                  T_batch_matmul_NN[((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner * 128)) + (b_inner * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner * 16)) + (i_inner * 8)) + j_outer_inner)] = (T_batch_matmul_NN[((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner * 128)) + (b_inner * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner * 16)) + (i_inner * 8)) + j_outer_inner)] + (ph_0[(((((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (b_outer_inner * 128)) + (b_inner * 64)) + ((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused / 5) * 32)) + (i_outer_inner * 16)) + (i_inner * 8)) + (k_outer * 2)) + k_inner)] * auto_scheduler_layout_transform[(((((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5) * 256) + (k_outer * 64)) + (b_outer_inner * 32)) + (j_outer_inner * 4)) + (k_inner * 2)) + b_inner)]));\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(40) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0, float* __restrict__ ph_6) {\n  float T_batch_matmul_NN_local[2];\n  __shared__ float ph_6_shared[50];\n  for (int b_c_inner_init = 0; b_c_inner_init < 2; ++b_c_inner_init) {\n    T_batch_matmul_NN_local[b_c_inner_init] = 0.000000e+00f;\n  }\n  for (int ax0_ax1_fused_ax2_fused_outer_outer = 0; ax0_ax1_fused_ax2_fused_outer_outer < 2; ++ax0_ax1_fused_ax2_fused_outer_outer) {\n    if (((ax0_ax1_fused_ax2_fused_outer_outer * 4) + (((int)threadIdx.x) / 10)) < 5) {\n      ph_6_shared[((ax0_ax1_fused_ax2_fused_outer_outer * 40) + ((int)threadIdx.x))] = ph_6[(((((int)blockIdx.x) * 50) + (ax0_ax1_fused_ax2_fused_outer_outer * 40)) + ((int)threadIdx.x))];\n    }\n  }\n  __syncthreads();\n  for (int k_inner = 0; k_inner < 5; ++k_inner) {\n    for (int b_c_inner = 0; b_c_inner < 2; ++b_c_inner) {\n      T_batch_matmul_NN_local[b_c_inner] = (T_batch_matmul_NN_local[b_c_inner] + (ph_0[((((((int)blockIdx.x) * 80) + (b_c_inner * 40)) + ((((int)threadIdx.x) / 5) * 5)) + k_inner)] * ph_6_shared[(((b_c_inner * 25) + (k_inner * 5)) + (((int)threadIdx.x) % 5))]));\n    }\n  }\n  for (int b_inner = 0; b_inner < 2; ++b_inner) {\n    T_batch_matmul_NN[(((((int)blockIdx.x) * 80) + (b_inner * 40)) + ((int)threadIdx.x))] = T_batch_matmul_NN_local[b_inner];\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 8, 8), \"float32\"), compute: T.Buffer((20, 8, 8), \"float32\"), T_batch_matmul_NN: T.Buffer((20, 8, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([1280], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((1280,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1280):\n            compute_1 = T.Buffer((1280,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.asinh(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], T.ceil(ph_0_1[i0_i1_fused_i2_fused])))\n        auto_scheduler_layout_transform_1 = T.Buffer((1280,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(5):\n            for ax4, ax5, ax6, ax7, ax8 in T.grid(4, 2, 8, 2, 2):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 256\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 64 + ax5 * 32 + ax6 * 4 + ax7 * 2 + ax8] = T.atan(ph_0_1[cse_var_1 + ax5 * 128 + ax8 * 64 + ax4 * 16 + ax7 * 8 + ax6])\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(10):\n            T_batch_matmul_NN_1 = T.Buffer((1280,), data=T_batch_matmul_NN.data)\n            for b_outer_inner_init, i_outer_inner_init, j_outer_inner_init, b_inner_init, i_inner_init in T.grid(2, 2, 8, 2, 2):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5 * 256 + b_outer_inner_init * 128 + b_inner_init * 64 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 5 * 32 + i_outer_inner_init * 16 + i_inner_init * 8 + j_outer_inner_init] = T.float32(0)\n            for k_outer, b_outer_inner, i_outer_inner, j_outer_inner, k_inner, b_inner, i_inner in T.grid(4, 2, 2, 8, 2, 2, 2):\n                cse_var_3: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused % 5 * 256\n                cse_var_4: T.int32 = cse_var_3 + b_outer_inner * 128 + b_inner * 64 + b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused // 5 * 32 + i_outer_inner * 16 + i_inner * 8\n                cse_var_2: T.int32 = cse_var_4 + j_outer_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + ph_0_1[cse_var_4 + k_outer * 2 + k_inner] * auto_scheduler_layout_transform_1[cse_var_3 + k_outer * 64 + b_outer_inner * 32 + j_outer_inner * 4 + k_inner * 2 + b_inner]",
        "op_args": [
            [
                "ceil",
                "mod",
                "asinh",
                "atan",
                "batch_matmul"
            ]
        ],
        "input_shape": [[20, 8, 8]],
        "output_shape": [[20, 8, 8], [20, 8, 8]],
        "deepseek_c": "void default_function_kernel(float* T_batch_matmul_NN, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer = 0; b_outer_outer_outer < 20; ++b_outer_outer_outer) {\n    for (int32_t i_outer_outer_outer = 0; i_outer_outer_outer < 8; ++i_outer_outer_outer) {\n      for (int32_t k_outer_outer_outer = 0; k_outer_outer_outer < 8; ++k_outer_outer_outer) {\n        compute[((((b_outer_outer_outer * 64) + (i_outer_outer_outer * 8)) + k_outer_outer_outer) + 0)] = asinhf(fmodf(ph_0[((((b_outer_outer_outer * 64) + (i_outer_outer_outer * 8)) + k_outer_outer_outer) + 0)], ceilf(ph_0[((((b_outer_outer_outer * 64) + (i_outer_outer_outer * 8)) + k_outer_outer_outer) + 0)])));\n        compute[((((b_outer_outer_outer * 64) + (i_outer_outer_outer * 8)) + k_outer_outer_outer) + 8)] = atanf(ph_0[((((b_outer_outer_outer * 64) + (i_outer_outer_outer * 8)) + k_outer_outer_outer) + 8)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer = 0; b_outer_outer < 20; ++b_outer_outer) {\n    for (int32_t b_inner_init = 0; b_inner_init < 2; ++b_inner_init) {\n      T_batch_matmul_NN[((b_outer_outer * 4) + b_inner_init)] = 0.000000e+00f;\n    }\n    for (int32_t k_outer_inner = 0; k_outer_inner < 8; ++k_outer_inner) {\n      for (int32_t b_inner = 0; b_inner < 2; ++b_inner) {\n        T_batch_matmul_NN[((b_outer_outer * 4) + b_inner)] = (T_batch_matmul_NN[((b_outer_outer * 4) + b_inner)] + (ph_0[((((b_outer_outer * 64) + (b_inner * 32)) + (k_outer_inner * 4)) + 0)] * ph_0[((((b_outer_outer * 64) + (b_inner * 32)) + (k_outer_inner * 4)) + 8)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_1 = 0; b_outer_outer_1 < 20; ++b_outer_outer_1) {\n    for (int32_t i_outer_outer_1 = 0; i_outer_outer_1 < 8; ++i_outer_outer_1) {\n      for (int32_t k_outer_inner_1 = 0; k_outer_inner_1 < 8; ++k_outer_inner_1) {\n        compute_1[((((b_outer_outer_1 * 64) + (i_outer_outer_1 * 8)) + k_outer_inner_1) + 0)] = atanf(ph_0[((((b_outer_outer_1 * 64) + (i_outer_outer_1 * 8)) + k_outer_inner_1) + 0)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 117; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_divide[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] / ph_3[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 117; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = atanf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1170; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (cosf(ph_0[ax0_ax1_fused_ax2_fused]) - ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  T_subtract[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))] = (__cosf(ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]) - ph_0[((((int)blockIdx.x) * 18) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 13, 10), \"float32\"), ph_3: T.Buffer((9, 13, 10), \"float32\"), T_divide: T.Buffer((9, 13, 10), \"float32\"), compute: T.Buffer((9, 13, 10), \"float32\"), T_subtract: T.Buffer((9, 13, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1170,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(117):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_divide_1 = T.Buffer((1170,), data=T_divide.data)\n                ph_3_1 = T.Buffer((1170,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused in T.parallel(117):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_1 = T.Buffer((1170,), data=compute.data)\n                compute_1[cse_var_2] = T.atan(ph_0_1[cse_var_2])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1170):\n            T_subtract_1 = T.Buffer((1170,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "divide",
                "atan",
                "cos",
                "subtract"
            ]
        ],
        "input_shape": [[9, 13, 10], [17, 7, 20], [9, 13, 10]],
        "output_shape": [[9, 13, 10], [17, 7, 20], [9, 13, 10], [9, 13, 10]],
        "deepseek_c": "void default_function_kernel(float* T_divide, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 13; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        T_divide[(((i0 * 130) + (i1 * 10)) + i2)] = (ph_0[(((i0 * 130) + (i1 * 10)) + i2)] / ph_3[(((i0 * 130) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 117; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 10; ++i2) {\n      compute[((i0_i1_fused * 10) + i2)] = atanf(ph_0[((i0_i1_fused * 10) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1170; ++i0_i1_fused_i2_fused) {\n    T_subtract[i0_i1_fused_i2_fused] = (cosf(ph_0[i0_i1_fused_i2_fused]) - ph_0[i0_i1_fused_i2_fused]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_batch_matmul_NN, float* T_subtract, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3, float* ph_8) {\n  float auto_scheduler_layout_transform[50];\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 750; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 750; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2; ++ax0_ax1_fused_ax2_fused) {\n    for (int32_t ax4 = 0; ax4 < 5; ++ax4) {\n      for (int32_t ax8 = 0; ax8 < 5; ++ax8) {\n        auto_scheduler_layout_transform[(((ax0_ax1_fused_ax2_fused * 25) + (ax4 * 5)) + ax8)] = ph_8[(((ax0_ax1_fused_ax2_fused * 25) + (ax8 * 5)) + ax4)];\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused = 0; b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused < 2; ++b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused) {\n    for (int32_t i_outer_inner_init = 0; i_outer_inner_init < 3; ++i_outer_inner_init) {\n      for (int32_t b_inner_init = 0; b_inner_init < 5; ++b_inner_init) {\n        for (int32_t i_inner_init = 0; i_inner_init < 5; ++i_inner_init) {\n          T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75) + (b_inner_init * 15)) + (i_outer_inner_init * 5)) + i_inner_init)] = 0.000000e+00f;\n        }\n      }\n    }\n    for (int32_t k_outer = 0; k_outer < 5; ++k_outer) {\n      for (int32_t i_outer_inner = 0; i_outer_inner < 3; ++i_outer_inner) {\n        for (int32_t b_inner = 0; b_inner < 5; ++b_inner) {\n          for (int32_t i_inner = 0; i_inner < 5; ++i_inner) {\n            T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75) + (b_inner * 15)) + (i_outer_inner * 5)) + i_inner)] = (T_batch_matmul_NN[((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75) + (b_inner * 15)) + (i_outer_inner * 5)) + i_inner)] + (acoshf(ph_0[(((((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 375) + (b_inner * 75)) + (i_outer_inner * 25)) + (i_inner * 5)) + k_outer)]) * auto_scheduler_layout_transform[(((b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 25) + (k_outer * 5)) + b_inner)]));\n          }\n        }\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 750; ++ax0_ax1_fused_ax2_fused_1) {\n    T_subtract[ax0_ax1_fused_ax2_fused_1] = (fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ph_3[ax0_ax1_fused_ax2_fused_1]) - ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 750; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = acosf(fmodf(ph_0[i0_i1_fused_i2_fused_2], ph_3[i0_i1_fused_i2_fused_2]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_4(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((int)blockIdx.x)] = (fmodf(ph_0[((int)blockIdx.x)], ph_3[((int)blockIdx.x)]) - ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(18) default_function_kernel_2(float* __restrict__ T_batch_matmul_NN, float* __restrict__ ph_0) {\n  float T_batch_matmul_NN_local[32];\n  for (int i_c_outer_inner_init = 0; i_c_outer_inner_init < 4; ++i_c_outer_inner_init) {\n    for (int j_c_outer_inner_init = 0; j_c_outer_inner_init < 4; ++j_c_outer_inner_init) {\n      for (int j_c_inner_init = 0; j_c_inner_init < 2; ++j_c_inner_init) {\n        T_batch_matmul_NN_local[(((i_c_outer_inner_init * 8) + (j_c_outer_inner_init * 2)) + j_c_inner_init)] = 0.000000e+00f;\n      }\n    }\n  }\n  for (int k_outer_outer = 0; k_outer_outer < 2; ++k_outer_outer) {\n    for (int i_c_outer_inner = 0; i_c_outer_inner < 4; ++i_c_outer_inner) {\n      for (int j_c_outer_inner = 0; j_c_outer_inner < 4; ++j_c_outer_inner) {\n        for (int k_inner = 0; k_inner < 4; ++k_inner) {\n          for (int j_c_inner = 0; j_c_inner < 2; ++j_c_inner) {\n            T_batch_matmul_NN_local[(((i_c_outer_inner * 8) + (j_c_outer_inner * 2)) + j_c_inner)] = (T_batch_matmul_NN_local[(((i_c_outer_inner * 8) + (j_c_outer_inner * 2)) + j_c_inner)] + (acoshf(ph_0[((((((int)threadIdx.x) * 32) + (i_c_outer_inner * 8)) + (k_outer_outer * 4)) + k_inner)]) * ph_0[((((((((int)threadIdx.x) >> 1) * 64) + (k_outer_outer * 32)) + (k_inner * 8)) + (j_c_outer_inner * 2)) + j_c_inner)]));\n          }\n        }\n      }\n    }\n  }\n  for (int i_inner = 0; i_inner < 4; ++i_inner) {\n    for (int j_inner = 0; j_inner < 8; ++j_inner) {\n      T_batch_matmul_NN[(((((int)threadIdx.x) * 32) + (i_inner * 8)) + j_inner)] = T_batch_matmul_NN_local[((i_inner * 8) + j_inner)];\n    }\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 15, 5), \"float32\"), ph_3: T.Buffer((10, 15, 5), \"float32\"), ph_8: T.Buffer((10, 5, 1), \"float32\"), compute: T.Buffer((10, 15, 5), \"float32\"), compute_1: T.Buffer((10, 15, 5), \"float32\"), T_batch_matmul_NN: T.Buffer((10, 15, 1), \"float32\"), T_subtract: T.Buffer((10, 15, 5), \"float32\"), compute_2: T.Buffer((10, 15, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        auto_scheduler_layout_transform = T.allocate([50], \"float32\", \"global\")\n        ph_0_1 = T.Buffer((750,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_3 = T.Buffer((750,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_3 = T.Buffer((750,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        auto_scheduler_layout_transform_1 = T.Buffer((50,), data=auto_scheduler_layout_transform)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2):\n            for ax4, ax8 in T.grid(5, 5):\n                cse_var_1: T.int32 = ax0_ax1_fused_ax2_fused * 25\n                ph_8_1 = T.Buffer((50,), data=ph_8.data)\n                auto_scheduler_layout_transform_1[cse_var_1 + ax4 * 5 + ax8] = ph_8_1[cse_var_1 + ax8 * 5 + ax4]\n        for b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused in T.parallel(2):\n            T_batch_matmul_NN_1 = T.Buffer((150,), data=T_batch_matmul_NN.data)\n            for i_outer_inner_init, b_inner_init, i_inner_init in T.grid(3, 5, 5):\n                T_batch_matmul_NN_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75 + b_inner_init * 15 + i_outer_inner_init * 5 + i_inner_init] = T.float32(0)\n            for k_outer, i_outer_inner, b_inner, i_inner in T.grid(5, 3, 5, 5):\n                cse_var_2: T.int32 = b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 75 + b_inner * 15 + i_outer_inner * 5 + i_inner\n                T_batch_matmul_NN_1[cse_var_2] = T_batch_matmul_NN_1[cse_var_2] + T.acosh(ph_0_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 375 + b_inner * 75 + i_outer_inner * 25 + i_inner * 5 + k_outer]) * auto_scheduler_layout_transform_1[b_outer_outer_outer_i_outer_outer_outer_fused_j_outer_outer_outer_fused_b_outer_outer_inner_fused_i_outer_outer_inner_fused_j_outer_outer_inner_fused * 25 + k_outer * 5 + b_inner]\n        ph_3_1 = T.Buffer((750,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(750):\n            T_subtract_1 = T.Buffer((750,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], ph_3_1[ax0_ax1_fused_ax2_fused]) - ph_0_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused_i2_fused in T.parallel(750):\n            compute_3 = T.Buffer((750,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.truncmod(ph_0_1[i0_i1_fused_i2_fused], ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "acosh",
                "acosh",
                "ceil",
                "batch_matmul",
                "subtract",
                "acos"
            ]
        ],
        "input_shape": [[10, 15, 5], [10, 20, 3], [10, 15, 5], [10, 5, 1]],
        "output_shape": [[10, 20, 3], [10, 15, 5], [10, 15, 5], [10, 15, 1], [10, 15, 5], [10, 15, 5]],
        "deepseek_c": "void default_function_kernel_3(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((int)blockIdx.x)] = (fmodf(ph_0[((int)blockIdx.x)], ph_3[((int)blockIdx.x)]) - ph_0[((int)blockIdx.x)]);\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 88; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = expf((ph_0[((i0_i1_fused * 3) + i2)] * asinhf(ph_0[((i0_i1_fused * 3) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = __expf((ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] * asinhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 8, 3), \"float32\"), compute: T.Buffer((11, 8, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused in T.parallel(88):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_1 = T.Buffer((264,), data=compute.data)\n                ph_0_1 = T.Buffer((264,), data=ph_0.data)\n                compute_1[cse_var_1] = T.exp(ph_0_1[cse_var_1] * T.asinh(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "asinh",
                "multiply",
                "exp"
            ]
        ],
        "input_shape": [[11, 8, 3]],
        "output_shape": [[11, 8, 3]],
        "deepseek_c": "void default_function_kernel(float* compute, float* ph_0) {\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 24) + (i1 * 3)) + i2)] = expf((ph_0[(((i0 * 24) + (i1 * 3)) + i2)] * asinhf(ph_0[(((i0 * 24) + (i1 * 3)) + i2)])));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 27; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 3; ++i2) {\n      compute[((i0_i1_fused * 3) + i2)] = acoshf(ph_0[((i0_i1_fused * 3) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 3; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 9; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 3; ++ax2) {\n        T_add[(((ax0 * 27) + (ax1 * 3)) + ax2)] = (ceilf(ph_0[(((ax0 * 27) + (ax1 * 3)) + ax2)]) + ph_0[(((ax0 * 27) + (ax1 * 3)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 81; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((3, 9, 3), \"float32\"), compute: T.Buffer((3, 9, 3), \"float32\"), T_add: T.Buffer((3, 9, 3), \"float32\"), compute_1: T.Buffer((3, 9, 3), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((81,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(27):\n            for i2 in range(3):\n                cse_var_1: T.int32 = i0_i1_fused * 3 + i2\n                compute_2 = T.Buffer((81,), data=compute.data)\n                compute_2[cse_var_1] = T.acosh(ph_0_1[cse_var_1])\n        for ax0 in T.parallel(3):\n            for ax1, ax2 in T.grid(9, 3):\n                cse_var_2: T.int32 = ax0 * 27 + ax1 * 3 + ax2\n                T_add_1 = T.Buffer((81,), data=T_add.data)\n                T_add_1[cse_var_2] = T.ceil(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(81):\n            compute_2 = T.Buffer((81,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "acosh",
                "ceil",
                "add",
                "cos"
            ]
        ],
        "input_shape": [[3, 9, 3]],
        "output_shape": [[3, 9, 3], [3, 9, 3], [3, 9, 3]],
        "deepseek_c": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 3; ++i0) {\n    for (int32_t i1 = 0; i1 < 9; ++i1) {\n      for (int32_t i2 = 0; i2 < 3; ++i2) {\n        compute[(((i0 * 27) + (i1 * 3)) + i2)] = acoshf(ph_0[(((i0 * 27) + (i1 * 3)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 3; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 9; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 3; ++i2_1) {\n        T_add[(((i0_1 * 27) + (i1_1 * 3)) + i2_1)] = (ceilf(ph_0[(((i0_1 * 27) + (i1_1 * 3)) + i2_1)]) + ph_0[(((i0_1 * 27) + (i1_1 * 3)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 3; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 9; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 3; ++i2_2) {\n        compute_1[(((i0_2 * 27) + (i1_2 * 3)) + i2_2)] = cosf(ph_0[(((i0_2 * 27) + (i1_2 * 3)) + i2_2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 4845; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 4845; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(cosf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 4845; ++i0_i1_fused_i2_fused_2) {\n    compute_2[i0_i1_fused_i2_fused_2] = sinf(ph_0[i0_i1_fused_i2_fused_2]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 15; ++i2) {\n        compute_3[(((i0 * 255) + (i1 * 15)) + i2)] = atanf(ph_0[(((i0 * 255) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 17, 15), \"float32\"), compute: T.Buffer((19, 17, 15), \"float32\"), compute_1: T.Buffer((19, 17, 15), \"float32\"), compute_2: T.Buffer((19, 17, 15), \"float32\"), compute_3: T.Buffer((19, 17, 15), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((4845,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(4845):\n            compute_4 = T.Buffer((4845,), data=compute.data)\n            compute_4[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(4845):\n            compute_4 = T.Buffer((4845,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(4845):\n            compute_4 = T.Buffer((4845,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0 in T.parallel(19):\n            for i1, i2 in T.grid(17, 15):\n                cse_var_1: T.int32 = i0 * 255 + i1 * 15 + i2\n                compute_4 = T.Buffer((4845,), data=compute_3.data)\n                compute_4[cse_var_1] = T.atan(ph_0_1[cse_var_1])",
        "op_args": [
            [
                "acos",
                "cos",
                "asinh",
                "sin",
                "atan"
            ]
        ],
        "input_shape": [[19, 17, 15]],
        "output_shape": [[19, 17, 15], [19, 17, 15], [19, 17, 15], [19, 17, 15]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  for (int i0 = 0; i0 < 19; ++i0) {\n    for (int i1 = 0; i1 < 17; ++i1) {\n      for (int i2 = 0; i2 < 15; ++i2) {\n        compute[(((i0 * 255) + (i1 * 15)) + i2)] = acosf(ph_0[(((i0 * 255) + (i1 * 15)) + i2)]);\n      }\n    }\n  }\n  for (int i0_1 = 0; i0_1 < 19; ++i0_1) {\n    for (int i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int i2_1 = 0; i2_1 < 15; ++i2_1) {\n        compute_1[(((i0_1 * 255) + (i1_1 * 15)) + i2_1)] = asinhf(cosf(ph_0[(((i0_1 * 255) + (i1_1 * 15)) + i2_1)]));\n      }\n    }\n  }\n  for (int i0_2 = 0; i0_2 < 19; ++i0_2) {\n    for (int i1_2 = 0; i1_2 < 17; ++i1_2) {\n      for (int i2_2 = 0; i2_2 < 15; ++i2_2) {\n        compute_2[(((i0_2 * 255) + (i1_2 * 15)) + i2_2)] = sinf(ph_0[(((i0_2 * 255) + (i1_2 * 15)) + i2_2)]);\n      }\n    }\n  }\n  for (int i0_3 = 0; i0_3 < 19; ++i0_3) {\n    for (int i1_3 = 0; i1_3 < 17; ++i1_3) {\n      for (int i2_3 = 0; i2_3 < 15; ++i2_3) {\n        compute_3[(((i0_3 * 255) + (i1_3 * 15)) + i2_3)] = atanf(ph_0[(((i0_3 * 255) + (i1_3 * 15)) + i2_3)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2142; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 238; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute_1[((i0_i1_fused * 9) + i2)] = ceilf(acosf(ph_0[((i0_i1_fused * 9) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_2[(((i0 * 153) + (i1 * 9)) + i2_1)] = sinf(ph_0[(((i0 * 153) + (i1 * 9)) + i2_1)]);\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = ceilf(acosf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 17, 9), \"float32\"), compute: T.Buffer((14, 17, 9), \"float32\"), compute_1: T.Buffer((14, 17, 9), \"float32\"), compute_2: T.Buffer((14, 17, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2142,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(2142):\n            compute_3 = T.Buffer((2142,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(238):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_3 = T.Buffer((2142,), data=compute_1.data)\n                compute_3[cse_var_1] = T.ceil(T.acos(ph_0_1[cse_var_1]))\n        for i0 in T.parallel(14):\n            for i1, i2 in T.grid(17, 9):\n                cse_var_2: T.int32 = i0 * 153 + i1 * 9 + i2\n                compute_3 = T.Buffer((2142,), data=compute_2.data)\n                compute_3[cse_var_2] = T.sin(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "acos",
                "ceil",
                "sin"
            ]
        ],
        "input_shape": [[14, 17, 9]],
        "output_shape": [[14, 17, 9], [14, 17, 9], [14, 17, 9]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 14; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 153) + (i1 * 9)) + i2)] = asinhf(ph_0[(((i0 * 153) + (i1 * 9)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 14; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 17; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_1[(((i0_1 * 153) + (i1_1 * 9)) + i2_1)] = ceilf(acosf(ph_0[(((i0_1 * 153) + (i1_1 * 9)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 14; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 17; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 9; ++i2_2) {\n        compute_2[(((i0_2 * 153) + (i1_2 * 9)) + i2_2)] = sinf(ph_0[(((i0_2 * 153) + (i1_2 * 9)) + i2_2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2565; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 285; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 9; ++i2) {\n      compute[((i0_i1_fused * 9) + i2)] = asinhf((ph_0[((i0_i1_fused * 9) + i2)] * (ph_0[((i0_i1_fused * 9) + i2)] - ph_3[((i0_i1_fused * 9) + i2)])));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2565; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] - ph_3[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((19, 15, 9), \"float32\"), ph_3: T.Buffer((19, 15, 9), \"float32\"), T_divide: T.Buffer((19, 15, 9), \"float32\"), compute: T.Buffer((19, 15, 9), \"float32\"), compute_1: T.Buffer((19, 15, 9), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2565,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2565,), data=ph_3.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(2565):\n            T_divide_1 = T.Buffer((2565,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] / ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0_i1_fused in T.parallel(285):\n            for i2 in range(9):\n                cse_var_1: T.int32 = i0_i1_fused * 9 + i2\n                compute_2 = T.Buffer((2565,), data=compute.data)\n                compute_2[cse_var_1] = T.asinh(ph_0_1[cse_var_1] * (ph_0_1[cse_var_1] - ph_3_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(2565):\n            compute_2 = T.Buffer((2565,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] - ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "subtract",
                "multiply",
                "asinh",
                "atanh"
            ]
        ],
        "input_shape": [[19, 15, 9], [19, 12, 17], [19, 15, 9]],
        "output_shape": [[19, 15, 9], [19, 12, 17], [19, 15, 9], [19, 15, 9]],
        "deepseek_c": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2565; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] / ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 19; ++i0) {\n    for (int32_t i1 = 0; i1 < 15; ++i1) {\n      for (int32_t i2 = 0; i2 < 9; ++i2) {\n        compute[(((i0 * 135) + (i1 * 9)) + i2)] = asinhf((ph_0[(((i0 * 135) + (i1 * 9)) + i2)] * (ph_0[(((i0 * 135) + (i1 * 9)) + i2)] - ph_3[(((i0 * 135) + (i1 * 9)) + i2)])));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 19; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 15; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 9; ++i2_1) {\n        compute_1[(((i0_1 * 135) + (i1_1 * 9)) + i2_1)] = atanhf((ph_0[(((i0_1 * 135) + (i1_1 * 9)) + i2_1)] * (ph_0[(((i0_1 * 135) + (i1_1 * 9)) + i2_1)] - ph_3[(((i0_1 * 135) + (i1_1 * 9)) + i2_1)])));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 180) + (i1 * 10)) + i2)] = asinhf(ph_0[(((i0 * 180) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2880; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = cosf(atanf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 2880; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n      compute_3[((i0_i1_fused * 10) + i2_1)] = acoshf(ph_0[((i0_i1_fused * 10) + i2_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 18, 10), \"float32\"), compute: T.Buffer((16, 18, 10), \"float32\"), compute_1: T.Buffer((16, 18, 10), \"float32\"), compute_2: T.Buffer((16, 18, 10), \"float32\"), compute_3: T.Buffer((16, 18, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2880,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(18, 10):\n                cse_var_1: T.int32 = i0 * 180 + i1 * 10 + i2\n                compute_4 = T.Buffer((2880,), data=compute.data)\n                compute_4[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_4 = T.Buffer((2880,), data=compute_1.data)\n            compute_4[i0_i1_fused_i2_fused] = T.cos(T.atan(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(2880):\n            compute_4 = T.Buffer((2880,), data=compute_2.data)\n            compute_4[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused in T.parallel(288):\n            for i2 in range(10):\n                cse_var_2: T.int32 = i0_i1_fused * 10 + i2\n                compute_4 = T.Buffer((2880,), data=compute_3.data)\n                compute_4[cse_var_2] = T.acosh(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "atan",
                "cos",
                "asinh",
                "acosh"
            ]
        ],
        "input_shape": [[16, 18, 10]],
        "output_shape": [[16, 18, 10], [16, 18, 10], [16, 18, 10], [16, 18, 10]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* compute_3, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 10; ++i2) {\n        compute[(((i0 * 180) + (i1 * 10)) + i2)] = asinhf(ph_0[(((i0 * 180) + (i1 * 10)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 16; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 18; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 10; ++i2_1) {\n        compute_1[(((i0_1 * 180) + (i1_1 * 10)) + i2_1)] = cosf(atanf(ph_0[(((i0_1 * 180) + (i1_1 * 10)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 16; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 18; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 10; ++i2_2) {\n        compute_2[(((i0_2 * 180) + (i1_2 * 10)) + i2_2)] = asinhf(ph_0[(((i0_2 * 180) + (i1_2 * 10)) + i2_2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 288; ++i0_i1_fused) {\n    for (int32_t i2_fused = 0; i2_fused < 10; ++i2_fused) {\n      compute_3[((i0_i1_fused * 10) + i2_fused)] = acoshf(ph_0[((i0_i1_fused * 10) + i2_fused)]);\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 176; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] / cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] / __cosf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((11, 2, 8), \"float32\"), compute: T.Buffer((11, 2, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(176):\n            compute_1 = T.Buffer((176,), data=compute.data)\n            ph_0_1 = T.Buffer((176,), data=ph_0.data)\n            compute_1[i0_i1_fused_i2_fused] = T.atanh(ph_0_1[i0_i1_fused_i2_fused] / T.cos(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "cos",
                "divide",
                "atanh"
            ]
        ],
        "input_shape": [[11, 2, 8]],
        "output_shape": [[11, 2, 8]],
        "deepseek_c": "void default_function_kernel(float* compute, float* ph_0) {\n  for (int32_t i0 = 0; i0 < 11; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 16) + (i1 * 8)) + i2)] = atanhf((ph_0[(((i0 * 16) + (i1 * 8)) + i2)] / cosf(ph_0[(((i0 * 16) + (i1 * 8)) + i2)])));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 4; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 19; ++i2) {\n      compute[((i0_i1_fused * 19) + i2)] = sinf(ph_0[((i0_i1_fused * 19) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n      compute_1[((i0 * 19) + i2_1)] = atanf(asinhf(ph_0[((i0 * 19) + i2_1)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 76; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = asinhf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanf(asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 1, 19), \"float32\"), compute: T.Buffer((4, 1, 19), \"float32\"), compute_1: T.Buffer((4, 1, 19), \"float32\"), compute_2: T.Buffer((4, 1, 19), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((76,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(4):\n            for i2 in range(19):\n                cse_var_1: T.int32 = i0_i1_fused * 19 + i2\n                compute_3 = T.Buffer((76,), data=compute.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for i0 in T.parallel(4):\n            for i2 in range(19):\n                cse_var_2: T.int32 = i0 * 19 + i2\n                compute_3 = T.Buffer((76,), data=compute_1.data)\n                compute_3[cse_var_2] = T.atan(T.asinh(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(76):\n            compute_3 = T.Buffer((76,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "sin",
                "asinh",
                "atan",
                "asinh"
            ]
        ],
        "input_shape": [[4, 1, 19]],
        "output_shape": [[4, 1, 19], [4, 1, 19], [4, 1, 19]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 1; ++i1) {\n      for (int32_t i2 = 0; i2 < 19; ++i2) {\n        compute[(((i0 * 19) + i1) + i2)] = sinf(ph_0[(((i0 * 19) + i1) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 4; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 1; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 19; ++i2_1) {\n        compute_1[(((i0_1 * 19) + i1_1) + i2_1)] = atanf(asinhf(ph_0[(((i0_1 * 19) + i1_1) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 4; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 1; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 19; ++i2_2) {\n        compute_2[(((i0_2 * 19) + i1_2) + i2_2)] = asinhf(ph_0[(((i0_2 * 19) + i1_2) + i2_2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 20; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 14; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n        T_subtract[(((ax0 * 56) + (ax1 * 4)) + ax2)] = (ph_0[(((ax0 * 56) + (ax1 * 4)) + ax2)] - ph_3[(((ax0 * 56) + (ax1 * 4)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1120; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1120; ++ax0_ax1_fused_ax2_fused) {\n    T_mod[ax0_ax1_fused_ax2_fused] = fmodf(ceilf(ph_0[ax0_ax1_fused_ax2_fused]), ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute_1[(((i0 * 56) + (i1 * 4)) + i2)] = ceilf(ceilf(ph_0[(((i0 * 56) + (i1 * 4)) + i2)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_subtract[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((20, 14, 4), \"float32\"), ph_3: T.Buffer((20, 14, 4), \"float32\"), T_subtract: T.Buffer((20, 14, 4), \"float32\"), compute: T.Buffer((20, 14, 4), \"float32\"), T_mod: T.Buffer((20, 14, 4), \"float32\"), compute_1: T.Buffer((20, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1120,), data=ph_0.data)\n        for ax0 in T.parallel(20):\n            for ax1, ax2 in T.grid(14, 4):\n                cse_var_1: T.int32 = ax0 * 56 + ax1 * 4 + ax2\n                T_subtract_1 = T.Buffer((1120,), data=T_subtract.data)\n                ph_3_1 = T.Buffer((1120,), data=ph_3.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(1120):\n            compute_2 = T.Buffer((1120,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0_ax1_fused_ax2_fused in T.parallel(1120):\n            T_mod_1 = T.Buffer((1120,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]), ph_0_1[ax0_ax1_fused_ax2_fused])\n        for i0 in T.parallel(20):\n            for i1, i2 in T.grid(14, 4):\n                cse_var_2: T.int32 = i0 * 56 + i1 * 4 + i2\n                compute_2 = T.Buffer((1120,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(T.ceil(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "subtract",
                "acos",
                "ceil",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[20, 14, 4], [12, 10, 16], [20, 14, 4]],
        "output_shape": [[20, 14, 4], [12, 10, 16], [20, 14, 4], [20, 14, 4], [20, 14, 4]],
        "deepseek_c": "void default_function_kernel(float* T_subtract, float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1120; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 20; ++i0) {\n    for (int32_t i1 = 0; i1 < 14; ++i1) {\n      for (int32_t i2 = 0; i2 < 4; ++i2) {\n        compute[(((i0 * 56) + (i1 * 4)) + i2)] = acosf(ph_0[(((i0 * 56) + (i1 * 4)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 20; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 14; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 4; ++i2_1) {\n        T_mod[(((i0_1 * 56) + (i1_1 * 4)) + i2_1)] = fmodf(ceilf(ph_0[(((i0_1 * 56) + (i1_1 * 4)) + i2_1)]), ph_0[(((i0_1 * 56) + (i1_1 * 4)) + i2_1)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 20; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 14; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 4; ++i2_2) {\n        compute_1[(((i0_2 * 56) + (i1_2 * 4)) + i2_2)] = ceilf(ceilf(ph_0[(((i0_2 * 56) + (i1_2 * 4)) + i2_2)]));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_subtract, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 304; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 8; ++ax2) {\n      T_subtract[((ax0_ax1_fused * 8) + ax2)] = (ph_0[((ax0_ax1_fused * 8) + ax2)] - ph_3[((ax0_ax1_fused * 8) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_1 = 0; ax0_ax1_fused_1 < 304; ++ax0_ax1_fused_1) {\n    for (int32_t ax2_1 = 0; ax2_1 < 8; ++ax2_1) {\n      T_mod[((ax0_ax1_fused_1 * 8) + ax2_1)] = fmodf((ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)] * (ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)] * ph_3[((ax0_ax1_fused_1 * 8) + ax2_1)])), ph_0[((ax0_ax1_fused_1 * 8) + ax2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 2432; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf((ph_0[i0_i1_fused_i2_fused] * (ph_0[i0_i1_fused_i2_fused] * ph_3[i0_i1_fused_i2_fused])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_subtract, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] - ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_mod[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = fmodf((ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * (ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))])), ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 19, 8), \"float32\"), ph_3: T.Buffer((16, 19, 8), \"float32\"), T_subtract: T.Buffer((16, 19, 8), \"float32\"), T_mod: T.Buffer((16, 19, 8), \"float32\"), compute: T.Buffer((16, 19, 8), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((2432,), data=ph_0.data)\n        ph_3_1 = T.Buffer((2432,), data=ph_3.data)\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(8):\n                cse_var_1: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_subtract_1 = T.Buffer((2432,), data=T_subtract.data)\n                T_subtract_1[cse_var_1] = ph_0_1[cse_var_1] - ph_3_1[cse_var_1]\n        for ax0_ax1_fused in T.parallel(304):\n            for ax2 in range(8):\n                cse_var_2: T.int32 = ax0_ax1_fused * 8 + ax2\n                T_mod_1 = T.Buffer((2432,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(ph_0_1[cse_var_2] * (ph_0_1[cse_var_2] * ph_3_1[cse_var_2]), ph_0_1[cse_var_2])\n        for i0_i1_fused_i2_fused in T.parallel(2432):\n            compute_1 = T.Buffer((2432,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused] * (ph_0_1[i0_i1_fused_i2_fused] * ph_3_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "subtract",
                "multiply",
                "multiply",
                "mod",
                "ceil"
            ]
        ],
        "input_shape": [[16, 19, 8], [18, 11, 11], [16, 19, 8]],
        "output_shape": [[16, 19, 8], [18, 11, 11], [16, 19, 8], [16, 19, 8]],
        "deepseek_c": "void default_function_kernel(float* T_subtract, float* T_mod, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 2432; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 2432; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf((ph_0[ax0_ax1_fused_ax2_fused_1] * (ph_0[ax0_ax1_fused_ax2_fused_1] * ph_3[ax0_ax1_fused_ax2_fused_1])), ph_0[ax0_ax1_fused_ax2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 19; ++i1) {\n      for (int32_t i2 = 0; i2 < 8; ++i2) {\n        compute[(((i0 * 152) + (i1 * 8)) + i2)] = ceilf((ph_0[(((i0 * 152) + (i1 * 8)) + i2)] * (ph_0[(((i0 * 152) + (i1 * 8)) + i2)] * ph_3[(((i0 * 152) + (i1 * 8)) + i2)])));\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 77; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = cosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 77; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = ceilf(acoshf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 7; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 11; ++i2) {\n      compute_2[((i0_i1_fused * 11) + i2)] = sinf(ph_0[((i0_i1_fused * 11) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 77; ++ax0_ax1_fused_ax2_fused) {\n    T_divide[ax0_ax1_fused_ax2_fused] = (fabsf(ph_0[ax0_ax1_fused_ax2_fused]) / ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ T_divide, float* __restrict__ ph_0) {\n  T_divide[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) / ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = ceilf(acoshf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 7, 11), \"float32\"), compute: T.Buffer((1, 7, 11), \"float32\"), compute_1: T.Buffer((1, 7, 11), \"float32\"), compute_2: T.Buffer((1, 7, 11), \"float32\"), T_divide: T.Buffer((1, 7, 11), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((77,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(77):\n            compute_3 = T.Buffer((77,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(77):\n            compute_3 = T.Buffer((77,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(T.acosh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(7):\n            for i2 in range(11):\n                cse_var_1: T.int32 = i0_i1_fused * 11 + i2\n                compute_3 = T.Buffer((77,), data=compute_2.data)\n                compute_3[cse_var_1] = T.sin(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(77):\n            T_divide_1 = T.Buffer((77,), data=T_divide.data)\n            T_divide_1[ax0_ax1_fused_ax2_fused] = T.fabs(ph_0_1[ax0_ax1_fused_ax2_fused]) / ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "cos",
                "acosh",
                "ceil",
                "sin",
                "abs",
                "divide"
            ]
        ],
        "input_shape": [[1, 7, 11]],
        "output_shape": [[1, 7, 11], [1, 7, 11], [1, 7, 11], [1, 7, 11]],
        "deepseek_c": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 7; ++i0) {\n    for (int32_t i1 = 0; i1 < 11; ++i1) {\n      compute[((i0 * 11) + i1)] = cosf(ph_0[((i0 * 11) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 7; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 11; ++i1_1) {\n      compute_1[((i0_1 * 11) + i1_1)] = sinf(ph_0[((i0_1 * 11) + i1_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 7; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 11; ++i1_2) {\n      compute_2[((i0_2 * 11) + i1_2)] = ceilf(acoshf(ph_0[((i0_2 * 11) + i1_2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 77; ++i0_i1_fused_i2_fused) {\n    T_divide[i0_i1_fused_i2_fused] = (fabsf(ph_0[i0_i1_fused_i2_fused]) / ph_0[i0_i1_fused_i2_fused]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 20; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] * ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = atanhf(ph_0[((i0 * 2) + i1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 10; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n      compute_1[((i0_1 * 2) + i1_1)] = ceilf(ph_0[((i0_1 * 2) + i1_1)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(9) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 9) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] * ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 2, 1), \"float32\"), ph_3: T.Buffer((10, 2, 1), \"float32\"), T_multiply: T.Buffer((10, 2, 1), \"float32\"), compute: T.Buffer((10, 2, 1), \"float32\"), compute_1: T.Buffer((10, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((20,), data=ph_0.data)\n        for ax0_ax1_fused_ax2_fused in T.parallel(20):\n            T_multiply_1 = T.Buffer((20,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((20,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] * ph_3_1[ax0_ax1_fused_ax2_fused]\n        for i0 in T.parallel(10):\n            for i1 in range(2):\n                cse_var_1: T.int32 = i0 * 2 + i1\n                compute_2 = T.Buffer((20,), data=compute.data)\n                compute_2[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(10):\n            for i1 in range(2):\n                cse_var_2: T.int32 = i0 * 2 + i1\n                compute_2 = T.Buffer((20,), data=compute_1.data)\n                compute_2[cse_var_2] = T.ceil(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "multiply",
                "atanh",
                "ceil"
            ]
        ],
        "input_shape": [[10, 2, 1], [4, 2, 7], [10, 2, 1]],
        "output_shape": [[10, 2, 1], [4, 2, 7], [10, 2, 1], [10, 2, 1]],
        "deepseek_c": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      for (int32_t i2 = 0; i2 < 1; ++i2) {\n        T_multiply[(((i0 * 2) + i1) * 1) + i2] = (ph_0[(((i0 * 2) + i1) * 1) + i2] * ph_3[(((i0 * 2) + i1) * 1) + i2]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 20; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 1; ++i2_1) {\n      compute[((i0_i1_fused * 1) + i2_1)] = atanhf(ph_0[((i0_i1_fused * 1) + i2_1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 20; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i2 = 0; i2 < 18; ++i2) {\n      compute[((i0 * 18) + i2)] = atanhf((ph_0[((i0 * 18) + i2)] + cosf(ph_0[((i0 * 18) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(24) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] = atanhf((ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))] + __cosf(ph_0[((((int)blockIdx.x) * 24) + ((int)threadIdx.x))])));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 1, 18), \"float32\"), compute: T.Buffer((8, 1, 18), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0 in T.parallel(8):\n            for i2 in range(18):\n                cse_var_1: T.int32 = i0 * 18 + i2\n                compute_1 = T.Buffer((144,), data=compute.data)\n                ph_0_1 = T.Buffer((144,), data=ph_0.data)\n                compute_1[cse_var_1] = T.atanh(ph_0_1[cse_var_1] + T.cos(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "cos",
                "add",
                "atanh"
            ]
        ],
        "input_shape": [[8, 1, 18]],
        "output_shape": [[8, 1, 18]],
        "deepseek_c": "void default_function_kernel(float* compute, float* ph_0) {\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 144; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanhf((ph_0[i0_i1_fused_i2_fused] + cosf(ph_0[i0_i1_fused_i2_fused])));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 3400; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 17, 20), \"float32\"), compute: T.Buffer((10, 17, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        for i0_i1_fused_i2_fused in T.parallel(3400):\n            compute_1 = T.Buffer((3400,), data=compute.data)\n            ph_0_1 = T.Buffer((3400,), data=ph_0.data)\n            compute_1[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "ceil"
            ]
        ],
        "input_shape": [[10, 17, 20]],
        "output_shape": [[10, 17, 20]],
        "deepseek_c": "void default_function_kernel(float* compute, float* ph_0) {\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 17; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 340) + (i1 * 20)) + i2)] = ceilf(ph_0[(((i0 * 340) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_mod_1, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acoshf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 2; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 12; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 20; ++ax2) {\n        T_mod[(((ax0 * 240) + (ax1 * 20)) + ax2)] = fmodf(asinhf(ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)]), ph_0[(((ax0 * 240) + (ax1 * 20)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_1 = 0; ax0_1 < 2; ++ax0_1) {\n    for (int32_t ax1_1 = 0; ax1_1 < 12; ++ax1_1) {\n      for (int32_t ax2_1 = 0; ax2_1 < 20; ++ax2_1) {\n        T_mod_1[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)] = fmodf(ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)], cosf(ph_0[(((ax0_1 * 240) + (ax1_1 * 20)) + ax2_1)]));\n      }\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = acoshf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(asinhf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))], __cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((2, 12, 20), \"float32\"), compute: T.Buffer((2, 12, 20), \"float32\"), T_mod: T.Buffer((2, 12, 20), \"float32\"), T_mod_1: T.Buffer((2, 12, 20), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((480,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(480):\n            compute_1 = T.Buffer((480,), data=compute.data)\n            compute_1[i0_i1_fused_i2_fused] = T.acosh(ph_0_1[i0_i1_fused_i2_fused])\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_1: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_mod_2 = T.Buffer((480,), data=T_mod.data)\n                T_mod_2[cse_var_1] = T.truncmod(T.asinh(ph_0_1[cse_var_1]), ph_0_1[cse_var_1])\n        for ax0 in T.parallel(2):\n            for ax1, ax2 in T.grid(12, 20):\n                cse_var_2: T.int32 = ax0 * 240 + ax1 * 20 + ax2\n                T_mod_2 = T.Buffer((480,), data=T_mod_1.data)\n                T_mod_2[cse_var_2] = T.truncmod(ph_0_1[cse_var_2], T.cos(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "acosh",
                "asinh",
                "mod",
                "cos",
                "mod"
            ]
        ],
        "input_shape": [[2, 12, 20]],
        "output_shape": [[2, 12, 20], [2, 12, 20], [2, 12, 20]],
        "deepseek_c": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 2; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 20; ++i2) {\n        compute[(((i0 * 240) + (i1 * 20)) + i2)] = acoshf(ph_0[(((i0 * 240) + (i1 * 20)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 24; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 20; ++i2) {\n      T_mod[((i0_i1_fused * 20) + i2)] = fmodf(asinhf(ph_0[((i0_i1_fused * 20) + i2)]), ph_0[((i0_i1_fused * 20) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 480; ++i0_i1_fused_i2_fused) {\n    T_mod[i0_i1_fused_i2_fused] = fmodf(ph_0[i0_i1_fused_i2_fused], cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 450; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = ceilf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 450; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = fabsf(asinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute_2[((i0_i1_fused * 5) + i2)] = asinhf((ph_0[((i0_i1_fused * 5) + i2)] * asinf(ph_0[((i0_i1_fused * 5) + i2)])));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ceilf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(2) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))] * asinf(ph_0[((((int)blockIdx.x) * 2) + ((int)threadIdx.x))])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fabsf(asinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 10, 5), \"float32\"), compute: T.Buffer((9, 10, 5), \"float32\"), compute_1: T.Buffer((9, 10, 5), \"float32\"), compute_2: T.Buffer((9, 10, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((450,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(450):\n            compute_3 = T.Buffer((450,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.ceil(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(450):\n            compute_3 = T.Buffer((450,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.fabs(T.asin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(90):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((450,), data=compute_2.data)\n                compute_3[cse_var_1] = T.asinh(ph_0_1[cse_var_1] * T.asin(ph_0_1[cse_var_1]))",
        "op_args": [
            [
                "ceil",
                "asin",
                "abs",
                "asin",
                "multiply",
                "asinh"
            ]
        ],
        "input_shape": [[9, 10, 5]],
        "output_shape": [[9, 10, 5], [9, 10, 5], [9, 10, 5]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 10; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 50) + (i1 * 5)) + i2)] = ceilf(ph_0[(((i0 * 50) + (i1 * 5)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 10; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n        compute_1[(((i0_1 * 50) + (i1_1 * 5)) + i2_1)] = fabsf(asinf(ph_0[(((i0_1 * 50) + (i1_1 * 5)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 90; ++i0_i1_fused) {\n    for (int32_t i2_fused = 0; i2_fused < 5; ++i2_fused) {\n      compute_2[((i0_i1_fused * 5) + i2_fused)] = asinhf((ph_0[((i0_i1_fused * 5) + i2_fused)] * asinf(ph_0[((i0_i1_fused * 5) + i2_fused)])));\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 16) + (i1 * 2)) + i2)] = fabsf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 128; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 2; ++ax2) {\n      T_add[((ax0_ax1_fused * 2) + ax2)] = (acoshf(ph_0[((ax0_ax1_fused * 2) + ax2)]) + ph_0[((ax0_ax1_fused * 2) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 256; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 256; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = acosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = acosf(__sinf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = __expf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_add, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_add[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (acoshf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]) + ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((16, 8, 2), \"float32\"), compute: T.Buffer((16, 8, 2), \"float32\"), T_add: T.Buffer((16, 8, 2), \"float32\"), compute_1: T.Buffer((16, 8, 2), \"float32\"), compute_2: T.Buffer((16, 8, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((256,), data=ph_0.data)\n        for i0 in T.parallel(16):\n            for i1, i2 in T.grid(8, 2):\n                cse_var_1: T.int32 = i0 * 16 + i1 * 2 + i2\n                compute_3 = T.Buffer((256,), data=compute.data)\n                compute_3[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(128):\n            for ax2 in range(2):\n                cse_var_2: T.int32 = ax0_ax1_fused * 2 + ax2\n                T_add_1 = T.Buffer((256,), data=T_add.data)\n                T_add_1[cse_var_2] = T.acosh(ph_0_1[cse_var_2]) + ph_0_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(256):\n            compute_3 = T.Buffer((256,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(256):\n            compute_3 = T.Buffer((256,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "abs",
                "acosh",
                "add",
                "exp",
                "sin",
                "acos"
            ]
        ],
        "input_shape": [[16, 8, 2]],
        "output_shape": [[16, 8, 2], [16, 8, 2], [16, 8, 2], [16, 8, 2]],
        "deepseek_c": "void default_function_kernel(float* T_add, float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 16; ++i0) {\n    for (int32_t i1 = 0; i1 < 8; ++i1) {\n      for (int32_t i2 = 0; i2 < 2; ++i2) {\n        compute[(((i0 * 16) + (i1 * 2)) + i2)] = fabsf(ph_0[(((i0 * 16) + (i1 * 2)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 128; ++i0_i1_fused) {\n    T_add[i0_i1_fused] = (acoshf(ph_0[i0_i1_fused]) + ph_0[i0_i1_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 256; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = expf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 128; ++i0_i1_fused_1) {\n    compute_2[i0_i1_fused_1] = acosf(sinf(ph_0[i0_i1_fused_1]));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_subtract, float* compute, float* compute_1, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 15; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 2; ++i2) {\n      compute[((i0_i1_fused * 2) + i2)] = fabsf(ph_0[((i0_i1_fused * 2) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 30; ++ax0_ax1_fused_ax2_fused) {\n    T_subtract[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] - asinhf(cosf(ph_0[ax0_ax1_fused_ax2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_1 = 0; i0_i1_fused_1 < 15; ++i0_i1_fused_1) {\n    for (int32_t i2_1 = 0; i2_1 < 2; ++i2_1) {\n      compute_1[((i0_i1_fused_1 * 2) + i2_1)] = asinf(atanhf(ph_0[((i0_i1_fused_1 * 2) + i2_1)]));\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ T_subtract, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_subtract[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(__cosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 8) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinf(atanhf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n  }\n}\n\nextern \"C\" __global__ void __launch_bounds__(6) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))] = fabsf(ph_0[((((int)blockIdx.x) * 6) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 15, 2), \"float32\"), compute: T.Buffer((1, 15, 2), \"float32\"), T_subtract: T.Buffer((1, 15, 2), \"float32\"), compute_1: T.Buffer((1, 15, 2), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((30,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(15):\n            for i2 in range(2):\n                cse_var_1: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((30,), data=compute.data)\n                compute_2[cse_var_1] = T.fabs(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(30):\n            T_subtract_1 = T.Buffer((30,), data=T_subtract.data)\n            T_subtract_1[ax0_ax1_fused_ax2_fused] = ph_0_1[ax0_ax1_fused_ax2_fused] - T.asinh(T.cos(ph_0_1[ax0_ax1_fused_ax2_fused]))\n        for i0_i1_fused in T.parallel(15):\n            for i2 in range(2):\n                cse_var_2: T.int32 = i0_i1_fused * 2 + i2\n                compute_2 = T.Buffer((30,), data=compute_1.data)\n                compute_2[cse_var_2] = T.asin(T.atanh(ph_0_1[cse_var_2]))",
        "op_args": [
            [
                "abs",
                "cos",
                "asinh",
                "subtract",
                "atanh",
                "asin"
            ]
        ],
        "input_shape": [[1, 15, 2]],
        "output_shape": [[1, 15, 2], [1, 15, 2], [1, 15, 2]],
        "deepseek_c": "void default_function_kernel(float* T_subtract, float* compute, float* ph_0) {\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 2; ++i1) {\n      compute[((i0 * 2) + i1)] = fabsf(ph_0[((i0 * 2) + i1)]);\n    }\n  }\n  for (int32_t i0_1 = 0; i0_1 < 15; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 2; ++i1_1) {\n      T_subtract[((i0_1 * 2) + i1_1)] = (ph_0[((i0_1 * 2) + i1_1)] - asinhf(cosf(ph_0[((i0_1 * 2) + i1_1)])));\n    }\n  }\n  for (int32_t i0_2 = 0; i0_2 < 15; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 2; ++i1_2) {\n      compute[((i0_2 * 2) + i1_2)] = asinf(atanhf(ph_0[((i0_2 * 2) + i1_2)]));\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 192) + (i1 * 16)) + i2)] = atanhf(ph_0[(((i0 * 192) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n        compute_1[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)] = cosf(atanf(ph_0[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1728; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = __cosf(atanf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = atanhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((9, 12, 16), \"float32\"), compute: T.Buffer((9, 12, 16), \"float32\"), compute_1: T.Buffer((9, 12, 16), \"float32\"), compute_2: T.Buffer((9, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1728,), data=ph_0.data)\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_1: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_3 = T.Buffer((1728,), data=compute.data)\n                compute_3[cse_var_1] = T.atanh(ph_0_1[cse_var_1])\n        for i0 in T.parallel(9):\n            for i1, i2 in T.grid(12, 16):\n                cse_var_2: T.int32 = i0 * 192 + i1 * 16 + i2\n                compute_3 = T.Buffer((1728,), data=compute_1.data)\n                compute_3[cse_var_2] = T.cos(T.atan(ph_0_1[cse_var_2]))\n        for i0_i1_fused_i2_fused in T.parallel(1728):\n            compute_3 = T.Buffer((1728,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.atan(ph_0_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "atanh",
                "atan",
                "cos",
                "atan"
            ]
        ],
        "input_shape": [[9, 12, 16]],
        "output_shape": [[9, 12, 16], [9, 12, 16], [9, 12, 16]],
        "deepseek_c": "void default_function_kernel(float* compute, float* compute_1, float* compute_2, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 9; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 192) + (i1 * 16)) + i2)] = atanhf(ph_0[(((i0 * 192) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 9; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 12; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 16; ++i2_1) {\n        compute_1[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)] = cosf(atanf(ph_0[(((i0_1 * 192) + (i1_1 * 16)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_2 = 0; i0_2 < 9; ++i0_2) {\n    for (int32_t i1_2 = 0; i1_2 < 12; ++i1_2) {\n      for (int32_t i2_2 = 0; i2_2 < 16; ++i2_2) {\n        compute_2[(((i0_2 * 192) + (i1_2 * 16)) + i2_2)] = atanf(ph_0[(((i0_2 * 192) + (i1_2 * 16)) + i2_2)]);\n      }\n    }\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_add, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 14; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 4; ++ax2) {\n      T_add[((ax0_ax1_fused * 4) + ax2)] = (ph_0[((ax0_ax1_fused * 4) + ax2)] + ph_3[((ax0_ax1_fused * 4) + ax2)]);\n    }\n  }\n  for (int32_t i1 = 0; i1 < 14; ++i1) {\n    for (int32_t i2 = 0; i2 < 4; ++i2) {\n      compute[((i1 * 4) + i2)] = atanf(ph_0[((i1 * 4) + i2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(36) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))] = atanf(ph_0[((((int)blockIdx.x) * 36) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ T_add, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_add[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((1, 14, 4), \"float32\"), ph_3: T.Buffer((1, 14, 4), \"float32\"), T_add: T.Buffer((1, 14, 4), \"float32\"), compute: T.Buffer((1, 14, 4), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((56,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(14):\n            for ax2 in range(4):\n                cse_var_1: T.int32 = ax0_ax1_fused * 4 + ax2\n                T_add_1 = T.Buffer((56,), data=T_add.data)\n                ph_3_1 = T.Buffer((56,), data=ph_3.data)\n                T_add_1[cse_var_1] = ph_0_1[cse_var_1] + ph_3_1[cse_var_1]\n        for i1, i2 in T.grid(14, 4):\n            cse_var_2: T.int32 = i1 * 4 + i2\n            compute_1 = T.Buffer((56,), data=compute.data)\n            compute_1[cse_var_2] = T.atan(ph_0_1[cse_var_2])",
        "op_args": [
            [
                "add",
                "atan"
            ]
        ],
        "input_shape": [[1, 14, 4], [14, 17, 6], [1, 14, 4]],
        "output_shape": [[1, 14, 4], [14, 17, 6], [1, 14, 4]],
        "deepseek_c": "void default_function_kernel(float* T_add, float* compute, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 56; ++ax0_ax1_fused_ax2_fused) {\n    T_add[ax0_ax1_fused_ax2_fused] = (ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 56; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = atanf(ph_0[i0_i1_fused_i2_fused]);\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 18; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 10; ++ax2) {\n      T_divide[((ax0_ax1_fused * 10) + ax2)] = (ph_0[((ax0_ax1_fused * 10) + ax2)] / ph_3[((ax0_ax1_fused * 10) + ax2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 180; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = fabsf((ph_0[i0_i1_fused_i2_fused] - asinhf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 180; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = expf((ph_0[i0_i1_fused_i2_fused_1] - asinhf(ph_0[i0_i1_fused_i2_fused_1])));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\nextern \"C\" __global__ void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf((ph_0[((int)blockIdx.x)] - asinhf(ph_0[((int)blockIdx.x)])));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  if (((((int)blockIdx.x) * 4) + (((int)threadIdx.x) >> 3)) < 9) {\n    compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] - asinhf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))])));\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((18, 1, 10), \"float32\"), ph_3: T.Buffer((18, 1, 10), \"float32\"), T_divide: T.Buffer((18, 1, 10), \"float32\"), compute: T.Buffer((18, 1, 10), \"float32\"), compute_1: T.Buffer((18, 1, 10), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((180,), data=ph_0.data)\n        for ax0_ax1_fused in T.parallel(18):\n            for ax2 in range(10):\n                cse_var_1: T.int32 = ax0_ax1_fused * 10 + ax2\n                T_divide_1 = T.Buffer((180,), data=T_divide.data)\n                ph_3_1 = T.Buffer((180,), data=ph_3.data)\n                T_divide_1[cse_var_1] = ph_0_1[cse_var_1] / ph_3_1[cse_var_1]\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_2 = T.Buffer((180,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.fabs(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused_i2_fused in T.parallel(180):\n            compute_2 = T.Buffer((180,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.exp(ph_0_1[i0_i1_fused_i2_fused] - T.asinh(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "divide",
                "asinh",
                "subtract",
                "abs",
                "exp"
            ]
        ],
        "input_shape": [[18, 1, 10], [16, 1, 3], [18, 1, 10]],
        "output_shape": [[18, 1, 10], [16, 1, 3], [18, 1, 10], [18, 1, 10]],
        "deepseek_c": "void default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((int)blockIdx.x)] = __expf((ph_0[((int)blockIdx.x)] - asinhf(ph_0[((int)blockIdx.x)])));\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 270; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 5; ++i2) {\n      compute[((i0_i1_fused * 5) + i2)] = asinhf(cosf(ph_0[((i0_i1_fused * 5) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1350; ++i0_i1_fused_i2_fused) {\n    compute_1[i0_i1_fused_i2_fused] = acosf(cosf(ph_0[i0_i1_fused_i2_fused]));\n  }\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 15; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 18; ++ax1) {\n      for (int32_t ax2 = 0; ax2 < 5; ++ax2) {\n        T_divide[(((ax0 * 90) + (ax1 * 5)) + ax2)] = (ph_0[(((ax0 * 90) + (ax1 * 5)) + ax2)] / ph_3[(((ax0 * 90) + (ax1 * 5)) + ax2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1350; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(16) default_function_kernel_2(float* __restrict__ T_divide, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_divide[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = (ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_3(float* __restrict__ compute, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = asinhf((ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] / ph_3[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = asinhf(__cosf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(48) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))] = acosf(__cosf(ph_0[((((int)blockIdx.x) * 48) + ((int)threadIdx.x))]));\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((15, 18, 5), \"float32\"), ph_3: T.Buffer((15, 18, 5), \"float32\"), T_divide: T.Buffer((15, 18, 5), \"float32\"), compute: T.Buffer((15, 18, 5), \"float32\"), compute_1: T.Buffer((15, 18, 5), \"float32\"), compute_2: T.Buffer((15, 18, 5), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1350,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(270):\n            for i2 in range(5):\n                cse_var_1: T.int32 = i0_i1_fused * 5 + i2\n                compute_3 = T.Buffer((1350,), data=compute.data)\n                compute_3[cse_var_1] = T.asinh(T.cos(ph_0_1[cse_var_1]))\n        for i0_i1_fused_i2_fused in T.parallel(1350):\n            compute_3 = T.Buffer((1350,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(T.cos(ph_0_1[i0_i1_fused_i2_fused]))\n        ph_3_1 = T.Buffer((1350,), data=ph_3.data)\n        for ax0 in T.parallel(15):\n            for ax1, ax2 in T.grid(18, 5):\n                cse_var_2: T.int32 = ax0 * 90 + ax1 * 5 + ax2\n                T_divide_1 = T.Buffer((1350,), data=T_divide.data)\n                T_divide_1[cse_var_2] = ph_0_1[cse_var_2] / ph_3_1[cse_var_2]\n        for i0_i1_fused_i2_fused in T.parallel(1350):\n            compute_3 = T.Buffer((1350,), data=compute_2.data)\n            compute_3[i0_i1_fused_i2_fused] = T.asinh(ph_0_1[i0_i1_fused_i2_fused] / ph_3_1[i0_i1_fused_i2_fused])",
        "op_args": [
            [
                "divide",
                "divide",
                "cos",
                "asinh",
                "acos",
                "asinh"
            ]
        ],
        "input_shape": [[15, 18, 5], [8, 9, 2], [15, 18, 5]],
        "output_shape": [[8, 9, 2], [15, 18, 5], [15, 18, 5], [15, 18, 5], [15, 18, 5]],
        "deepseek_c": "void default_function_kernel(float* T_divide, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 15; ++i0) {\n    for (int32_t i1 = 0; i1 < 18; ++i1) {\n      for (int32_t i2 = 0; i2 < 5; ++i2) {\n        compute[(((i0 * 90) + (i1 * 5)) + i2)] = asinhf(cosf(ph_0[(((i0 * 90) + (i1 * 5)) + i2)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_1 = 0; i0_1 < 15; ++i0_1) {\n    for (int32_t i1_1 = 0; i1_1 < 18; ++i1_1) {\n      for (int32_t i2_1 = 0; i2_1 < 5; ++i2_1) {\n        compute_1[(((i0_1 * 90) + (i1_1 * 5)) + i2_1)] = acosf(cosf(ph_0[(((i0_1 * 90) + (i1_1 * 5)) + i2_1)]));\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1350; ++i0_i1_fused_i2_fused) {\n    T_divide[i0_i1_fused_i2_fused] = (ph_0[i0_i1_fused_i2_fused] / ph_3[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1350; ++i0_i1_fused_i2_fused_1) {\n    compute_2[i0_i1_fused_i2_fused_1] = asinhf((ph_0[i0_i1_fused_i2_fused_1] / ph_3[i0_i1_fused_i2_fused_1]));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* T_multiply, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 10; ++i0) {\n    for (int32_t i1 = 0; i1 < 5; ++i1) {\n      for (int32_t i2 = 0; i2 < 12; ++i2) {\n        compute[(((i0 * 60) + (i1 * 12)) + i2)] = acosf(ph_0[(((i0 * 60) + (i1 * 12)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 600; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = (atanf(ph_0[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused_1 = 0; ax0_ax1_fused_ax2_fused_1 < 600; ++ax0_ax1_fused_ax2_fused_1) {\n    T_mod[ax0_ax1_fused_ax2_fused_1] = fmodf(ph_0[ax0_ax1_fused_ax2_fused_1], ceilf(ph_0[ax0_ax1_fused_ax2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_2(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))], ceilf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((int)blockIdx.x)] = (atanf(ph_0[((int)blockIdx.x)]) * ph_0[((int)blockIdx.x)]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((10, 5, 12), \"float32\"), compute: T.Buffer((10, 5, 12), \"float32\"), T_multiply: T.Buffer((10, 5, 12), \"float32\"), T_mod: T.Buffer((10, 5, 12), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((600,), data=ph_0.data)\n        for i0 in T.parallel(10):\n            for i1, i2 in T.grid(5, 12):\n                cse_var_1: T.int32 = i0 * 60 + i1 * 12 + i2\n                compute_1 = T.Buffer((600,), data=compute.data)\n                compute_1[cse_var_1] = T.acos(ph_0_1[cse_var_1])\n        for ax0_ax1_fused_ax2_fused in T.parallel(600):\n            T_multiply_1 = T.Buffer((600,), data=T_multiply.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = T.atan(ph_0_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]\n        for ax0_ax1_fused_ax2_fused in T.parallel(600):\n            T_mod_1 = T.Buffer((600,), data=T_mod.data)\n            T_mod_1[ax0_ax1_fused_ax2_fused] = T.truncmod(ph_0_1[ax0_ax1_fused_ax2_fused], T.ceil(ph_0_1[ax0_ax1_fused_ax2_fused]))",
        "op_args": [
            [
                "acos",
                "atan",
                "multiply",
                "ceil",
                "mod"
            ]
        ],
        "input_shape": [[10, 5, 12]],
        "output_shape": [[10, 5, 12], [10, 5, 12], [10, 5, 12]],
        "deepseek_c": "void default_function_kernel_1(float* __restrict__ T_multiply, float* __restrict__ ph_0) {\n  T_multiply[((int)blockIdx.x)] = (atanf(ph_0[((int)blockIdx.x)]) * ph_0[((int)blockIdx.x)]);\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1536; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = acosf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1536; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = cosf(sinf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 16; ++i2) {\n      compute_2[((i0_i1_fused * 16) + i2)] = fabsf(sinf(ph_0[((i0_i1_fused * 16) + i2)]));\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused_ax2_fused = 0; ax0_ax1_fused_ax2_fused < 1536; ++ax0_ax1_fused_ax2_fused) {\n    T_multiply[ax0_ax1_fused_ax2_fused] = ((ph_0[ax0_ax1_fused_ax2_fused] + ph_3[ax0_ax1_fused_ax2_fused]) * ph_0[ax0_ax1_fused_ax2_fused]);\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = __cosf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_3(float* __restrict__ T_multiply, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  T_multiply[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = ((ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] + ph_3[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]) * ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = fabsf(__sinf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(32) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))] = acosf(ph_0[((((int)blockIdx.x) * 32) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((8, 12, 16), \"float32\"), ph_3: T.Buffer((8, 12, 16), \"float32\"), compute: T.Buffer((8, 12, 16), \"float32\"), compute_1: T.Buffer((8, 12, 16), \"float32\"), compute_2: T.Buffer((8, 12, 16), \"float32\"), T_multiply: T.Buffer((8, 12, 16), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((1536,), data=ph_0.data)\n        for i0_i1_fused_i2_fused in T.parallel(1536):\n            compute_3 = T.Buffer((1536,), data=compute.data)\n            compute_3[i0_i1_fused_i2_fused] = T.acos(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(1536):\n            compute_3 = T.Buffer((1536,), data=compute_1.data)\n            compute_3[i0_i1_fused_i2_fused] = T.cos(T.sin(ph_0_1[i0_i1_fused_i2_fused]))\n        for i0_i1_fused in T.parallel(96):\n            for i2 in range(16):\n                cse_var_1: T.int32 = i0_i1_fused * 16 + i2\n                compute_3 = T.Buffer((1536,), data=compute_2.data)\n                compute_3[cse_var_1] = T.fabs(T.sin(ph_0_1[cse_var_1]))\n        for ax0_ax1_fused_ax2_fused in T.parallel(1536):\n            T_multiply_1 = T.Buffer((1536,), data=T_multiply.data)\n            ph_3_1 = T.Buffer((1536,), data=ph_3.data)\n            T_multiply_1[ax0_ax1_fused_ax2_fused] = (ph_0_1[ax0_ax1_fused_ax2_fused] + ph_3_1[ax0_ax1_fused_ax2_fused]) * ph_0_1[ax0_ax1_fused_ax2_fused]",
        "op_args": [
            [
                "add",
                "acos",
                "sin",
                "cos",
                "abs",
                "multiply"
            ]
        ],
        "input_shape": [[8, 12, 16], [2, 9, 7], [8, 12, 16]],
        "output_shape": [[2, 9, 7], [8, 12, 16], [8, 12, 16], [8, 12, 16], [8, 12, 16]],
        "deepseek_c": "void default_function_kernel(float* T_multiply, float* compute, float* compute_1, float* compute_2, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 8; ++i0) {\n    for (int32_t i1 = 0; i1 < 12; ++i1) {\n      for (int32_t i2 = 0; i2 < 16; ++i2) {\n        compute[(((i0 * 192) + (i1 * 16)) + i2)] = acosf(ph_0[(((i0 * 192) + (i1 * 16)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 96; ++i0_i1_fused) {\n    compute_1[i0_i1_fused] = cosf(sinf(ph_0[i0_i1_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 1536; ++i0_i1_fused_i2_fused) {\n    compute_2[i0_i1_fused_i2_fused] = fabsf(sinf(ph_0[i0_i1_fused_i2_fused])));\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 1536; ++i0_i1_fused_i2_fused_1) {\n    T_multiply[i0_i1_fused_i2_fused_1] = ((ph_0[i0_i1_fused_i2_fused_1] + ph_3[i0_i1_fused_i2_fused_1]) * ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t ax0 = 0; ax0 < 14; ++ax0) {\n    for (int32_t ax1 = 0; ax1 < 2; ++ax1) {\n      T_mod[((ax0 * 2) + ax1)] = fmodf(ph_0[((ax0 * 2) + ax1)], ph_3[((ax0 * 2) + ax1)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 28; ++i0_i1_fused_i2_fused) {\n    compute[i0_i1_fused_i2_fused] = sinf(ph_0[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 28; ++i0_i1_fused_i2_fused_1) {\n    compute_1[i0_i1_fused_i2_fused_1] = asinhf(ceilf(ph_0[i0_i1_fused_i2_fused_1]));\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(8) default_function_kernel_2(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))] = asinhf(ceilf(ph_0[((((int)blockIdx.x) * 8) + ((int)threadIdx.x))]));\n}\n\nextern \"C\" __global__ void __launch_bounds__(4) default_function_kernel_1(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))] = __sinf(ph_0[((((int)blockIdx.x) * 4) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(16) default_function_kernel(float* __restrict__ T_mod, float* __restrict__ ph_0, float* __restrict__ ph_3) {\n  if (((((int)blockIdx.x) * 2) + (((int)threadIdx.x) >> 3)) < 9) {\n    T_mod[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))] = fmodf(ph_0[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))], ph_3[((((int)blockIdx.x) * 16) + ((int)threadIdx.x))]);\n  }\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((14, 2, 1), \"float32\"), ph_3: T.Buffer((14, 2, 1), \"float32\"), T_mod: T.Buffer((14, 2, 1), \"float32\"), compute: T.Buffer((14, 2, 1), \"float32\"), compute_1: T.Buffer((14, 2, 1), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((28,), data=ph_0.data)\n        for ax0 in T.parallel(14):\n            for ax1 in range(2):\n                cse_var_1: T.int32 = ax0 * 2 + ax1\n                T_mod_1 = T.Buffer((28,), data=T_mod.data)\n                ph_3_1 = T.Buffer((28,), data=ph_3.data)\n                T_mod_1[cse_var_1] = T.truncmod(ph_0_1[cse_var_1], ph_3_1[cse_var_1])\n        for i0_i1_fused_i2_fused in T.parallel(28):\n            compute_2 = T.Buffer((28,), data=compute.data)\n            compute_2[i0_i1_fused_i2_fused] = T.sin(ph_0_1[i0_i1_fused_i2_fused])\n        for i0_i1_fused_i2_fused in T.parallel(28):\n            compute_2 = T.Buffer((28,), data=compute_1.data)\n            compute_2[i0_i1_fused_i2_fused] = T.asinh(T.ceil(ph_0_1[i0_i1_fused_i2_fused]))",
        "op_args": [
            [
                "mod",
                "sin",
                "ceil",
                "asinh"
            ]
        ],
        "input_shape": [[14, 2, 1], [20, 9, 19], [14, 2, 1]],
        "output_shape": [[14, 2, 1], [20, 9, 19], [14, 2, 1], [14, 2, 1]],
        "deepseek_c": "void default_function_kernel(float* T_mod, float* compute, float* compute_1, float* ph_0, float* ph_3) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused = 0; i0_i1_fused_i2_fused < 28; ++i0_i1_fused_i2_fused) {\n    T_mod[i0_i1_fused_i2_fused] = fmodf(ph_0[i0_i1_fused_i2_fused], ph_3[i0_i1_fused_i2_fused]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_1 = 0; i0_i1_fused_i2_fused_1 < 28; ++i0_i1_fused_i2_fused_1) {\n    compute[i0_i1_fused_i2_fused_1] = sinf(ph_0[i0_i1_fused_i2_fused_1]);\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused_i2_fused_2 = 0; i0_i1_fused_i2_fused_2 < 28; ++i0_i1_fused_i2_fused_2) {\n    compute_1[i0_i1_fused_i2_fused_2] = asinhf(ceilf(ph_0[i0_i1_fused_i2_fused_2]));\n  }\n}"
    },
    {
        "op_name": "topology_expansion",
        "c_code": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 12; ++i0_i1_fused) {\n    for (int32_t i2 = 0; i2 < 17; ++i2) {\n      compute[((i0_i1_fused * 17) + i2)] = asinhf(ph_0[((i0_i1_fused * 17) + i2)]);\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t ax0_ax1_fused = 0; ax0_ax1_fused < 12; ++ax0_ax1_fused) {\n    for (int32_t ax2 = 0; ax2 < 17; ++ax2) {\n      T_mod[((ax0_ax1_fused * 17) + ax2)] = fmodf(sinf(ph_0[((ax0_ax1_fused * 17) + ax2)]), ph_0[((ax0_ax1_fused * 17) + ax2)]);\n    }\n  }\n}\n\n",
        "cuda_code": "extern \"C\" __global__ void __launch_bounds__(64) default_function_kernel_1(float* __restrict__ T_mod, float* __restrict__ ph_0) {\n  T_mod[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))] = fmodf(__sinf(ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]), ph_0[((((int)blockIdx.x) * 64) + ((int)threadIdx.x))]);\n}\n\nextern \"C\" __global__ void __launch_bounds__(12) default_function_kernel(float* __restrict__ compute, float* __restrict__ ph_0) {\n  compute[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))] = asinhf(ph_0[((((int)blockIdx.x) * 12) + ((int)threadIdx.x))]);\n}\n\n",
        "ir_code": "# from tvm.script import ir as I\n# from tvm.script import tir as T\n\n@I.ir_module\nclass Module:\n    @T.prim_func\n    def main(ph_0: T.Buffer((4, 3, 17), \"float32\"), compute: T.Buffer((4, 3, 17), \"float32\"), T_mod: T.Buffer((4, 3, 17), \"float32\")):\n        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"tir.noalias\": T.bool(True)})\n        ph_0_1 = T.Buffer((204,), data=ph_0.data)\n        for i0_i1_fused in T.parallel(12):\n            for i2 in range(17):\n                cse_var_1: T.int32 = i0_i1_fused * 17 + i2\n                compute_1 = T.Buffer((204,), data=compute.data)\n                compute_1[cse_var_1] = T.asinh(ph_0_1[cse_var_1])\n        for ax0_ax1_fused in T.parallel(12):\n            for ax2 in range(17):\n                cse_var_2: T.int32 = ax0_ax1_fused * 17 + ax2\n                T_mod_1 = T.Buffer((204,), data=T_mod.data)\n                T_mod_1[cse_var_2] = T.truncmod(T.sin(ph_0_1[cse_var_2]), ph_0_1[cse_var_2])",
        "op_args": [
            [
                "asinh",
                "sin",
                "mod"
            ]
        ],
        "input_shape": [[4, 3, 17]],
        "output_shape": [[4, 3, 17], [4, 3, 17]],
        "deepseek_c": "void default_function_kernel(float* T_mod, float* compute, float* ph_0) {\n  #pragma omp parallel for\n  for (int32_t i0 = 0; i0 < 4; ++i0) {\n    for (int32_t i1 = 0; i1 < 3; ++i1) {\n      for (int32_t i2 = 0; i2 < 17; ++i2) {\n        compute[(((i0 * 51) + (i1 * 17)) + i2)] = asinhf(ph_0[(((i0 * 51) + (i1 * 17)) + i2)]);\n      }\n    }\n  }\n  #pragma omp parallel for\n  for (int32_t i0_i1_fused = 0; i0_i1_fused < 12; ++i0_i1_fused) {\n    for (int32_t i2_1 = 0; i2_1 < 17; ++i2_1) {\n      T_mod[((i0_i1_fused * 17) + i2_1)] = fmodf(sinf(ph_0[((i0_i1_fused * 17) + i2_1)]), ph_0[((i0_i1_fused * 17) + i2_1)]);\n    }\n  }\n}"
    }
]